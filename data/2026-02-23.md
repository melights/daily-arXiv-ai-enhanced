<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 37]
- [cs.RO](#cs.RO) [Total: 17]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding](https://arxiv.org/abs/2602.17768)
*Boda Lin,Yongjie Zhu,Xiaocheng Gong,Wenyu Qin,Meng Wang*

Main category: cs.CV

TL;DR: 该论文针对视频字幕生成中细粒度运动描述不准确和幻觉问题，提出了KPM-Bench数据集和MoPE算法，通过运动解析和提取技术改善运动中心视频的字幕质量。


<details>
  <summary>Details</summary>
Motivation: 当前视频字幕模型在描述细粒度运动细节方面存在局限，特别是在运动中心视频中，对复杂动作和肢体动态的精确描述不足，且存在严重的幻觉问题。

Method: 1) 开发自动化标注流程，结合运动学计算和语言解析；2) 构建KPM-Bench数据集，包含细粒度视频-字幕对、运动理解问答对和幻觉评估集；3) 提出MoPE算法从文本字幕中提取运动属性；4) 将MoPE集成到GRPO后训练框架中。

Result: 创建了KPM-Bench开源数据集，提出了独立的幻觉评估指标，通过MoPE算法显著改善了运动中心视频字幕的可靠性，有效缓解了幻觉问题。

Conclusion: 该研究通过创新的数据集构建和算法设计，为细粒度运动理解和幻觉缓解提供了系统解决方案，提升了视频字幕模型在运动描述方面的准确性和可靠性。

Abstract: Despite recent advancements, video captioning models still face significant limitations in accurately describing fine-grained motion details and suffer from severe hallucination issues. These challenges become particularly prominent when generating captions for motion-centric videos, where precise depiction of intricate movements and limb dynamics is crucial yet often neglected. To alleviate this gap, we introduce an automated annotation pipeline that integrates kinematic-based motion computation with linguistic parsing, enabling detailed decomposition and description of complex human motions. Based on this pipeline, we construct and release the Kinematic Parsing Motion Benchmark (KPM-Bench), a novel open-source dataset designed to facilitate fine-grained motion understanding. KPM-Bench consists of (i) fine-grained video-caption pairs that comprehensively illustrate limb-level dynamics in complex actions, (ii) diverse and challenging question-answer pairs focusing specifically on motion understanding, and (iii) a meticulously curated evaluation set specifically designed to assess hallucination phenomena associated with motion descriptions. Furthermore, to address hallucination issues systematically, we propose the linguistically grounded Motion Parsing and Extraction (MoPE) algorithm, capable of accurately extracting motion-specific attributes directly from textual captions. Leveraging MoPE, we introduce a precise hallucination evaluation metric that functions independently of large-scale vision-language or language-only models. By integrating MoPE into the GRPO post-training framework, we effectively mitigate hallucination problems, significantly improving the reliability of motion-centric video captioning models.

</details>


### [2] [CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild](https://arxiv.org/abs/2602.17770)
*Balamurugan Thambiraja,Omid Taheri,Radek Danecek,Giorgio Becherini,Gerard Pons-Moll,Justus Thies*

Main category: cs.CV

TL;DR: 本文提出了3D-HIW数据集和CLUTCH系统，用于解决野外环境下3D手部动作建模的挑战，在文本到动作和动作到文本任务上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 手部动作在日常生活中至关重要，但现有方法依赖工作室采集的数据集，动作和上下文有限，难以扩展到野外环境，且现有模型在动画保真度和文本-动作对齐方面存在困难。

Method: 1) 构建3D-HIW数据集：结合视觉语言模型和先进3D手部跟踪器的数据标注流程；2) 提出CLUTCH系统：基于LLM的手部动画系统，包含SHIFT（部分模态分解的VQ-VAE架构）和几何细化阶段（通过重建损失微调LLM）。

Result: 在文本到动作和动作到文本任务上实现了最先进的性能，建立了可扩展的野外手部动作建模的首个基准。

Conclusion: 通过3D-HIW数据集和CLUTCH系统，成功解决了野外环境下3D手部动作建模的挑战，为可扩展的野外手部动作建模提供了首个基准。

Abstract: Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to "in-the-wild" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.

</details>


### [3] [LGD-Net: Latent-Guided Dual-Stream Network for HER2 Scoring with Task-Specific Domain Knowledge](https://arxiv.org/abs/2602.17793)
*Peide Zhu,Linbin Lu,Zhiqin Chen,Xiong Chen*

Main category: cs.CV

TL;DR: 提出LGD-Net框架，通过跨模态特征幻觉而非像素级图像生成，从H&E切片直接预测HER2表达水平，避免重建伪影，提高计算效率


<details>
  <summary>Details</summary>
Motivation: 标准IHC染色资源密集、昂贵耗时，许多地区无法获得；现有基于H&E切片的虚拟IHC方法计算昂贵且易产生重建伪影，可能导致诊断错误

Method: 提出Latent-Guided Dual-Stream Network (LGD-Net)，学习将形态学H&E特征直接映射到分子潜在空间，通过教师IHC编码器指导训练；使用核分布和膜染色强度等任务特定领域知识进行轻量级辅助正则化

Result: 在公开BCI数据集上的实验表明，LGD-Net达到最先进性能，显著优于基线方法，同时支持单模态H&E输入的高效推理

Conclusion: LGD-Net通过跨模态特征幻觉而非像素级生成，有效解决了虚拟IHC方法的计算成本和伪影问题，为HER2表达评估提供了更实用可靠的替代方案

Abstract: It is a critical task to evalaute HER2 expression level accurately for breast cancer evaluation and targeted treatment therapy selection. However, the standard multi-step Immunohistochemistry (IHC) staining is resource-intensive, expensive, and time-consuming, which is also often unavailable in many areas. Consequently, predicting HER2 levels directly from H&E slides has emerged as a potential alternative solution. It has been shown to be effective to use virtual IHC images from H&E images for automatic HER2 scoring. However, the pixel-level virtual staining methods are computationally expensive and prone to reconstruction artifacts that can propagate diagnostic errors. To address these limitations, we propose the Latent-Guided Dual-Stream Network (LGD-Net), a novel framework that employes cross-modal feature hallucination instead of explicit pixel-level image generation. LGD-Net learns to map morphological H&E features directly to the molecular latent space, guided by a teacher IHC encoder during training. To ensure the hallucinated features capture clinically relevant phenotypes, we explicitly regularize the model training with task-specific domain knowledge, specifically nuclei distribution and membrane staining intensity, via lightweight auxiliary regularization tasks. Extensive experiments on the public BCI dataset demonstrate that LGD-Net achieves state-of-the-art performance, significantly outperforming baseline methods while enabling efficient inference using single-modality H&E inputs.

</details>


### [4] [Enabling Training-Free Text-Based Remote Sensing Segmentation](https://arxiv.org/abs/2602.17799)
*Jose Sosa,Danila Rukhovich,Anis Kacem,Djamila Aouada*

Main category: cs.CV

TL;DR: 提出一种无需额外训练即可实现遥感图像文本引导分割的方法，结合对比式和生成式视觉语言模型与SAM，在19个遥感基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型和视觉基础模型为零样本遥感图像分割提供了新机会，但大多数方法仍需额外可训练组件，限制了泛化能力和实际应用。本研究旨在探索仅依赖现有基础模型、无需额外训练即可实现文本引导遥感分割的可能性。

Method: 提出两种方法：1) 对比式方法：使用CLIP作为SAM网格提议的掩码选择器，实现完全零样本的开放词汇语义分割；2) 生成式方法：使用GPT-5（零样本）和LoRA微调的Qwen-VL模型生成点击提示给SAM，实现推理和指代分割。

Result: 在19个遥感基准测试（包括开放词汇、指代和基于推理的任务）上进行了广泛实验，对比式方法在完全零样本设置下实现了最先进的开放词汇语义分割，生成式方法中LoRA微调的Qwen-VL模型表现最佳。

Conclusion: 研究表明，通过整合现有基础模型，无需额外训练即可实现有效的文本引导遥感图像分割，为实际应用提供了更通用和实用的解决方案。

Abstract: Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery. However, most existing approaches still rely on additional trainable components, limiting their generalisation and practical applicability. In this work, we investigate to what extent text-based remote sensing segmentation can be achieved without additional training, by relying solely on existing foundation models. We propose a simple yet effective approach that integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. Our contrastive approach employs CLIP as mask selector for SAM's grid-based proposals, achieving state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. In parallel, our generative approach enables reasoning and referring segmentation by generating click prompts for SAM using GPT-5 in a zero-shot setting and a LoRA-tuned Qwen-VL model, with the latter yielding the best results. Extensive experiments across 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks, demonstrate the strong capabilities of our approach. Code will be released at https://github.com/josesosajs/trainfree-rs-segmentation.

</details>


### [5] [VQPP: Video Query Performance Prediction Benchmark](https://arxiv.org/abs/2602.17814)
*Adrian Catalin Lutu,Eduard Poesina,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 本文提出了首个视频查询性能预测（VQPP）基准，包含两个文本到视频检索数据集和两个CBVR系统，共56K文本查询和51K视频，旨在填补视频领域QPP研究的空白。


<details>
  <summary>Details</summary>
Motivation: 查询性能预测（QPP）在信息检索中很重要，但主要研究集中在文本和图像检索领域，基于内容的视频检索（CBVR）的QPP研究严重不足，需要建立专门的基准来推动该领域发展。

Method: 构建了VQPP基准，包含两个文本到视频检索数据集和两个CBVR系统，探索了多种检索前和检索后性能预测器，并使用最佳检索前预测器作为奖励模型，通过直接偏好优化训练大语言模型进行查询重写。

Result: 检索前预测器取得了有竞争力的性能，能够在执行检索步骤之前实现应用；VQPP基准为视频领域的QPP研究提供了可比较和可复现的基础。

Conclusion: VQPP是首个视频查询性能预测基准，填补了该领域的研究空白，展示了检索前预测器的实用性，并为未来视频领域的QPP研究提供了标准化平台。

Abstract: Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval performance predictors, creating a representative benchmark for future exploration of QPP in the video domain. Our results show that pre-retrieval predictors obtain competitive performance, enabling applications before performing the retrieval step. We also demonstrate the applicability of VQPP by employing the best performing pre-retrieval predictor as reward model for training a large language model (LLM) on the query reformulation task via direct preference optimization (DPO). We release our benchmark and code at https://github.com/AdrianLutu/VQPP.

</details>


### [6] [On the Evaluation Protocol of Gesture Recognition for UAV-based Rescue Operation based on Deep Learning: A Subject-Independence Perspective](https://arxiv.org/abs/2602.17854)
*Domonkos Varga*

Main category: cs.CV

TL;DR: 本文对Liu和Szirányi提出的手势识别方法进行了方法学分析，重点指出其评估协议存在严重的数据泄露问题，导致报告的高准确率无法反映模型对未见个体的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机是揭示Liu和Szirányi手势识别方法评估协议中的缺陷。作者发现原论文报告的接近完美的准确率指标源于帧级别的随机训练-测试分割，这种分割方式不可避免地混合了相同受试者的样本，导致严重的数据泄露问题。

Method: 作者通过分析已发表的混淆矩阵、学习曲线和数据集构建方式，系统性地检验了原论文的评估协议。他们重点关注数据分割方法是否能够真正衡量模型对未见个体的泛化能力，特别强调了受试者独立数据分区的重要性。

Result: 分析结果表明，原论文的评估协议存在严重缺陷：帧级别的随机分割导致训练集和测试集中混合了相同受试者的数据，造成了数据泄露。这使得报告的高准确率无法反映模型对未见个体的真实泛化能力，特别是在无人机-人交互等需要识别新用户手势的应用场景中。

Conclusion: 本文结论强调了在基于视觉的手势识别研究中，采用受试者独立数据分区的重要性。对于需要识别未见个体手势的应用（如无人机-人交互），必须确保评估协议能够真实反映模型的泛化能力，避免数据泄露导致的虚假高准确率。

Abstract: This paper presents a methodological analysis of the gesture-recognition approach proposed by Liu and Szirányi, with a particular focus on the validity of their evaluation protocol. We show that the reported near-perfect accuracy metrics result from a frame-level random train-test split that inevitably mixes samples from the same subjects across both sets, causing severe data leakage. By examining the published confusion matrix, learning curves, and dataset construction, we demonstrate that the evaluation does not measure generalization to unseen individuals. Our findings underscore the importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for applications - such as UAV-human interaction - that require reliable recognition of gestures performed by previously unseen people.

</details>


### [7] [Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models](https://arxiv.org/abs/2602.17869)
*Yuxiao Chen,Jue Wang,Zhikang Zhang,Jingru Yi,Xu Zhang,Yang Zou,Zhaowei Cai,Jianbo Yuan,Xinyu Li,Hao Yang,Davide Modolo*

Main category: cs.CV

TL;DR: 提出了一种用于长视频理解的新型端到端框架，包含基于信息密度的自适应视频采样器和基于自动编码器的时空视频压缩器，与多模态大语言模型集成，有效解决长视频冗余问题。


<details>
  <summary>Details</summary>
Motivation: 随着视频骨干架构和大型语言模型的发展，分析长达数十分钟的长视频变得可行且普遍，但视频序列固有的冗余性给现有模型带来挑战：1) 在内存限制内高效处理更多帧；2) 从大量输入数据中提取判别性信息。

Method: 提出端到端长视频理解框架，包含：1) 基于信息密度的自适应视频采样器(AVS)，自适应捕获不同时长视频的关键信息；2) 基于自动编码器的时空视频压缩器(SVC)，实现高压缩率同时保留关键判别信息；3) 与多模态大语言模型(MLLM)集成。

Result: 该框架在多个基准测试中表现优异，在长视频理解任务和标准视频理解基准上都表现出色，证明了其在处理长视频复杂性方面的有效性和通用性。

Conclusion: 提出的框架通过自适应采样和高效压缩机制，有效解决了长视频理解中的冗余问题，展示了在处理长视频序列复杂性的多功能性和高效性，为长视频分析提供了有前景的解决方案。

Abstract: With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.

</details>


### [8] [Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models](https://arxiv.org/abs/2602.17871)
*Dhruba Ghosh,Yuhui Zhang,Ludwig Schmidt*

Main category: cs.CV

TL;DR: 该研究发现当前视觉语言模型在细粒度图像分类任务上表现不佳，通过实验发现更好的视觉编码器能显著提升细粒度分类性能，而预训练阶段对模型性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型在多种视觉问答基准测试中取得了显著进展，但在传统的图像分类基准测试（特别是细粒度视觉知识测试）中表现落后。研究者希望探索这种性能差距的原因，并找出影响细粒度视觉理解的关键因素。

Method: 研究者测试了大量最新的视觉语言模型在细粒度分类基准上的表现，通过一系列消融实验分析性能差距的原因。实验重点关注不同组件（如语言模型、视觉编码器）对性能的影响，以及预训练阶段的重要性。

Result: 研究发现：1）使用更好的语言模型能同等提升所有基准测试分数；2）更好的视觉编码器能不成比例地显著提升细粒度分类性能；3）预训练阶段对细粒度性能至关重要，特别是在语言模型权重未冻结的情况下。

Conclusion: 该研究揭示了视觉语言模型中细粒度视觉理解能力不足的原因，并指出改进视觉编码器和优化预训练策略是提升细粒度视觉理解能力的关键途径，为增强视觉语言模型的视觉中心能力提供了重要见解。

Abstract: Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.

</details>


### [9] [A Single Image and Multimodality Is All You Need for Novel View Synthesis](https://arxiv.org/abs/2602.17909)
*Amirhosein Javadi,Chi-Shiang Gau,Konstantinos D. Polyzos,Tara Javidi*

Main category: cs.CV

TL;DR: 该论文提出了一种利用稀疏多模态测距数据（如雷达或激光雷达）改进基于扩散模型的单图像新视角合成的方法，通过高斯过程重建密集深度图来替代单目深度估计，显著提升了几何一致性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 基于扩散模型的新视角合成方法依赖于单目深度估计的几何信息，但在低纹理、恶劣天气和遮挡严重的真实场景中，深度估计的可靠性有限，导致合成视图的质量和一致性受到影响。

Method: 提出多模态深度重建框架，利用极稀疏的测距数据（如汽车雷达或激光雷达），采用局部高斯过程在角度域建模深度，实现计算高效推理并显式量化观测有限区域的不确定性。重建的深度和不确定性可直接替代现有扩散渲染流程中的单目深度估计器。

Result: 在真实世界多模态驾驶场景实验中，用稀疏测距重建深度替代纯视觉深度，显著提升了单图像新视角视频生成的几何一致性和视觉质量。

Conclusion: 研究强调了可靠几何先验对基于扩散模型的视角合成的重要性，并证明了即使在极端稀疏情况下，多模态传感也具有实际优势。

Abstract: Diffusion-based approaches have recently demonstrated strong performance for single-image novel view synthesis by conditioning generative models on geometry inferred from monocular depth estimation. However, in practice, the quality and consistency of the synthesized views are fundamentally limited by the reliability of the underlying depth estimates, which are often fragile under low texture, adverse weather, and occlusion-heavy real-world conditions. In this work, we show that incorporating sparse multimodal range measurements provides a simple yet effective way to overcome these limitations. We introduce a multimodal depth reconstruction framework that leverages extremely sparse range sensing data, such as automotive radar or LiDAR, to produce dense depth maps that serve as robust geometric conditioning for diffusion-based novel view synthesis. Our approach models depth in an angular domain using a localized Gaussian Process formulation, enabling computationally efficient inference while explicitly quantifying uncertainty in regions with limited observations. The reconstructed depth and uncertainty are used as a drop-in replacement for monocular depth estimators in existing diffusion-based rendering pipelines, without modifying the generative model itself. Experiments on real-world multimodal driving scenes demonstrate that replacing vision-only depth with our sparse range-based reconstruction substantially improves both geometric consistency and visual quality in single-image novel-view video generation. These results highlight the importance of reliable geometric priors for diffusion-based view synthesis and demonstrate the practical benefits of multimodal sensing even at extreme levels of sparsity.

</details>


### [10] [ZACH-ViT: Regime-Dependent Inductive Bias in Compact Vision Transformers for Medical Imaging](https://arxiv.org/abs/2602.17929)
*Athanasios Angelakis*

Main category: cs.CV

TL;DR: ZACH-ViT是一种紧凑型视觉Transformer，移除了位置嵌入和[CLS]标记，通过全局平均池化实现排列不变性，在医学图像任务中表现良好，特别适合资源受限的临床环境。


<details>
  <summary>Details</summary>
Motivation: 传统Vision Transformer中的位置嵌入和类别标记编码了固定的空间先验，这在自然图像中有效，但在医学成像中可能阻碍泛化能力，因为医学图像的空间布局信息较弱或不一致。

Method: 提出ZACH-ViT，移除位置嵌入和[CLS]标记，通过全局平均池化处理补丁表示实现排列不变性。采用自适应残差投影保持训练稳定性，同时严格控制参数预算。

Result: 在7个MedMNIST数据集上的评估显示：ZACH-ViT（0.25M参数）在BloodMNIST上表现最强，在PathMNIST上与TransMIL竞争，在具有强解剖先验的数据集（OCTMNIST、OrganAMNIST）上相对优势减弱，验证了架构假设。

Conclusion: 将架构归纳偏置与数据结构对齐比追求通用基准优势更重要。ZACH-ViT尽管规模小且无预训练，仍能实现竞争性性能并保持亚秒级推理时间，适合资源受限的临床环境部署。

Abstract: Vision Transformers rely on positional embeddings and class tokens that encode fixed spatial priors. While effective for natural images, these priors may hinder generalization when spatial layout is weakly informative or inconsistent, a frequent condition in medical imaging and edge-deployed clinical systems. We introduce ZACH-ViT (Zero-token Adaptive Compact Hierarchical Vision Transformer), a compact Vision Transformer that removes both positional embeddings and the [CLS] token, achieving permutation invariance through global average pooling over patch representations. The term "Zero-token" specifically refers to removing the dedicated [CLS] aggregation token and positional embeddings; patch tokens remain unchanged and are processed normally. Adaptive residual projections preserve training stability in compact configurations while maintaining a strict parameter budget.
  Evaluation is performed across seven MedMNIST datasets spanning binary and multi-class tasks under a strict few-shot protocol (50 samples per class, fixed hyperparameters, five random seeds). The empirical analysis demonstrates regime-dependent behavior: ZACH-ViT (0.25M parameters, trained from scratch) achieves its strongest advantage on BloodMNIST and remains competitive with TransMIL on PathMNIST, while its relative advantage decreases on datasets with strong anatomical priors (OCTMNIST, OrganAMNIST), consistent with the architectural hypothesis. These findings support the view that aligning architectural inductive bias with data structure can be more important than pursuing universal benchmark dominance. Despite its minimal size and lack of pretraining, ZACH-ViT achieves competitive performance while maintaining sub-second inference times, supporting deployment in resource-constrained clinical environments. Code and models are available at https://github.com/Bluesman79/ZACH-ViT.

</details>


### [11] [ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models](https://arxiv.org/abs/2602.17951)
*Guoheng Sun,Tingting Du,Kaixi Feng,Chenxiang Luo,Xingguo Ding,Zheyu Shen,Ziyao Wang,Yexiao He,Ang Li*

Main category: cs.CV

TL;DR: ROCKET提出了一种残差导向的多层表示对齐框架，通过共享投影器将VLA模型的多个层与3D视觉基础模型的多个层对齐，减少梯度冲突，显著提升3D空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型通常在2D数据上预训练，缺乏3D空间理解能力。现有表示对齐方法通常只在单层进行监督，无法充分利用深度分布信息，而简单的多层对齐会导致梯度干扰。

Method: ROCKET采用残差导向的多层表示对齐框架，将多层对齐建模为将一个残差流对齐到另一个残差流。使用共享投影器通过层不变映射将VLA骨干的多个层与强大的3D视觉基础模型的多个层对齐，减少梯度冲突。提出Matryoshka风格的稀疏激活方案来平衡多个对齐损失，并结合免训练层选择策略。

Result: ROCKET仅需约4%的计算预算，在LIBERO上达到98.5%的最先进成功率。在LIBERO-Plus和RoboTwin等多个基准测试中表现出优越性能，适用于多种VLA模型。

Conclusion: ROCKET通过残差导向的多层表示对齐框架，有效解决了现有VLA模型缺乏3D空间理解的问题，显著提升了指令跟随机器人操作能力，同时大幅降低了计算成本。

Abstract: Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, naïve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts. We provide both theoretical justification and empirical analyses showing that a shared projector is sufficient and outperforms prior designs, and further propose a Matryoshka-style sparse activation scheme for the shared projector to balance multiple alignment losses. Our experiments show that, combined with a training-free layer selection strategy, ROCKET requires only about 4% of the compute budget while achieving 98.5% state-of-the-art success rate on LIBERO. We further demonstrate the superior performance of ROCKET across LIBERO-Plus and RoboTwin, as well as multiple VLA models. The code and model weights can be found at https://github.com/CASE-Lab-UMD/ROCKET-VLA.

</details>


### [12] [Image Quality Assessment: Exploring Quality Awareness via Memory-driven Distortion Patterns Matching](https://arxiv.org/abs/2602.18000)
*Xuting Lan,Mingliang Zhou,Xuekai Wei,Jielu Yan,Yueting Huang,Huayan Pu,Jun Luo,Weijia Jia*

Main category: cs.CV

TL;DR: 提出基于记忆驱动的质量感知框架（MQAF），通过建立存储失真模式的记忆库，在有无参考图像时动态切换双模式质量评估策略，减少对高质量参考图像的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有全参考图像质量评估（FR-IQA）方法依赖参考图像质量，限制了在理想参考源不可用的实际应用。受人类视觉系统积累视觉记忆能力的启发，提出减少对高质量参考图像依赖的新方法。

Method: 建立存储失真模式的记忆库，实现双模式质量评估策略：有参考图像时，通过自适应加权参考信息并与记忆库中失真模式比较获得参考引导质量分数；无参考图像时，依赖记忆库中的失真模式推断图像质量，实现无参考质量评估。

Result: 实验结果表明，该方法在多个数据集上优于现有最先进方法，同时能够适应无参考和全参考任务。

Conclusion: 提出的记忆驱动质量感知框架通过模拟人类视觉记忆机制，有效减少了对高质量参考图像的依赖，在有无参考图像情况下都能实现高质量评估，具有更好的实际应用价值。

Abstract: Existing full-reference image quality assessment (FR-IQA) methods achieve high-precision evaluation by analysing feature differences between reference and distorted images. However, their performance is constrained by the quality of the reference image, which limits real-world applications where ideal reference sources are unavailable. Notably, the human visual system has the ability to accumulate visual memory, allowing image quality assessment on the basis of long-term memory storage. Inspired by this biological memory mechanism, we propose a memory-driven quality-aware framework (MQAF), which establishes a memory bank for storing distortion patterns and dynamically switches between dual-mode quality assessment strategies to reduce reliance on high-quality reference images. When reference images are available, MQAF obtains reference-guided quality scores by adaptively weighting reference information and comparing the distorted image with stored distortion patterns in the memory bank. When the reference image is absent, the framework relies on distortion patterns in the memory bank to infer image quality, enabling no-reference quality assessment (NR-IQA). The experimental results show that our method outperforms state-of-the-art approaches across multiple datasets while adapting to both no-reference and full-reference tasks.

</details>


### [13] [MUOT_3M: A 3 Million Frame Multimodal Underwater Benchmark and the MUTrack Tracking Method](https://arxiv.org/abs/2602.18006)
*Ahsan Baidar Bakht,Mohamad Alansari,Muhayy Ud Din,Muzammal Naseer,Sajid Javed,Irfan Hussain,Jiri Matas,Arif Mahmood*

Main category: cs.CV

TL;DR: MUOT_3M是首个包含300万帧、3030个视频的多模态水下目标跟踪基准数据集，MUTrack是基于SAM的多模态到单模态跟踪器，在五个基准测试中性能超越SOTA方法8.40% AUC和7.80%精度。


<details>
  <summary>Details</summary>
Motivation: 水下目标跟踪对海洋机器人、生态监测和海洋探索至关重要，但现有基准数据集规模小且仅支持RGB模态，限制了在颜色失真、浑浊和低能见度条件下的鲁棒性。

Method: 提出MUOT_3M多模态基准数据集，包含RGB、增强RGB、估计深度和语言模态；基于此提出MUTrack跟踪器，采用视觉几何对齐、视觉语言融合和四级知识蒸馏，将多模态知识迁移到单模态学生模型。

Result: MUOT_3M包含300万帧、3030个视频（27.8小时），标注了32个跟踪属性、677个细粒度类别；MUTrack在五个UOT基准测试中达到24FPS，比最强SOTA基线高8.40% AUC和7.80%精度。

Conclusion: MUOT_3M和MUTrack为可扩展、多模态训练但实际可部署的水下跟踪建立了新基础，解决了数据稀缺和模态单一的限制问题。

Abstract: Underwater Object Tracking (UOT) is crucial for efficient marine robotics, large scale ecological monitoring, and ocean exploration; however, progress has been hindered by the scarcity of large, multimodal, and diverse datasets. Existing benchmarks remain small and RGB only, limiting robustness under severe color distortion, turbidity, and low visibility conditions. We introduce MUOT_3M, the first pseudo multimodal UOT benchmark comprising 3 million frames from 3,030 videos (27.8h) annotated with 32 tracking attributes, 677 fine grained classes, and synchronized RGB, estimated enhanced RGB, estimated depth, and language modalities validated by a marine biologist. Building upon MUOT_3M, we propose MUTrack, a SAM-based multimodal to unimodal tracker featuring visual geometric alignment, vision language fusion, and four level knowledge distillation that transfers multimodal knowledge into a unimodal student model. Extensive evaluations across five UOT benchmarks demonstrate that MUTrack achieves up to 8.40% higher AUC and 7.80% higher precision than the strongest SOTA baselines while running at 24 FPS. MUOT_3M and MUTrack establish a new foundation for scalable, multimodally trained yet practically deployable underwater tracking.

</details>


### [14] [Towards LLM-centric Affective Visual Customization via Efficient and Precise Emotion Manipulating](https://arxiv.org/abs/2602.18016)
*Jiamin Luo,Xuqian Gu,Jingjing Wang,Jiahong Lu*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态大语言模型的LLM中心化情感视觉定制（L-AVC）任务，旨在通过编辑图像的主观情感内容来生成图像，并提出了高效精确的情感操纵方法（EPEM）。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定制研究主要关注各种控制信号（如语言、布局、边缘检测）与编辑图像之间的客观对齐，但忽略了主观情感内容，且缺乏通用的情感视觉定制基础模型。

Method: 提出高效精确的情感操纵方法（EPEM），包含两个核心模块：高效情感间转换（EIC）模块使LLM在编辑前后高效对齐情感语义转换；精确情感外保留（PER）模块精确保留与情感无关的内容。

Result: 在构建的L-AVC数据集上进行综合实验评估，结果显示EPEM方法在L-AVC任务上优于多个最先进的基线方法，证明了情感信息对L-AVC的重要性以及EPEM在高效精确操纵情感信息方面的有效性。

Conclusion: 情感信息对LLM中心化视觉定制任务至关重要，提出的EPEM方法能够高效且精确地操纵图像中的情感内容，为情感视觉定制提供了有效的解决方案。

Abstract: Previous studies on visual customization primarily rely on the objective alignment between various control signals (e.g., language, layout and canny) and the edited images, which largely ignore the subjective emotional contents, and more importantly lack general-purpose foundation models for affective visual customization. With this in mind, this paper proposes an LLM-centric Affective Visual Customization (L-AVC) task, which focuses on generating images within modifying their subjective emotions via Multimodal LLM. Further, this paper contends that how to make the model efficiently align emotion conversion in semantics (named inter-emotion semantic conversion) and how to precisely retain emotion-agnostic contents (named exter-emotion semantic retaining) are rather important and challenging in this L-AVC task. To this end, this paper proposes an Efficient and Precise Emotion Manipulating approach for editing subjective emotions in images. Specifically, an Efficient Inter-emotion Converting (EIC) module is tailored to make the LLM efficiently align emotion conversion in semantics before and after editing, followed by a Precise Exter-emotion Retaining (PER) module to precisely retain the emotion-agnostic contents. Comprehensive experimental evaluations on our constructed L-AVC dataset demonstrate the great advantage of the proposed EPEM approach to the L-AVC task over several state-of-the-art baselines. This justifies the importance of emotion information for L-AVC and the effectiveness of EPEM in efficiently and precisely manipulating such information.

</details>


### [15] [DeepSVU: Towards In-depth Security-oriented Video Understanding via Unified Physical-world Regularized MoE](https://arxiv.org/abs/2602.18019)
*Yujie Jin,Wenxin Zhang,Jingjing Wang,Guodong Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种新的深度安全导向视频理解任务DeepSVU，不仅检测威胁，还分析威胁原因，并提出了统一的物理世界正则化MoE方法UPRM来解决该任务中的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 现有安全导向视频理解研究主要关注检测和定位视频中的威胁（如枪击、抢劫），但缺乏生成和评估威胁原因的有效能力。为填补这一空白，本文提出了新的深度安全导向视频理解任务DeepSVU。

Method: 提出了统一的物理世界正则化MoE方法UPRM，包含两个关键组件：统一的物理世界增强MoE块（UPE）用于建模从粗到细的物理世界信息；物理世界权衡正则化器（PTR）用于自适应权衡这些因素。

Result: 在DeepSVU指令数据集（UCF-C指令和CUVA指令）上的大量实验表明，UPRM优于多个先进的视频-语言模型和非VLM方法，证明了从粗到细的物理世界信息在DeepSVU任务中的重要性以及UPRM捕获此类信息的有效性。

Conclusion: 论文成功提出了DeepSVU任务并开发了有效的UPRM方法，解决了安全导向视频理解中威胁原因分析的空白，为视频安全分析提供了更深入的理解框架。

Abstract: In the literature, prior research on Security-oriented Video Understanding (SVU) has predominantly focused on detecting and localize the threats (e.g., shootings, robberies) in videos, while largely lacking the effective capability to generate and evaluate the threat causes. Motivated by these gaps, this paper introduces a new chat paradigm SVU task, i.e., In-depth Security-oriented Video Understanding (DeepSVU), which aims to not only identify and locate the threats but also attribute and evaluate the causes threatening segments. Furthermore, this paper reveals two key challenges in the proposed task: 1) how to effectively model the coarse-to-fine physical-world information (e.g., human behavior, object interactions and background context) to boost the DeepSVU task; and 2) how to adaptively trade off these factors. To tackle these challenges, this paper proposes a new Unified Physical-world Regularized MoE (UPRM) approach. Specifically, UPRM incorporates two key components: the Unified Physical-world Enhanced MoE (UPE) Block and the Physical-world Trade-off Regularizer (PTR), to address the above two challenges, respectively. Extensive experiments conduct on our DeepSVU instructions datasets (i.e., UCF-C instructions and CUVA instructions) demonstrate that UPRM outperforms several advanced Video-LLMs as well as non-VLM approaches. Such information.These justify the importance of the coarse-to-fine physical-world information in the DeepSVU task and demonstrate the effectiveness of our UPRM in capturing such information.

</details>


### [16] [UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models](https://arxiv.org/abs/2602.18020)
*Jiabing Yang,Yixiang Chen,Yuan Xu,Peiyan Li,Xiangnan Wu,Zichen Wen,Bowen Fang,Tao Yu,Zhengbo Zhang,Yingda Li,Kai Wang,Jing Liu,Nianfeng Liu,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: 提出UAOR模块，无需训练即可增强VLA模型性能，通过不确定性感知的观测信息重注入机制，在语言模型层不确定性高时，通过注意力检索将关键观测信息注入下一层FFN，提升动作生成的置信度和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型增强方法通常需要额外的观测线索（如深度图、点云）或辅助模块（如目标检测器），这些方法需要昂贵的数据收集和额外训练。作者希望开发一种无需训练、即插即用的模块来提升VLA模型性能。

Method: 提出不确定性感知观测重注入（UAOR）模块，当当前语言模型层的不确定性（通过动作熵衡量）较高时，通过注意力检索机制将关键观测信息重新注入下一层的FFN中，帮助模型更好地关注观测信息。

Result: 综合实验表明，该方法能持续提升多种VLA模型在仿真和真实世界任务中的性能，且开销极小。UAOR无需额外观测线索或模块，可作为现有VLA管道的通用即插即用插件。

Conclusion: UAOR是一种有效、无需训练、即插即用的VLA模型增强模块，通过不确定性感知的观测信息重注入机制，显著提升模型性能，为现有VLA系统提供了实用且通用的改进方案。

Abstract: Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as "key-value memory", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.

</details>


### [17] [Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers](https://arxiv.org/abs/2602.18022)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: 本文提出DCAG框架，通过同时操控DiT架构中的Key和Value通道实现无需训练的图像编辑强度控制，相比仅操控Key的方法在编辑保真度权衡上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的注意力操控方法仅关注Key空间来调节注意力路由，而完全忽略了控制特征聚合的Value空间。需要一种无需训练的方法来更精确地控制基于DiT的图像编辑模型的编辑强度。

Method: 提出Dual-Channel Attention Guidance (DCAG)框架，基于DiT多模态注意力层中Key和Value投影都表现出明显的偏差-增量结构这一观察，同时操控Key通道（控制注意力位置）和Value通道（控制聚合内容）。理论分析表明Key通道通过非线性softmax函数作为粗粒度控制，Value通道通过线性加权求和作为细粒度补充。

Result: 在PIE-Bench基准测试（700张图像，10个编辑类别）上，DCAG在所有保真度指标上都优于仅使用Key引导的方法，在对象删除（LPIPS降低4.9%）和对象添加（LPIPS降低3.2%）等局部化编辑任务中改进最为显著。二维参数空间(δ_k, δ_v)能实现比任何单通道方法更精确的编辑-保真度权衡。

Conclusion: DCAG框架通过同时操控Key和Value通道，为基于DiT的图像编辑模型提供了更精确的无需训练编辑强度控制，在编辑质量和保真度之间实现了更好的平衡。

Abstract: Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT's multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(δ_k, δ_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).

</details>


### [18] [Spatio-temporal Decoupled Knowledge Compensator for Few-Shot Action Recognition](https://arxiv.org/abs/2602.18043)
*Hongyu Qu,Xiangbo Shu,Rui Yan,Hailiang Gao,Wenguan Wang,Jinhui Tang*

Main category: cs.CV

TL;DR: DiST提出了一种基于分解-融合框架的少样本动作识别方法，利用大语言模型提供的解耦空间和时间知识来学习表达性多粒度原型


<details>
  <summary>Details</summary>
Motivation: 现有的少样本动作识别方法通常使用语义粗糙的类别名称作为辅助上下文来指导视觉特征学习，但动作名称提供的上下文过于有限，无法为捕捉动作中的新颖空间和时间概念提供足够的背景知识

Method: 提出DiST框架：1）分解阶段：将普通动作名称解耦为多样化的时空属性描述；2）融合阶段：提出空间/时间知识补偿器（SKC/TKC）分别发现判别性的对象级和帧级原型，SKC在空间知识指导下自适应聚合重要补丁标记，TKC利用时间属性辅助帧间时间关系建模

Result: 实验结果显示DiST在五个标准FSAR数据集上取得了最先进的结果

Conclusion: 通过解耦和融合大语言模型提供的空间和时间知识，DiST能够学习表达性多粒度原型，有效捕捉细粒度空间细节和多样化时间模式，在少样本动作识别任务中表现出色

Abstract: Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features. However, such context provided by the action names is too limited to provide sufficient background knowledge for capturing novel spatial and temporal concepts in actions. In this paper, we propose DiST, an innovative Decomposition-incorporation framework for FSAR that makes use of decoupled Spatial and Temporal knowledge provided by large language models to learn expressive multi-granularity prototypes. In the decomposition stage, we decouple vanilla action names into diverse spatio-temporal attribute descriptions (action-related knowledge). Such commonsense knowledge complements semantic contexts from spatial and temporal perspectives. In the incorporation stage, we propose Spatial/Temporal Knowledge Compensators (SKC/TKC) to discover discriminative object-level and frame-level prototypes, respectively. In SKC, object-level prototypes adaptively aggregate important patch tokens under the guidance of spatial knowledge. Moreover, in TKC, frame-level prototypes utilize temporal attributes to assist in inter-frame temporal relation modeling. These learned prototypes thus provide transparency in capturing fine-grained spatial details and diverse temporal patterns. Experimental results show DiST achieves state-of-the-art results on five standard FSAR datasets.

</details>


### [19] [CityGuard: Graph-Aware Private Descriptors for Bias-Resilient Identity Search Across Urban Cameras](https://arxiv.org/abs/2602.18047)
*Rong Fu,Wenxin Zhang,Yibo Meng,Jia Yee Tan,Jiaxuan Lu,Rui Lu,Jiekai Wu,Zhaolu Kang,Simon Fong*

Main category: cs.CV

TL;DR: CityGuard：一种拓扑感知的Transformer框架，用于分布式摄像头下的隐私保护行人重识别，通过分散自适应度量学习、空间条件注意力和差分隐私嵌入映射，在保护隐私的同时实现高效身份检索。


<details>
  <summary>Details</summary>
Motivation: 城市规模的行人重识别面临视角变化、遮挡和域偏移等挑战，同时需要遵守数据保护法规，防止原始图像共享。现有方法在隐私保护和检索性能之间存在权衡。

Method: 1. 分散自适应度量学习：根据特征分布调整实例级边界，增强类内紧凑性；2. 空间条件注意力：将GPS或部署平面图等粗略几何信息注入基于图的自注意力，实现投影一致的跨视角对齐；3. 差分隐私嵌入映射：结合紧凑近似索引，支持安全高效部署。

Result: 在Market-1501和其他公开基准测试中，检索精度和查询吞吐量均优于强基线方法，数据库规模检索研究证实了框架的实际可行性。

Conclusion: CityGuard框架在隐私关键的城市身份匹配应用中，实现了视角变化、遮挡和域偏移的鲁棒性，并在严格的差分隐私约束下提供了隐私与效用的可调平衡。

Abstract: City-scale person re-identification across distributed cameras must handle severe appearance changes from viewpoint, occlusion, and domain shift while complying with data protection rules that prevent sharing raw imagery. We introduce CityGuard, a topology-aware transformer for privacy-preserving identity retrieval in decentralized surveillance. The framework integrates three components. A dispersion-adaptive metric learner adjusts instance-level margins according to feature spread, increasing intra-class compactness. Spatially conditioned attention injects coarse geometry, such as GPS or deployment floor plans, into graph-based self-attention to enable projectively consistent cross-view alignment using only coarse geometric priors without requiring survey-grade calibration. Differentially private embedding maps are coupled with compact approximate indexes to support secure and cost-efficient deployment. Together these designs produce descriptors robust to viewpoint variation, occlusion, and domain shifts, and they enable a tunable balance between privacy and utility under rigorous differential-privacy accounting. Experiments on Market-1501 and additional public benchmarks, complemented by database-scale retrieval studies, show consistent gains in retrieval precision and query throughput over strong baselines, confirming the practicality of the framework for privacy-critical urban identity matching.

</details>


### [20] [Temporal Consistency-Aware Text-to-Motion Generation](https://arxiv.org/abs/2602.18057)
*Hongsong Wang,Wenjing Yan,Qiuxia Lai,Xin Geng*

Main category: cs.CV

TL;DR: TCA-T2M：一种时序一致性感知的文本到动作生成框架，通过跨序列时序对齐和运动约束提升动作生成的语义对齐和物理合理性


<details>
  <summary>Details</summary>
Motivation: 现有两阶段文本到动作生成框架通常忽略跨序列时序一致性（即同一动作不同实例间的共享时序结构），导致语义错位和物理上不可信的动作

Method: 提出TCA-T2M框架：1）时序一致性感知的空间VQ-VAE用于跨序列时序对齐；2）掩码运动变换器用于文本条件动作生成；3）运动学约束块减轻离散化伪影确保物理合理性

Result: 在HumanML3D和KIT-ML基准测试中达到最先进性能，证明了时序一致性对鲁棒和连贯文本到动作生成的重要性

Conclusion: 时序一致性是文本到动作生成的关键因素，TCA-T2M通过跨序列时序对齐和物理约束有效提升了动作生成的语义准确性和物理合理性

Abstract: Text-to-Motion (T2M) generation aims to synthesize realistic human motion sequences from natural language descriptions. While two-stage frameworks leveraging discrete motion representations have advanced T2M research, they often neglect cross-sequence temporal consistency, i.e., the shared temporal structures present across different instances of the same action. This leads to semantic misalignments and physically implausible motions. To address this limitation, we propose TCA-T2M, a framework for temporal consistency-aware T2M generation. Our approach introduces a temporal consistency-aware spatial VQ-VAE (TCaS-VQ-VAE) for cross-sequence temporal alignment, coupled with a masked motion transformer for text-conditioned motion generation. Additionally, a kinematic constraint block mitigates discretization artifacts to ensure physical plausibility. Experiments on HumanML3D and KIT-ML benchmarks demonstrate that TCA-T2M achieves state-of-the-art performance, highlighting the importance of temporal consistency in robust and coherent T2M generation.

</details>


### [21] [Diff2DGS: Reliable Reconstruction of Occluded Surgical Scenes via 2D Gaussian Splatting](https://arxiv.org/abs/2602.18314)
*Tianyi Song,Danail Stoyanov,Evangelos Mazomenos,Francisco Vasconcelos*

Main category: cs.CV

TL;DR: Diff2DGS：用于手术场景实时3D重建的两阶段框架，通过扩散模型修复被器械遮挡的组织，结合可学习变形模型的2D高斯溅射技术，在图像质量和几何精度上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前手术场景实时重建方法虽然能实现密集重建，但在遮挡区域的重建质量有限，且深度精度评估不足，因为现有基准数据集缺乏3D真实数据。需要一种既能处理遮挡又能保证几何精度的可靠重建方法。

Method: 提出两阶段框架：第一阶段使用基于扩散的视频模块，利用时间先验修复被手术器械遮挡的组织，保证时空一致性；第二阶段采用2D高斯溅射技术，结合可学习变形模型来捕捉动态组织变形和解剖几何结构。

Result: 在EndoNeRF数据集上达到38.02 dB PSNR，在StereoMIS数据集上达到34.40 dB PSNR，均优于现有方法。实验表明仅优化图像质量并不能保证最优的3D重建精度，因此进一步优化深度质量以获得更准确的几何重建。

Conclusion: Diff2DGS在手术场景的3D重建中同时实现了高保真外观和准确几何结构，通过两阶段方法有效解决了遮挡问题和深度精度评估不足的挑战，为机器人手术的实时重建提供了可靠解决方案。

Abstract: Real-time reconstruction of deformable surgical scenes is vital for advancing robotic surgery, improving surgeon guidance, and enabling automation. Recent methods achieve dense reconstructions from da Vinci robotic surgery videos, with Gaussian Splatting (GS) offering real-time performance via graphics acceleration. However, reconstruction quality in occluded regions remains limited, and depth accuracy has not been fully assessed, as benchmarks like EndoNeRF and StereoMIS lack 3D ground truth. We propose Diff2DGS, a novel two-stage framework for reliable 3D reconstruction of occluded surgical scenes. In the first stage, a diffusion-based video module with temporal priors inpaints tissue occluded by instruments with high spatial-temporal consistency. In the second stage, we adapt 2D Gaussian Splatting (2DGS) with a Learnable Deformation Model (LDM) to capture dynamic tissue deformation and anatomical geometry. We also extend evaluation beyond prior image-quality metrics by performing quantitative depth accuracy analysis on the SCARED dataset. Diff2DGS outperforms state-of-the-art approaches in both appearance and geometry, reaching 38.02 dB PSNR on EndoNeRF and 34.40 dB on StereoMIS. Furthermore, our experiments demonstrate that optimizing for image quality alone does not necessarily translate into optimal 3D reconstruction accuracy. To address this, we further optimize the depth quality of the reconstructed 3D results, ensuring more faithful geometry in addition to high-fidelity appearance.

</details>


### [22] [CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation](https://arxiv.org/abs/2602.18424)
*Xia Su,Ruiqi Chen,Benlin Liu,Jingwei Ma,Zonglin Di,Ranjay Krishna,Jon Froehlich*

Main category: cs.CV

TL;DR: CapNav是一个评估视觉语言模型在考虑智能体物理能力约束下进行室内导航的新基准，包含5种代表性智能体、45个真实室内场景、473个导航任务，测试发现现有VLM在移动约束严格时性能显著下降


<details>
  <summary>Details</summary>
Motivation: 现实世界导航受智能体移动能力约束（如扫地机器人不能爬楼梯，四足机器人可以），但现有视觉语言导航研究未充分考虑这些物理限制，需要评估VLM在考虑智能体具体能力下的导航性能

Method: 定义了5种代表性人类和机器人智能体，描述其物理尺寸、移动能力和环境交互能力；构建包含45个真实室内场景、473个导航任务和2365个问答对的基准数据集；评估了13个现代VLM模型

Result: 当前VLM的导航性能随移动约束收紧而急剧下降；即使最先进的模型在处理需要空间维度推理的障碍类型时也表现不佳；模型在考虑智能体能力约束下的导航能力有限

Conclusion: 需要开发能力感知的导航系统，未来VLM应增强具身空间推理能力，考虑智能体物理约束对现实世界导航至关重要

Abstract: Vision-Language Models (VLMs) have shown remarkable progress in Vision-Language Navigation (VLN), offering new possibilities for navigation decision-making that could benefit both robotic platforms and human users. However, real-world navigation is inherently conditioned by the agent's mobility constraints. For example, a sweeping robot cannot traverse stairs, while a quadruped can. We introduce Capability-Conditioned Navigation (CapNav), a benchmark designed to evaluate how well VLMs can navigate complex indoor spaces given an agent's specific physical and operational capabilities. CapNav defines five representative human and robot agents, each described with physical dimensions, mobility capabilities, and environmental interaction abilities. CapNav provides 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test if VLMs can traverse indoor environments based on agent capabilities. We evaluate 13 modern VLMs and find that current VLM's navigation performance drops sharply as mobility constraints tighten, and that even state-of-the-art models struggle with obstacle types that require reasoning on spatial dimensions. We conclude by discussing the implications for capability-aware navigation and the opportunities for advancing embodied spatial reasoning in future VLMs. The benchmark is available at https://github.com/makeabilitylab/CapNav

</details>


### [23] [Faster Training, Fewer Labels: Self-Supervised Pretraining for Fine-Grained BEV Segmentation](https://arxiv.org/abs/2602.18066)
*Daniel Busch,Christian Bohn,Thomas Kurbiel,Klaus Friedrichs,Richard Meyes,Tobias Meisen*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段训练策略，用于鸟瞰图语义分割，通过自监督预训练和半监督微调，在减少50%标注数据和三分之二训练时间的情况下，性能仍优于全监督基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前多摄像头鸟瞰图语义分割方法依赖于昂贵且标注不一致的地面真值数据，这限制了自动驾驶感知系统的可扩展性。

Method: 采用两阶段训练策略：1）自监督预训练阶段，将BEVFormer预测通过可微分重投影到图像平面，使用Mask2Former生成的多视角语义伪标签进行训练，并加入时间一致性损失；2）监督微调阶段，仅使用50%数据集进行训练。

Result: 在nuScenes数据集上，该方法比全监督基线模型提升高达2.5个百分点的mIoU，同时将标注数据使用量减半，总训练时间减少三分之二。

Conclusion: 可微分重投影加相机视角伪标签能够产生可迁移的BEV特征，为减少标注的自动驾驶感知提供了一条可扩展的路径。

Abstract: Dense Bird's Eye View (BEV) semantic maps are central to autonomous driving, yet current multi-camera methods depend on costly, inconsistently annotated BEV ground truth. We address this limitation with a two-phase training strategy for fine-grained road marking segmentation that removes full supervision during pretraining and halves the amount of training data during fine-tuning while still outperforming the comparable supervised baseline model. During the self-supervised pretraining, BEVFormer predictions are differentiably reprojected into the image plane and trained against multi-view semantic pseudo-labels generated by the widely used semantic segmentation model Mask2Former. A temporal loss encourages consistency across frames. The subsequent supervised fine-tuning phase requires only 50% of the dataset and significantly less training time. With our method, the fine-tuning benefits from rich priors learned during pretraining boosting the performance and BEV segmentation quality (up to +2.5pp mIoU over the fully supervised baseline) on nuScenes. It simultaneously halves the usage of annotation data and reduces total training time by up to two thirds. The results demonstrate that differentiable reprojection plus camera perspective pseudo labels yields transferable BEV features and a scalable path toward reduced-label autonomous perception.

</details>


### [24] [DohaScript: A Large-Scale Multi-Writer Dataset for Continuous Handwritten Hindi Text](https://arxiv.org/abs/2602.18089)
*Kunwar Arpit Singh,Ankush Prakash,Haroon R Lone*

Main category: cs.CV

TL;DR: DohaScript是一个大规模、多书写者的手写印地语数据集，包含531名贡献者书写的六首传统印地语对句，旨在解决德瓦纳格里文字手写数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 德瓦纳格里文字手写文本在公开基准数据集中严重不足，现有资源规模有限，主要关注孤立字符或短词，缺乏受控词汇内容和书写者多样性，无法捕捉德瓦纳格里手写体的连续、融合和结构复杂特性。

Method: 创建DohaScript数据集，作为平行风格语料库，所有书写者转录相同的六首传统印地语对句。数据集包含去识别化的人口统计元数据，基于客观清晰度和分辨率标准的严格质量筛选，以及页面级布局难度标注。

Result: 基线实验显示了清晰的质量分离和对未见书写者的强泛化能力，突显了数据集的可靠性和实用价值。数据集支持手写识别、书写者识别、风格分析和生成建模等任务。

Conclusion: DohaScript旨在作为标准化、可复现的基准，推动低资源脚本环境下连续手写德瓦纳格里文本的研究进展。

Abstract: Despite having hundreds of millions of speakers, handwritten Devanagari text remains severely underrepresented in publicly available benchmark datasets. Existing resources are limited in scale, focus primarily on isolated characters or short words, and lack controlled lexical content and writer level diversity, which restricts their utility for modern data driven handwriting analysis. As a result, they fail to capture the continuous, fused, and structurally complex nature of Devanagari handwriting, where characters are connected through a shared shirorekha (horizontal headline) and exhibit rich ligature formations. We introduce DohaScript, a large scale, multi writer dataset of handwritten Hindi text collected from 531 unique contributors. The dataset is designed as a parallel stylistic corpus, in which all writers transcribe the same fixed set of six traditional Hindi dohas (couplets). This controlled design enables systematic analysis of writer specific variation independent of linguistic content, and supports tasks such as handwriting recognition, writer identification, style analysis, and generative modeling. The dataset is accompanied by non identifiable demographic metadata, rigorous quality curation based on objective sharpness and resolution criteria, and page level layout difficulty annotations that facilitate stratified benchmarking. Baseline experiments demonstrate clear quality separation and strong generalization to unseen writers, highlighting the dataset's reliability and practical value. DohaScript is intended to serve as a standardized and reproducible benchmark for advancing research on continuous handwritten Devanagari text in low resource script settings.

</details>


### [25] [Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.18093)
*Hanshuai Cui,Zhiqing Tang,Qianli Ma,Zhi Yao,Weijia Jia*

Main category: cs.CV

TL;DR: PrediT：一种无需训练的DiT加速框架，通过线性多步方法预测模型输出，结合校正器和动态步长调制，实现5.54倍加速且保持生成质量


<details>
  <summary>Details</summary>
Motivation: 扩散变换器（DiT）在图像和视频生成中表现出色，但其迭代去噪过程计算成本高昂。现有基于特征缓存的加速方法假设时间稳定性，但特征重用可能导致潜在漂移和视觉质量下降

Method: 提出PrediT框架：1) 将特征预测建模为线性多步问题，使用经典线性多步方法从历史信息预测未来模型输出；2) 在高动态区域激活校正器防止误差累积；3) 动态步长调制机制通过监测特征变化率自适应调整预测范围

Result: 实验验证PrediT在各种基于DiT的图像和视频生成模型中实现了高达5.54倍的延迟减少，同时质量下降可忽略不计

Conclusion: PrediT是一种有效的训练免费加速框架，通过预测而非简单重用特征，在保持生成保真度的同时显著提升DiT模型的推理效率

Abstract: Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.

</details>


### [26] [OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models](https://arxiv.org/abs/2602.18094)
*Ling Lin,Yang Bai,Heng Su,Congcong Zhu,Yaoxing Wang,Yang Zhou,Huazhu Fu,Jingrun Chen*

Main category: cs.CV

TL;DR: OODBench：一个自动化构建的基准测试，用于评估视觉语言模型在处理分布外数据时的性能，包含40K实例级OOD数据对，并提出了渐进式提示评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在大规模数据集上训练时假设数据独立同分布，但现实应用中常遇到分布外数据，处理不当可能带来安全风险（如自动驾驶、医疗辅助）。目前缺乏全面评估VLM处理OOD数据能力的有效基准。

Method: 提出OODBench方法，采用最小人工验证的自动化方式构建新基准，包含40K实例级OOD实例-类别对。同时提出可靠的自动化评估指标，采用从基础到高级的渐进式提示问题来更全面地评估OOD数据对不同难度问题的影响。

Result: 当前VLM在OODBench上即使面对常见图像类别仍表现出显著性能下降。通过渐进式提示评估方法，能够更全面地揭示OOD数据对模型性能的影响。

Conclusion: OODBench为评估VLM处理分布外数据能力提供了有效基准，揭示了当前模型的局限性，并总结了重要发现和见解，有助于未来OOD数据获取和评估研究。

Abstract: Existing Visual-Language Models (VLMs) have achieved significant progress by being trained on massive-scale datasets, typically under the assumption that data are independent and identically distributed (IID). However, in real-world scenarios, it is often impractical to expect that all data processed by an AI system satisfy this assumption. Furthermore, failure to appropriately handle out-of-distribution (OOD) objects may introduce safety risks in real-world applications (e.g., autonomous driving or medical assistance). Unfortunately, current research has not yet provided valid benchmarks that can comprehensively assess the performance of VLMs in response to OOD data. Therefore, we propose OODBench, a predominantly automated method with minimal human verification, for constructing new benchmarks and evaluating the ability of VLMs to process OOD data. OODBench contains 40K instance-level OOD instance-category pairs, and we show that current VLMs still exhibit notable performance degradation on OODBench, even when the underlying image categories are common. In addition, we propose a reliable automated assessment metric that employs a Basic-to-Advanced Progression of prompted questions to assess the impact of OOD data on questions of varying difficulty more fully. Lastly, we summarize substantial findings and insights to facilitate future research in the acquisition and evaluation of OOD data.

</details>


### [27] [Evaluating Graphical Perception Capabilities of Vision Transformers](https://arxiv.org/abs/2602.18178)
*Poonam Poonam,Pere-Pau Vázquez,Timo Ropinski*

Main category: cs.CV

TL;DR: ViTs在可视化图形感知任务中表现不如人类，与CNN相比也存在感知差异


<details>
  <summary>Details</summary>
Motivation: 虽然ViTs在各种图像任务中表现出色，但其在可视化图形感知任务中的能力尚未被充分探索，而图形感知对于可视化解释至关重要

Method: 基于Cleveland和McGill的基础研究，设计了一系列受控的图形感知任务，将ViTs与CNNs和人类参与者的表现进行对比评估

Result: ViTs虽然在通用视觉任务中表现强劲，但在可视化领域的图形感知任务中与人类感知的一致性有限，存在明显的感知差距

Conclusion: ViTs在可视化系统和图形感知建模中的应用需要谨慎考虑，研究揭示了重要的感知差异，为未来研究提供了方向

Abstract: Vision Transformers, ViTs, have emerged as a powerful alternative to convolutional neural networks, CNNs, in a variety of image-based tasks. While CNNs have previously been evaluated for their ability to perform graphical perception tasks, which are essential for interpreting visualizations, the perceptual capabilities of ViTs remain largely unexplored. In this work, we investigate the performance of ViTs in elementary visual judgment tasks inspired by the foundational studies of Cleveland and McGill, which quantified the accuracy of human perception across different visual encodings. Inspired by their study, we benchmark ViTs against CNNs and human participants in a series of controlled graphical perception tasks. Our results reveal that, although ViTs demonstrate strong performance in general vision tasks, their alignment with human-like graphical perception in the visualization domain is limited. This study highlights key perceptual gaps and points to important considerations for the application of ViTs in visualization systems and graphical perceptual modeling.

</details>


### [28] [BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards](https://arxiv.org/abs/2602.18193)
*Yiran Yang,Zhaowei Liu,Yuan Yuan,Yukun Song,Xiong Ma,Yinghao Song,Xiangji Zeng,Lu Sun,Yulu Wang,Hai Zhou,Shuai Cui,Zhaohan Gong,Jiefei Zhang*

Main category: cs.CV

TL;DR: BLM-Guard是一个针对短视频平台商业广告的内容审核框架，通过结合思维链推理、基于规则的政策原则和批评者引导的奖励，实现更精细的多模态广告审核。


<details>
  <summary>Details</summary>
Motivation: 短视频平台上的多模态广告包含欺骗性的视觉、语音和字幕内容，需要比社区安全过滤器更精细、基于政策的审核机制。

Method: 1. 使用规则驱动的ICoT数据合成管道生成结构化场景描述、推理链和标签；2. 强化学习通过平衡因果连贯性和政策遵从性的复合奖励来优化模型；3. 多任务架构建模模态内操纵（如夸张图像）和跨模态不匹配（如字幕-语音漂移）。

Result: 在真实短视频广告上的实验表明，BLM-Guard在准确性、一致性和泛化能力方面超越了强基线方法。

Conclusion: BLM-Guard框架为商业广告内容审核提供了一种有效的解决方案，能够处理多模态欺骗性内容，同时降低标注成本并提高审核质量。

Abstract: Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.

</details>


### [29] [A Self-Supervised Approach on Motion Calibration for Enhancing Physical Plausibility in Text-to-Motion](https://arxiv.org/abs/2602.18199)
*Gahyeon Shim,Soogeun Park,Hyemin Ahn*

Main category: cs.CV

TL;DR: DMC是一个后处理模块，通过自监督数据驱动方法，在保持语义一致性的同时，修正文本生成动作中的物理不合理问题（如脚部漂浮），提升动作的物理真实感。


<details>
  <summary>Details</summary>
Motivation: 当前文本到动作生成技术虽能生成语义对齐的动作，但难以同时保证语义和物理真实性。现有方法常产生物理上不合理的动作（如脚部漂浮），需要一种既能保持语义一致性又能提升物理真实性的解决方案。

Method: 提出Distortion-aware Motion Calibrator (DMC)，这是一个后处理模块，采用自监督数据驱动方法。DMC学习在给定故意扭曲的动作和原始文本描述时，生成物理上合理的动作，无需复杂的物理建模。

Result: DMC显著提升动作质量：在T2M上FID分数降低42.74%，在T2M-GPT上降低13.20%，同时达到最高的R-Precision。应用于MoMask等高质量模型时，穿透率减少33.0%，漂浮伪影更接近真实参考。

Conclusion: DMC作为一个有前景的后处理动作精炼框架，能够为各种文本到动作生成模型提升物理合理性，同时保持文本语义一致性，为动作生成领域提供了有效的质量提升方案。

Abstract: Generating semantically aligned human motion from textual descriptions has made rapid progress, but ensuring both semantic and physical realism in motion remains a challenge. In this paper, we introduce the Distortion-aware Motion Calibrator (DMC), a post-hoc module that refines physically implausible motions (e.g., foot floating) while preserving semantic consistency with the original textual description. Rather than relying on complex physical modeling, we propose a self-supervised and data-driven approach, whereby DMC learns to obtain physically plausible motions when an intentionally distorted motion and the original textual descriptions are given as inputs. We evaluate DMC as a post-hoc module to improve motions obtained from various text-to-motion generation models and demonstrate its effectiveness in improving physical plausibility while enhancing semantic consistency. The experimental results show that DMC reduces FID score by 42.74% on T2M and 13.20% on T2M-GPT, while also achieving the highest R-Precision. When applied to high-quality models like MoMask, DMC improves the physical plausibility of motions by reducing penetration by 33.0% as well as adjusting floating artifacts closer to the ground-truth reference. These results highlight that DMC can serve as a promising post-hoc motion refinement framework for any kind of text-to-motion models by incorporating textual semantics and physical plausibility.

</details>


### [30] [On the Adversarial Robustness of Discrete Image Tokenizers](https://arxiv.org/abs/2602.18252)
*Rishika Bhagwatkar,Irina Rish,Nicolas Flammarion,Francesco Croce*

Main category: cs.CV

TL;DR: 该论文首次研究了离散图像分词器在对抗攻击下的脆弱性，提出了高效的应用无关攻击方法，并设计了无监督对抗训练防御方案，显著提升了分词器的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 离散图像分词器在多模态系统中日益流行，但其对抗攻击脆弱性尚未被探索。与CLIP编码器不同，离散分词器的安全性研究存在空白，这影响了多模态基础模型的安全性。

Method: 1. 提出针对离散分词器的对抗攻击方法，旨在扰动特征提取并改变生成的tokens；2. 受鲁棒CLIP编码器启发，采用无监督对抗训练微调流行分词器，保持其他组件冻结。

Result: 攻击方法计算高效、应用无关，在分类、多模态检索和字幕生成任务中均有效。防御方法显著提升了对无监督和端到端监督攻击的鲁棒性，并能良好泛化到未见任务和数据。

Conclusion: 该工作强调了分词器鲁棒性在下游任务中的关键作用，为开发安全的多模态基础模型迈出了重要一步。无监督对抗训练相比监督方法更具通用性，可利用未标记图像。

Abstract: Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.

</details>


### [31] [DEIG: Detail-Enhanced Instance Generation with Fine-Grained Semantic Control](https://arxiv.org/abs/2602.18282)
*Shiyan Du,Conghan Yue,Xinyu Cheng,Dongyu Zhang*

Main category: cs.CV

TL;DR: DEIG是一个用于细粒度可控多实例生成的新框架，通过实例细节提取器和细节融合模块解决现有方法在复杂文本描述下的语义理解问题，在空间一致性、语义准确性和组合泛化方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多实例生成方法在空间布局和属性绑定方面已有进步，但在处理复杂文本描述时仍面临细粒度语义理解的挑战，特别是在处理丰富的局部化文本描述时存在困难。

Method: 提出DEIG框架，包含实例细节提取器（将文本编码器嵌入转换为紧凑的实例感知表示）和细节融合模块（应用基于实例的掩码注意力防止属性跨实例泄漏）。构建了由VLM生成的详细组合实例标注的高质量数据集，并引入了DEIG-Bench基准测试。

Result: 实验表明DEIG在多个基准测试中在空间一致性、语义准确性和组合泛化方面始终优于现有方法。DEIG可作为即插即用模块，易于集成到标准基于扩散的流程中。

Conclusion: DEIG通过细粒度可控的多实例生成框架，有效解决了复杂文本描述下的语义理解问题，能够生成与丰富局部化文本描述精确匹配的视觉连贯多实例场景。

Abstract: Multi-Instance Generation has advanced significantly in spatial placement and attribute binding. However, existing approaches still face challenges in fine-grained semantic understanding, particularly when dealing with complex textual descriptions. To overcome these limitations, we propose DEIG, a novel framework for fine-grained and controllable multi-instance generation. DEIG integrates an Instance Detail Extractor (IDE) that transforms text encoder embeddings into compact, instance-aware representations, and a Detail Fusion Module (DFM) that applies instance-based masked attention to prevent attribute leakage across instances. These components enable DEIG to generate visually coherent multi-instance scenes that precisely match rich, localized textual descriptions. To support fine-grained supervision, we construct a high-quality dataset with detailed, compositional instance captions generated by VLMs. We also introduce DEIG-Bench, a new benchmark with region-level annotations and multi-attribute prompts for both humans and objects. Experiments demonstrate that DEIG consistently outperforms existing approaches across multiple benchmarks in spatial consistency, semantic accuracy, and compositional generalization. Moreover, DEIG functions as a plug-and-play module, making it easily integrable into standard diffusion-based pipelines.

</details>


### [32] [Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation](https://arxiv.org/abs/2602.18309)
*Ziyue Liu,Davide Talon,Federico Girella,Zanxi Ruan,Mattia Mondo,Loris Bazzani,Yiming Wang,Marco Cristani*

Main category: cs.CV

TL;DR: LOTS框架通过多级条件引导，结合全局草图结构和局部文本-草图对，提升时尚图像生成质量，并创建了首个多文本-草图对时尚数据集Sketchy。


<details>
  <summary>Details</summary>
Motivation: 在时尚设计早期阶段，草图能表达结构、轮廓和空间关系，而文本描述能补充材料、颜色和风格细节。如何有效结合文本和视觉模态，在利用文本局部属性指导的同时保持草图视觉结构，是当前研究的挑战。

Method: 提出LOTS框架：1）多级条件阶段，在共享潜在空间中独立编码局部特征，同时保持全局结构协调；2）扩散对引导阶段，通过注意力机制在扩散模型的多步去噪过程中整合局部和全局条件。

Result: 创建了首个时尚数据集Sketchy，包含每张图像对应的多个文本-草图对，包含专业草图和高变异性非专业草图。实验表明该方法在保持全局结构的同时利用更丰富的局部语义指导，优于现有方法。

Conclusion: LOTS框架通过多级条件引导有效结合文本和草图模态，在时尚图像生成中实现了更好的结构保持和语义表达，为多模态时尚设计提供了新方法。

Abstract: Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an "in the wild" split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.

</details>


### [33] [Unifying Color and Lightness Correction with View-Adaptive Curve Adjustment for Robust 3D Novel View Synthesis](https://arxiv.org/abs/2602.18322)
*Ziteng Cui,Shuhong Liu,Xiaoyu Dong,Xuangeng Chu,Lin Gu,Ming-Hsuan Yang,Tatsuya Harada*

Main category: cs.CV

TL;DR: Luminance-GS++：基于3D高斯泼溅的鲁棒新视角合成框架，通过全局自适应亮度调整和局部像素级残差细化处理多视角光照不一致问题，保持实时渲染效率。


<details>
  <summary>Details</summary>
Motivation: 真实环境中多视角图像采集面临复杂光照变化和相机成像管线限制，导致光度不一致性，违反光度一致性假设，影响NeRF和3DGS等新视角合成方法的重建和渲染质量。

Method: 结合全局视角自适应亮度调整和局部像素级残差细化进行精确色彩校正，设计无监督目标联合执行亮度校正与多视角几何和光度一致性约束，保持显式3DGS表示不变。

Result: 在低光照、过曝光及复杂亮度和色彩变化等挑战性场景中实现最先进性能，显著提升重建保真度，同时保持实时渲染效率。

Conclusion: Luminance-GS++通过创新的光度校正方法有效解决了多视角光照不一致问题，在不改变底层表示的情况下提升了3D高斯泼溅在复杂光照条件下的鲁棒性和重建质量。

Abstract: High-quality image acquisition in real-world environments remains challenging due to complex illumination variations and inherent limitations of camera imaging pipelines. These issues are exacerbated in multi-view capture, where differences in lighting, sensor responses, and image signal processor (ISP) configurations introduce photometric and chromatic inconsistencies that violate the assumptions of photometric consistency underlying modern 3D novel view synthesis (NVS) methods, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), leading to degraded reconstruction and rendering quality. We propose Luminance-GS++, a 3DGS-based framework for robust NVS under diverse illumination conditions. Our method combines a globally view-adaptive lightness adjustment with a local pixel-wise residual refinement for precise color correction. We further design unsupervised objectives that jointly enforce lightness correction and multi-view geometric and photometric consistency. Extensive experiments demonstrate state-of-the-art performance across challenging scenarios, including low-light, overexposure, and complex luminance and chromatic variations. Unlike prior approaches that modify the underlying representation, our method preserves the explicit 3DGS formulation, improving reconstruction fidelity while maintaining real-time rendering efficiency.

</details>


### [34] [G-LoG Bi-filtration for Medical Image Classification](https://arxiv.org/abs/2602.18329)
*Qingsong Wang,Jiaxing He,Bingzhe Hou,Tieru Wu,Yang Cao,Cailing Yao*

Main category: cs.CV

TL;DR: 本文提出了一种基于高斯-拉普拉斯算子(G-LoG)的双参数过滤方法，用于医学图像拓扑数据分析，在MedMNIST数据集上表现优于单参数过滤，且基于拓扑特征的简单MLP模型性能接近复杂深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 在拓扑数据分析中，构建实用的过滤方法来检测拓扑和几何特征是一个重要任务。本文旨在利用拉普拉斯高斯算子增强医学图像边界的能力，定义更适合多参数持久性模块的特征。

Method: 提出G-LoG（高斯-拉普拉斯高斯）双参数过滤方法：1）利用拉普拉斯高斯算子增强医学图像边界；2）将体积图像建模为有界函数；3）证明从双过滤获得的持久性模块的交错距离相对于有界函数的最大范数是稳定的。

Result: 在MedMNIST数据集上的实验表明：1）G-LoG双参数过滤显著优于单参数过滤；2）基于拓扑特征的简单多层感知器（MLP）性能与在原始数据集上训练的复杂深度学习模型（Google AutoML Vision、ResNet、AutoKeras、auto-sklearn）相当。

Conclusion: G-LoG双参数过滤方法为医学图像拓扑数据分析提供了有效的特征提取工具，其稳定性和性能优势使其成为传统深度学习方法的有效替代方案。

Abstract: Building practical filtrations on objects to detect topological and geometric features is an important task in the field of Topological Data Analysis (TDA). In this paper, leveraging the ability of the Laplacian of Gaussian operator to enhance the boundaries of medical images, we define the G-LoG (Gaussian-Laplacian of Gaussian) bi-filtration to generate the features more suitable for multi-parameter persistence module. By modeling volumetric images as bounded functions, then we prove the interleaving distance on the persistence modules obtained from our bi-filtrations on the bounded functions is stable with respect to the maximum norm of the bounded functions. Finally, we conduct experiments on the MedMNIST dataset, comparing our bi-filtration against single-parameter filtration and the established deep learning baselines, including Google AutoML Vision, ResNet, AutoKeras and auto-sklearn. Experiments results demonstrate that our bi-filtration significantly outperforms single-parameter filtration. Notably, a simple Multi-Layer Perceptron (MLP) trained on the topological features generated by our bi-filtration achieves performance comparable to complex deep learning models trained on the original dataset.

</details>


### [35] [Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control](https://arxiv.org/abs/2602.18422)
*Linxi Xie,Lisong C. Sun,Ashley Neall,Tong Wu,Shengqu Cai,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 提出了一种基于头部和手部姿态控制的人为中心视频世界模型，用于扩展现实中的交互式虚拟环境生成


<details>
  <summary>Details</summary>
Motivation: 当前视频世界模型仅接受文本或键盘等粗粒度控制信号，无法响应真实世界中的用户运动追踪，限制了在具身交互中的实用性

Method: 1) 评估现有扩散变换器条件策略；2) 提出有效的3D头部和手部控制机制；3) 训练双向视频扩散模型教师，并蒸馏为因果交互系统

Result: 通过人类受试者评估，相比相关基线，该系统在任务表现上有改进，且在感知控制程度上显著更高

Conclusion: 该人中心视频世界模型能够响应追踪的头部和手部姿态，实现灵巧的手-物交互，为扩展现实中的具身交互提供了有效解决方案

Abstract: Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.

</details>


### [36] [SARAH: Spatially Aware Real-time Agentic Humans](https://arxiv.org/abs/2602.18432)
*Evonne Ng,Siwei Zhang,Zhang Chen,Michael Zollhoefer,Alexander Richard*

Main category: cs.CV

TL;DR: 首个实时、完全因果的对话运动空间感知方法，可在VR头显上部署，结合用户位置和音频生成全身运动，实现300+FPS的实时性能


<details>
  <summary>Details</summary>
Motivation: 当前方法缺乏空间感知能力，无法让智能体转向用户、响应用户动作并保持自然注视，限制了VR、远程呈现和数字人应用中智能体的真实感

Method: 结合因果transformer VAE与流匹配模型，使用交错潜在令牌进行流式推理，通过用户轨迹和音频条件化，引入注视评分机制和分类器自由引导实现注视偏好控制

Result: 在Embody 3D数据集上达到最先进的运动质量，性能超过300FPS（比非因果基线快3倍），能捕捉自然对话的微妙空间动态，并在实时VR系统中验证

Conclusion: 提出了首个实时、完全因果的空间感知对话运动方法，实现了高质量、低延迟的智能体运动生成，为实时部署空间感知对话智能体提供了可行方案

Abstract: As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.

</details>


### [37] [Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory](https://arxiv.org/abs/2602.18434)
*Vatsal Agarwal,Saksham Suri,Matthew Gwilliam,Pulkit Kumar,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: MemStream通过增加token预算、自适应选择策略和无训练检索专家混合，提升流式视频理解性能，在多个基准测试中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有流式视频理解方法使用有限的每帧token数，导致细粒度视觉细节丢失；同时这些方法在处理密集视频流时存在查询-帧相似度随时间增加的问题，使检索偏向后期帧。

Method: 1) 扩大token预算以实现更细粒度的时空理解；2) 引入自适应选择策略减少token冗余同时保留局部时空信息；3) 提出无训练检索专家混合，利用外部模型更好地识别相关帧。

Result: 在CG-Bench上提升8.0%，LVBench上提升8.5%，VideoMME (Long)上提升2.4%（相比ReKV with Qwen2.5-VL-7B）。

Conclusion: MemStream通过扩展token预算和创新的自适应选择与检索机制，显著提升了流式视频理解任务的性能，解决了现有方法在处理密集视频流时的局限性。

Abstract: Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [38] [Nested Training for Mutual Adaptation in Human-AI Teaming](https://arxiv.org/abs/2602.17737)
*Upasana Biswas,Durgesh Kalwar,Subbarao Kambhampati,Sarath Sreedharan*

Main category: cs.RO

TL;DR: 该研究提出了一种基于I-POMDP框架的嵌套训练方法，用于解决人机协作中的相互适应问题，避免训练中出现隐式协调策略，提高与未见过的自适应伙伴的协作性能。


<details>
  <summary>Details</summary>
Motivation: 人机协作中的核心挑战是相互适应问题，人类会自然调整策略以响应机器人策略。现有方法通过增加训练伙伴多样性来近似人类行为，但这些伙伴是静态的，无法捕捉人类的自适应行为。当两个智能体在多智能体环境中同时学习时，它们通常会收敛到不透明的隐式协调策略，这些策略只适用于共同训练的伙伴，无法泛化到新伙伴。

Method: 将人机协作场景建模为交互式部分可观测马尔可夫决策过程（I-POMDP），将人类适应行为明确建模为状态的一部分。提出嵌套训练机制来近似学习有限层级I-POMDP的解，每一层级的智能体都针对下一层级的自适应智能体进行训练，确保自我智能体在训练中暴露于自适应行为，同时避免隐式协调策略的出现。

Result: 在Overcooked领域的多回合必需合作设置中训练该方法，并与几个人机协作基线智能体进行比较。评估结果表明，当与训练中未见过的自适应伙伴配对时，该方法不仅在这些自适应伙伴上实现了更高的任务性能，而且在团队交互中表现出显著更强的适应性。

Conclusion: 通过I-POMDP框架和嵌套训练方法，成功解决了人机协作中的相互适应问题，避免了隐式协调策略的出现，使智能体能够更好地泛化到新的自适应伙伴，提高了人机协作的适应性和任务性能。

Abstract: Mutual adaptation is a central challenge in human--AI teaming, as humans naturally adjust their strategies in response to a robot's policy. Existing approaches aim to improve diversity in training partners to approximate human behavior, but these partners are static and fail to capture adaptive behavior of humans. Exposing robots to adaptive behaviors is critical, yet when both agents learn simultaneously in a multi-agent setting, they often converge to opaque implicit coordination strategies that only work with the agents they were co-trained with. Such agents fail to generalize when paired with new partners. In order to capture the adaptive behavior of humans, we model the human-robot teaming scenario as an Interactive Partially Observable Markov Decision Process (I-POMDP), explicitly modeling human adaptation as part of the state. We propose a nested training regime to approximately learn the solution to a finite-level I-POMDP. In this framework, agents at each level are trained against adaptive agents from the level below. This ensures that the ego agent is exposed to adaptive behavior during training while avoiding the emergence of implicit coordination strategies, since the training partners are not themselves learning. We train our method in a multi-episode, required cooperation setup in the Overcooked domain, comparing it against several baseline agents designed for human-robot teaming. We evaluate the performance of our agent when paired with adaptive partners that were not seen during training. Our results demonstrate that our agent not only achieves higher task performance with these adaptive partners but also exhibits significantly greater adaptability during team interactions.

</details>


### [39] [WHED: A Wearable Hand Exoskeleton for Natural, High-Quality Demonstration Collection](https://arxiv.org/abs/2602.17908)
*Mingzhang Zhu,Alvin Zhu,Jose Victor S. H. Ramos,Beom Jun Kim,Yike Shi,Yufeng Wu,Ruochen Hou,Quanyou Wang,Eric Song,Tony Fan,Yuchen Cui,Dennis W. Hong*

Main category: cs.RO

TL;DR: WHED是一个可穿戴手部外骨骼系统，用于在真实环境中采集灵巧操作演示数据，解决了多指手演示数据采集困难的问题。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作的规模化学习受限于高质量人类演示数据的采集困难，主要问题包括遮挡、复杂手部运动学和接触丰富的交互。

Method: 设计WHED可穿戴手部外骨骼系统，采用可穿戴优先设计便于长时间使用，具有姿态容忍的自由移动拇指耦合机制，保持自然拇指行为同时映射到机器人拇指自由度。系统包含连杆驱动的手指接口、被动适配、改进的被动手部本体感知传感以及板载传感/电源模块。

Result: 展示了在代表性抓取和操作序列上的可行性，涵盖精确捏取和全手包裹抓取，并显示采集的演示与重放执行之间的定性一致性。

Conclusion: WHED系统为在真实环境中采集高质量灵巧操作演示数据提供了可行解决方案，支持端到端的数据管道处理。

Abstract: Scalable learning of dexterous manipulation remains bottlenecked by the difficulty of collecting natural, high-fidelity human demonstrations of multi-finger hands due to occlusion, complex hand kinematics, and contact-rich interactions. We present WHED, a wearable hand-exoskeleton system designed for in-the-wild demonstration capture, guided by two principles: wearability-first operation for extended use and a pose-tolerant, free-to-move thumb coupling that preserves natural thumb behaviors while maintaining a consistent mapping to the target robot thumb degrees of freedom. WHED integrates a linkage-driven finger interface with passive fit accommodation, a modified passive hand with robust proprioceptive sensing, and an onboard sensing/power module. We also provide an end-to-end data pipeline that synchronizes joint encoders, AR-based end-effector pose, and wrist-mounted visual observations, and supports post-processing for time alignment and replay. We demonstrate feasibility on representative grasping and manipulation sequences spanning precision pinch and full-hand enclosure grasps, and show qualitative consistency between collected demonstrations and replayed executions.

</details>


### [40] [Latent Diffeomorphic Co-Design of End-Effectors for Deformable and Fragile Object Manipulation](https://arxiv.org/abs/2602.17921)
*Kei Ikemura,Yifei Dong,Florian T. Pokorny*

Main category: cs.RO

TL;DR: 提出首个共同设计框架，联合优化末端执行器形态和操控策略，用于可变形和易碎物体的操作


<details>
  <summary>Details</summary>
Motivation: 可变形和易碎物体的操作是机器人学中的基本挑战，现有方法通常孤立地优化末端执行器设计或控制策略，限制了可实现的性能

Method: 1) 引入潜在微分同胚形状参数化，实现表达性强且可处理的末端执行器几何优化；2) 提出应力感知的双层共同设计流程，耦合形态和控制优化；3) 采用特权到点云的策略蒸馏方案，实现零样本真实世界部署

Result: 在具有挑战性的食物操作任务（包括抓取和推动果冻、舀取鱼片）上进行评估，仿真和真实世界实验证明了所提方法的有效性

Conclusion: 该共同设计框架能够有效处理可变形和易碎物体的操作问题，通过联合优化末端执行器形态和控制策略，实现了更好的操作性能

Abstract: Manipulating deformable and fragile objects remains a fundamental challenge in robotics due to complex contact dynamics and strict requirements on object integrity. Existing approaches typically optimize either end-effector design or control strategies in isolation, limiting achievable performance. In this work, we present the first co-design framework that jointly optimizes end-effector morphology and manipulation control for deformable and fragile object manipulation. We introduce (1) a latent diffeomorphic shape parameterization enabling expressive yet tractable end-effector geometry optimization, (2) a stress-aware bi-level co-design pipeline coupling morphology and control optimization, and (3) a privileged-to-pointcloud policy distillation scheme for zero-shot real-world deployment. We evaluate our approach on challenging food manipulation tasks, including grasping and pushing jelly and scooping fillets. Simulation and real-world experiments demonstrate the effectiveness of the proposed method.

</details>


### [41] [Homotopic information gain for sparse active target tracking](https://arxiv.org/abs/2602.17926)
*Jennifer Wakulicz,Ki Myung Brian Lee,Teresa Vidal-Calleja,Robert Fitch*

Main category: cs.RO

TL;DR: 论文提出了一种用于主动目标跟踪的新规划方法，通过最大化目标同伦类信息而非传统度量信息，在减少观测次数的同时获得更准确的目标轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 在多模态运动模型中，传统的信息增益概念定义不明确，难以有效规划移动机器人的感知轨迹来预测目标未来轨迹。

Method: 提出同伦信息增益的概念，作为测量给定观测所能提供的高层轨迹信息的度量。该方法规划感知轨迹以最大化同伦信息，而非传统的度量信息。

Result: 同伦信息增益是度量信息增益的下界，在环境中分布稀疏如障碍物。相比度量信息方法，该方法能用更少的测量获得更准确的轨迹估计。

Conclusion: 通过关注目标的高层运动（同伦类）而非低层度量信息，提出的规划方法在主动目标跟踪中更高效，在真实和模拟行人数据上验证了其优越性。

Abstract: The problem of planning sensing trajectories for a mobile robot to collect observations of a target and predict its future trajectory is known as active target tracking. Enabled by probabilistic motion models, one may solve this problem by exploring the belief space of all trajectory predictions given future sensing actions to maximise information gain. However, for multi-modal motion models the notion of information gain is often ill-defined. This paper proposes a planning approach designed around maximising information regarding the target's homotopy class, or high-level motion. We introduce homotopic information gain, a measure of the expected high-level trajectory information given by a measurement. We show that homotopic information gain is a lower bound for metric or low-level information gain, and is as sparsely distributed in the environment as obstacles are. Planning sensing trajectories to maximise homotopic information results in highly accurate trajectory estimates with fewer measurements than a metric information approach, as supported by our empirical evaluation on real and simulated pedestrian data.

</details>


### [42] [Quasi-Periodic Gaussian Process Predictive Iterative Learning Control](https://arxiv.org/abs/2602.18014)
*Unnati Nigam,Radhendushka Srivastava,Faezeh Marzbanrad,Michael Burke*

Main category: cs.RO

TL;DR: 该研究将准周期高斯过程（QPGP）融入预测性迭代学习控制（ILC）框架，用于建模和预测重复运动任务中的扰动和漂移，显著降低计算复杂度并实现更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 机器人重复运动任务中，环境变化和设备磨损会导致性能随时间下降。传统迭代学习控制（ILC）虽然能利用历史信息补偿误差，但在处理时变扰动和计算效率方面存在局限，需要更高效的预测性控制方法。

Method: 采用准周期高斯过程（QPGP）的近期结构方程公式，将其整合到预测性ILC框架中。该方法通过QPGP建模迭代间的扰动和漂移，实现复杂度为𝒪(p³)的高效推理（而非传统方法的𝒪(i²p³)），支持参数估计而不损失信息，使持续高斯过程学习在控制回路中计算可行。

Result: 在三个任务（自动驾驶车辆轨迹跟踪、三连杆机械臂、真实Stretch机器人实验）中，该方法相比标准ILC和传统GP预测性ILC收敛更快，在注入和自然扰动下保持鲁棒性，同时降低计算成本。

Conclusion: 提出的QPGP预测性ILC方法在多种重复动态系统中具有实际应用价值，能够实现更快的收敛速度、保持鲁棒性并降低计算复杂度，特别适合处理时变扰动和长期重复任务。

Abstract: Repetitive motion tasks are common in robotics, but performance can degrade over time due to environmental changes and robot wear and tear. Iterative learning control (ILC) improves performance by using information from previous iterations to compensate for expected errors in future iterations. This work incorporates the use of Quasi-Periodic Gaussian Processes (QPGPs) into a predictive ILC framework to model and forecast disturbances and drift across iterations. Using a recent structural equation formulation of QPGPs, the proposed approach enables efficient inference with complexity $\mathcal{O}(p^3)$ instead of $\mathcal{O}(i^2p^3)$, where $p$ denotes the number of points within an iteration and $i$ represents the total number of iterations, specially for larger $i$. This formulation also enables parameter estimation without loss of information, making continual GP learning computationally feasible within the control loop. By predicting next-iteration error profiles rather than relying only on past errors, the controller achieves faster convergence and maintains this under time-varying disturbances. We benchmark the method against both standard ILC and conventional Gaussian Process (GP)-based predictive ILC on three tasks, autonomous vehicle trajectory tracking, a three-link robotic manipulator, and a real-world Stretch robot experiment. Across all cases, the proposed approach converges faster and remains robust under injected and natural disturbances while reducing computational cost. This highlights its practicality across a range of repetitive dynamical systems.

</details>


### [43] [EgoPush: Learning End-to-End Egocentric Multi-Object Rearrangement for Mobile Robots](https://arxiv.org/abs/2602.18071)
*Boyuan An,Zhexiong Wang,Yipeng Wang,Jiaqi Li,Sihang Li,Jing Zhang,Chen Feng*

Main category: cs.RO

TL;DR: EgoPush是一个用于移动机器人多物体非抓取重排的端到端学习框架，使用单目自中心相机，通过特权强化学习教师和视觉学生策略的蒸馏实现，无需全局状态估计。


<details>
  <summary>Details</summary>
Motivation: 受人类在杂乱环境中通过自中心感知重排物体的能力启发，研究移动机器人的长时程多物体非抓取重排问题，解决传统方法依赖全局状态估计在动态场景中容易失效的问题。

Method: 设计物体中心的潜在空间编码物体间相对空间关系而非绝对位姿；使用特权强化学习教师从稀疏关键点联合学习潜在状态和移动动作，然后蒸馏到纯视觉学生策略；限制教师观察为视觉可访问线索以减少监督差距；使用时间衰减的阶段局部完成奖励分解长时程任务。

Result: 大量仿真实验表明EgoPush在成功率上显著优于端到端强化学习基线，消融研究验证了每个设计选择的有效性；进一步在真实世界移动平台上展示了零样本仿真到真实的迁移能力。

Conclusion: EgoPush框架成功实现了基于自中心感知的移动机器人多物体重排，无需全局状态估计，通过特权学习蒸馏和主动感知设计解决了长时程任务中的部分观测和信用分配问题。

Abstract: Humans can rearrange objects in cluttered environments using egocentric perception, navigating occlusions without global coordinates. Inspired by this capability, we study long-horizon multi-object non-prehensile rearrangement for mobile robots using a single egocentric camera. We introduce EgoPush, a policy learning framework that enables egocentric, perception-driven rearrangement without relying on explicit global state estimation that often fails in dynamic scenes. EgoPush designs an object-centric latent space to encode relative spatial relations among objects, rather than absolute poses. This design enables a privileged reinforcement-learning (RL) teacher to jointly learn latent states and mobile actions from sparse keypoints, which is then distilled into a purely visual student policy. To reduce the supervision gap between the omniscient teacher and the partially observed student, we restrict the teacher's observations to visually accessible cues. This induces active perception behaviors that are recoverable from the student's viewpoint. To address long-horizon credit assignment, we decompose rearrangement into stage-level subproblems using temporally decayed, stage-local completion rewards. Extensive simulation experiments demonstrate that EgoPush significantly outperforms end-to-end RL baselines in success rate, with ablation studies validating each design choice. We further demonstrate zero-shot sim-to-real transfer on a mobile platform in the real world. Code and videos are available at https://ai4ce.github.io/EgoPush/.

</details>


### [44] [Interacting safely with cyclists using Hamilton-Jacobi reachability and reinforcement learning](https://arxiv.org/abs/2602.18097)
*Aarati Andrea Noronha,Jean Oh*

Main category: cs.RO

TL;DR: 提出一个结合Hamilton-Jacobi可达性分析和深度Q学习的框架，使自动驾驶车辆能够与骑行者安全高效交互，平衡安全性和最优性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需要与骑行者等弱势道路使用者安全交互，同时保持行驶效率。现有方法难以在提供安全保证的同时实现时间最优导航。

Method: 1) 使用Hamilton-Jacobi可达性分析计算安全度量值函数；2) 将该安全度量作为结构化奖励信号集成到强化学习框架；3) 建模骑行者对车辆的潜在响应，将人类舒适度和行为适应作为扰动输入。

Result: 通过仿真评估，并与人类驾驶行为和现有最先进方法进行比较，验证了所提框架的有效性。

Conclusion: 提出的框架成功平衡了自动驾驶车辆与骑行者交互的安全性和最优性，为安全关键场景下的自主导航提供了新思路。

Abstract: In this paper, we present a framework for enabling autonomous vehicles to interact with cyclists in a manner that balances safety and optimality. The approach integrates Hamilton-Jacobi reachability analysis with deep Q-learning to jointly address safety guarantees and time-efficient navigation. A value function is computed as the solution to a time-dependent Hamilton-Jacobi-Bellman inequality, providing a quantitative measure of safety for each system state. This safety metric is incorporated as a structured reward signal within a reinforcement learning framework. The method further models the cyclist's latent response to the vehicle, allowing disturbance inputs to reflect human comfort and behavioral adaptation. The proposed framework is evaluated through simulation and comparison with human driving behavior and an existing state-of-the-art method.

</details>


### [45] [GrandTour: A Legged Robotics Dataset in the Wild for Multi-Modal Perception and State Estimation](https://arxiv.org/abs/2602.18164)
*Jonas Frey,Turcan Tuna,Frank Fu,Katharine Patterson,Tianao Xu,Maurice Fallon,Cesar Cadena,Marco Hutter*

Main category: cs.RO

TL;DR: GrandTour是一个大规模多模态四足机器人数据集，包含各种室内外环境下的传感器数据和高精度地面真值轨迹，支持SLAM、状态估计和多模态学习研究。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏大规模公开的四足机器人数据集来开发和评估复杂环境下的状态估计、感知和导航算法，特别是在真实世界条件下。

Method: 使用ANYmal-D四足机器人配备多模态传感器载荷，在多种挑战性环境（高山、森林、拆除建筑、城市等）中收集数据，提供时间同步的激光雷达、RGB相机、本体感受传感器、立体深度相机数据，以及RTK-GNSS和全站仪的高精度地面真值轨迹。

Result: 创建了迄今为止最大的开源四足机器人数据集GrandTour，涵盖广泛的环境类型、规模、复杂性、光照和天气条件，支持SLAM、高精度状态估计和多模态学习研究。

Conclusion: GrandTour数据集填补了四足机器人研究领域大规模多模态数据集的空白，为传感器融合算法的严格评估和新方法开发提供了重要资源，数据集已在多个平台公开提供。

Abstract: Accurate state estimation and multi-modal perception are prerequisites for autonomous legged robots in complex, large-scale environments. To date, no large-scale public legged-robot dataset captures the real-world conditions needed to develop and benchmark algorithms for legged-robot state estimation, perception, and navigation. To address this, we introduce the GrandTour dataset, a multi-modal legged-robotics dataset collected across challenging outdoor and indoor environments, featuring an ANYbotics ANYmal-D quadruped equipped with the \boxi multi-modal sensor payload. GrandTour spans a broad range of environments and operational scenarios across distinct test sites, ranging from alpine scenery and forests to demolished buildings and urban areas, and covers a wide variation in scale, complexity, illumination, and weather conditions. The dataset provides time-synchronized sensor data from spinning LiDARs, multiple RGB cameras with complementary characteristics, proprioceptive sensors, and stereo depth cameras. Moreover, it includes high-precision ground-truth trajectories from satellite-based RTK-GNSS and a Leica Geosystems total station. This dataset supports research in SLAM, high-precision state estimation, and multi-modal learning, enabling rigorous evaluation and development of new approaches to sensor fusion in legged robotic systems. With its extensive scope, GrandTour represents the largest open-access legged-robotics dataset to date. The dataset is available at https://grand-tour.leggedrobotics.com, on HuggingFace (ROS-independent), and in ROS formats, along with tools and demo resources.

</details>


### [46] [Have We Mastered Scale in Deep Monocular Visual SLAM? The ScaleMaster Dataset and Benchmark](https://arxiv.org/abs/2602.18174)
*Hyoseok Ju,Bokeon Suh,Giseop Kim*

Main category: cs.RO

TL;DR: ScaleMaster数据集是首个专门评估大规模室内环境中尺度一致性的基准，揭示了现有深度单目视觉SLAM系统在真实场景中的严重尺度漂移问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度单目视觉SLAM系统在尺度一致性方面缺乏充分评估，特别是在大规模室内环境中存在尺度漂移和尺度模糊问题，而现有基准仅限于房间尺度或简单结构场景。

Method: 提出了ScaleMaster数据集，专门设计用于评估多楼层结构、长轨迹、重复视图和低纹理区域等挑战性场景下的尺度一致性。采用系统分析方法，不仅评估轨迹指标，还使用Chamfer距离等指标进行直接地图到地图的质量评估。

Result: 研究发现，尽管现有深度单目视觉SLAM系统在传统基准上表现良好，但在真实大规模室内环境中存在严重的尺度相关故障。通过定量和定性评估揭示了系统的脆弱性。

Conclusion: ScaleMaster数据集和基线结果的发布为未来开发尺度一致且可靠的视觉SLAM系统研究奠定了基础，强调了解决大规模室内环境中尺度一致性问题的重要性。

Abstract: Recent advances in deep monocular visual Simultaneous Localization and Mapping (SLAM) have achieved impressive accuracy and dense reconstruction capabilities, yet their robustness to scale inconsistency in large-scale indoor environments remains largely unexplored. Existing benchmarks are limited to room-scale or structurally simple settings, leaving critical issues of intra-session scale drift and inter-session scale ambiguity insufficiently addressed. To fill this gap, we introduce the ScaleMaster Dataset, the first benchmark explicitly designed to evaluate scale consistency under challenging scenarios such as multi-floor structures, long trajectories, repetitive views, and low-texture regions. We systematically analyze the vulnerability of state-of-the-art deep monocular visual SLAM systems to scale inconsistency, providing both quantitative and qualitative evaluations. Crucially, our analysis extends beyond traditional trajectory metrics to include a direct map-to-map quality assessment using metrics like Chamfer distance against high-fidelity 3D ground truth. Our results reveal that while recent deep monocular visual SLAM systems demonstrate strong performance on existing benchmarks, they suffer from severe scale-related failures in realistic, large-scale indoor environments. By releasing the ScaleMaster dataset and baseline results, we aim to establish a foundation for future research toward developing scale-consistent and reliable visual SLAM systems.

</details>


### [47] [Design and Characterization of a Dual-DOF Soft Shoulder Exosuit with Volume-Optimized Pneumatic Actuator](https://arxiv.org/abs/2602.18212)
*Rui Chen,Domenico Chiaradia,Daniele Leonardis,Antonio Frisoli*

Main category: cs.RO

TL;DR: 该研究开发了一种便携式气动肩部外骨骼，通过优化执行器几何形状解决了扭矩输出与动态响应之间的权衡问题，实现了多模态肩部辅助功能。


<details>
  <summary>Details</summary>
Motivation: 便携式气动系统在2自由度软肩外骨骼中仍未被充分探索，面临着扭矩输出与动态响应之间的基本权衡问题，且需要多个执行器来支持复杂的肩部运动。

Method: 采用体积优化的纺锤形角度执行器几何设计，减少35.7%的体积；基于此开发弯曲外展执行器和基于囊袋电机原理的水平内收执行器，集成到双自由度纺织基肩部外骨骼中。

Result: SSAA执行器在减少35.7%体积的同时保持了94.2%的输出扭矩，动态响应速度提高了35.2%。用户研究表明，外骨骼在肩部外展和屈曲任务中显著降低了肌电活动，最高减少63.7%。

Conclusion: 该研究通过执行器几何优化解决了便携式气动肩部外骨骼的关键设计挑战，为多自由度外骨骼系统提供了设计指导，但在健康用户中增加第二个执行器的增量效益有限。

Abstract: Portable pneumatic systems for 2 degree-of-freedom (DOF) soft shoulder exosuits remain underexplored, and face fundamental trade-offs between torque output and dynamic response that are further compounded by the need for multiple actuators to support complex shoulder movement. This work addresses these constraints through a volume-optimized spindle-shaped angled actuator (SSAA) geometry: by reducing actuator volume by 35.7% (357mL vs. 555mL), the SSAA maintains 94.2% of output torque while achieving 35.2% faster dynamic response compared to uniform cylindrical designs. Building on the SSAA, we develop a curved abduction actuator (CAA) based on the SSAA geometry and a horizontal adduction actuator (HAA) based on the pouch motor principle, integrating both into a dual-DOF textile-based shoulder exosuit (390 g). The exosuit delivers multi-modal assistance spanning shoulder abduction, flexion, and horizontal adduction, depending on the actuation.
  User studies with 10 healthy participants reveal that the exosuit substantially reduces electromyographic (EMG) activity across both shoulder abduction and flexion tasks. For abduction with HAA only, the exosuit achieved up to 59% muscle activity reduction across seven muscles. For flexion, both the single-actuator configuration (HAA only) and the dual-actuator configuration (HAA,+,CAA) reduced EMG activity by up to 63.7% compared to no assistance. However, the incremental benefit of adding the CAA to existing HAA support was limited in healthy users during flexion, with statistically significant additional reductions observed only in pectoralis major. These experimental findings characterize actuator contributions in healthy users and provide design guidance for multi-DOF exosuit systems.

</details>


### [48] [RoEL: Robust Event-based 3D Line Reconstruction](https://arxiv.org/abs/2602.18258)
*Gwangtak Bae,Jaeho Shin,Seunggu Kang,Junho Kim,Ayoung Kim,Young Min Kim*

Main category: cs.RO

TL;DR: 提出一种基于事件相机的鲁棒线特征提取与跟踪方法，通过多时间切片观测补偿事件数据中的噪声，利用几何代价函数优化3D线地图和相机位姿，适用于多模态场景。


<details>
  <summary>Details</summary>
Motivation: 事件相机在运动中主要检测物体边界或纹理边缘，产生亮度变化线，但线特征的稀疏性可能导致微小估计误差下的性能急剧下降。现有方法多依赖额外传感器，难以应对事件传感器的严重域差异和不可预测噪声特性。

Method: 1) 通过观察事件数据多个时间切片的多重表示，设计巧妙的算法过程稳定提取各种外观的线轨迹；2) 提出几何代价函数消除投影畸变和深度模糊，优化3D线地图和相机位姿；3) 构建高度紧凑的3D线地图，代价函数可适配任何能检测提取线结构或其投影的观测数据。

Result: 在多个数据集上验证了该方法在基于事件的建图和位姿细化方面带来显著性能提升，可灵活应用于多模态场景，证明线基公式是事件感知模块实际部署的鲁棒有效方法。

Conclusion: 提出的基于线特征的公式为事件相机提供了鲁棒的中间表示，通过多时间观测补偿事件数据噪声，几何优化消除投影问题，紧凑的3D线地图和通用代价函数使其成为事件感知系统实际部署的有效解决方案。

Abstract: Event cameras in motion tend to detect object boundaries or texture edges, which produce lines of brightness changes, especially in man-made environments. While lines can constitute a robust intermediate representation that is consistently observed, the sparse nature of lines may lead to drastic deterioration with minor estimation errors. Only a few previous works, often accompanied by additional sensors, utilize lines to compensate for the severe domain discrepancies of event sensors along with unpredictable noise characteristics. We propose a method that can stably extract tracks of varying appearances of lines using a clever algorithmic process that observes multiple representations from various time slices of events, compensating for potential adversaries within the event data. We then propose geometric cost functions that can refine the 3D line maps and camera poses, eliminating projective distortions and depth ambiguities. The 3D line maps are highly compact and can be equipped with our proposed cost function, which can be adapted for any observations that can detect and extract line structures or projections of them, including 3D point cloud maps or image observations. We demonstrate that our formulation is powerful enough to exhibit a significant performance boost in event-based mapping and pose refinement across diverse datasets, and can be flexibly applied to multimodal scenarios. Our results confirm that the proposed line-based formulation is a robust and effective approach for the practical deployment of event-based perceptual modules. Project page: https://gwangtak.github.io/roel/

</details>


### [49] [Role-Adaptive Collaborative Formation Planning for Team of Quadruped Robots in Cluttered Environments](https://arxiv.org/abs/2602.18260)
*Magnus Norén,Marios-Nektarios Stamatopoulos,Avijit Banerjee,George Nikolakopoulos*

Main category: cs.RO

TL;DR: 提出了一种基于角色自适应领导者-跟随者的四足机器人编队规划与控制框架，能够在复杂环境中实现灵活、无碰撞的导航。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用固定领导者或刚性编队角色，在复杂环境中缺乏灵活性，难以适应动态障碍物和复杂场景的需求。

Method: 集成动态角色分配和部分目标规划，采用虚拟弹簧阻尼系统确保编队稳定性，结合新型避障层自适应调整机器人速度，使用Fast Marching Square算法进行全局和局部路径规划。

Result: 通过仿真和真实世界实验验证，展示了平滑协调、自适应角色切换以及在复杂非结构化环境中稳健的编队维护能力。

Conclusion: 该框架为四足机器人团队在复杂环境中的编队导航提供了一种有效解决方案，实现了灵活、安全的协调运动。

Abstract: This paper presents a role-adaptive Leader-Follower-based formation planning and control framework for teams of quadruped robots operating in cluttered environments. Unlike conventional methods with fixed leaders or rigid formation roles, the proposed approach integrates dynamic role assignment and partial goal planning, enabling flexible, collision-free navigation in complex scenarios. Formation stability and inter-robot safety are ensured through a virtual spring-damper system coupled with a novel obstacle avoidance layer that adaptively adjusts each agent's velocity. A dynamic look-ahead reference generator further enhances flexibility, allowing temporary formation deformation to maneuver around obstacles while maintaining goal-directed motion. The Fast Marching Square (FM2) algorithm provides the global path for the leader and local paths for the followers as the planning backbone. The framework is validated through extensive simulations and real-world experiments with teams of quadruped robots. Results demonstrate smooth coordination, adaptive role switching, and robust formation maintenance in complex, unstructured environments. A video featuring the simulation and physical experiments along with their associated visualizations can be found at https://youtu.be/scq37Tua9W4.

</details>


### [50] [Tendon-Driven Reciprocating and Non-Reciprocating Motion via Snapping Metabeams](https://arxiv.org/abs/2602.18330)
*Mohsen Jafarpour,Ayberk Yüksek,Shahab Eshghi,Stanislav Gorb,Edoardo Milana*

Main category: cs.RO

TL;DR: 研究开发了一种基于螺旋结构肌腱驱动的可调谐拍打梁机制，通过边界条件调节实现可逆和非可逆运动，并成功应用于游泳机器人推进。


<details>
  <summary>Details</summary>
Motivation: 利用非线性失稳的拍打梁实现软体机器人快速几何转变，开发可调谐的肌腱驱动机制来产生高效运动。

Method: 采用螺旋结构设计代谢梁，使用PLA材料通过熔融沉积成型制造，通过调整边界条件调节机械特性，并集成到游泳机器人中测试推进性能。

Result: 边界约束可单独调节临界力和稳定性；螺旋几何允许大范围可逆变形；游泳机器人实现两种驱动模式，非可逆运动达到81mm/s推进速度（0.4体长/秒）。

Conclusion: 几何驱动的拍打结构在软体机器人系统中具有高效可编程驱动的潜力，为可控拍打行为提供了直接的设计概念。

Abstract: Snapping beams enable rapid geometric transitions through nonlinear instability, offering an efficient means of generating motion in soft robotic systems. In this study, a tendon-driven mechanism consisting of spiral-based metabeams was developed to exploit this principle for producing both reciprocating and non-reciprocating motion. The snapping structures were fabricated using fused deposition modeling with polylactic acid (PLA) and experimentally tested under different boundary conditions to analyze their nonlinear behavior. The results show that the mechanical characteristics, including critical forces and stability, can be tuned solely by adjusting the boundary constraints. The spiral geometry allows large reversible deformation even when made from a relatively stiff material such as PLA, providing a straightforward design concept for controllable snapping behavior. The developed mechanism was further integrated into a swimming robot, where tendon-driven fins exhibited two distinct actuation modes: reciprocating and non-reciprocating motion. The latter enabled efficient propulsion, producing a forward displacement of about 32 mm per 0.4 s cycle ($\approx$ 81 mm/s, equivalent to 0.4 body lengths per second). This study highlights the potential of geometry-driven snapping structures for efficient and programmable actuation in soft robotic systems.

</details>


### [51] [Downwash-aware Configuration Optimization for Modular Aerial Systems](https://arxiv.org/abs/2602.18344)
*Mengguang Li,Heinz Koeppl*

Main category: cs.RO

TL;DR: 提出一个框架，为大规模同构模块化空中系统生成并优化选择任务特定的装配配置，明确限制模块间的下洗气流。


<details>
  <summary>Details</summary>
Motivation: 先前工作主要关注平面布局且常忽略空气动力学干扰，需要解决模块化空中系统的装配配置优化问题，考虑下洗气流约束。

Method: 首先大规模枚举非同构连接拓扑；其次求解非线性规划问题检查可行性，并选择在驱动限制和下洗约束下最小化控制输入的配置。

Result: 在基于物理的仿真中评估框架，并在真实世界实验中展示其有效性。

Conclusion: 该框架能够为大规模模块化空中系统生成满足空气动力学约束的优化装配配置，在仿真和实验中均验证了其可行性。

Abstract: This work proposes a framework that generates and optimally selects task-specific assembly configurations for a large group of homogeneous modular aerial systems, explicitly enforcing bounds on inter-module downwash. Prior work largely focuses on planar layouts and often ignores aerodynamic interference. In contrast, firstly we enumerate non-isomorphic connection topologies at scale; secondly, we solve a nonlinear program to check feasibility and select the configuration that minimizes control input subject to actuation limits and downwash constraints. We evaluate the framework in physics-based simulation and demonstrate it in real-world experiments.

</details>


### [52] [Zero-shot Interactive Perception](https://arxiv.org/abs/2602.18374)
*Venkatesh Sripada,Frank Guerin,Amir Ghalamzan*

Main category: cs.RO

TL;DR: ZS-IP框架结合多策略操作（推和抓）与记忆驱动的视觉语言模型，通过物理交互解决复杂场景中的语义查询问题


<details>
  <summary>Details</summary>
Motivation: 交互感知对于机器人在复杂、部分可观察场景中提取隐藏信息、解决遮挡和模糊性至关重要，现有方法在接触丰富的推动作方面存在局限

Method: 提出ZS-IP框架，包含三个关键组件：1）增强观察模块（使用关键点和pushlines视觉增强），2）记忆引导动作模块（通过上下文查找强化语义推理），3）基于VLM输出的机器人控制器（执行推、拉、抓动作）

Result: 在7-DOF Franka Panda机械臂上的实验表明，ZS-IP在推动作任务中优于被动和基于视点的感知技术（如MOKA），同时保持非目标元素的完整性

Conclusion: ZS-IP通过结合pushlines视觉增强和记忆驱动的VLM，显著提升了机器人交互感知能力，特别是在推动作任务中表现出色

Abstract: Interactive perception (IP) enables robots to extract hidden information in their workspace and execute manipulation plans by physically interacting with objects and altering the state of the environment -- crucial for resolving occlusions and ambiguity in complex, partially observable scenarios. We present Zero-Shot IP (ZS-IP), a novel framework that couples multi-strategy manipulation (pushing and grasping) with a memory-driven Vision Language Model (VLM) to guide robotic interactions and resolve semantic queries. ZS-IP integrates three key components: (1) an Enhanced Observation (EO) module that augments the VLM's visual perception with both conventional keypoints and our proposed pushlines -- a novel 2D visual augmentation tailored to pushing actions, (2) a memory-guided action module that reinforces semantic reasoning through context lookup, and (3) a robotic controller that executes pushing, pulling, or grasping based on VLM output. Unlike grid-based augmentations optimized for pick-and-place, pushlines capture affordances for contact-rich actions, substantially improving pushing performance. We evaluate ZS-IP on a 7-DOF Franka Panda arm across diverse scenes with varying occlusions and task complexities. Our experiments demonstrate that ZS-IP outperforms passive and viewpoint-based perception techniques such as Mark-Based Visual Prompting (MOKA), particularly in pushing tasks, while preserving the integrity of non-target elements.

</details>


### [53] [Ori-Sense: origami capacitive sensing for soft robotic applications](https://arxiv.org/abs/2602.18379)
*Hugo de Souza Oliveira,Xin Li,Mohsen Jafarpour,Edoardo Milana*

Main category: cs.RO

TL;DR: Ori-Sense是一种基于倒置Kresling折纸结构的柔性电容传感器，可将扭转变形转化为可测量的电容变化，为软体机器人提供本体感知反馈。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够将机械变形直接转化为电信号的集成式柔性传感器，为软体机器人系统提供可靠的本体感知能力，解决传统刚性传感器与软体系统兼容性差的问题。

Method: 采用可溶解芯模铸造技术制造单块硅胶结构，嵌入导电TPU电极形成集成软电容器；通过机械测试评估刚度特性，有限元模拟验证应力分布，并进行电气测试测量电容变化。

Result: 传感器表现出低刚度和最小阻抗，扭矩值在轴向位移±15mm时低于0.01N·mm，压缩状态下30度扭转时达到0.03N·mm；电容调制可达30%，与扭转角度直接相关，在5mm轴向变形时最大灵敏度为0.0067pF/度。

Conclusion: Ori-Sense成功实现了将扭转变形转化为电容变化的传感机制，为软体机器人提供了有效的本体感知解决方案，其集成制造方法和可靠的性能表现展示了在软体机器人系统中的应用潜力。

Abstract: This work introduces Ori-Sense, a compliant capacitive sensor inspired by the inverted Kresling origami pattern. The device translates torsional deformation into measurable capacitance changes, enabling proprioceptive feedback for soft robotic systems. Using dissolvable-core molding, we fabricated a monolithic silicone structure with embedded conductive TPU electrodes, forming an integrated soft capacitor. Mechanical characterization revealed low stiffness and minimal impedance, with torque values below 0.01 N mm for axial displacements between -15 mm and 15 mm, and up to 0.03 N mm at 30 degrees twist under compression. Finite-element simulations confirmed localized stresses along fold lines and validated the measured torque-rotation response. Electrical tests showed consistent capacitance modulation up to 30%, directly correlated with the twist angle, and maximal sensitivity of S_theta ~ 0.0067 pF/deg at 5 mm of axial deformation.

</details>


### [54] [Learning to Tune Pure Pursuit in Autonomous Racing: Joint Lookahead and Steering-Gain Control with PPO](https://arxiv.org/abs/2602.18386)
*Mohamed Elgouhary,Amr S. El-Wakeel*

Main category: cs.RO

TL;DR: 使用强化学习在线联合调整纯追踪算法的前瞻距离和转向增益，提升自动驾驶赛车路径跟踪性能


<details>
  <summary>Details</summary>
Motivation: 纯追踪算法在自动驾驶赛车中广泛使用，但其性能高度依赖于前瞻距离和转向增益参数的选择。传统的基于速度的调度方法调整不够精确，且在不同赛道和速度配置下难以迁移。

Method: 提出强化学习方法，使用PPO算法在线联合选择前瞻距离Ld和转向增益g。策略观察紧凑的状态特征（速度和曲率信息），在每个控制步骤输出(Ld, g)参数。在F1TENTH Gym中训练，部署在ROS 2栈中，直接驱动纯追踪算法（带轻微平滑）。

Result: 在仿真和真实车辆测试中，提出的RL-PP控制器（联合选择Ld和g）在圈速、路径跟踪精度和转向平滑度方面一致优于固定前瞻距离PP、速度调度自适应PP、仅前瞻距离RL变体，并且在评估设置下超过了运动学MPC参考线跟踪器。

Conclusion: 策略引导的参数调优能够可靠地改进基于几何的经典控制方法，证明了强化学习在优化传统控制算法参数方面的有效性。

Abstract: Pure Pursuit (PP) is widely used in autonomous racing for real-time path tracking due to its efficiency and geometric clarity, yet performance is highly sensitive to how key parameters-lookahead distance and steering gain-are chosen. Standard velocity-based schedules adjust these only approximately and often fail to transfer across tracks and speed profiles. We propose a reinforcement-learning (RL) approach that jointly chooses the lookahead Ld and a steering gain g online using Proximal Policy Optimization (PPO). The policy observes compact state features (speed and curvature taps) and outputs (Ld, g) at each control step. Trained in F1TENTH Gym and deployed in a ROS 2 stack, the policy drives PP directly (with light smoothing) and requires no per-map retuning. Across simulation and real-car tests, the proposed RL-PP controller that jointly selects (Ld, g) consistently outperforms fixed-lookahead PP, velocity-scheduled adaptive PP, and an RL lookahead-only variant, and it also exceeds a kinematic MPC raceline tracker under our evaluated settings in lap time, path-tracking accuracy, and steering smoothness, demonstrating that policy-guided parameter tuning can reliably improve classical geometry-based control.

</details>
