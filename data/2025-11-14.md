<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 80]
- [cs.RO](#cs.RO) [Total: 16]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [FedeCouple: Fine-Grained Balancing of Global-Generalization and Local-Adaptability in Federated Learning](https://arxiv.org/abs/2511.09599)
*Ming Yang,Dongrun Li,Xin Wang,Feng Li,Lisheng Fan,Chunxiao Wang,Xiaoming Wu,Peng Cheng*

Main category: cs.CV

TL;DR: FedeCouple是一种联邦学习方法，通过细粒度平衡全局泛化与局部适应性，联合学习全局和局部特征表示，使用动态知识蒸馏增强个性化分类器的泛化能力，并通过锚点优化特征空间，在隐私保护和通信效率方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决现有个性化联邦学习方法在特征提取器局部适应性和分类器全局泛化方面的不足，提升组件间的协调性和耦合度，从而提高整体模型性能。

Method: 联合学习全局和局部特征表示；采用动态知识蒸馏增强分类器泛化；引入锚点优化特征空间（具有严格局部性和非传输特性）；提供非凸目标收敛的理论分析。

Result: 在五个图像分类数据集上的实验表明，FedeCouple在有效性、稳定性、可扩展性和安全性方面均优于九种基线方法，有效性实验中比最佳基线方法高出4.3%。

Conclusion: FedeCouple通过平衡全局泛化与局部适应性，有效解决了联邦学习中组件协调不足的问题，在保持隐私和降低通信开销的同时显著提升了模型性能。

Abstract: In privacy-preserving mobile network transmission scenarios with heterogeneous client data, personalized federated learning methods that decouple feature extractors and classifiers have demonstrated notable advantages in enhancing learning capability. However, many existing approaches primarily focus on feature space consistency and classification personalization during local training, often neglecting the local adaptability of the extractor and the global generalization of the classifier. This oversight results in insufficient coordination and weak coupling between the components, ultimately degrading the overall model performance. To address this challenge, we propose FedeCouple, a federated learning method that balances global generalization and local adaptability at a fine-grained level. Our approach jointly learns global and local feature representations while employing dynamic knowledge distillation to enhance the generalization of personalized classifiers. We further introduce anchors to refine the feature space; their strict locality and non-transmission inherently preserve privacy and reduce communication overhead. Furthermore, we provide a theoretical analysis proving that FedeCouple converges for nonconvex objectives, with iterates approaching a stationary point as the number of communication rounds increases. Extensive experiments conducted on five image-classification datasets demonstrate that FedeCouple consistently outperforms nine baseline methods in effectiveness, stability, scalability, and security. Notably, in experiments evaluating effectiveness, FedeCouple surpasses the best baseline by a significant margin of 4.3%.

</details>


### [2] [MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation](https://arxiv.org/abs/2511.09611)
*Ye Tian,Ling Yang,Jiongfan Yang,Anran Wang,Yu Tian,Jiani Zheng,Haochen Wang,Zhiyang Teng,Zhuochen Wang,Yinjie Wang,Yunhai Tong,Mengdi Wang,Xiangtai Li*

Main category: cs.CV

TL;DR: 本文提出ParaBench基准测试，揭示现有顺序自回归方法在思维感知生成中因错误传播导致性能下降的问题，并提出并行多模态扩散框架MMaDA-Parallel，通过并行强化学习优化跨模态一致性，在ParaBench上比最先进模型Bagel提升6.9%的输出对齐度。


<details>
  <summary>Details</summary>
Motivation: 现有顺序自回归方法在复杂任务的思维感知生成中存在性能下降问题，主要原因是错误传播和生成推理与最终图像之间的对齐不佳。

Method: 提出并行多模态扩散框架MMaDA-Parallel，支持文本和图像在整个去噪轨迹中的连续双向交互；使用监督微调训练，并通过新颖的并行强化学习策略在轨迹上应用语义奖励来强制跨模态一致性。

Result: 实验验证模型显著改善了跨模态对齐和语义一致性，在ParaBench基准测试中比最先进模型Bagel在输出对齐度上提升了6.9%。

Conclusion: MMaDA-Parallel为思维感知图像合成建立了更鲁棒的范式，通过并行交互和强化学习优化解决了现有方法的性能退化问题。

Abstract: While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9\% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at https://github.com/tyfeld/MMaDA-Parallel

</details>


### [3] [Classifying Phonotrauma Severity from Vocal Fold Images with Soft Ordinal Regression](https://arxiv.org/abs/2511.09702)
*Katie Matton,Purvaja Balaji,Hamzeh Ghasemzadeh,Jameson C. Cooper,Daryush D. Mehta,Jarrad H. Van Stan,Robert E. Hillman,Rosalind Picard,John Guttag,S. Mazdak Abulnaga*

Main category: cs.CV

TL;DR: 本文提出了首个从声带图像自动分类语音创伤严重程度的方法，采用序数回归框架处理有序标签，并引入软标签处理标注不确定性，性能接近临床专家水平。


<details>
  <summary>Details</summary>
Motivation: 语音创伤严重程度评估依赖临床专家判断，成本高且可靠性差异大，需要自动化工具来支持大规模研究和改善患者护理。

Method: 采用广泛使用的序数回归框架处理有序标签，提出新颖的序数回归损失函数修改，使其能够处理反映标注者评分分布的软标签。

Result: 提出的软序数回归方法预测性能接近临床专家水平，同时产生良好校准的不确定性估计。

Conclusion: 通过提供语音创伤严重程度评估的自动化工具，本研究能够实现语音创伤的大规模研究，最终改善临床理解和患者护理。

Abstract: Phonotrauma refers to vocal fold tissue damage resulting from exposure to forces during voicing. It occurs on a continuum from mild to severe, and treatment options can vary based on severity. Assessment of severity involves a clinician's expert judgment, which is costly and can vary widely in reliability. In this work, we present the first method for automatically classifying phonotrauma severity from vocal fold images. To account for the ordinal nature of the labels, we adopt a widely used ordinal regression framework. To account for label uncertainty, we propose a novel modification to ordinal regression loss functions that enables them to operate on soft labels reflecting annotator rating distributions. Our proposed soft ordinal regression method achieves predictive performance approaching that of clinical experts, while producing well-calibrated uncertainty estimates. By providing an automated tool for phonotrauma severity assessment, our work can enable large-scale studies of phonotrauma, ultimately leading to improved clinical understanding and patient care.

</details>


### [4] [SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control](https://arxiv.org/abs/2511.09715)
*Arman Zarei,Samyadeep Basu,Mobina Pournemat,Sayan Nag,Ryan Rossi,Soheil Feizi*

Main category: cs.CV

TL;DR: SliderEdit是一个用于指令式图像编辑的框架，通过引入可调节的滑块实现对单个编辑指令强度的连续精细控制，解决了现有模型固定强度编辑的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有指令式图像编辑模型在处理多指令提示时，对每个指令应用固定强度，限制了用户对单个编辑强度的精确连续控制能力。

Method: 提出SliderEdit框架，将多部分编辑指令解耦，为每个指令训练全局滑块，使用单一低秩适应矩阵集泛化到多样化的编辑、属性和组合指令。

Result: 在FLUX-Kontext和Qwen-Image-Edit等最先进图像编辑模型上应用SliderEdit，显著提升了编辑可控性、视觉一致性和用户可引导性。

Conclusion: SliderEdit首次探索并提出了指令式图像编辑模型中连续精细指令控制的框架，为实现交互式、指令驱动的图像操作提供了连续组合控制的新途径。

Abstract: Instruction-based image editing models have recently achieved impressive performance, enabling complex edits to an input image from a multi-instruction prompt. However, these models apply each instruction in the prompt with a fixed strength, limiting the user's ability to precisely and continuously control the intensity of individual edits. We introduce SliderEdit, a framework for continuous image editing with fine-grained, interpretable instruction control. Given a multi-part edit instruction, SliderEdit disentangles the individual instructions and exposes each as a globally trained slider, allowing smooth adjustment of its strength. Unlike prior works that introduced slider-based attribute controls in text-to-image generation, typically requiring separate training or fine-tuning for each attribute or concept, our method learns a single set of low-rank adaptation matrices that generalize across diverse edits, attributes, and compositional instructions. This enables continuous interpolation along individual edit dimensions while preserving both spatial locality and global semantic consistency. We apply SliderEdit to state-of-the-art image editing models, including FLUX-Kontext and Qwen-Image-Edit, and observe substantial improvements in edit controllability, visual consistency, and user steerability. To the best of our knowledge, we are the first to explore and propose a framework for continuous, fine-grained instruction control in instruction-based image editing models. Our results pave the way for interactive, instruction-driven image manipulation with continuous and compositional control.

</details>


### [5] [Density Estimation and Crowd Counting](https://arxiv.org/abs/2511.09723)
*Balachandra Devarangadi Sunil,Rakshith Venkatesh,Shantanu Todmal*

Main category: cs.CV

TL;DR: 本研究将原本用于图像分析的群体密度估计算法改进为适用于视频场景，通过扩散过程的去噪概率模型生成高质量密度图，结合事件驱动采样技术减少计算负担，在密集和稀疏场景下都能有效捕捉群体动态。


<details>
  <summary>Details</summary>
Motivation: 解决视频分析中特有的时间挑战，为公共安全、灾害响应和事件管理等实时群体监控应用提供可扩展且高效的框架。

Method: 集成去噪概率模型使用扩散过程生成密度图，采用窄高斯核生成多个密度图输出，结合回归分支进行精确特征提取，通过相似度评分机制整合密度图，并引入基于Farneback光流算法的事件驱动采样技术选择性地捕获重要群体运动帧。

Result: 通过定性和定量评估（包括叠加图和平均绝对误差），模型在密集和稀疏设置下都能有效捕捉群体动态，采样方法能够显著减少帧数同时保持关键的群体事件。

Conclusion: 该工作提供了一个可扩展且高效的实时群体监控框架，成功解决了视频分析中的时间挑战，适用于各种实际应用场景。

Abstract: This study enhances a crowd density estimation algorithm originally designed for image-based analysis by adapting it for video-based scenarios. The proposed method integrates a denoising probabilistic model that utilizes diffusion processes to generate high-quality crowd density maps. To improve accuracy, narrow Gaussian kernels are employed, and multiple density map outputs are generated. A regression branch is incorporated into the model for precise feature extraction, while a consolidation mechanism combines these maps based on similarity scores to produce a robust final result. An event-driven sampling technique, utilizing the Farneback optical flow algorithm, is introduced to selectively capture frames showing significant crowd movements, reducing computational load and storage by focusing on critical crowd dynamics. Through qualitative and quantitative evaluations, including overlay plots and Mean Absolute Error (MAE), the model demonstrates its ability to effectively capture crowd dynamics in both dense and sparse settings. The efficiency of the sampling method is further assessed, showcasing its capability to decrease frame counts while maintaining essential crowd events. By addressing the temporal challenges unique to video analysis, this work offers a scalable and efficient framework for real-time crowd monitoring in applications such as public safety, disaster response, and event management.

</details>


### [6] [PALMS+: Modular Image-Based Floor Plan Localization Leveraging Depth Foundation Model](https://arxiv.org/abs/2511.09724)
*Yunqian Cheng,Benjamin Princen,Roberto Manduchi*

Main category: cs.CV

TL;DR: PALMS+是一个基于图像的室内定位系统，通过单目深度估计重建3D点云并与平面图进行几何匹配，解决了传统方法在GPS缺失环境中的局限性，无需训练即可实现高精度定位。


<details>
  <summary>Details</summary>
Motivation: 在GPS缺失的室内环境中实现精确定位对应急响应和辅助导航至关重要。传统视觉方法如PALMS受限于智能手机LiDAR的短距离和室内布局的模糊性，需要改进。

Method: 使用基础单目深度估计模型从RGB图像重建尺度对齐的3D点云，然后通过卷积与平面图进行几何布局匹配，输出位置和方向的后验分布。

Result: 在Structured3D和自定义校园数据集上，PALMS+在静态定位精度上优于PALMS和F3Loc，无需训练。在33条真实轨迹上的序列定位中，定位误差更低，展示了鲁棒性。

Conclusion: PALMS+为无基础设施应用提供了有效的室内定位解决方案，特别适用于无摄像头跟踪场景，具有实际应用潜力。

Abstract: Indoor localization in GPS-denied environments is crucial for applications like emergency response and assistive navigation. Vision-based methods such as PALMS enable infrastructure-free localization using only a floor plan and a stationary scan, but are limited by the short range of smartphone LiDAR and ambiguity in indoor layouts. We propose PALMS$+$, a modular, image-based system that addresses these challenges by reconstructing scale-aligned 3D point clouds from posed RGB images using a foundation monocular depth estimation model (Depth Pro), followed by geometric layout matching via convolution with the floor plan. PALMS$+$ outputs a posterior over the location and orientation, usable for direct or sequential localization. Evaluated on the Structured3D and a custom campus dataset consisting of 80 observations across four large campus buildings, PALMS$+$ outperforms PALMS and F3Loc in stationary localization accuracy -- without requiring any training. Furthermore, when integrated with a particle filter for sequential localization on 33 real-world trajectories, PALMS$+$ achieved lower localization errors compared to other methods, demonstrating robustness for camera-free tracking and its potential for infrastructure-free applications. Code and data are available at https://github.com/Head-inthe-Cloud/PALMS-Plane-based-Accessible-Indoor-Localization-Using-Mobile-Smartphones

</details>


### [7] [Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction](https://arxiv.org/abs/2511.09735)
*Ahmed Alia,Mohcine Chraibi,Armin Seyfried*

Main category: cs.CV

TL;DR: 本文提出了一种改进的Social LSTM模型，通过引入动态占用空间损失函数，在保持位移误差不增加的同时有效降低行人轨迹预测中的碰撞率。


<details>
  <summary>Details</summary>
Motivation: 现有行人轨迹预测方法通常将行人视为点实体，忽略了每个人实际占用的物理空间，导致在动态拥挤环境中无法有效避免现实碰撞。

Method: 提出增强型Social LSTM模型，结合新的动态占用空间损失函数，该函数将平均位移误差与对场景密度和个体空间占用敏感的碰撞惩罚相结合。

Result: 模型在五个数据集上测试，相比基线模型碰撞率降低达31%，平均位移误差和最终位移误差分别平均降低5%和6%，并在大多数测试集上优于多个先进深度学习模型。

Conclusion: 所提出的动态占用空间损失函数能有效指导模型学习避免现实碰撞，同时保持位移预测精度，在不同密度场景下均表现优异。

Abstract: In dynamic and crowded environments, realistic pedestrian trajectory prediction remains a challenging task due to the complex nature of human motion and the mutual influences among individuals. Deep learning models have recently achieved promising results by implicitly learning such patterns from 2D trajectory data. However, most approaches treat pedestrians as point entities, ignoring the physical space that each person occupies. To address these limitations, this paper proposes a novel deep learning model that enhances the Social LSTM with a new Dynamic Occupied Space loss function. This loss function guides Social LSTM in learning to avoid realistic collisions without increasing displacement error across different crowd densities, ranging from low to high, in both homogeneous and heterogeneous density settings. Such a function achieves this by combining the average displacement error with a new collision penalty that is sensitive to scene density and individual spatial occupancy. For efficient training and evaluation, five datasets were generated from real pedestrian trajectories recorded during the Festival of Lights in Lyon 2022. Four datasets represent homogeneous crowd conditions -- low, medium, high, and very high density -- while the fifth corresponds to a heterogeneous density distribution. The experimental findings indicate that the proposed model not only lowers collision rates but also enhances displacement prediction accuracy in each dataset. Specifically, the model achieves up to a 31% reduction in the collision rate and reduces the average displacement error and the final displacement error by 5% and 6%, respectively, on average across all datasets compared to the baseline. Moreover, the proposed model consistently outperforms several state-of-the-art deep learning models across most test sets.

</details>


### [8] [Soiling detection for Advanced Driver Assistance Systems](https://arxiv.org/abs/2511.09740)
*Filip Beránek,Václav Diviš,Ivan Gruber*

Main category: cs.CV

TL;DR: 该论文将汽车摄像头污染检测视为语义分割问题，比较了流行的分割方法并展示了其相对于图块级分类方法的性能优势。作者发现Woodscape数据集存在数据泄漏和标注不精确问题，创建了新的数据子集，在更小规模下实现了可比的结果。


<details>
  <summary>Details</summary>
Motivation: 汽车摄像头污染检测是高级驾驶辅助系统的关键部分，需要提高系统对外部条件（如天气、灰尘等）的鲁棒性。

Method: 将污染检测视为语义分割问题，比较了流行的分割方法，并创建了新的数据子集来解决原始数据集的数据泄漏和标注不精确问题。

Result: 分割方法在性能上优于图块级分类方法，新创建的数据子集虽然规模更小，但能在更短时间内达到可比的结果。

Conclusion: 语义分割方法在汽车摄像头污染检测中表现优越，通过解决数据集问题可以更高效地实现检测性能。

Abstract: Soiling detection for automotive cameras is a crucial part of advanced driver assistance systems to make them more robust to external conditions like weather, dust, etc. In this paper, we regard the soiling detection as a semantic segmentation problem. We provide a comprehensive comparison of popular segmentation methods and show their superiority in performance while comparing them to tile-level classification approaches. Moreover, we present an extensive analysis of the Woodscape dataset showing that the original dataset contains a data-leakage and imprecise annotations. To address these problems, we create a new data subset, which, despite being much smaller, provides enough information for the segmentation method to reach comparable results in a much shorter time. All our codes and dataset splits are available at https://github.com/filipberanek/woodscape_revision.

</details>


### [9] [Feature Quality and Adaptability of Medical Foundation Models: A Comparative Evaluation for Radiographic Classification and Segmentation](https://arxiv.org/abs/2511.09742)
*Frank Li,Theo Dapamede,Mohammadreza Chavoshi,Young Seok Jeon,Bardia Khosravi,Abdulhameed Dere,Beatrice Brown-Mulry,Rohan Satya Isaac,Aawez Mansuri,Chiratidzo Sanyika,Janice Newsome,Saptarshi Purkayastha,Imon Banerjee,Hari Trivedi,Judy Gichoya*

Main category: cs.CV

TL;DR: 本研究评估了8个医学和通用领域基础模型在胸部X光分析中的表现，发现医学领域预训练显著优于通用领域模型，但特征有效性高度依赖任务类型。对于复杂、细微的病理分割，所有基础模型在未充分微调时表现不佳，而监督式端到端基线模型在分割任务中仍保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 基础模型在医学影像中的有效性存在差异，但预训练领域、范式和架构如何影响嵌入质量尚不清楚，这阻碍了为特定放射学任务选择最佳编码器。

Method: 评估8个医学和通用领域基础模型的视觉编码器，使用线性探测和微调方法对胸部X光进行分类（气胸、心脏肥大）和分割（气胸、心脏边界）任务。

Result: 医学领域预训练具有显著优势，在线性探测中医学基础模型始终优于通用领域模型。但对于复杂、细微的病理分割，所有基础模型在未充分微调时表现不佳。文本-图像对齐不是必要条件，仅图像和标签监督的基础模型表现优异。监督式端到端基线模型在分割任务中与最佳基础模型相当或更优。

Conclusion: 医学预训练有益但非万能，架构选择（如多尺度）至关重要。预训练特征并非普遍有效，特别是在复杂定位任务中，监督模型仍然是强有力的替代方案。

Abstract: Foundation models (FMs) promise to generalize medical imaging, but their effectiveness varies. It remains unclear how pre-training domain (medical vs. general), paradigm (e.g., text-guided), and architecture influence embedding quality, hindering the selection of optimal encoders for specific radiology tasks. To address this, we evaluate vision encoders from eight medical and general-domain FMs for chest X-ray analysis. We benchmark classification (pneumothorax, cardiomegaly) and segmentation (pneumothorax, cardiac boundary) using linear probing and fine-tuning. Our results show that domain-specific pre-training provides a significant advantage; medical FMs consistently outperformed general-domain models in linear probing, establishing superior initial feature quality. However, feature utility is highly task-dependent. Pre-trained embeddings were strong for global classification and segmenting salient anatomy (e.g., heart). In contrast, for segmenting complex, subtle pathologies (e.g., pneumothorax), all FMs performed poorly without significant fine-tuning, revealing a critical gap in localizing subtle disease. Subgroup analysis showed FMs use confounding shortcuts (e.g., chest tubes for pneumothorax) for classification, a strategy that fails for precise segmentation. We also found that expensive text-image alignment is not a prerequisite; image-only (RAD-DINO) and label-supervised (Ark+) FMs were among top performers. Notably, a supervised, end-to-end baseline remained highly competitive, matching or exceeding the best FMs on segmentation tasks. These findings show that while medical pre-training is beneficial, architectural choices (e.g., multi-scale) are critical, and pre-trained features are not universally effective, especially for complex localization tasks where supervised models remain a strong alternative.

</details>


### [10] [Gradient-Guided Exploration of Generative Model's Latent Space for Controlled Iris Image Augmentations](https://arxiv.org/abs/2511.09749)
*Mahsa Mitcheff,Siamul Karim Khan,Adam Czajka*

Main category: cs.CV

TL;DR: 本文提出了一种新的虹膜图像增强策略，通过在生成模型的潜在空间中遍历，操纵特定虹膜图像属性同时保持身份不变。


<details>
  <summary>Details</summary>
Motivation: 开发可靠的虹膜识别和呈现攻击检测方法需要能够捕捉虹膜特征真实变化和各种异常情况的多样化数据集。由于虹膜图像纹理丰富且空间频率范围广，合成相同身份但控制特定属性的虹膜图像具有挑战性。

Method: 通过引导生成模型潜在空间的遍历，沿着特定几何、纹理或质量相关虹膜图像特征的梯度方向，操纵所需属性（如清晰度、瞳孔大小、虹膜大小或瞳孔-虹膜比例），同时保持图像的身份信息。该方法可扩展到任何可制定可微分损失项的属性，并能使用预训练GAN模型生成的随机图像或真实虹膜图像。

Result: 该方法能够合成相同身份的虹膜图像，同时控制特定图像属性，为虹膜识别和呈现攻击检测提供多样化的训练数据。

Conclusion: 提出的潜在空间遍历方法为虹膜图像增强提供了一种有效的策略，能够在保持身份一致性的前提下操纵各种图像属性，有助于提升虹膜识别系统的性能。

Abstract: Developing reliable iris recognition and presentation attack detection methods requires diverse datasets that capture realistic variations in iris features and a wide spectrum of anomalies. Because of the rich texture of iris images, which spans a wide range of spatial frequencies, synthesizing same-identity iris images while controlling specific attributes remains challenging. In this work, we introduce a new iris image augmentation strategy by traversing a generative model's latent space toward latent codes that represent same-identity samples but with some desired iris image properties manipulated. The latent space traversal is guided by a gradient of specific geometrical, textural, or quality-related iris image features (e.g., sharpness, pupil size, iris size, or pupil-to-iris ratio) and preserves the identity represented by the image being manipulated. The proposed approach can be easily extended to manipulate any attribute for which a differentiable loss term can be formulated. Additionally, our approach can use either randomly generated images using either a pre-train GAN model or real-world iris images. We can utilize GAN inversion to project any given iris image into the latent space and obtain its corresponding latent code.

</details>


### [11] [STORM: Segment, Track, and Object Re-Localization from a Single 3D Model](https://arxiv.org/abs/2511.09771)
*Yu Deng,Teng Cao,Hikaru Shindo,Jiahong Xue,Quentin Delfosse,Kristian Kersting*

Main category: cs.CV

TL;DR: STORM是一个无需人工标注的实时6D姿态估计系统，通过结合视觉语言理解和自监督特征匹配的三阶段流程，实现了在遮挡和快速运动情况下的鲁棒姿态估计和跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有6D姿态估计方法依赖第一帧的手动标注分割掩码，这既费时又容易在遮挡或快速运动时性能下降。STORM旨在解决这些限制，提供无需人工标注的鲁棒解决方案。

Method: 采用三阶段流程：上下文对象描述指导定位、自交叉注意力机制识别候选区域、分割模型生成精确掩码用于姿态估计。关键创新是自动重新注册机制，通过特征相似性监测检测跟踪失败并从严重遮挡或快速运动中恢复。

Result: 在具有多目标遮挡、高速运动和变化光照的挑战性工业数据集上实现了最先进的精度，同时以实时速度运行且无需额外训练。

Conclusion: 这种无标注方法显著减少了部署开销，为柔性制造和智能质量控制等现代应用提供了实用解决方案。

Abstract: Accurate 6D pose estimation and tracking are fundamental capabilities for physical AI systems such as robots. However, existing approaches typically rely on a manually annotated segmentation mask of the target in the first frame, which is labor-intensive and leads to reduced performance when faced with occlusions or rapid movement. To address these limi- tations, we propose STORM (Segment, Track, and Object Re-localization from a single 3D Model), an open-source robust real-time 6D pose estimation system that requires no manual annotation. STORM employs a novel three-stage pipeline combining vision-language understanding with self-supervised feature matching: contextual object descriptions guide localization, self-cross-attention mechanisms identify candidate regions, and a segmentation model produces precise masks for accurate pose estimation. Another key innovation is our automatic re-registration mechanism that detects tracking failures through feature similarity monitoring and recovers from severe occlusions or rapid motion. STORM achieves state-of-the-art accuracy on challenging industrial datasets featuring multi-object occlusions, high-speed motion, and varying illumination, while operating at real-time speeds without additional training. This annotation-free approach significantly reduces deployment overhead, providing a practical solution for modern applications, such as flexible manufacturing and intelligent quality control.

</details>


### [12] [PANDA - Patch And Distribution-Aware Augmentation for Long-Tailed Exemplar-Free Continual Learning](https://arxiv.org/abs/2511.09791)
*Siddeshwar Raghavan,Jiangpeng He,Fengqing Zhu*

Main category: cs.CV

TL;DR: PANDA是一个面向无示例持续学习的补丁和分布感知增强框架，通过CLIP编码器识别代表性区域并移植到高频类别样本中，同时利用自适应平衡策略平滑任务间不平衡，有效缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据流存在双重不平衡问题：数据集级分布不平衡与任务内极端或反向偏斜，这种任务内和任务间的不平衡阻碍了有效学习和泛化。

Method: 提出PANDA框架，使用CLIP编码器识别低频率类别的代表性区域并移植到高频类别样本中，同时采用自适应平衡策略利用先前任务分布来平滑任务间不平衡。

Result: 广泛的实验和消融研究表明PANDA能够与现有的基于预训练模型的持续学习方法协同工作，提高准确性并减少灾难性遗忘。

Conclusion: PANDA框架有效解决了无示例持续学习中的双重不平衡问题，通过补丁级增强和分布感知平衡策略显著提升了学习效果。

Abstract: Exemplar-Free Continual Learning (EFCL) restricts the storage of previous task data and is highly susceptible to catastrophic forgetting. While pre-trained models (PTMs) are increasingly leveraged for EFCL, existing methods often overlook the inherent imbalance of real-world data distributions. We discovered that real-world data streams commonly exhibit dual-level imbalances, dataset-level distributions combined with extreme or reversed skews within individual tasks, creating both intra-task and inter-task disparities that hinder effective learning and generalization. To address these challenges, we propose PANDA, a Patch-and-Distribution-Aware Augmentation framework that integrates seamlessly with existing PTM-based EFCL methods. PANDA amplifies low-frequency classes by using a CLIP encoder to identify representative regions and transplanting those into frequent-class samples within each task. Furthermore, PANDA incorporates an adaptive balancing strategy that leverages prior task distributions to smooth inter-task imbalances, reducing the overall gap between average samples across tasks and enabling fairer learning with frozen PTMs. Extensive experiments and ablation studies demonstrate PANDA's capability to work with existing PTM-based CL methods, improving accuracy and reducing catastrophic forgetting.

</details>


### [13] [Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models](https://arxiv.org/abs/2511.09809)
*Konstantinos M. Dafnis,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: STS是一种轻量级测试时适应框架，通过提取文本嵌入的谱子空间来定义主要语义方向，并以频谱感知方式调整潜在表示，仅需少量参数即可实现高效域适应。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在测试时域偏移下性能下降，而传统的测试时适应方法需要反向传播或修改模型组件，计算开销大。

Method: 从文本嵌入中提取谱子空间定义语义方向，通过适应少量每样本偏移参数来最小化增强视图间的熵，完全在潜在空间中进行推理，无需反向传播。

Result: STS在标准评估协议下显著优于或与最先进的测试时适应方法相当，同时参数更少、推理速度提升8倍、内存占用减少12倍。

Conclusion: STS提供了一种高效轻量的测试时适应解决方案，无需修改冻结编码器即可实现优异的域适应性能。

Abstract: Vision-Language Models (VLMs) excel at zero-shot inference but often degrade under test-time domain shifts. For this reason, episodic test-time adaptation strategies have recently emerged as powerful techniques for adapting VLMs to a single unlabeled image. However, existing adaptation strategies, such as test-time prompt tuning, typically require backpropagating through large encoder weights or altering core model components. In this work, we introduce Spectrum-Aware Test-Time Steering (STS), a lightweight adaptation framework that extracts a spectral subspace from the textual embeddings to define principal semantic directions and learns to steer latent representations in a spectrum-aware manner by adapting a small number of per-sample shift parameters to minimize entropy across augmented views. STS operates entirely at inference in the latent space, without backpropagation through or modification of the frozen encoders. Building on standard evaluation protocols, our comprehensive experiments demonstrate that STS largely surpasses or compares favorably against state-of-the-art test-time adaptation methods, while introducing only a handful of additional parameters and achieving inference speeds up to 8x faster with a 12x smaller memory footprint than conventional test-time prompt tuning. The code is available at https://github.com/kdafnis/STS.

</details>


### [14] [AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting](https://arxiv.org/abs/2511.09827)
*Aymen Mir,Jian Wang,Riza Alp Guler,Chuan Guo,Gerard Pons-Moll,Bing Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯泼溅(3DGS)的新框架，用于在3D场景中动画化人类角色，实现了几何一致的自由视点渲染和自然的人-场景交互。


<details>
  <summary>Details</summary>
Motivation: 现有动画管道主要使用网格或点云作为3D表示，而3DGS在新型视图合成方面取得了最先进的光照真实效果，但在人-场景动画和交互方面仍未被充分探索。

Method: 使用3DGS作为3D表示，提出高斯对齐运动模块合成运动而无需显式场景几何，使用基于不透明度的线索和投影高斯结构来指导人体放置和姿态对齐，并进一步提出人-场景高斯细化优化来确保自然交互。

Result: 在Scannet++和SuperSplat库的场景上进行了评估，展示了从稀疏和密集多视角人体捕获重建的虚拟角色，能够实现几何一致的自由视点渲染。

Conclusion: 该框架支持新颖应用，如使用新动画人类编辑单目RGB视频的几何一致自由视点渲染，展示了3DGS在单目视频人体动画方面的独特优势。

Abstract: We present a novel framework for animating humans in 3D scenes using 3D Gaussian Splatting (3DGS), a neural scene representation that has recently achieved state-of-the-art photorealistic results for novel-view synthesis but remains under-explored for human-scene animation and interaction. Unlike existing animation pipelines that use meshes or point clouds as the underlying 3D representation, our approach introduces the use of 3DGS as the 3D representation to the problem of animating humans in scenes. By representing humans and scenes as Gaussians, our approach allows for geometry-consistent free-viewpoint rendering of humans interacting with 3D scenes. Our key insight is that the rendering can be decoupled from the motion synthesis and each sub-problem can be addressed independently, without the need for paired human-scene data. Central to our method is a Gaussian-aligned motion module that synthesizes motion without explicit scene geometry, using opacity-based cues and projected Gaussian structures to guide human placement and pose alignment. To ensure natural interactions, we further propose a human-scene Gaussian refinement optimization that enforces realistic contact and navigation. We evaluate our approach on scenes from Scannet++ and the SuperSplat library, and on avatars reconstructed from sparse and dense multi-view human capture. Finally, we demonstrate that our framework allows for novel applications such as geometry-consistent free-viewpoint rendering of edited monocular RGB videos with new animated humans, showcasing the unique advantage of 3DGS for monocular video-based human animation.

</details>


### [15] [CertMask: Certifiable Defense Against Adversarial Patches via Theoretically Optimal Mask Coverage](https://arxiv.org/abs/2511.09834)
*Xuntao Lyu,Ching-Chi Lin,Abdullah Al Arafat,Georg von der Brüggen,Jian-Jia Chen,Zhishan Guo*

Main category: cs.CV

TL;DR: CertMask是一种可证明鲁棒的防御方法，通过单轮掩码操作有效对抗对抗性补丁攻击，相比现有方法在计算效率和认证鲁棒性方面都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 对抗性补丁攻击通过局部扰动误导深度视觉模型，在现实应用中构成严重威胁。现有防御方法如PatchCleanser需要两轮掩码且计算复杂度高，因此需要更高效的防御方案。

Method: 提出CertMask方法，采用数学严谨的覆盖策略构建二进制掩码集，确保每个可能的补丁位置至少被覆盖k次。仅需单轮掩码操作，时间复杂度为O(n)。

Result: 在ImageNet、ImageNette和CIFAR-10数据集上的实验表明，CertMask相比PatchCleanser将认证鲁棒准确率提升高达+13.4%，同时保持与原始模型几乎相同的干净准确率。

Conclusion: CertMask通过理论保证的覆盖策略实现了高效且鲁棒的防御，显著优于现有方法，为对抗性补丁攻击提供了实用的解决方案。

Abstract: Adversarial patch attacks inject localized perturbations into images to mislead deep vision models. These attacks can be physically deployed, posing serious risks to real-world applications. In this paper, we propose CertMask, a certifiably robust defense that constructs a provably sufficient set of binary masks to neutralize patch effects with strong theoretical guarantees. While the state-of-the-art approach (PatchCleanser) requires two rounds of masking and incurs $O(n^2)$ inference cost, CertMask performs only a single round of masking with $O(n)$ time complexity, where $n$ is the cardinality of the mask set to cover an input image. Our proposed mask set is computed using a mathematically rigorous coverage strategy that ensures each possible patch location is covered at least $k$ times, providing both efficiency and robustness. We offer a theoretical analysis of the coverage condition and prove its sufficiency for certification. Experiments on ImageNet, ImageNette, and CIFAR-10 show that CertMask improves certified robust accuracy by up to +13.4\% over PatchCleanser, while maintaining clean accuracy nearly identical to the vanilla model.

</details>


### [16] [Remember Me: Bridging the Long-Range Gap in LVLMs with Three-Step Inference-Only Decay Resilience Strategies](https://arxiv.org/abs/2511.09868)
*Peng Gao,Yujian Lee,Xiaofeng Zhang,Zailong Chen,Hui Zhang*

Main category: cs.CV

TL;DR: 提出了T-DRS方法，通过三种策略缓解LVLMs中ROPE导致的远程依赖建模问题，无需训练即可提升VQA任务性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在使用ROPE时面临远程依赖建模挑战，ROPE虽然能精确建模token位置，但随着token距离增加会导致注意力衰减，损害模型记忆全局上下文的能力。

Method: 提出仅推理阶段使用的三步衰减恢复策略：1)语义驱动DRS通过内容感知残差放大语义重要但距离远的信号；2)距离感知控制DRS基于位置距离平滑调节权重；3)重新强化远程DRS巩固剩余信息性远程依赖。

Result: 在视觉问答基准测试上的广泛实验表明，T-DRS能以无需训练的方式持续提升性能。

Conclusion: T-DRS策略能恢复被抑制的远程token对而不损害局部归纳偏差，有效解决ROPE导致的注意力衰减问题。

Abstract: Large Vision-Language Models (LVLMs) have achieved impressive performance across a wide range of multimodal tasks. However, they still face critical challenges in modeling long-range dependencies under the usage of Rotary Positional Encoding (ROPE). Although it can facilitate precise modeling of token positions, it induces progressive attention decay as token distance increases, especially with progressive attention decay over distant token pairs, which severely impairs the model's ability to remember global context. To alleviate this issue, we propose inference-only Three-step Decay Resilience Strategies (T-DRS), comprising (1) Semantic-Driven DRS (SD-DRS), amplifying semantically meaningful but distant signals via content-aware residuals, (2) Distance-aware Control DRS (DC-DRS), which can purify attention by smoothly modulating weights based on positional distances, suppressing noise while preserving locality, and (3) re-Reinforce Distant DRS (reRD-DRS), consolidating the remaining informative remote dependencies to maintain global coherence. Together, the T-DRS recover suppressed long-range token pairs without harming local inductive biases. Extensive experiments on Vision Question Answering (VQA) benchmarks demonstrate that T-DRS can consistently improve performance in a training-free manner. The code can be accessed in https://github.com/labixiaoq-qq/Remember-me

</details>


### [17] [SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection](https://arxiv.org/abs/2511.09870)
*Jia Lin,Xiaofei Zhou,Jiyuan Liu,Runmin Cong,Guodao Zhang,Zhi Liu,Jiyong Zhang*

Main category: cs.CV

TL;DR: 提出SAM-DAQ方法，将SAM2模型适配于RGB-D视频显著目标检测任务，通过深度引导自适应查询和查询驱动时序记忆模块，在无需手动提示的情况下实现更好的性能。


<details>
  <summary>Details</summary>
Motivation: 解决直接应用SAM基础模型到RGB-D视频显著目标检测任务时面临的三个挑战：依赖手动提示、序列适配器内存消耗高、记忆注意力计算负担重。

Method: 1. 部署基于并行适配器的多模态图像编码器(PAMIE)，包含深度引导并行适配器(DPA)；2. 部署查询驱动时序记忆(QTM)模块，统一记忆库和提示嵌入到可学习管道中，同时利用帧级和视频级查询。

Result: 在三个RGB-D VSOD数据集上的广泛实验表明，所提出的SAM-DAQ在所有评估指标上均优于最先进的方法。

Conclusion: SAM-DAQ通过无缝集成深度和时序线索，成功将SAM2适配于视频显著目标检测，在无需手动提示的情况下实现了优越性能。

Abstract: Recently segment anything model (SAM) has attracted widespread concerns, and it is often treated as a vision foundation model for universal segmentation. Some researchers have attempted to directly apply the foundation model to the RGB-D video salient object detection (RGB-D VSOD) task, which often encounters three challenges, including the dependence on manual prompts, the high memory consumption of sequential adapters, and the computational burden of memory attention. To address the limitations, we propose a novel method, namely Segment Anything Model with Depth-guided Adaptive Queries (SAM-DAQ), which adapts SAM2 to pop-out salient objects from videos by seamlessly integrating depth and temporal cues within a unified framework. Firstly, we deploy a parallel adapter-based multi-modal image encoder (PAMIE), which incorporates several depth-guided parallel adapters (DPAs) in a skip-connection way. Remarkably, we fine-tune the frozen SAM encoder under prompt-free conditions, where the DPA utilizes depth cues to facilitate the fusion of multi-modal features. Secondly, we deploy a query-driven temporal memory (QTM) module, which unifies the memory bank and prompt embeddings into a learnable pipeline. Concretely, by leveraging both frame-level queries and video-level queries simultaneously, the QTM module can not only selectively extract temporal consistency features but also iteratively update the temporal representations of the queries. Extensive experiments are conducted on three RGB-D VSOD datasets, and the results show that the proposed SAM-DAQ consistently outperforms state-of-the-art methods in terms of all evaluation metrics.

</details>


### [18] [HCC-3D: Hierarchical Compensatory Compression for 98% 3D Token Reduction in Vision-Language Models](https://arxiv.org/abs/2511.09883)
*Liheng Zhang,Jin Wang,Hui Li,Bingfeng Zhang,Weifeng Liu*

Main category: cs.CV

TL;DR: 本文提出HCC-3D方法，通过分层补偿压缩技术显著降低3D视觉语言模型的运算成本，在保持98%压缩率的同时实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前3D视觉语言模型直接将点云嵌入为3D token，导致在大型语言模型部分处理所有3D token时产生巨大计算开销，限制了实际应用。

Method: 提出分层补偿压缩方法：1) 全局结构压缩(GSC)使用全局查询将3D token压缩为少量关键token；2) 自适应细节挖掘(ADM)模块通过互补评分选择性重新压缩显著但未被充分关注的特征。

Result: 实验表明HCC-3D实现了约98%的极端压缩率，同时达到了新的最先进性能，在效率和性能方面均有显著提升。

Conclusion: HCC-3D通过高效压缩3D token同时保持关键细节，成功解决了3D视觉语言模型的计算瓶颈问题。

Abstract: 3D understanding has drawn significant attention recently, leveraging Vision-Language Models (VLMs) to enable multi-modal reasoning between point cloud and text data. Current 3D-VLMs directly embed the 3D point clouds into 3D tokens, following large 2D-VLMs with powerful reasoning capabilities. However, this framework has a great computational cost limiting its application, where we identify that the bottleneck lies in processing all 3D tokens in the Large Language Model (LLM) part. This raises the question: how can we reduce the computational overhead introduced by 3D tokens while preserving the integrity of their essential information? To address this question, we introduce Hierarchical Compensatory Compression (HCC-3D) to efficiently compress 3D tokens while maintaining critical detail retention. Specifically, we first propose a global structure compression (GSC), in which we design global queries to compress all 3D tokens into a few key tokens while keeping overall structural information. Then, to compensate for the information loss in GSC, we further propose an adaptive detail mining (ADM) module that selectively recompresses salient but under-attended features through complementary scoring. Extensive experiments demonstrate that HCC-3D not only achieves extreme compression ratios (approximately 98%) compared to previous 3D-VLMs, but also achieves new state-of-the-art performance, showing the great improvements on both efficiency and performance.

</details>


### [19] [VISTA: A Vision and Intent-Aware Social Attention Framework for Multi-Agent Trajectory Prediction](https://arxiv.org/abs/2511.10203)
*Stephane Da Silva Martins,Emanuel Aldea,Sylvie Le Hégarat-Mascle*

Main category: cs.CV

TL;DR: VISTA是一种基于递归目标条件变换器的多智能体轨迹预测方法，结合长期意图与细粒度社交交互建模，在MADRAS和SDD数据集上实现了最先进的精度和显著降低的碰撞率。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时捕捉智能体的长期目标和细粒度社交交互，导致生成的多智能体未来轨迹不真实。

Method: 提出VISTA模型，包含：(1)交叉注意力融合模块整合长期意图与历史运动；(2)社交令牌注意力机制实现灵活的跨智能体交互建模；(3)成对注意力图使社交影响模式在推理时可解释。

Result: 在MADRAS基准测试中，将强基线的平均碰撞率从2.14%降低到0.03%；在SDD上实现零碰撞，同时改进ADE、FDE和minFDE指标。

Conclusion: VISTA能够生成社交合规、目标感知且可解释的轨迹，在安全关键自主系统中具有应用前景。

Abstract: Multi-agent trajectory prediction is crucial for autonomous systems operating in dense, interactive environments. Existing methods often fail to jointly capture agents' long-term goals and their fine-grained social interactions, which leads to unrealistic multi-agent futures. We propose VISTA, a recursive goal-conditioned transformer for multi-agent trajectory forecasting. VISTA combines (i) a cross-attention fusion module that integrates long-horizon intent with past motion, (ii) a social-token attention mechanism for flexible interaction modeling across agents, and (iii) pairwise attention maps that make social influence patterns interpretable at inference time. Our model turns single-agent goal-conditioned prediction into a coherent multi-agent forecasting framework. Beyond standard displacement metrics, we evaluate trajectory collision rates as a measure of joint realism. On the high-density MADRAS benchmark and on SDD, VISTA achieves state-of-the-art accuracy and substantially fewer collisions. On MADRAS, it reduces the average collision rate of strong baselines from 2.14 to 0.03 percent, and on SDD it attains zero collisions while improving ADE, FDE, and minFDE. These results show that VISTA generates socially compliant, goal-aware, and interpretable trajectories, making it promising for safety-critical autonomous systems.

</details>


### [20] [Regional Attention-Enhanced Swin Transformer for Clinically Relevant Medical Image Captioning](https://arxiv.org/abs/2511.09893)
*Zubia Naz,Farhan Asghar,Muhammad Ishfaq Hussain,Yahya Hadadi,Muhammad Aasim Rafique,Wookjin Choi,Moongu Jeon*

Main category: cs.CV

TL;DR: 提出了一种基于Swin-BART编码器-解码器的医学图像自动标注系统，通过轻量级区域注意力模块增强诊断关键区域，在ROCO数据集上实现了最先进的语义保真度。


<details>
  <summary>Details</summary>
Motivation: 将复杂的放射学图像转化为诊断性叙述，以支持报告工作流程，同时保持模型的紧凑性和可解释性。

Method: 采用Swin-BART编码器-解码器架构，集成轻量级区域注意力模块，在交叉注意力之前放大诊断关键区域。使用束搜索解码（束大小=4，长度惩罚=1.1，无重复n-gram大小=3，最大长度=128）。

Result: 在ROCO数据集上，相比基线模型显著提升了ROUGE（0.603 vs 0.356/0.255）和BERTScore（0.807 vs 0.645/0.623），同时在BLEU、CIDEr和METEOR指标上表现优异。提供了消融实验、模态分析和定性热图。

Conclusion: 所提出的设计能够生成准确、临床化的标注，并提供透明的区域归因，支持在人工监督下的安全研究使用。

Abstract: Automated medical image captioning translates complex radiological images into diagnostic narratives that can support reporting workflows. We present a Swin-BART encoder-decoder system with a lightweight regional attention module that amplifies diagnostically salient regions before cross-attention. Trained and evaluated on ROCO, our model achieves state-of-the-art semantic fidelity while remaining compact and interpretable. We report results as mean$\pm$std over three seeds and include $95\%$ confidence intervals. Compared with baselines, our approach improves ROUGE (proposed 0.603, ResNet-CNN 0.356, BLIP2-OPT 0.255) and BERTScore (proposed 0.807, BLIP2-OPT 0.645, ResNet-CNN 0.623), with competitive BLEU, CIDEr, and METEOR. We further provide ablations (regional attention on/off and token-count sweep), per-modality analysis (CT/MRI/X-ray), paired significance tests, and qualitative heatmaps that visualize the regions driving each description. Decoding uses beam search (beam size $=4$), length penalty $=1.1$, $no\_repeat\_ngram\_size$ $=3$, and max length $=128$. The proposed design yields accurate, clinically phrased captions and transparent regional attributions, supporting safe research use with a human in the loop.

</details>


### [21] [MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation](https://arxiv.org/abs/2511.10376)
*Xun Huang,Shijia Zhao,Yunxiang Wang,Xin Lu,Wanfa Zhang,Rongsheng Qu,Weixin Li,Yunhong Wang,Chenglu Wen*

Main category: cs.CV

TL;DR: 提出了M3DSG多模态3D场景图和MSGNav零样本导航系统，通过保留视觉线索和动态图像分配解决现有方法的问题，在GOAT-Bench和HM3D-OVON数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有零样本导航方法中3D场景图压缩视觉观察为纯文本关系导致的高构建成本、视觉证据丢失和词汇受限问题，实现开放词汇泛化和低训练开销。

Method: 引入M3DSG多模态3D场景图，用动态分配图像替代文本关系边；构建MSGNav系统，包含关键子图选择、自适应词汇更新、闭环推理和基于可见性的视点决策模块。

Result: 在GOAT-Bench和HM3D-OVON数据集上实现了最先进的性能表现。

Conclusion: MSGNav通过多模态场景图和专门模块有效解决了零样本导航中的关键问题，特别是最后一英里问题，为机器人导航提供了高效可靠的解决方案。

Abstract: Embodied navigation is a fundamental capability for robotic agents operating. Real-world deployment requires open vocabulary generalization and low training overhead, motivating zero-shot methods rather than task-specific RL training. However, existing zero-shot methods that build explicit 3D scene graphs often compress rich visual observations into text-only relations, leading to high construction cost, irreversible loss of visual evidence, and constrained vocabularies. To address these limitations, we introduce the Multi-modal 3D Scene Graph (M3DSG), which preserves visual cues by replacing textual relational edges with dynamically assigned images. Built on M3DSG, we propose MSGNav, a zero-shot navigation system that includes a Key Subgraph Selection module for efficient reasoning, an Adaptive Vocabulary Update module for open vocabulary support, and a Closed-Loop Reasoning module for accurate exploration reasoning. Additionally, we further identify the last-mile problem in zero-shot navigation - determining the feasible target location with a suitable final viewpoint, and propose a Visibility-based Viewpoint Decision module to explicitly resolve it. Comprehensive experimental results demonstrate that MSGNav achieves state-of-the-art performance on GOAT-Bench and HM3D-OVON datasets. The open-source code will be publicly available.

</details>


### [22] [SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation](https://arxiv.org/abs/2511.10518)
*Wei Li,Renshan Zhang,Rui Shao,Zhijian Fang,Kaiwen Zhou,Zhuotao Tian,Liqiang Nie*

Main category: cs.CV

TL;DR: 本文提出SemanticVLA框架，通过语义对齐的稀疏化和增强技术解决VLA模型在机器人操作中的感知冗余和指令-视觉对齐不足问题，显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在机器人操作中存在两个主要限制：1）感知冗余，处理无关视觉输入效率低下；2）指令-视觉对齐浅层化，阻碍动作的语义基础。

Method: 提出SemanticVLA框架，包含三个核心组件：1）语义引导的双重视觉剪枝器（SD-Pruner）进行感知稀疏化；2）语义互补的分层融合器（SH-Fuser）融合稀疏特征；3）语义条件动作耦合器（SA-Coupler）增强感知到动作的转换。

Result: 在仿真和真实世界任务中，SemanticVLA在LIBERO基准上比OpenVLA成功率提升21.1%，同时训练成本和推理延迟分别降低3.0倍和2.7倍。

Conclusion: SemanticVLA在性能和效率方面都达到了新的最先进水平，为高效的机器人操作提供了有效的解决方案。

Abstract: Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions. In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation. Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2. 2) To exploit sparsified features and integrate semantics with spatial geometry, Semantic-complementary Hierarchical Fuser (SH-Fuser) fuses dense patches and sparse tokens across SigLIP and DINOv2 for coherent representation. 3) To enhance the transformation from perception to action, Semantic-conditioned Action Coupler (SA-Coupler) replaces the conventional observation-to-DoF approach, yielding more efficient and interpretable behavior modeling for manipulation tasks. Extensive experiments on simulation and real-world tasks show that SemanticVLA sets a new SOTA in both performance and efficiency. SemanticVLA surpasses OpenVLA on LIBERO benchmark by 21.1% in success rate, while reducing training cost and inference latency by 3.0-fold and 2.7-fold.SemanticVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/SemanticVLA

</details>


### [23] [MosaicDoc: A Large-Scale Bilingual Benchmark for Visually Rich Document Understanding](https://arxiv.org/abs/2511.09919)
*Ketong Chen,Yuhao Chen,Yang Xue*

Main category: cs.CV

TL;DR: DocWeaver是一个多智能体管道，利用大语言模型自动生成MosaicDoc基准测试，这是一个大规模双语（中文和英文）的视觉丰富文档理解资源，包含72K图像和600K+问答对，用于评估模型在复杂布局文档上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要是英文中心、布局简单且任务有限，无法充分评估视觉语言模型在视觉丰富文档理解（VRDU）方面的能力，特别是处理复杂布局和密集文本的挑战。

Method: 开发了DocWeaver多智能体管道，利用大语言模型自动生成新的基准测试MosaicDoc，该资源源自报纸和杂志，具有多样复杂的布局（包括多列和非曼哈顿布局）、196个出版商的丰富风格变化，以及全面的多任务标注。

Result: 创建了MosaicDoc基准测试，包含72,000张图像和超过600,000个问答对，涵盖OCR、视觉问答、阅读顺序和定位等多任务。对最先进模型的广泛评估揭示了它们在处理真实世界文档复杂性方面的局限性。

Conclusion: MosaicDoc为视觉丰富文档理解领域提供了一个权威的基准测试，揭示了当前模型的局限性，并为未来研究指明了清晰的方向。

Abstract: Despite the rapid progress of Vision-Language Models (VLMs), their capabilities are inadequately assessed by existing benchmarks, which are predominantly English-centric, feature simplistic layouts, and support limited tasks. Consequently, they fail to evaluate model performance for Visually Rich Document Understanding (VRDU), a critical challenge involving complex layouts and dense text. To address this, we introduce DocWeaver, a novel multi-agent pipeline that leverages Large Language Models to automatically generate a new benchmark. The result is MosaicDoc, a large-scale, bilingual (Chinese and English) resource designed to push the boundaries of VRDU. Sourced from newspapers and magazines, MosaicDoc features diverse and complex layouts (including multi-column and non-Manhattan), rich stylistic variety from 196 publishers, and comprehensive multi-task annotations (OCR, VQA, reading order, and localization). With 72K images and over 600K QA pairs, MosaicDoc serves as a definitive benchmark for the field. Our extensive evaluation of state-of-the-art models on this benchmark reveals their current limitations in handling real-world document complexity and charts a clear path for future research.

</details>


### [24] [Compensating Distribution Drifts in Class-incremental Learning of Pre-trained Vision Transformers](https://arxiv.org/abs/2511.09926)
*Xuan Rao,Simian Xu,Zheng Li,Bo Zhao,Derong Liu,Mingming Ha,Cesare Alippi*

Main category: cs.CV

TL;DR: 本文提出SLDC方法来解决顺序微调中的分布漂移问题，通过潜在空间转换算子和知识蒸馏来对齐特征分布，显著提升了SeqFT在类增量学习中的性能。


<details>
  <summary>Details</summary>
Motivation: 顺序微调方法在类增量学习中存在分布漂移问题，导致先前学习类别与更新模型之间的分布不匹配，影响分类器性能。

Method: 提出SLDC方法，包含线性变体（通过正则化最小二乘学习线性算子）和弱非线性变体（使用可学习的弱非线性映射），并结合知识蒸馏来减少表示漂移。

Result: 在标准CIL基准测试中，SLDC显著提升了SeqFT性能，结合KD后达到与联合训练相当的性能水平。

Conclusion: SLDC通过补偿分布漂移有效解决了顺序微调中的关键问题，为类增量学习提供了有效的解决方案。

Abstract: Recent advances have shown that sequential fine-tuning (SeqFT) of pre-trained vision transformers (ViTs), followed by classifier refinement using approximate distributions of class features, can be an effective strategy for class-incremental learning (CIL). However, this approach is susceptible to distribution drift, caused by the sequential optimization of shared backbone parameters. This results in a mismatch between the distributions of the previously learned classes and that of the updater model, ultimately degrading the effectiveness of classifier performance over time. To address this issue, we introduce a latent space transition operator and propose Sequential Learning with Drift Compensation (SLDC). SLDC aims to align feature distributions across tasks to mitigate the impact of drift. First, we present a linear variant of SLDC, which learns a linear operator by solving a regularized least-squares problem that maps features before and after fine-tuning. Next, we extend this with a weakly nonlinear SLDC variant, which assumes that the ideal transition operator lies between purely linear and fully nonlinear transformations. This is implemented using learnable, weakly nonlinear mappings that balance flexibility and generalization. To further reduce representation drift, we apply knowledge distillation (KD) in both algorithmic variants. Extensive experiments on standard CIL benchmarks demonstrate that SLDC significantly improves the performance of SeqFT. Notably, by combining KD to address representation drift with SLDC to compensate distribution drift, SeqFT achieves performance comparable to joint training across all evaluated datasets. Code: https://github.com/raoxuan98-hash/sldc.git.

</details>


### [25] [Debiased Dual-Invariant Defense for Adversarially Robust Person Re-Identification](https://arxiv.org/abs/2511.09933)
*Yuhang Zhou,Yanxiang Zhao,Zhongyun Hua,Zhipu Liu,Zhaoquan Gu,Qing Liao,Leo Yu Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种针对行人重识别任务的对抗防御框架，通过数据平衡和双对抗自元防御两阶段方法，解决了模型偏差和复合泛化需求等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习行人重识别模型容易受到对抗攻击，而现有防御策略主要针对分类任务，未能有效解决行人重识别特有的对抗鲁棒性挑战。

Method: 提出去偏双不变防御框架：1）数据平衡阶段使用扩散模型进行数据重采样；2）双对抗自元防御阶段引入度量对抗训练和最远负样本扩展软化，以及对抗增强的自元机制。

Result: 实验表明该方法显著优于现有最先进的防御方法。

Conclusion: 该方法有效解决了行人重识别对抗防御中的模型偏差和复合泛化问题，为度量学习任务的对抗鲁棒性提供了新思路。

Abstract: Person re-identification (ReID) is a fundamental task in many real-world applications such as pedestrian trajectory tracking. However, advanced deep learning-based ReID models are highly susceptible to adversarial attacks, where imperceptible perturbations to pedestrian images can cause entirely incorrect predictions, posing significant security threats. Although numerous adversarial defense strategies have been proposed for classification tasks, their extension to metric learning tasks such as person ReID remains relatively unexplored. Moreover, the several existing defenses for person ReID fail to address the inherent unique challenges of adversarially robust ReID. In this paper, we systematically identify the challenges of adversarial defense in person ReID into two key issues: model bias and composite generalization requirements. To address them, we propose a debiased dual-invariant defense framework composed of two main phases. In the data balancing phase, we mitigate model bias using a diffusion-model-based data resampling strategy that promotes fairness and diversity in training data. In the bi-adversarial self-meta defense phase, we introduce a novel metric adversarial training approach incorporating farthest negative extension softening to overcome the robustness degradation caused by the absence of classifier. Additionally, we introduce an adversarially-enhanced self-meta mechanism to achieve dual-generalization for both unseen identities and unseen attack types. Experiments demonstrate that our method significantly outperforms existing state-of-the-art defenses.

</details>


### [26] [TSPE-GS: Probabilistic Depth Extraction for Semi-Transparent Surface Reconstruction via 3D Gaussian Splatting](https://arxiv.org/abs/2511.09944)
*Zhiyuan Xu,Nan Min,Yuhang Guo,Tong Wei*

Main category: cs.CV

TL;DR: TSPE-GS通过均匀采样透射率来建模像素级多模态不透明度和深度分布，解决3D高斯溅射在半透明表面重建中的问题，替代了先前的单峰假设，消除跨表面深度模糊。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射在速度-质量权衡方面表现良好，但在重建半透明表面时遇到困难，因为大多数方法假设每个像素只有一个深度，这在多个表面可见时会失败。

Method: TSPE-GS通过均匀采样透射率来建模像素级多模态不透明度和深度分布，逐步融合截断符号距离函数，在统一框架内分别重建外部和内部表面。该方法可泛化到其他基于高斯的重建流程，无需额外训练开销。

Result: 在公开和自收集的半透明和不透明数据集上的广泛实验表明，TSPE-GS显著改善了半透明几何重建，同时在不透明场景上保持性能。

Conclusion: TSPE-GS成功解决了3D高斯溅射在半透明表面重建中的限制，提供了一种有效的方法来处理多表面可见的情况，同时保持了对不透明场景的良好性能。

Abstract: 3D Gaussian Splatting offers a strong speed-quality trade-off but struggles to reconstruct semi-transparent surfaces because most methods assume a single depth per pixel, which fails when multiple surfaces are visible. We propose TSPE-GS (Transparent Surface Probabilistic Extraction for Gaussian Splatting), which uniformly samples transmittance to model a pixel-wise multi-modal distribution of opacity and depth, replacing the prior single-peak assumption and resolving cross-surface depth ambiguity. By progressively fusing truncated signed distance functions, TSPE-GS reconstructs external and internal surfaces separately within a unified framework. The method generalizes to other Gaussian-based reconstruction pipelines without extra training overhead. Extensive experiments on public and self-collected semi-transparent and opaque datasets show TSPE-GS significantly improves semi-transparent geometry reconstruction while maintaining performance on opaque scenes.

</details>


### [27] [Beyond Cosine Similarity Magnitude-Aware CLIP for No-Reference Image Quality Assessment](https://arxiv.org/abs/2511.09948)
*Zhicheng Liao,Dongxu Wu,Zhenshan Shi,Sijie Mai,Hanwei Zhu,Lingyu Zhu,Yuncheng Jiang,Baoliang Chen*

Main category: cs.CV

TL;DR: 本文提出了一种自适应融合框架，将CLIP图像特征的余弦相似度与特征幅值相结合，用于无参考图像质量评估，无需任务特定训练即可超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的NR-IQA方法仅使用语义相似度（余弦相似度），但忽略了CLIP图像特征幅值与感知质量之间的强相关性这一关键线索。

Method: 提出自适应融合框架：提取CLIP图像特征的绝对值，应用Box-Cox变换进行统计归一化，生成语义归一化的辅助线索；设计置信度引导的融合方案，根据相对强度自适应加权两个线索。

Result: 在多个基准IQA数据集上的广泛实验表明，该方法始终优于标准的基于CLIP的IQA和最先进的基线方法。

Conclusion: 通过结合余弦相似度和特征幅值线索，提出的自适应融合框架显著提升了无参考图像质量评估的性能，无需额外训练即可实现优越表现。

Abstract: Recent efforts have repurposed the Contrastive Language-Image Pre-training (CLIP) model for No-Reference Image Quality Assessment (NR-IQA) by measuring the cosine similarity between the image embedding and textual prompts such as "a good photo" or "a bad photo." However, this semantic similarity overlooks a critical yet underexplored cue: the magnitude of the CLIP image features, which we empirically find to exhibit a strong correlation with perceptual quality. In this work, we introduce a novel adaptive fusion framework that complements cosine similarity with a magnitude-aware quality cue. Specifically, we first extract the absolute CLIP image features and apply a Box-Cox transformation to statistically normalize the feature distribution and mitigate semantic sensitivity. The resulting scalar summary serves as a semantically-normalized auxiliary cue that complements cosine-based prompt matching. To integrate both cues effectively, we further design a confidence-guided fusion scheme that adaptively weighs each term according to its relative strength. Extensive experiments on multiple benchmark IQA datasets demonstrate that our method consistently outperforms standard CLIP-based IQA and state-of-the-art baselines, without any task-specific training.

</details>


### [28] [Robust Object Detection with Pseudo Labels from VLMs using Per-Object Co-teaching](https://arxiv.org/abs/2511.09955)
*Uday Bhaskar,Rishabh Bhattacharya,Avinash Patel,Sarthak Khoche,Praveen Anil Kulkarni,Naresh Manwani*

Main category: cs.CV

TL;DR: 提出一种利用视觉语言模型自动生成伪标签来训练高效实时目标检测器的新方法，通过基于每个目标的协同教学训练策略来减轻VLM生成标签中的噪声问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在零样本目标检测方面表现出色，但存在检测延迟和幻觉预测问题，无法直接部署。手动标注成本高昂，需要一种自动生成高质量训练标签的方法。

Method: 使用VLM自动生成伪标签，提出基于每个目标的协同教学训练策略，两个YOLO模型协作学习，根据同伴的每个目标损失值过滤不可靠的边界框。

Result: 在KITTI数据集上，方法相比基线YOLOv5m模型显著提升mAP@0.5（31.12%到46.61%），保持实时检测延迟。加入10%真实标签后达到57.97% mAP@0.5，在ACDC和BDD100k数据集上也观察到类似性能提升。

Conclusion: 该流程为自动驾驶提供了一种高效、鲁棒且可扩展的高性能目标检测器训练方法，显著减少了对昂贵人工标注的依赖。

Abstract: Foundation models, especially vision-language models (VLMs), offer compelling zero-shot object detection for applications like autonomous driving, a domain where manual labelling is prohibitively expensive. However, their detection latency and tendency to hallucinate predictions render them unsuitable for direct deployment. This work introduces a novel pipeline that addresses this challenge by leveraging VLMs to automatically generate pseudo-labels for training efficient, real-time object detectors. Our key innovation is a per-object co-teaching-based training strategy that mitigates the inherent noise in VLM-generated labels. The proposed per-object coteaching approach filters noisy bounding boxes from training instead of filtering the entire image. Specifically, two YOLO models learn collaboratively, filtering out unreliable boxes from each mini-batch based on their peers' per-object loss values. Overall, our pipeline provides an efficient, robust, and scalable approach to train high-performance object detectors for autonomous driving, significantly reducing reliance on costly human annotation. Experimental results on the KITTI dataset demonstrate that our method outperforms a baseline YOLOv5m model, achieving a significant mAP@0.5 boost ($31.12\%$ to $46.61\%$) while maintaining real-time detection latency. Furthermore, we show that supplementing our pseudo-labelled data with a small fraction of ground truth labels ($10\%$) leads to further performance gains, reaching $57.97\%$ mAP@0.5 on the KITTI dataset. We observe similar performance improvements for the ACDC and BDD100k datasets.

</details>


### [29] [Equivariant Sampling for Improving Diffusion Model-based Image Restoration](https://arxiv.org/abs/2511.09965)
*Chenxu Wu,Qingpeng Kong,Peiang Zhao,Wendi Yang,Wenxin Ma,Fenghe Tang,Zihang Jiang,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 本文提出EquS和EquS+方法，通过双采样轨迹施加等变信息，结合时间步感知调度，提升扩散模型在图像修复中的性能，不增加计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有问题无关的扩散模型图像修复方法未能充分利用扩散先验，导致性能欠佳。本文旨在解决这些方法的局限性。

Method: 提出EquS方法，通过双采样轨迹施加等变信息；进一步提出EquS+，引入时间步感知调度(TAS)，优先确定性步骤以增强确定性和采样效率。

Result: 在基准测试上的广泛实验表明，该方法与先前的问题无关DMIR方法兼容，并能显著提升其性能。

Conclusion: EquS和EquS+方法有效解决了现有问题无关DMIR方法的局限性，显著提升了图像修复性能而不增加计算成本。

Abstract: Recent advances in generative models, especially diffusion models, have significantly improved image restoration (IR) performance. However, existing problem-agnostic diffusion model-based image restoration (DMIR) methods face challenges in fully leveraging diffusion priors, resulting in suboptimal performance. In this paper, we address the limitations of current problem-agnostic DMIR methods by analyzing their sampling process and providing effective solutions. We introduce EquS, a DMIR method that imposes equivariant information through dual sampling trajectories. To further boost EquS, we propose the Timestep-Aware Schedule (TAS) and introduce EquS$^+$. TAS prioritizes deterministic steps to enhance certainty and sampling efficiency. Extensive experiments on benchmarks demonstrate that our method is compatible with previous problem-agnostic DMIR methods and significantly boosts their performance without increasing computational costs. Our code is available at https://github.com/FouierL/EquS.

</details>


### [30] [Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models](https://arxiv.org/abs/2511.09973)
*Satoshi Suzuki,Shin'ya Yamaguchi,Shoichiro Takeda,Taiga Yamane,Naoki Makishima,Naotaka Kawata,Mana Ihori,Tomohiro Tanaka,Shota Orihashi,Ryo Masumura*

Main category: cs.CV

TL;DR: 本文提出DiVE方法，通过约束预训练模型和微调模型嵌入之间的差异向量来保持几何结构，从而在ID数据上稳健微调视觉语言模型而不损害OOD和零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于对比学习的微调方法会扭曲嵌入的几何结构，限制了OOD和零样本性能。本文旨在在ID数据上稳健微调视觉语言模型，同时保持其OOD和零样本泛化能力。

Method: 提出差异向量均衡化(DiVE)方法，通过平均向量损失(AVL)和成对向量损失(PVL)约束差异向量，全局和局部地保持嵌入几何结构。AVL约束差异向量等于其加权平均值，PVL确保一致的多模态对齐。

Result: 实验表明DiVE有效保持了几何结构，在ID、OOD和零样本指标上都取得了强劲结果。

Conclusion: DiVE方法通过约束差异向量来保持几何结构，成功实现了在ID数据上稳健微调而不损害OOD和零样本泛化能力。

Abstract: Contrastive pre-trained vision-language models, such as CLIP, demonstrate strong generalization abilities in zero-shot classification by leveraging embeddings extracted from image and text encoders. This paper aims to robustly fine-tune these vision-language models on in-distribution (ID) data without compromising their generalization abilities in out-of-distribution (OOD) and zero-shot settings. Current robust fine-tuning methods tackle this challenge by reusing contrastive learning, which was used in pre-training, for fine-tuning. However, we found that these methods distort the geometric structure of the embeddings, which plays a crucial role in the generalization of vision-language models, resulting in limited OOD and zero-shot performance. To address this, we propose Difference Vector Equalization (DiVE), which preserves the geometric structure during fine-tuning. The idea behind DiVE is to constrain difference vectors, each of which is obtained by subtracting the embeddings extracted from the pre-trained and fine-tuning models for the same data sample. By constraining the difference vectors to be equal across various data samples, we effectively preserve the geometric structure. Therefore, we introduce two losses: average vector loss (AVL) and pairwise vector loss (PVL). AVL preserves the geometric structure globally by constraining difference vectors to be equal to their weighted average. PVL preserves the geometric structure locally by ensuring a consistent multimodal alignment. Our experiments demonstrate that DiVE effectively preserves the geometric structure, achieving strong results across ID, OOD, and zero-shot metrics.

</details>


### [31] [STELLAR: Scene Text Editor for Low-Resource Languages and Real-World Data](https://arxiv.org/abs/2511.09977)
*Yongdeuk Seo,Hyun-seok Min,Sungchul Choi*

Main category: cs.CV

TL;DR: STELLAR是一个用于低资源语言和真实世界数据的场景文本编辑模型，通过语言自适应字形编码器和多阶段训练策略解决现有方法的局限性，并提出新的评估指标TAS来衡量样式保持效果。


<details>
  <summary>Details</summary>
Motivation: 现有场景文本编辑方法存在三个主要问题：缺乏对低资源语言的支持、合成数据与真实数据之间的领域差距、以及缺乏合适的文本样式保持评估指标。

Method: 提出STELLAR模型，包含语言自适应字形编码器和多阶段训练策略（先在合成数据上预训练，然后在真实图像上微调），并构建了新的数据集STIPLAR用于训练和评估。

Result: 实验结果表明STELLAR在视觉一致性和识别准确性方面优于最先进模型，在TAS指标上平均比基线方法提升了2.2%。

Conclusion: STELLAR通过创新的架构设计和训练策略，有效解决了场景文本编辑中的多语言支持、领域适应和评估问题，为低资源语言的文本编辑提供了可靠解决方案。

Abstract: Scene Text Editing (STE) is the task of modifying text content in an image while preserving its visual style, such as font, color, and background. While recent diffusion-based approaches have shown improvements in visual quality, key limitations remain: lack of support for low-resource languages, domain gap between synthetic and real data, and the absence of appropriate metrics for evaluating text style preservation. To address these challenges, we propose STELLAR (Scene Text Editor for Low-resource LAnguages and Real-world data). STELLAR enables reliable multilingual editing through a language-adaptive glyph encoder and a multi-stage training strategy that first pre-trains on synthetic data and then fine-tunes on real images. We also construct a new dataset, STIPLAR(Scene Text Image Pairs of Low-resource lAnguages and Real-world data), for training and evaluation. Furthermore, we propose Text Appearance Similarity (TAS), a novel metric that assesses style preservation by independently measuring font, color, and background similarity, enabling robust evaluation even without ground truth. Experimental results demonstrate that STELLAR outperforms state-of-the-art models in visual consistency and recognition accuracy, achieving an average TAS improvement of 2.2% across languages over the baselines.

</details>


### [32] [MOBA: A Material-Oriented Backdoor Attack against LiDAR-based 3D Object Detection Systems](https://arxiv.org/abs/2511.09999)
*Saket S. Chaturvedi,Gaurav Bagwe,Lan Zhang,Pan He,Xiaoyong Yuan*

Main category: cs.CV

TL;DR: MOBA是一种针对LiDAR 3D物体检测系统的物理可实现后门攻击框架，通过建模真实世界触发器的材料特性来弥合数字-物理差距，在先进检测模型上达到93.50%的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击缺乏物理可实现性，数字触发器在真实世界中因忽略材料相关的LiDAR反射特性而失效，物理构造的触发器则未经优化导致效果差或易被检测。

Method: 1) 系统选择鲁棒触发材料（二氧化钛TiO₂）；2) 开发新颖模拟管道：角度无关的Oren-Nayar BRDF模型近似生成真实LiDAR强度，距离感知缩放机制保持空间一致性。

Result: 在先进的LiDAR和相机-LiDAR融合模型上进行广泛实验，MOBA达到93.50%的攻击成功率，比先前方法提高超过41%。

Conclusion: MOBA揭示了一类新的物理可实现威胁，强调了在真实环境中考虑材料级属性的防御措施的紧迫需求。

Abstract: LiDAR-based 3D object detection is widely used in safety-critical systems. However, these systems remain vulnerable to backdoor attacks that embed hidden malicious behaviors during training. A key limitation of existing backdoor attacks is their lack of physical realizability, primarily due to the digital-to-physical domain gap. Digital triggers often fail in real-world settings because they overlook material-dependent LiDAR reflection properties. On the other hand, physically constructed triggers are often unoptimized, leading to low effectiveness or easy detectability.This paper introduces Material-Oriented Backdoor Attack (MOBA), a novel framework that bridges the digital-physical gap by explicitly modeling the material properties of real-world triggers. MOBA tackles two key challenges in physical backdoor design: 1) robustness of the trigger material under diverse environmental conditions, 2) alignment between the physical trigger's behavior and its digital simulation. First, we propose a systematic approach to selecting robust trigger materials, identifying titanium dioxide (TiO_2) for its high diffuse reflectivity and environmental resilience. Second, to ensure the digital trigger accurately mimics the physical behavior of the material-based trigger, we develop a novel simulation pipeline that features: (1) an angle-independent approximation of the Oren-Nayar BRDF model to generate realistic LiDAR intensities, and (2) a distance-aware scaling mechanism to maintain spatial consistency across varying depths. We conduct extensive experiments on state-of-the-art LiDAR-based and Camera-LiDAR fusion models, showing that MOBA achieves a 93.50% attack success rate, outperforming prior methods by over 41%. Our work reveals a new class of physically realizable threats and underscores the urgent need for defenses that account for material-level properties in real-world environments.

</details>


### [33] [DBGroup: Dual-Branch Point Grouping for Weakly Supervised 3D Instance Segmentation](https://arxiv.org/abs/2511.10003)
*Xuexun Liu,Xiaoxu Xu,Qiudan Zhang,Lin Ma,Xu Wang*

Main category: cs.CV

TL;DR: DBGroup是一个两阶段弱监督3D实例分割框架，使用场景级标注而非点级标注，通过双分支点分组生成伪标签，并采用多轮自训练方法，在减少标注成本的同时达到竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有弱监督3D实例分割方法（如单点标注和边界框标注）仍存在的标注劳动密集、复杂度高和依赖专家标注的问题，提出更高效可扩展的场景级标注方法。

Method: 两阶段框架：第一阶段使用双分支点分组模块生成伪标签，结合粒度感知实例合并和语义选择传播策略优化标签质量；第二阶段进行多轮自训练，并采用实例掩码过滤策略处理伪标签不一致性。

Result: 在实验中，DBGroup与稀疏点级监督的3D实例分割方法相比达到竞争性性能，同时超越了最先进的场景级监督3D语义分割方法。

Conclusion: DBGroup证明了使用场景级标注进行3D实例分割的可行性，提供了一种更高效、可扩展的弱监督解决方案，显著降低了标注成本。

Abstract: Weakly supervised 3D instance segmentation is essential for 3D scene understanding, especially as the growing scale of data and high annotation costs associated with fully supervised approaches. Existing methods primarily rely on two forms of weak supervision: one-thing-one-click annotations and bounding box annotations, both of which aim to reduce labeling efforts. However, these approaches still encounter limitations, including labor-intensive annotation processes, high complexity, and reliance on expert annotators. To address these challenges, we propose \textbf{DBGroup}, a two-stage weakly supervised 3D instance segmentation framework that leverages scene-level annotations as a more efficient and scalable alternative. In the first stage, we introduce a Dual-Branch Point Grouping module to generate pseudo labels guided by semantic and mask cues extracted from multi-view images. To further improve label quality, we develop two refinement strategies: Granularity-Aware Instance Merging and Semantic Selection and Propagation. The second stage involves multi-round self-training on an end-to-end instance segmentation network using the refined pseudo-labels. Additionally, we introduce an Instance Mask Filter strategy to address inconsistencies within the pseudo labels. Extensive experiments demonstrate that DBGroup achieves competitive performance compared to sparse-point-level supervised 3D instance segmentation methods, while surpassing state-of-the-art scene-level supervised 3D semantic segmentation approaches. Code is available at https://github.com/liuxuexun/DBGroup.

</details>


### [34] [LampQ: Towards Accurate Layer-wise Mixed Precision Quantization for Vision Transformers](https://arxiv.org/abs/2511.10004)
*Minjun Kim,Jaeri Lee,Jongjin Kim,Jeongin Yun,Yongmo Kwon,U Kang*

Main category: cs.CV

TL;DR: LampQ是一种用于视觉Transformer的层级混合精度量化方法，通过类型感知的Fisher度量、整数线性规划和迭代更新，解决了现有方法在粒度、度量尺度和比特分配方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有ViT量化方法采用统一精度，忽略了不同组件对量化的敏感度差异。基于度量的混合精度量化虽然前景广阔，但面临三个主要问题：粒度粗糙、跨组件类型度量尺度不匹配、量化无关的比特分配。

Method: 提出LampQ方法：1）执行层级量化实现细粒度控制和高效加速；2）引入类型感知的Fisher度量来评估敏感度；3）通过整数线性规划优化比特分配；4）迭代更新比特宽度。

Result: 大量实验表明，在图像分类、目标检测和零样本量化等多种任务上，LampQ在量化预训练ViT模型时提供了最先进的性能。

Conclusion: LampQ通过层级混合精度量化方法，有效克服了现有ViT量化方法的局限性，在各种视觉任务中实现了优异的量化性能。

Abstract: How can we accurately quantize a pre-trained Vision Transformer model? Quantization algorithms compress Vision Transformers (ViTs) into low-bit formats, reducing memory and computation demands with minimal accuracy degradation. However, existing methods rely on uniform precision, ignoring the diverse sensitivity of ViT components to quantization. Metric-based Mixed Precision Quantization (MPQ) is a promising alternative, but previous MPQ methods for ViTs suffer from three major limitations: 1) coarse granularity, 2) mismatch in metric scale across component types, and 3) quantization-unaware bit allocation. In this paper, we propose LampQ (Layer-wise Mixed Precision Quantization for Vision Transformers), an accurate metric-based MPQ method for ViTs to overcome these limitations. LampQ performs layer-wise quantization to achieve both fine-grained control and efficient acceleration, incorporating a type-aware Fisher-based metric to measure sensitivity. Then, LampQ assigns bit-widths optimally through integer linear programming and further updates them iteratively. Extensive experiments show that LampQ provides the state-of-the-art performance in quantizing ViTs pre-trained on various tasks such as image classification, object detection, and zero-shot quantization.

</details>


### [35] [Anomagic: Crossmodal Prompt-driven Zero-shot Anomaly Generation](https://arxiv.org/abs/2511.10020)
*Yuxin Jiang,Wei Luo,Hui Zhang,Qiyu Chen,Haiming Yao,Weiming Shen,Yunkang Cao*

Main category: cs.CV

TL;DR: Anomagic是一种零样本异常生成方法，无需异常样本即可生成语义一致的异常。通过跨模态提示编码方案整合视觉和文本线索，利用上下文信息指导基于修复的生成流程，并通过对比细化策略增强异常与掩码的对齐，提升异常检测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有异常生成方法通常需要异常样本作为参考，限制了其应用范围。本文旨在开发一种无需异常样本的零样本异常生成方法，能够为任何正常类别图像生成用户定义的异常。

Method: 提出跨模态提示编码方案统一视觉和文本线索；采用基于修复的生成流程；引入对比细化策略强化异常与掩码的对齐；构建AnomVerse数据集（12,987个异常-掩码-描述三元组）用于训练。

Result: 实验表明Anomagic能生成比现有方法更真实和多样化的异常，显著提升下游异常检测性能。该方法可为任何正常类别图像生成用户定义异常，成为异常生成的通用基础模型。

Conclusion: Anomagic成功实现了零样本异常生成，无需异常样本即可生成语义一致的异常，在异常检测任务中表现出优越性能，并具备为任意正常图像生成用户定义异常的通用能力。

Abstract: We propose Anomagic, a zero-shot anomaly generation method that produces semantically coherent anomalies without requiring any exemplar anomalies. By unifying both visual and textual cues through a crossmodal prompt encoding scheme, Anomagic leverages rich contextual information to steer an inpainting-based generation pipeline. A subsequent contrastive refinement strategy enforces precise alignment between synthesized anomalies and their masks, thereby bolstering downstream anomaly detection accuracy. To facilitate training, we introduce AnomVerse, a collection of 12,987 anomaly-mask-caption triplets assembled from 13 publicly available datasets, where captions are automatically generated by multimodal large language models using structured visual prompts and template-based textual hints. Extensive experiments demonstrate that Anomagic trained on AnomVerse can synthesize more realistic and varied anomalies than prior methods, yielding superior improvements in downstream anomaly detection. Furthermore, Anomagic can generate anomalies for any normal-category image using user-defined prompts, establishing a versatile foundation model for anomaly generation.

</details>


### [36] [DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection](https://arxiv.org/abs/2511.10035)
*Feiyang Jia,Caiyan Jia,Ailin Liu,Shaoqing Xu,Qiming Xia,Lin Liu,Lei Yang,Yan Gong,Ziying Song*

Main category: cs.CV

TL;DR: DGFusion基于双引导范式，通过难度感知实例配对匹配器和双引导模块，解决了多模态3D物体检测中远距离、小尺寸和遮挡物体检测的挑战，在nuScenes数据集上实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有多模态3D物体检测方法采用单引导范式，无法处理硬实例在不同模态间信息密度的差异，这直接影响了自动驾驶系统的安全性。

Method: 提出DGFusion方法，包含难度感知实例配对匹配器(DIPM)和双引导模块，通过实例级特征匹配生成简单和困难实例对，并利用两种配对类型的优势进行有效的多模态特征融合。

Result: 在nuScenes数据集上，DGFusion相比基线方法分别提升了+1.0% mAP、+0.8% NDS和+1.3%平均召回率，在距离、尺寸、可见性和小规模训练场景下对硬实例检测均表现出稳健的性能提升。

Conclusion: DGFusion通过双引导范式有效解决了多模态3D物体检测中硬实例检测的挑战，为自动驾驶感知系统提供了更安全可靠的解决方案。

Abstract: As a critical task in autonomous driving perception systems, 3D object detection is used to identify and track key objects, such as vehicles and pedestrians. However, detecting distant, small, or occluded objects (hard instances) remains a challenge, which directly compromises the safety of autonomous driving systems. We observe that existing multi-modal 3D object detection methods often follow a single-guided paradigm, failing to account for the differences in information density of hard instances between modalities. In this work, we propose DGFusion, based on the Dual-guided paradigm, which fully inherits the advantages of the Point-guide-Image paradigm and integrates the Image-guide-Point paradigm to address the limitations of the single paradigms. The core of DGFusion, the Difficulty-aware Instance Pair Matcher (DIPM), performs instance-level feature matching based on difficulty to generate easy and hard instance pairs, while the Dual-guided Modules exploit the advantages of both pair types to enable effective multi-modal feature fusion. Experimental results demonstrate that our DGFusion outperforms the baseline methods, with respective improvements of +1.0\% mAP, +0.8\% NDS, and +1.3\% average recall on nuScenes. Extensive experiments demonstrate consistent robustness gains for hard instance detection across ego-distance, size, visibility, and small-scale training scenarios.

</details>


### [37] [LoG3D: Ultra-High-Resolution 3D Shape Modeling via Local-to-Global Partitioning](https://arxiv.org/abs/2511.10040)
*Xinran Yang,Shuichang Lai,Jiangjing Lyu,Hongjie Li,Bowen Pan,Yuanqi Li,Jie Guo,Zhou Zhengkang,Yanwen Guo*

Main category: cs.CV

TL;DR: 提出基于无符号距离场（UDF）的3D变分自编码器框架，通过局部到全局架构处理复杂拓扑结构，支持超高分辨率3D内容生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理任意拓扑结构（如开放表面和复杂内部结构）时面临挑战，基于SDF的方法需要昂贵的水密预处理且难以处理非流形几何，点云表示存在采样伪影和表面不连续问题。

Method: 使用无符号距离场（UDF）作为基础表示，提出局部到全局（LoG）架构，将UDF划分为均匀子体积（UBlocks），结合3D卷积捕获局部细节和稀疏变换器确保全局一致性，采用Pad-Average策略保证子体积边界平滑过渡。

Result: 实验证明在重建精度和生成质量方面达到最先进水平，实现了更好的表面平滑度和几何灵活性，能够扩展到2048^3的超高分辨率。

Conclusion: 所提出的基于UDF的3D VAE框架能够有效处理复杂拓扑结构，在保持几何细节的同时支持超高分辨率3D内容生成，为3D表示学习提供了新的解决方案。

Abstract: Generating high-fidelity 3D contents remains a fundamental challenge due to the complexity of representing arbitrary topologies-such as open surfaces and intricate internal structures-while preserving geometric details. Prevailing methods based on signed distance fields (SDFs) are hampered by costly watertight preprocessing and struggle with non-manifold geometries, while point-cloud representations often suffer from sampling artifacts and surface discontinuities. To overcome these limitations, we propose a novel 3D variational autoencoder (VAE) framework built upon unsigned distance fields (UDFs)-a more robust and computationally efficient representation that naturally handles complex and incomplete shapes. Our core innovation is a local-to-global (LoG) architecture that processes the UDF by partitioning it into uniform subvolumes, termed UBlocks. This architecture couples 3D convolutions for capturing local detail with sparse transformers for enforcing global coherence. A Pad-Average strategy further ensures smooth transitions at subvolume boundaries during reconstruction. This modular design enables seamless scaling to ultra-high resolutions up to 2048^3-a regime previously unattainable for 3D VAEs. Experiments demonstrate state-of-the-art performance in both reconstruction accuracy and generative quality, yielding superior surface smoothness and geometric flexibility.

</details>


### [38] [FreDFT: Frequency Domain Fusion Transformer for Visible-Infrared Object Detection](https://arxiv.org/abs/2511.10046)
*Wencong Wu,Xiuwei Zhang,Hanlin Yin,Shun Dai,Hongxi Zhang,Yanning Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种用于可见光-红外目标检测的频率域融合变换器FreDFT，通过频率域注意力机制和混合尺度频率特征融合来解决模态间信息不平衡问题，在复杂场景下提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有可见光-红外目标检测方法存在模态间信息不平衡问题，导致跨模态融合不充分，检测性能下降。同时，大多数方法在空间域使用变换器，忽略了在频率域开发变换器挖掘互补信息的优势。

Method: 提出频率域融合变换器FreDFT，包含多模态频率域注意力(MFDA)挖掘模态间互补信息，频率域前馈层(FDFFL)通过混合尺度频率特征融合增强多模态特征，跨模态全局建模模块(CGMM)消除模态信息不平衡，以及局部特征增强模块(LFEM)强化多模态局部特征表示。

Result: 在多个公共数据集上的广泛实验结果表明，FreDFT相比其他最先进方法取得了优异的性能。

Conclusion: FreDFT通过频率域变换器有效解决了可见光-红外目标检测中的模态信息不平衡问题，显著提升了检测性能。

Abstract: Visible-infrared object detection has gained sufficient attention due to its detection performance in low light, fog, and rain conditions. However, visible and infrared modalities captured by different sensors exist the information imbalance problem in complex scenarios, which can cause inadequate cross-modal fusion, resulting in degraded detection performance. \textcolor{red}{Furthermore, most existing methods use transformers in the spatial domain to capture complementary features, ignoring the advantages of developing frequency domain transformers to mine complementary information.} To solve these weaknesses, we propose a frequency domain fusion transformer, called FreDFT, for visible-infrared object detection. The proposed approach employs a novel multimodal frequency domain attention (MFDA) to mine complementary information between modalities and a frequency domain feed-forward layer (FDFFL) via a mixed-scale frequency feature fusion strategy is designed to better enhance multimodal features. To eliminate the imbalance of multimodal information, a cross-modal global modeling module (CGMM) is constructed to perform pixel-wise inter-modal feature interaction in a spatial and channel manner. Moreover, a local feature enhancement module (LFEM) is developed to strengthen multimodal local feature representation and promote multimodal feature fusion by using various convolution layers and applying a channel shuffle. Extensive experimental results have verified that our proposed FreDFT achieves excellent performance on multiple public datasets compared with other state-of-the-art methods. The code of our FreDFT is linked at https://github.com/WenCongWu/FreDFT.

</details>


### [39] [Image Aesthetic Reasoning via HCM-GRPO: Empowering Compact Model for Superior Performance](https://arxiv.org/abs/2511.10055)
*Zhiyuan Hu,Zheng Sun,Yi Wei,Long Yu*

Main category: cs.CV

TL;DR: 本文提出了一个完整的图像筛选解决方案，包括构建包含128k样本、640k图像的综合数据集，以及引入HCM-GRPO方法来提升多模态大语言模型的图像美学推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成性能显著提升，但图像筛选研究较少，且多模态大语言模型在图像美学推理方面表现不佳，主要由于缺乏数据和模型推理能力弱。

Method: 收集综合图像筛选数据集，采用多种标注方法获取高质量思维链数据；提出HCM-GRPO方法，在GRPO框架中引入硬案例挖掘策略和动态比例精度奖励。

Result: 实验表明，即使最先进的闭源MLLMs在图像美学推理中表现接近随机猜测，而使用HCM-GRPO的小模型能够超越大型开源和领先闭源模型的得分。

Conclusion: 通过提出的数据和方法论解决方案，显著提升了图像美学推理能力，证明了HCM-GRPO方法的有效性。

Abstract: The performance of image generation has been significantly improved in recent years. However, the study of image screening is rare and its performance with Multimodal Large Language Models (MLLMs) is unsatisfactory due to the lack of data and the weak image aesthetic reasoning ability in MLLMs. In this work, we propose a complete solution to address these problems in terms of data and methodology. For data, we collect a comprehensive image screening dataset with over 128k samples, about 640k images. Each sample consists of an original image, four generated images. The dataset evaluates the image aesthetic reasoning ability under four aspects: appearance deformation, physical shadow, placement layout, and extension rationality. Regarding data annotation, we investigate multiple approaches, including purely manual, fully automated, and answer-driven annotations, to acquire high-quality chains of thought (CoT) data in the most cost-effective manner. Methodologically, we introduce a Hard Cases Mining (HCM) strategy with a Dynamic Proportional Accuracy (DPA) reward into the Group Relative Policy Optimization (GRPO) framework, called HCM-GRPO. This enhanced method demonstrates superior image aesthetic reasoning capabilities compared to the original GRPO. Our experimental results reveal that even state-of-the-art closed-source MLLMs, such as GPT4o and Qwen-VL-Max, exhibit performance akin to random guessing in image aesthetic reasoning. In contrast, by leveraging the HCM-GRPO, we are able to surpass the scores of both large-scale open-source and leading closed-source models with a much smaller model.

</details>


### [40] [When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?](https://arxiv.org/abs/2511.10059)
*Qilang Ye,Wei Zeng,Meng Liu,Jie Zhang,Yupeng Hu,Zitong Yu,Yu Zhou*

Main category: cs.CV

TL;DR: 本文提出了AV-ConfuseBench基准测试，发现多模态大语言模型在音频-视觉混淆场景中表现不佳，并开发了RL-CoMM方法通过强化学习提升模型的音频-视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究多模态大语言模型是否能识别视觉存在但音频缺失的混淆对象，解决模型在音频-视觉推理中的视觉主导偏差问题。

Method: 提出RL-CoMM方法：1）引入大型音频语言模型作为参考模型生成纯音频推理，设计逐步推理奖励函数；2）引入答案中心置信度优化减少异构推理差异。

Result: 在音频-视觉问答和音频-视觉幻觉任务上，RL-CoMM相比基线模型准确率提升了10-30%，且仅需有限训练数据。

Conclusion: RL-CoMM通过强化学习和多模型协作有效缓解了多模态大语言模型的视觉主导偏差，显著提升了音频-视觉推理性能。

Abstract: Can Multimodal Large Language Models (MLLMs) discern confused objects that are visually present but audio-absent? To study this, we introduce a new benchmark, AV-ConfuseBench, which simulates an ``Audio-Visual Confusion'' scene by modifying the corresponding sound of an object in the video, e.g., mute the sounding object and ask MLLMs Is there a/an muted-object sound''. Experimental results reveal that MLLMs, such as Qwen2.5-Omni and Gemini 2.5, struggle to discriminate non-existent audio due to visually dominated reasoning. Motivated by this observation, we introduce RL-CoMM, a Reinforcement Learning-based Collaborative Multi-MLLM that is built upon the Qwen2.5-Omni foundation. RL-CoMM includes two stages: 1) To alleviate visually dominated ambiguities, we introduce an external model, a Large Audio Language Model (LALM), as the reference model to generate audio-only reasoning. Then, we design a Step-wise Reasoning Reward function that enables MLLMs to self-improve audio-visual reasoning with the audio-only reference. 2) To ensure an accurate answer prediction, we introduce Answer-centered Confidence Optimization to reduce the uncertainty of potential heterogeneous reasoning differences. Extensive experiments on audio-visual question answering and audio-visual hallucination show that RL-CoMM improves the accuracy by 10~30\% over the baseline model with limited training data. Follow: https://github.com/rikeilong/AVConfusion.

</details>


### [41] [Multivariate Gaussian Representation Learning for Medical Action Evaluation](https://arxiv.org/abs/2511.10060)
*Luming Yang,Haoxian Liu,Siqing Li,Alper Yilmaz*

Main category: cs.CV

TL;DR: 本文提出了GaussMedAct框架和CPREval-6k数据集，用于医疗动作评估，通过多元高斯表示和混合空间编码实现了92.1%的Top-1准确率，比基线方法提升了5.9%。


<details>
  <summary>Details</summary>
Motivation: 医疗视觉中的细粒度动作评估面临数据集不完整、精度要求严格以及快速动作时空动态建模不足的挑战。

Method: 提出了GaussMedAct框架，使用多元高斯表示将关节运动投影到时间尺度多维空间，分解为自适应3D高斯作为标记；采用笛卡尔和向量双流策略的混合空间编码有效利用骨骼信息。

Result: 在CPREval-6k基准测试中达到92.1%的Top-1准确率，实时推理，仅使用10%的FLOPs就比ST-GCN基线提升了+5.9%准确率；跨数据集实验验证了方法的鲁棒性。

Conclusion: 该方法在医疗动作分析中表现出色，通过自适应时空表示学习实现了高精度和高效性，在鲁棒性方面具有优势。

Abstract: Fine-grained action evaluation in medical vision faces unique challenges due to the unavailability of comprehensive datasets, stringent precision requirements, and insufficient spatiotemporal dynamic modeling of very rapid actions. To support development and evaluation, we introduce CPREval-6k, a multi-view, multi-label medical action benchmark containing 6,372 expert-annotated videos with 22 clinical labels. Using this dataset, we present GaussMedAct, a multivariate Gaussian encoding framework, to advance medical motion analysis through adaptive spatiotemporal representation learning. Multivariate Gaussian Representation projects the joint motions to a temporally scaled multi-dimensional space, and decomposes actions into adaptive 3D Gaussians that serve as tokens. These tokens preserve motion semantics through anisotropic covariance modeling while maintaining robustness to spatiotemporal noise. Hybrid Spatial Encoding, employing a Cartesian and Vector dual-stream strategy, effectively utilizes skeletal information in the form of joint and bone features. The proposed method achieves 92.1% Top-1 accuracy with real-time inference on the benchmark, outperforming the ST-GCN baseline by +5.9% accuracy with only 10% FLOPs. Cross-dataset experiments confirm the superiority of our method in robustness.

</details>


### [42] [Perceive, Act and Correct: Confidence Is Not Enough for Hyperspectral Classification](https://arxiv.org/abs/2511.10068)
*Muzhou Yang,Wuzhou Quan,Mingqiang Wei*

Main category: cs.CV

TL;DR: CABIN是一个半监督学习框架，通过认知感知的行为学习解决高光谱图像分类中置信度误导的问题，包含感知、行动和修正的闭环学习过程。


<details>
  <summary>Details</summary>
Motivation: 传统高光谱图像分类中，模型往往将高预测分数误认为正确性，缺乏不确定性意识，导致在稀疏标注或类别不平衡情况下产生确认偏差和过拟合。

Method: CABIN框架包含三个核心策略：1）感知阶段通过估计认知不确定性识别模糊区域；2）行动阶段采用不确定性引导的双重采样策略；3）修正阶段引入细粒度动态分配策略对伪标签数据进行分类处理。

Result: 实验结果表明，多种最先进方法在集成CABIN后都获得了性能提升，提高了标注效率和分类性能。

Conclusion: CABIN通过认知感知的行为学习机制有效解决了高光谱图像分类中的置信度误导问题，提高了模型在稀疏标注和类别不平衡情况下的泛化能力。

Abstract: Confidence alone is often misleading in hyperspectral image classification, as models tend to mistake high predictive scores for correctness while lacking awareness of uncertainty. This leads to confirmation bias, especially under sparse annotations or class imbalance, where models overfit confident errors and fail to generalize. We propose CABIN (Cognitive-Aware Behavior-Informed learNing), a semi-supervised framework that addresses this limitation through a closed-loop learning process of perception, action, and correction. CABIN first develops perceptual awareness by estimating epistemic uncertainty, identifying ambiguous regions where errors are likely to occur. It then acts by adopting an Uncertainty-Guided Dual Sampling Strategy, selecting uncertain samples for exploration while anchoring confident ones as stable pseudo-labels to reduce bias. To correct noisy supervision, CABIN introduces a Fine-Grained Dynamic Assignment Strategy that categorizes pseudo-labeled data into reliable, ambiguous, and noisy subsets, applying tailored losses to enhance generalization. Experimental results show that a wide range of state-of-the-art methods benefit from the integration of CABIN, with improved labeling efficiency and performance.

</details>


### [43] [VLF-MSC: Vision-Language Feature-Based Multimodal Semantic Communication System](https://arxiv.org/abs/2511.10074)
*Gwangyeon Ahn,Jiwan Seo,Joonhyuk Kang*

Main category: cs.CV

TL;DR: VLF-MSC是一种基于视觉语言特征的多模态语义通信系统，通过传输单一紧凑的视觉语言表示来同时支持接收端的图像和文本生成。


<details>
  <summary>Details</summary>
Motivation: 现有语义通信技术分别处理每种模态，导致频谱效率低下和适应性差。VLF-MSC旨在通过统一表示消除对模态特定流或重传的需求。

Method: 使用预训练视觉语言模型将源图像编码为视觉语言语义特征，通过无线信道传输。接收端基于语言模型的解码器和基于扩散的图像生成器都以此特征为条件生成描述性文本和语义对齐图像。

Result: 实验表明VLF-MSC在低信噪比下优于仅文本和仅图像的基线方法，对两种模态都实现了更高的语义准确性，同时显著减少了带宽需求。

Conclusion: VLF-MSC通过利用基础模型实现了对信道噪声的鲁棒性，同时保持语义保真度，提供了一种高效的多模态语义通信解决方案。

Abstract: We propose Vision-Language Feature-based Multimodal Semantic Communication (VLF-MSC), a unified system that transmits a single compact vision-language representation to support both image and text generation at the receiver. Unlike existing semantic communication techniques that process each modality separately, VLF-MSC employs a pre-trained vision-language model (VLM) to encode the source image into a vision-language semantic feature (VLF), which is transmitted over the wireless channel. At the receiver, a decoder-based language model and a diffusion-based image generator are both conditioned on the VLF to produce a descriptive text and a semantically aligned image. This unified representation eliminates the need for modality-specific streams or retransmissions, improving spectral efficiency and adaptability. By leveraging foundation models, the system achieves robustness to channel noise while preserving semantic fidelity. Experiments demonstrate that VLF-MSC outperforms text-only and image-only baselines, achieving higher semantic accuracy for both modalities under low SNR with significantly reduced bandwidth.

</details>


### [44] [Mitigating Error Accumulation in Co-Speech Motion Generation via Global Rotation Diffusion and Multi-Level Constraints](https://arxiv.org/abs/2511.10076)
*Xiangyue Zhang,Jianfang Li,Jianqiang Ren,Jiaxu Zhang*

Main category: cs.CV

TL;DR: GlobalDiff是一个基于扩散模型的框架，首次直接在全局关节旋转空间中进行语音驱动动作生成，通过多级约束方案解决全局旋转空间中结构先验缺失的问题，显著提升了动作生成的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有语音驱动动作生成方法通常在局部关节旋转上操作，基于骨架结构分层定义，导致生成过程中累积误差，表现为末端执行器的不稳定和不合理动作。

Method: 提出GlobalDiff框架，直接在全局关节旋转空间操作，解耦各关节预测与上游依赖关系；引入多级约束方案：关节结构约束使用虚拟锚点捕捉细粒度方向，骨架结构约束保持骨骼角度一致性，时间结构约束使用多尺度变分编码器对齐生成动作与真实时间模式。

Result: 在标准语音驱动动作基准测试上的广泛评估显示，GlobalDiff生成平滑准确的动作，在多个说话者身份下相比当前最优方法性能提升46.0%。

Conclusion: GlobalDiff通过全局旋转空间操作和多级约束方案，有效解决了分层误差累积问题，显著提升了语音驱动动作生成的质量和稳定性。

Abstract: Reliable co-speech motion generation requires precise motion representation and consistent structural priors across all joints. Existing generative methods typically operate on local joint rotations, which are defined hierarchically based on the skeleton structure. This leads to cumulative errors during generation, manifesting as unstable and implausible motions at end-effectors. In this work, we propose GlobalDiff, a diffusion-based framework that operates directly in the space of global joint rotations for the first time, fundamentally decoupling each joint's prediction from upstream dependencies and alleviating hierarchical error accumulation. To compensate for the absence of structural priors in global rotation space, we introduce a multi-level constraint scheme. Specifically, a joint structure constraint introduces virtual anchor points around each joint to better capture fine-grained orientation. A skeleton structure constraint enforces angular consistency across bones to maintain structural integrity. A temporal structure constraint utilizes a multi-scale variational encoder to align the generated motion with ground-truth temporal patterns. These constraints jointly regularize the global diffusion process and reinforce structural awareness. Extensive evaluations on standard co-speech benchmarks show that GlobalDiff generates smooth and accurate motions, improving the performance by 46.0 % compared to the current SOTA under multiple speaker identities.

</details>


### [45] [GridPrune: From "Where to Look" to "What to Select" in Visual Token Pruning for MLLMs](https://arxiv.org/abs/2511.10081)
*Yuxiang Duan,Ao Li,Yingqin Li,Luyu Li,Pengwei Wang*

Main category: cs.CV

TL;DR: GridPrune是一种用于多模态大语言模型的高效视觉token剪枝方法，采用"全局引导、局部选择"的两阶段策略，通过文本条件指导在空间区域间分配token预算，然后在每个区域内进行局部选择，显著提升模型效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉token剪枝方法主要关注"选择什么"，忽略了"看哪里"的空间分配问题，导致效率低下、位置偏差和冗余token保留。受人类视觉系统启发，需要先确定关注区域再进行细粒度选择。

Method: 提出GridPrune方法，将剪枝过程分为两步：首先使用文本条件指导动态分配token预算到不同空间区域，然后在每个预算区域内进行局部选择，替代全局Top-K机制。

Result: 在LLaVA-NeXT-7B上，GridPrune仅使用11.1%的token即可保持96.98%的完整性能，在相同剪枝率下比最佳基线方法提升2.34%。

Conclusion: GridPrune通过模拟人类视觉系统的两阶段处理策略，有效解决了现有剪枝方法在空间分配上的不足，显著提升了多模态大语言模型的效率。

Abstract: Multimodal large language models (MLLMs) have shown remarkable capabilities in a wide range of vision-language tasks. However, the large number of visual tokens introduces significant computational overhead. To address this issue, visual token pruning has emerged as a key technique for enhancing the efficiency of MLLMs. In cognitive science, humans tend to first determine which regions of a scene to attend to ("where to look") before deciding which specific elements within those regions to process in detail ("what to select"). This two-stage strategy enables the visual system to efficiently allocate attention at a coarse spatial level before performing fine-grained selection. However, existing pruning methods primarily focus on directly optimizing "what to select", typically using attention scores or similarity metrics. They rarely consider "where to look", which has been shown to lead to inefficient spatial allocation, positional bias, and the retention of irrelevant or redundant tokens. In this paper, we propose GridPrune, a method that replaces the global Top-K mechanism with a "guide-globally, select-locally" zonal selection system. GridPrune splits the pruning process into two steps: first, it uses text-conditional guidance to dynamically allocate a token budget across spatial zones; and then, it performs local selection within each budgeted zone. Experimental results demonstrate that GridPrune achieves superior performance across various MLLM architectures. On LLaVA-NeXT-7B, GridPrune retains 96.98% of the full performance while using 11.1% of the tokens, outperforming the best-performing baseline by 2.34% at the same pruning rate.

</details>


### [46] [SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition](https://arxiv.org/abs/2511.10091)
*Qilang Ye,Yu Zhou,Lian He,Jie Zhang,Xuanming Guo,Jiayu Zhang,Mingkui Tan,Weicheng Xie,Yue Sun,Tao Tan,Xiaochen Yuan,Ghada Khoriba,Zitong Yu*

Main category: cs.CV

TL;DR: SUGAR是一种结合LLMs与人体骨骼进行动作识别和描述的新范式，通过视觉-运动知识监督骨骼学习，生成离散表示供LLM理解，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索如何让LLMs理解骨骼数据并进行动作分类和描述，解决LLMs作为识别器时的两个关键问题：理解骨骼数据和区分不同动作。

Method: 提出SUGAR范式：1)使用大规模视频模型生成视觉和运动信息作为知识库；2)通过先验知识监督骨骼学习生成离散表示；3)使用预训练权重的LLM理解这些表示并生成动作目标和描述；4)提出TQP模块处理长序列骨骼信号。

Result: 在多个骨骼动作分类基准测试中验证了SUGAR的有效性，在零样本场景下比基于线性方法更具通用性。

Conclusion: SUGAR成功地将LLMs与骨骼数据结合，通过视觉-运动知识监督实现了有效的动作识别和描述，在零样本场景下表现出色。

Abstract: Large Language Models (LLMs) hold rich implicit knowledge and powerful transferability. In this paper, we explore the combination of LLMs with the human skeleton to perform action classification and description. However, when treating LLM as a recognizer, two questions arise: 1) How can LLMs understand skeleton? 2) How can LLMs distinguish among actions? To address these problems, we introduce a novel paradigm named learning Skeleton representation with visUal-motion knowledGe for Action Recognition (SUGAR). In our pipeline, we first utilize off-the-shelf large-scale video models as a knowledge base to generate visual, motion information related to actions. Then, we propose to supervise skeleton learning through this prior knowledge to yield discrete representations. Finally, we use the LLM with untouched pre-training weights to understand these representations and generate the desired action targets and descriptions. Notably, we present a Temporal Query Projection (TQP) module to continuously model the skeleton signals with long sequences. Experiments on several skeleton-based action classification benchmarks demonstrate the efficacy of our SUGAR. Moreover, experiments on zero-shot scenarios show that SUGAR is more versatile than linear-based methods.

</details>


### [47] [RobIA: Robust Instance-aware Continual Test-time Adaptation for Deep Stereo](https://arxiv.org/abs/2511.10107)
*Jueun Ko,Hyewon Park,Hyesong Choi,Dongbo Min*

Main category: cs.CV

TL;DR: 本文提出RobIA框架，用于立体深度估计中的持续测试时适应，通过动态路由的专家混合模块和鲁棒教师模型来解决域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 解决真实环境中立体深度估计面临的动态域偏移、稀疏监督和高成本标注问题，现有测试时适应方法在持续域变化下效果有限。

Method: 提出RobIA框架，包含AttEx-MoE模块（基于自注意力的动态专家路由）和Robust AdaptBN Teacher（基于PEFT的教师模型提供密集伪监督）。

Result: 在动态目标域上实现了优越的适应性能，同时保持计算效率。

Conclusion: RobIA框架通过输入特定的灵活性和广泛的监督覆盖，在域偏移下提高了泛化能力。

Abstract: Stereo Depth Estimation in real-world environments poses significant challenges due to dynamic domain shifts, sparse or unreliable supervision, and the high cost of acquiring dense ground-truth labels. While recent Test-Time Adaptation (TTA) methods offer promising solutions, most rely on static target domain assumptions and input-invariant adaptation strategies, limiting their effectiveness under continual shifts. In this paper, we propose RobIA, a novel Robust, Instance-Aware framework for Continual Test-Time Adaptation (CTTA) in stereo depth estimation. RobIA integrates two key components: (1) Attend-and-Excite Mixture-of-Experts (AttEx-MoE), a parameter-efficient module that dynamically routes input to frozen experts via lightweight self-attention mechanism tailored to epipolar geometry, and (2) Robust AdaptBN Teacher, a PEFT-based teacher model that provides dense pseudo-supervision by complementing sparse handcrafted labels. This strategy enables input-specific flexibility, broad supervision coverage, improving generalization under domain shift. Extensive experiments demonstrate that RobIA achieves superior adaptation performance across dynamic target domains while maintaining computational efficiency.

</details>


### [48] [Explicit Temporal-Semantic Modeling for Dense Video Captioning via Context-Aware Cross-Modal Interaction](https://arxiv.org/abs/2511.10134)
*Mingda Jia,Weiliang Meng,Zenghuang Fu,Yiheng Li,Qi Zeng,Yifan Zhang,Ju Xin,Rongtao Xu,Jiguang Zhang,Xiaopeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为CACMI的显式时空语义建模框架，通过跨模态帧聚合和上下文感知特征增强来解决密集视频描述中时间连贯性和语义理解不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有密集视频描述方法主要依赖隐式建模，使用帧级或碎片化视频特征，无法捕捉事件序列的时间连贯性和视觉上下文的全面语义。

Method: 提出CACMI框架，包含两个核心组件：跨模态帧聚合通过跨模态检索提取时间连贯的事件对齐文本特征；上下文感知特征增强利用查询引导注意力整合视觉动态与伪事件语义。

Result: 在ActivityNet Captions和YouCook2数据集上的大量实验表明，CACMI在密集视频描述任务上达到了最先进的性能。

Conclusion: CACMI通过显式建模视频中的潜在时间特性和文本语料库的语言语义，有效提升了密集视频描述的性能。

Abstract: Dense video captioning jointly localizes and captions salient events in untrimmed videos. Recent methods primarily focus on leveraging additional prior knowledge and advanced multi-task architectures to achieve competitive performance. However, these pipelines rely on implicit modeling that uses frame-level or fragmented video features, failing to capture the temporal coherence across event sequences and comprehensive semantics within visual contexts. To address this, we propose an explicit temporal-semantic modeling framework called Context-Aware Cross-Modal Interaction (CACMI), which leverages both latent temporal characteristics within videos and linguistic semantics from text corpus. Specifically, our model consists of two core components: Cross-modal Frame Aggregation aggregates relevant frames to extract temporally coherent, event-aligned textual features through cross-modal retrieval; and Context-aware Feature Enhancement utilizes query-guided attention to integrate visual dynamics with pseudo-event semantics. Extensive experiments on the ActivityNet Captions and YouCook2 datasets demonstrate that CACMI achieves the state-of-the-art performance on dense video captioning task.

</details>


### [49] [Right Looks, Wrong Reasons: Compositional Fidelity in Text-to-Image Generation](https://arxiv.org/abs/2511.10136)
*Mayank Vatsa,Aparna Bharati,Richa Singh*

Main category: cs.CV

TL;DR: 本文调查了当前文本到图像模型在处理逻辑组合（否定、计数和空间关系）方面的根本缺陷，揭示了模型在组合这些基本要素时性能急剧下降的问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机是发现当今领先的文本到图像模型架构存在根本性缺陷：无法处理逻辑组合，这限制了模型的实际应用能力。

Method: 通过分析三个核心要素（否定、计数和空间关系）来调查性能崩溃现象，追踪失败原因到训练数据缺失、注意力架构不适用和评估指标偏差三个关键因素。

Result: 分析显示模型在单个要素上准确，但在组合时性能急剧下降，暴露严重干扰问题。当前解决方案和简单扩展无法弥合这一差距。

Conclusion: 实现真正的组合性需要在表示和推理方面进行根本性进步，而非对现有架构进行渐进式调整。

Abstract: The architectural blueprint of today's leading text-to-image models contains a fundamental flaw: an inability to handle logical composition. This survey investigates this breakdown across three core primitives-negation, counting, and spatial relations. Our analysis reveals a dramatic performance collapse: models that are accurate on single primitives fail precipitously when these are combined, exposing severe interference. We trace this failure to three key factors. First, training data show a near-total absence of explicit negations. Second, continuous attention architectures are fundamentally unsuitable for discrete logic. Third, evaluation metrics reward visual plausibility over constraint satisfaction. By analyzing recent benchmarks and methods, we show that current solutions and simple scaling cannot bridge this gap. Achieving genuine compositionality, we conclude, will require fundamental advances in representation and reasoning rather than incremental adjustments to existing architectures.

</details>


### [50] [Split-Layer: Enhancing Implicit Neural Representation by Maximizing the Dimensionality of Feature Space](https://arxiv.org/abs/2511.10142)
*Zhicheng Cai,Hao Zhu,Linsen Chen,Qiu Shen,Xun Cao*

Main category: cs.CV

TL;DR: 提出了一种名为split-layer的新型MLP重构方法，通过将每层分成多个并行分支并使用Hadamard乘积整合输出，有效构建高维多项式空间，显著提升隐式神经表示(INR)的表征能力，同时避免计算开销的二次增长。


<details>
  <summary>Details</summary>
Motivation: 传统多层感知机(MLP)架构中低维特征空间限制了INR的表征能力，而扩大MLP宽度会导致计算和内存成本的二次增长，需要一种既能提升表征能力又避免过高计算开销的方法。

Method: 提出split-layer方法，将MLP的每一层分成多个并行分支，通过Hadamard乘积整合分支输出，构建高维多项式空间来增强特征空间维度。

Result: 实验表明split-layer显著提升了INR性能，在2D图像拟合、2D CT重建、3D形状表示和5D新视角合成等多个任务中超越了现有方法。

Conclusion: split-layer通过创新的MLP重构有效解决了INR表征能力受限的问题，在不引入过高计算成本的前提下显著提升了性能表现。

Abstract: Implicit neural representation (INR) models signals as continuous functions using neural networks, offering efficient and differentiable optimization for inverse problems across diverse disciplines. However, the representational capacity of INR defined by the range of functions the neural network can characterize, is inherently limited by the low-dimensional feature space in conventional multilayer perceptron (MLP) architectures. While widening the MLP can linearly increase feature space dimensionality, it also leads to a quadratic growth in computational and memory costs. To address this limitation, we propose the split-layer, a novel reformulation of MLP construction. The split-layer divides each layer into multiple parallel branches and integrates their outputs via Hadamard product, effectively constructing a high-degree polynomial space. This approach significantly enhances INR's representational capacity by expanding the feature space dimensionality without incurring prohibitive computational overhead. Extensive experiments demonstrate that the split-layer substantially improves INR performance, surpassing existing methods across multiple tasks, including 2D image fitting, 2D CT reconstruction, 3D shape representation, and 5D novel view synthesis.

</details>


### [51] [Decoupling Bias, Aligning Distributions: Synergistic Fairness Optimization for Deepfake Detection](https://arxiv.org/abs/2511.10150)
*Feng Ding,Wenhui Yi,Yunpeng Zhou,Xinan He,Hong Rao,Shu Hu*

Main category: cs.CV

TL;DR: 提出了一种双机制协同优化框架，通过结构公平解耦和全局分布对齐来提升深度伪造检测模型的公平性，同时保持检测精度。


<details>
  <summary>Details</summary>
Motivation: 深度伪造检测模型在不同人口统计群体（如性别、种族）上存在偏见，可能导致系统性误判，加剧数字鸿沟和社会不平等。现有公平性增强检测器往往以牺牲检测精度为代价来提升公平性。

Method: 创新性地整合了结构公平解耦和全局分布对齐：在模型架构层面解耦对人口统计群体敏感的通道，在特征层面减少整体样本分布与各人口统计群体对应分布之间的距离。

Result: 实验结果表明，与其他方法相比，该框架在保持跨领域整体检测精度的同时，提高了组间和组内公平性。

Conclusion: 提出的双机制协同优化框架能够有效解决深度伪造检测中的公平性问题，在提升公平性的同时不牺牲检测精度。

Abstract: Fairness is a core element in the trustworthy deployment of deepfake detection models, especially in the field of digital identity security. Biases in detection models toward different demographic groups, such as gender and race, may lead to systemic misjudgments, exacerbating the digital divide and social inequities. However, current fairness-enhanced detectors often improve fairness at the cost of detection accuracy. To address this challenge, we propose a dual-mechanism collaborative optimization framework. Our proposed method innovatively integrates structural fairness decoupling and global distribution alignment: decoupling channels sensitive to demographic groups at the model architectural level, and subsequently reducing the distance between the overall sample distribution and the distributions corresponding to each demographic group at the feature level. Experimental results demonstrate that, compared with other methods, our framework improves both inter-group and intra-group fairness while maintaining overall detection accuracy across domains.

</details>


### [52] [GEA: Generation-Enhanced Alignment for Text-to-Image Person Retrieval](https://arxiv.org/abs/2511.10154)
*Hao Zou,Runqing Zhang,Xue Zhou,Jianxiao Zou*

Main category: cs.CV

TL;DR: 本文提出了一种基于生成视角的生成增强对齐方法GEA，用于解决文本到图像行人检索中的模态差距问题。GEA包含两个并行模块：文本引导的令牌增强和生成中间融合，通过扩散生成图像作为中间语义表示来弥合文本与视觉模式之间的差距。


<details>
  <summary>Details</summary>
Motivation: 文本查询有时无法准确全面地反映图像内容，导致跨模态对齐效果差和对有限数据集的过拟合。文本和图像之间的固有模态差距进一步放大了这些问题，使得准确的跨模态检索更具挑战性。

Method: 提出生成增强对齐方法GEA，包含两个并行模块：(1)文本引导的令牌增强，引入扩散生成的图像作为中间语义表示；(2)生成中间融合，结合生成图像、原始图像和文本特征的交叉注意力，生成通过三元组对齐损失优化的统一表示。

Result: 在三个公开的TIPR数据集CUHK-PEDES、RSTPReid和ICFG-PEDES上进行了广泛实验，结果证明了该方法的有效性。

Conclusion: GEA方法通过生成视角有效解决了文本到图像行人检索中的模态差距问题，实验结果表明该方法具有显著效果。

Abstract: Text-to-Image Person Retrieval (TIPR) aims to retrieve person images based on natural language descriptions. Although many TIPR methods have achieved promising results, sometimes textual queries cannot accurately and comprehensively reflect the content of the image, leading to poor cross-modal alignment and overfitting to limited datasets. Moreover, the inherent modality gap between text and image further amplifies these issues, making accurate cross-modal retrieval even more challenging. To address these limitations, we propose the Generation-Enhanced Alignment (GEA) from a generative perspective. GEA contains two parallel modules: (1) Text-Guided Token Enhancement (TGTE), which introduces diffusion-generated images as intermediate semantic representations to bridge the gap between text and visual patterns. These generated images enrich the semantic representation of text and facilitate cross-modal alignment. (2) Generative Intermediate Fusion (GIF), which combines cross-attention between generated images, original images, and text features to generate a unified representation optimized by triplet alignment loss. We conduct extensive experiments on three public TIPR datasets, CUHK-PEDES, RSTPReid, and ICFG-PEDES, to evaluate the performance of GEA. The results justify the effectiveness of our method. More implementation details and extended results are available at https://github.com/sugelamyd123/Sup-for-GEA.

</details>


### [53] [Utilizing a Geospatial Foundation Model for Coastline Delineation in Small Sandy Islands](https://arxiv.org/abs/2511.10177)
*Tishya Chhabra,Manisha Bajpai,Walter Zesk,Skylar Tibbits*

Main category: cs.CV

TL;DR: 评估NASA和IBM的Prithvi-EO-2.0地理空间基础模型在小沙岛海岸线划定中的应用，使用少量训练图像即可获得高性能。


<details>
  <summary>Details</summary>
Motivation: 探索Prithvi地理空间基础模型在数据贫乏地区海岸监测中的潜力，特别是对小沙岛海岸线划定的应用。

Method: 收集并标注了225张马尔代夫岛屿多光谱图像数据集，对Prithvi模型的300M和600M参数版本进行微调，训练集规模从5到181张图像不等。

Result: 即使仅使用5张训练图像，模型也能达到高性能（F1分数0.94，IoU 0.79），展示了强大的迁移学习能力。

Conclusion: Prithvi模型具有强大的迁移学习能力，在数据贫乏地区支持海岸监测方面具有巨大潜力。

Abstract: We present an initial evaluation of NASA and IBM's Prithvi-EO-2.0 geospatial foundation model on shoreline delineation of small sandy islands using satellite images. We curated and labeled a dataset of 225 multispectral images of two Maldivian islands, which we publicly release, and fine-tuned both the 300M and 600M parameter versions of Prithvi on training subsets ranging from 5 to 181 images. Our experiments show that even with as few as 5 training images, the models achieve high performance (F1 of 0.94, IoU of 0.79). Our results demonstrate the strong transfer learning capability of Prithvi, underscoring the potential of such models to support coastal monitoring in data-poor regions.

</details>


### [54] [HeatV2X: Scalable Heterogeneous Collaborative Perception via Efficient Alignment and Interaction](https://arxiv.org/abs/2511.10211)
*Yueran Zhao,Zhang Zhang,Chao Sun,Tianze Wang,Chao Yue,Nuoran Li*

Main category: cs.CV

TL;DR: 本文提出HeatV2X框架，解决V2X协同感知中多模态异构性和可扩展性挑战，通过局部异构微调和全局协同微调实现高效特征对齐与交互。


<details>
  <summary>Details</summary>
Motivation: 随着更多智能体参与V2X协同感知，现有框架面临两个关键挑战：(1)参与智能体本质上是多模态和异构的；(2)协作框架必须可扩展以适应新智能体。前者需要有效的跨智能体特征对齐以减轻异构性损失，后者使全参数训练不切实际，凸显了可扩展适应的重要性。

Method: 提出异构适应框架HeatV2X：首先基于异构图注意力训练高性能智能体作为协作学习基础；然后设计局部异构微调和全局协同微调，前者使用异构感知适配器高效提取模态特定差异，后者使用多认知适配器增强跨智能体协作并充分利用融合潜力。

Result: 在OPV2V-H和DAIR-V2X数据集上的实验结果表明，该方法以显著减少的训练开销实现了优越的感知性能，优于现有最先进方法。

Conclusion: HeatV2X框架通过异构感知适配器和多认知适配器的设计，能够在最小训练成本下显著提升协作框架性能，有效解决了V2X协同感知中的异构性和可扩展性问题。

Abstract: Vehicle-to-Everything (V2X) collaborative perception extends sensing beyond single vehicle limits through transmission. However, as more agents participate, existing frameworks face two key challenges: (1) the participating agents are inherently multi-modal and heterogeneous, and (2) the collaborative framework must be scalable to accommodate new agents. The former requires effective cross-agent feature alignment to mitigate heterogeneity loss, while the latter renders full-parameter training impractical, highlighting the importance of scalable adaptation. To address these issues, we propose Heterogeneous Adaptation (HeatV2X), a scalable collaborative framework. We first train a high-performance agent based on heterogeneous graph attention as the foundation for collaborative learning. Then, we design Local Heterogeneous Fine-Tuning and Global Collaborative Fine-Tuning to achieve effective alignment and interaction among heterogeneous agents. The former efficiently extracts modality-specific differences using Hetero-Aware Adapters, while the latter employs the Multi-Cognitive Adapter to enhance cross-agent collaboration and fully exploit the fusion potential. These designs enable substantial performance improvement of the collaborative framework with minimal training cost. We evaluate our approach on the OPV2V-H and DAIR-V2X datasets. Experimental results demonstrate that our method achieves superior perception performance with significantly reduced training overhead, outperforming existing state-of-the-art approaches. Our implementation will be released soon.

</details>


### [55] [Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization](https://arxiv.org/abs/2511.10212)
*Ashutosh Anshul,Shreyas Gopal,Deepu Rajan,Eng Siong Chng*

Main category: cs.CV

TL;DR: 提出一种单阶段训练框架，通过结合单模态和跨模态特征的下一帧预测来增强深度伪造检测的泛化能力，并引入窗口级注意力机制来捕获预测帧与实际帧之间的差异，实现精确的时序定位。


<details>
  <summary>Details</summary>
Motivation: 现有多模态深度伪造检测方法主要关注音频-视觉不一致性，可能忽略模态内伪影，且在保持音频-视觉对齐的伪造操作上表现不佳；同时这些方法需要真实样本的预训练。

Method: 采用单阶段训练框架，结合单模态和跨模态特征的下一帧预测，引入窗口级注意力机制来捕获预测帧与实际帧之间的局部差异。

Result: 在多个基准数据集上的评估表明，该模型具有强大的泛化能力和精确的时序定位性能。

Conclusion: 所提出的单阶段训练框架能够有效提升深度伪造检测的泛化能力，并实现精确的时序定位，克服了现有方法的局限性。

Abstract: Recent multimodal deepfake detection methods designed for generalization conjecture that single-stage supervised training struggles to generalize across unseen manipulations and datasets. However, such approaches that target generalization require pretraining over real samples. Additionally, these methods primarily focus on detecting audio-visual inconsistencies and may overlook intra-modal artifacts causing them to fail against manipulations that preserve audio-visual alignment. To address these limitations, we propose a single-stage training framework that enhances generalization by incorporating next-frame prediction for both uni-modal and cross-modal features. Additionally, we introduce a window-level attention mechanism to capture discrepancies between predicted and actual frames, enabling the model to detect local artifacts around every frame, which is crucial for accurately classifying fully manipulated videos and effectively localizing deepfake segments in partially spoofed samples. Our model, evaluated on multiple benchmark datasets, demonstrates strong generalization and precise temporal localization.

</details>


### [56] [TubeRMC: Tube-conditioned Reconstruction with Mutual Constraints for Weakly-supervised Spatio-Temporal Video Grounding](https://arxiv.org/abs/2511.10241)
*Jinxuan Li,Yi Zhang,Jian-Fang Hu,Chaolei Tan,Tianming Liang,Beihao Xia*

Main category: cs.CV

TL;DR: 本文提出TubeRMC框架，通过管状条件重建与相互约束来解决弱监督时空视频定位中的目标识别和跟踪不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有的弱监督STVG方法通常采用简单的后期融合方式，生成的管状区域独立于文本描述，导致目标识别失败和跟踪不一致。

Method: 提出TubeRMC框架，使用预训练视觉定位模型生成文本条件候选管，并通过管状条件重建与时空约束进行细化。设计了三种重建策略（时间、空间、时空），并引入空间和时间提案之间的相互约束。

Result: 在VidSTG和HCSTVG两个公共基准测试中优于现有方法，可视化显示有效缓解了目标识别错误和跟踪不一致问题。

Conclusion: TubeRMC通过管状条件重建和相互约束机制，显著提升了弱监督时空视频定位的性能。

Abstract: Spatio-Temporal Video Grounding (STVG) aims to localize a spatio-temporal tube that corresponds to a given language query in an untrimmed video. This is a challenging task since it involves complex vision-language understanding and spatiotemporal reasoning. Recent works have explored weakly-supervised setting in STVG to eliminate reliance on fine-grained annotations like bounding boxes or temporal stamps. However, they typically follow a simple late-fusion manner, which generates tubes independent of the text description, often resulting in failed target identification and inconsistent target tracking. To address this limitation, we propose a Tube-conditioned Reconstruction with Mutual Constraints (\textbf{TubeRMC}) framework that generates text-conditioned candidate tubes with pre-trained visual grounding models and further refine them via tube-conditioned reconstruction with spatio-temporal constraints. Specifically, we design three reconstruction strategies from temporal, spatial, and spatio-temporal perspectives to comprehensively capture rich tube-text correspondences. Each strategy is equipped with a Tube-conditioned Reconstructor, utilizing spatio-temporal tubes as condition to reconstruct the key clues in the query. We further introduce mutual constraints between spatial and temporal proposals to enhance their quality for reconstruction. TubeRMC outperforms existing methods on two public benchmarks VidSTG and HCSTVG. Further visualization shows that TubeRMC effectively mitigates both target identification errors and inconsistent tracking.

</details>


### [57] [FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment](https://arxiv.org/abs/2511.10250)
*Yongji Zhang,Siqi Li,Yue Gao,Yu Jiang*

Main category: cs.CV

TL;DR: 本文构建了首个包含细粒度子分数和扣分标注的空中滑雪动作质量评估数据集，并提出JudgeMind方法模拟专业裁判的评分思维，通过分段评分、阶段感知特征增强融合和基于知识的等级感知解码器来提升评估性能和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有AQA方法主要基于整个视频提取特征预测分数，解释性和可靠性有限，且现有数据集缺乏细粒度的动作分数标注，特别是扣分项和子分数标注。

Method: 提出JudgeMind方法：1）将输入动作视频分段并对每段评分；2）阶段感知特征增强融合模块增强阶段特定关键区域感知和对频繁摄像机视角切换的鲁棒性；3）基于知识的等级感知解码器将可能的扣分项作为先验知识来预测更准确可靠的分数。

Result: 实验结果表明，该方法实现了最先进的性能。

Conclusion: JudgeMind方法通过模拟专业裁判的评分思维，显著提升了动作质量评估的性能和可靠性，在新建的数据集上表现出色。

Abstract: Action Quality Assessment (AQA) aims to evaluate and score sports actions, which has attracted widespread interest in recent years. Existing AQA methods primarily predict scores based on features extracted from the entire video, resulting in limited interpretability and reliability. Meanwhile, existing AQA datasets also lack fine-grained annotations for action scores, especially for deduction items and sub-score annotations. In this paper, we construct the first AQA dataset containing fine-grained sub-score and deduction annotations for aerial skiing, which will be released as a new benchmark. For the technical challenges, we propose a novel AQA method, named JudgeMind, which significantly enhances performance and reliability by simulating the judgment and scoring mindset of professional referees. Our method segments the input action video into different stages and scores each stage to enhance accuracy. Then, we propose a stage-aware feature enhancement and fusion module to boost the perception of stage-specific key regions and enhance the robustness to visual changes caused by frequent camera viewpoints switching. In addition, we propose a knowledge-based grade-aware decoder to incorporate possible deduction items as prior knowledge to predict more accurate and reliable scores. Experimental results demonstrate that our method achieves state-of-the-art performance.

</details>


### [58] [Facial-R1: Aligning Reasoning and Recognition for Facial Emotion Analysis](https://arxiv.org/abs/2511.10254)
*Jiulong Wu,Yucheng Shen,Lingyong Yan,Haixin Sun,Deguo Xia,Jizhou Huang,Min Cao*

Main category: cs.CV

TL;DR: Facial-R1是一个三阶段对齐框架，通过指令微调、强化训练和数据合成来解决面部情感分析中的幻觉推理和识别-推理不对齐问题，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统面部情感分析方法存在两个关键限制：(1) 幻觉推理 - 视觉语言模型生成看似合理但不准确的解释；(2) 情感推理与识别之间的不对齐 - 观察到的面部特征与最终标签之间连接碎片化。

Method: 提出三阶段对齐框架：1) 指令微调建立基本情感推理能力；2) 强化训练以情感和动作单元标签作为奖励信号，显式对齐生成推理过程与预测情感；3) 数据合成管道迭代利用前阶段扩展训练数据集，实现可扩展的自我改进。

Result: 构建了FEA-20K基准数据集（17,737训练和1,688测试样本），在八个标准基准测试中实现最先进的FEA性能，具有强泛化能力和鲁棒可解释性。

Conclusion: Facial-R1框架有效解决了面部情感分析中的关键挑战，通过最小监督实现了推理与识别的对齐，为细粒度情感分析提供了可靠解决方案。

Abstract: Facial Emotion Analysis (FEA) extends traditional facial emotion recognition by incorporating explainable, fine-grained reasoning. The task integrates three subtasks: emotion recognition, facial Action Unit (AU) recognition, and AU-based emotion reasoning to model affective states jointly. While recent approaches leverage Vision-Language Models (VLMs) and achieve promising results, they face two critical limitations: (1) hallucinated reasoning, where VLMs generate plausible but inaccurate explanations due to insufficient emotion-specific knowledge; and (2) misalignment between emotion reasoning and recognition, caused by fragmented connections between observed facial features and final labels. We propose Facial-R1, a three-stage alignment framework that effectively addresses both challenges with minimal supervision. First, we employ instruction fine-tuning to establish basic emotional reasoning capability. Second, we introduce reinforcement training guided by emotion and AU labels as reward signals, which explicitly aligns the generated reasoning process with the predicted emotion. Third, we design a data synthesis pipeline that iteratively leverages the prior stages to expand the training dataset, enabling scalable self-improvement of the model. Built upon this framework, we introduce FEA-20K, a benchmark dataset comprising 17,737 training and 1,688 test samples with fine-grained emotion analysis annotations. Extensive experiments across eight standard benchmarks demonstrate that Facial-R1 achieves state-of-the-art performance in FEA, with strong generalization and robust interpretability.

</details>


### [59] [H3Former: Hypergraph-based Semantic-Aware Aggregation via Hyperbolic Hierarchical Contrastive Loss for Fine-Grained Visual Classification](https://arxiv.org/abs/2511.10260)
*Yongji Zhang,Siqi Li,Kuiyang Huang,Yue Gao,Yu Jiang*

Main category: cs.CV

TL;DR: H3Former是一个新颖的token-to-region框架，利用高阶语义关系聚合局部细粒度表示，通过语义感知聚合模块和双曲层次对比损失，在细粒度视觉分类任务中取得了优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的细粒度视觉分类方法通常依赖特征选择机制或区域提议策略来定位判别性区域，但这些方法往往无法全面捕捉判别性线索，同时引入大量类别无关的冗余信息。

Method: 提出H3Former框架，包含语义感知聚合模块(SAAM)和双曲层次对比损失(HHCL)。SAAM利用多尺度上下文线索动态构建token间的加权超图，通过超图卷积捕获高阶语义依赖；HHCL在非欧几里得嵌入空间中强制执行层次语义约束。

Result: 在四个标准FGVC基准数据集上进行的全面实验验证了H3Former框架的优越性。

Conclusion: H3Former通过结构化区域级建模和高阶语义关系聚合，有效解决了细粒度视觉分类中的挑战，在多个基准数据集上表现出色。

Abstract: Fine-Grained Visual Classification (FGVC) remains a challenging task due to subtle inter-class differences and large intra-class variations. Existing approaches typically rely on feature-selection mechanisms or region-proposal strategies to localize discriminative regions for semantic analysis. However, these methods often fail to capture discriminative cues comprehensively while introducing substantial category-agnostic redundancy. To address these limitations, we propose H3Former, a novel token-to-region framework that leverages high-order semantic relations to aggregate local fine-grained representations with structured region-level modeling. Specifically, we propose the Semantic-Aware Aggregation Module (SAAM), which exploits multi-scale contextual cues to dynamically construct a weighted hypergraph among tokens. By applying hypergraph convolution, SAAM captures high-order semantic dependencies and progressively aggregates token features into compact region-level representations. Furthermore, we introduce the Hyperbolic Hierarchical Contrastive Loss (HHCL), which enforces hierarchical semantic constraints in a non-Euclidean embedding space. The HHCL enhances inter-class separability and intra-class consistency while preserving the intrinsic hierarchical relationships among fine-grained categories. Comprehensive experiments conducted on four standard FGVC benchmarks validate the superiority of our H3Former framework.

</details>


### [60] [Adaptive Residual-Update Steering for Low-Overhead Hallucination Mitigation in Large Vision Language Models](https://arxiv.org/abs/2511.10292)
*Zhengtao Zou,Ya Gao,Jiarui Guan,Bin Li,Pekka Marttinen*

Main category: cs.CV

TL;DR: RUDDER是一个低开销框架，通过单次前向传播提取视觉证据向量，并使用自适应门控机制来减少大型视觉语言模型的对象幻觉问题，在保持高效的同时达到与最先进方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有缓解对象幻觉的推理时干预方法存在计算开销大的问题，通常需要额外的前向传播，限制了在延迟敏感的实际部署中的实用性。

Method: 提出RUDDER框架，包含两个关键创新：1）CARD向量，在单次标准前向传播中从自注意力层的残差更新中提取每样本视觉证据向量；2）贝叶斯启发的自适应门控，根据模型偏离视觉上下文程度进行标记级注入。

Result: 在POPE和CHAIR等关键幻觉基准测试上的广泛实验表明，RUDDER在引入可忽略计算延迟的同时，实现了与最先进方法相当的性能。

Conclusion: RUDDER是一种实用有效的改进LVLM可靠性的方法，无需在效率上做出显著妥协。

Abstract: Large Vision-Language Models (LVLMs) often suffer from object hallucination, generating text inconsistent with visual inputs, which can critically undermine their reliability. Existing inference-time interventions to mitigate this issue present a challenging trade-off: while methods that steer internal states or adjust output logits can be effective, they often incur substantial computational overhead, typically requiring extra forward passes. This efficiency bottleneck can limit their practicality for real-world, latency-sensitive deployments. In this work, we aim to address this trade-off with Residual-Update Directed DEcoding Regulation (RUDDER), a low-overhead framework that steers LVLMs towards visually-grounded generation. RUDDER is built on two key innovations: (1) Contextual Activation Residual Direction (CARD) vector, a per-sample visual evidence vector extracted from the residual update of a self-attention layer during a single, standard forward pass. (2) A Bayesian-inspired adaptive gate that performs token-wise injection, applying a corrective signal whose strength is conditioned on the model's deviation from the visual context. Extensive experiments on key hallucination benchmarks, including POPE and CHAIR, indicate that RUDDER achieves performance comparable to state-of-the-art methods while introducing negligible computational latency, validating RUDDER as a pragmatic and effective approach for improving LVLMs' reliability without a significant compromise on efficiency.

</details>


### [61] [Rethinking Visual Information Processing in Multimodal LLMs](https://arxiv.org/abs/2511.10301)
*Dongwan Kim,Viresh Ranjan,Takashi Nagata,Arnab Dhua,Amit Kumar K C*

Main category: cs.CV

TL;DR: LLaViT将LLM同时用作语言模型和视觉编码器，通过三个关键修改解决视觉特征集成问题，显著超越LLaVA基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLaVA架构在视觉特征集成方面的固有不足，由于文本和视觉模态之间的不匹配导致效果受限。

Method: 1) 为视觉模态学习独立的QKV投影；2) 在视觉token上启用双向注意力；3) 结合全局和局部视觉表示。

Result: 在多个基准测试中显著优于LLaVA基线方法，甚至超过参数数量翻倍的模型。

Conclusion: LLaViT为视觉语言建模提供了一种更有效的方法，使LLM能够同时作为强大的视觉编码器。

Abstract: Despite the remarkable success of the LLaVA architecture for vision-language tasks, its design inherently struggles to effectively integrate visual features due to the inherent mismatch between text and vision modalities. We tackle this issue from a novel perspective in which the LLM not only serves as a language model but also a powerful vision encoder. To this end, we present LLaViT - Large Language Models as extended Vision Transformers - which enables the LLM to simultaneously function as a vision encoder through three key modifications: (1) learning separate QKV projections for vision modality, (2) enabling bidirectional attention on visual tokens, and (3) incorporating both global and local visual representations. Through extensive controlled experiments on a wide range of LLMs, we demonstrate that LLaViT significantly outperforms the baseline LLaVA method on a multitude of benchmarks, even surpassing models with double its parameter count, establishing a more effective approach to vision-language modeling.

</details>


### [62] [Depth-Consistent 3D Gaussian Splatting via Physical Defocus Modeling and Multi-View Geometric Supervision](https://arxiv.org/abs/2511.10316)
*Yu Deng,Baozhu Zhao,Junyan Su,Xiaohan Zhang,Qi Liu*

Main category: cs.CV

TL;DR: 提出了一种结合景深监督和多视角一致性监督的3D高斯泼溅框架，解决了极端深度变化场景中深度估计不准确和结构退化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法同时解决远场区域深度估计不准确和近场区域结构退化的问题，特别是在具有极端深度变化的场景中。

Method: 采用两个核心组件：1) 景深监督使用尺度恢复的单目深度估计器生成深度先验，利用散焦卷积合成物理准确的散焦图像，通过景深损失增强几何一致性；2) 多视角一致性监督使用基于LoFTR的半稠密特征匹配最小化跨视角几何误差，通过可靠匹配点的最小二乘优化强制深度一致性。

Result: 在Waymo Open Dataset上相比最先进方法实现了0.8 dB的PSNR提升，在远场和近场区域都获得了优越的深度保真度。

Conclusion: 该框架将物理成像原理与基于学习的深度正则化相结合，为城市环境中复杂深度分层提供了可扩展的解决方案。

Abstract: Three-dimensional reconstruction in scenes with extreme depth variations remains challenging due to inconsistent supervisory signals between near-field and far-field regions. Existing methods fail to simultaneously address inaccurate depth estimation in distant areas and structural degradation in close-range regions. This paper proposes a novel computational framework that integrates depth-of-field supervision and multi-view consistency supervision to advance 3D Gaussian Splatting. Our approach comprises two core components: (1) Depth-of-field Supervision employs a scale-recovered monocular depth estimator (e.g., Metric3D) to generate depth priors, leverages defocus convolution to synthesize physically accurate defocused images, and enforces geometric consistency through a novel depth-of-field loss, thereby enhancing depth fidelity in both far-field and near-field regions; (2) Multi-View Consistency Supervision employing LoFTR-based semi-dense feature matching to minimize cross-view geometric errors and enforce depth consistency via least squares optimization of reliable matched points. By unifying defocus physics with multi-view geometric constraints, our method achieves superior depth fidelity, demonstrating a 0.8 dB PSNR improvement over the state-of-the-art method on the Waymo Open Dataset. This framework bridges physical imaging principles and learning-based depth regularization, offering a scalable solution for complex depth stratification in urban environments.

</details>


### [63] [FOUND: Fourier-based von Mises Distribution for Robust Single Domain Generalization in Object Detection](https://arxiv.org/abs/2511.10352)
*Mengzhu Wang,Changyuan Deng,Shanshan Wang,Nan Yin,Long Lan,Liang Yang*

Main category: cs.CV

TL;DR: 本文提出了一种结合von Mises-Fisher分布和傅里叶变换的CLIP引导框架，用于增强单域泛化目标检测，通过建模方向特征和频率域扰动来提升跨域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的语义增强方法往往忽略了特征分布的基本结构和频率域特性，而这些对于鲁棒性至关重要。

Method: 使用von Mises-Fisher分布建模对象表示的方向特征，捕捉嵌入空间中的域不变语义结构；引入基于傅里叶的增强策略，扰动幅度和相位分量来模拟频率域中的域偏移。

Result: 在多样化天气驾驶基准测试上的广泛实验表明，该方法优于现有的最先进方法。

Conclusion: 该方法不仅保留了CLIP的语义对齐优势，还丰富了跨域的特征多样性和结构一致性。

Abstract: Single Domain Generalization (SDG) for object detection aims to train a model on a single source domain that can generalize effectively to unseen target domains. While recent methods like CLIP-based semantic augmentation have shown promise, they often overlook the underlying structure of feature distributions and frequency-domain characteristics that are critical for robustness. In this paper, we propose a novel framework that enhances SDG object detection by integrating the von Mises-Fisher (vMF) distribution and Fourier transformation into a CLIP-guided pipeline. Specifically, we model the directional features of object representations using vMF to better capture domain-invariant semantic structures in the embedding space. Additionally, we introduce a Fourier-based augmentation strategy that perturbs amplitude and phase components to simulate domain shifts in the frequency domain, further improving feature robustness. Our method not only preserves the semantic alignment benefits of CLIP but also enriches feature diversity and structural consistency across domains. Extensive experiments on the diverse weather-driving benchmark demonstrate that our approach outperforms the existing state-of-the-art method.

</details>


### [64] [DermAI: Clinical dermatology acquisition through quality-driven image collection for AI classification in mobile](https://arxiv.org/abs/2511.10367)
*Thales Bezerra,Emanoel Thyago,Kelvin Cunha,Rodrigo Abreu,Fábio Papais,Francisco Mauro,Natália Lopes,Érico Medeiros,Jéssica Guido,Shirley Cruz,Paulo Borba,Tsang Ing Ren*

Main category: cs.CV

TL;DR: DermAI是一个基于智能手机的轻量级应用，可在常规咨询中实时捕获、注释和分类皮肤病变，解决了AI皮肤病学中数据集偏见、图像质量变化和验证有限的问题。


<details>
  <summary>Details</summary>
Motivation: AI皮肤病学的采用受到偏见数据集、可变图像质量和有限验证的限制，需要开发能够适应不同皮肤色调、种族和设备的解决方案。

Method: 开发DermAI应用，在设备上进行质量检查和本地模型适配，创建包含广泛皮肤色调、种族和来源设备的临床数据集。

Result: 在初步实验中，基于公共数据集训练的模型无法泛化到我们的样本，而使用本地数据进行微调提高了性能。

Conclusion: 结果强调了标准化、多样化数据收集的重要性，这些数据应与医疗需求对齐并面向机器学习开发。

Abstract: AI-based dermatology adoption remains limited by biased datasets, variable image quality, and limited validation. We introduce DermAI, a lightweight, smartphone-based application that enables real-time capture, annotation, and classification of skin lesions during routine consultations. Unlike prior dermoscopy-focused tools, DermAI performs on-device quality checks, and local model adaptation. The DermAI clinical dataset, encompasses a wide range of skin tones, ethinicity and source devices. In preliminary experiments, models trained on public datasets failed to generalize to our samples, while fine-tuning with local data improved performance. These results highlight the importance of standardized, diverse data collection aligned with healthcare needs and oriented to machine learning development.

</details>


### [65] [SHRUG-FM: Reliability-Aware Foundation Models for Earth Observation](https://arxiv.org/abs/2511.10370)
*Kai-Hendrik Cohrs,Zuzanna Osika,Maria Gonzalez-Calabuig,Vishal Nedungadi,Ruben Cartuyvels,Steffen Knoblauch,Joppe Massant,Shruti Nath,Patrick Ebel,Vasileios Sitokonstantinou*

Main category: cs.CV

TL;DR: SHRUG-FM是一个用于地球观测地理空间基础模型的可靠性感知预测框架，通过整合输入空间OOD检测、嵌入空间OOD检测和任务特定预测不确定性三个信号，提高模型在预训练数据代表性不足环境中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 地理空间基础模型在预训练期间代表性不足的环境中往往表现不可靠，需要开发能够识别和应对这些不可靠预测的方法。

Method: SHRUG-FM框架整合三种互补信号：输入空间的OOD检测、嵌入空间的OOD检测和任务特定的预测不确定性，应用于烧伤疤痕分割任务。

Result: SHRUG-FM显示OOD分数与特定环境条件下较低性能相关，基于不确定性的标志有助于丢弃许多表现不佳的预测。失败集中在某些地理区域，如低海拔区域和大型河流区域，这可能是由于预训练数据代表性不足所致。

Conclusion: SHRUG-FM为在气候敏感应用中更安全、更可解释地部署地理空间基础模型提供了途径，有助于弥合基准性能与现实世界可靠性之间的差距。

Abstract: Geospatial foundation models for Earth observation often fail to perform reliably in environments underrepresented during pretraining. We introduce SHRUG-FM, a framework for reliability-aware prediction that integrates three complementary signals: out-of-distribution (OOD) detection in the input space, OOD detection in the embedding space and task-specific predictive uncertainty. Applied to burn scar segmentation, SHRUG-FM shows that OOD scores correlate with lower performance in specific environmental conditions, while uncertainty-based flags help discard many poorly performing predictions. Linking these flags to land cover attributes from HydroATLAS shows that failures are not random but concentrated in certain geographies, such as low-elevation zones and large river areas, likely due to underrepresentation in pretraining data. SHRUG-FM provides a pathway toward safer and more interpretable deployment of GFMs in climate-sensitive applications, helping bridge the gap between benchmark performance and real-world reliability.

</details>


### [66] [Fragile by Design: On the Limits of Adversarial Defenses in Personalized Generation](https://arxiv.org/abs/2511.10382)
*Zhen Chen,Yi Zhang,Xiangyu Yin,Chengxuan Qin,Xingyu Zhao,Xiaowei Huang,Wenjie Ruan*

Main category: cs.CV

TL;DR: 本文分析了当前针对DreamBooth等个性化AI应用的身份泄露防御方法（如Anti-DreamBooth）的局限性，发现现有方法存在可感知的伪影和易被简单过滤器清除的脆弱性，并提出了AntiDB_Purify评估框架来系统评估防御方法在净化威胁下的有效性。


<details>
  <summary>Details</summary>
Motivation: 个性化AI应用如DreamBooth虽然能生成定制内容，但也带来严重的隐私风险，特别是面部身份泄露。现有防御机制试图通过注入对抗性扰动来防止成功个性化，但这些方法存在被忽视的关键限制。

Method: 提出了AntiDB_Purify评估框架，系统评估现有防御方法在现实净化威胁下的表现，包括传统图像过滤器和对抗性净化方法。

Result: 结果显示，当前所有防御方法在净化威胁下都无法保持其保护效果，现有防御只提供了虚假的安全感。

Conclusion: 当前防御方法存在严重脆弱性，迫切需要开发更不可察觉和鲁棒的保护机制来保护个性化生成中的用户身份安全。

Abstract: Personalized AI applications such as DreamBooth enable the generation of customized content from user images, but also raise significant privacy concerns, particularly the risk of facial identity leakage. Recent defense mechanisms like Anti-DreamBooth attempt to mitigate this risk by injecting adversarial perturbations into user photos to prevent successful personalization. However, we identify two critical yet overlooked limitations of these methods. First, the adversarial examples often exhibit perceptible artifacts such as conspicuous patterns or stripes, making them easily detectable as manipulated content. Second, the perturbations are highly fragile, as even a simple, non-learned filter can effectively remove them, thereby restoring the model's ability to memorize and reproduce user identity. To investigate this vulnerability, we propose a novel evaluation framework, AntiDB_Purify, to systematically evaluate existing defenses under realistic purification threats, including both traditional image filters and adversarial purification. Results reveal that none of the current methods maintains their protective effectiveness under such threats. These findings highlight that current defenses offer a false sense of security and underscore the urgent need for more imperceptible and robust protections to safeguard user identity in personalized generation.

</details>


### [67] [SAMIRO: Spatial Attention Mutual Information Regularization with a Pre-trained Model as Oracle for Lane Detection](https://arxiv.org/abs/2511.10385)
*Hyunjong Lee,Jangho Lee,Jaekoo Lee*

Main category: cs.CV

TL;DR: 提出SAMIRO方法，通过预训练模型作为Oracle，使用空间注意力互信息正则化来提升车道线检测性能，保持领域无关的空间信息。


<details>
  <summary>Details</summary>
Motivation: 现实环境中的背景杂乱、光照变化和遮挡等挑战对车道线检测造成显著障碍，特别是依赖数据驱动方法需要大量数据收集和标注工作。需要利用周围车道和物体的上下文和全局信息。

Method: 提出SAMIRO方法，使用预训练模型作为Oracle，通过空间注意力互信息正则化来转移知识，同时保持领域无关的空间信息。具有即插即用特性，可集成到各种最先进的车道线检测方法中。

Result: 在CULane、Tusimple和LLAMAS等主要基准测试上进行广泛实验，结果表明SAMIRO在不同模型和数据集上都能持续提升性能。

Conclusion: SAMIRO方法通过知识转移和空间信息保持，有效提升了车道线检测在各种挑战性环境下的性能，具有广泛的适用性。

Abstract: Lane detection is an important topic in the future mobility solutions. Real-world environmental challenges such as background clutter, varying illumination, and occlusions pose significant obstacles to effective lane detection, particularly when relying on data-driven approaches that require substantial effort and cost for data collection and annotation. To address these issues, lane detection methods must leverage contextual and global information from surrounding lanes and objects. In this paper, we propose a Spatial Attention Mutual Information Regularization with a pre-trained model as an Oracle, called SAMIRO. SAMIRO enhances lane detection performance by transferring knowledge from a pretrained model while preserving domain-agnostic spatial information. Leveraging SAMIRO's plug-and-play characteristic, we integrate it into various state-of-the-art lane detection approaches and conduct extensive experiments on major benchmarks such as CULane, Tusimple, and LLAMAS. The results demonstrate that SAMIRO consistently improves performance across different models and datasets. The code will be made available upon publication.

</details>


### [68] [MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns](https://arxiv.org/abs/2511.10390)
*Jiarui Zhang,Yuliang Liu,Zijun Wu,Guosheng Pang,Zhili Ye,Yupei Zhong,Junteng Ma,Tao Wei,Haiyang Xu,Weikai Chen,Zeen Wang,Qiangjun Ji,Fanxi Zhou,Qi Zhang,Yuanrui Hu,Jiahao Liu,Zhang Li,Ziyang Zhang,Qiang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: MonkeyOCR v1.5是一个统一的视觉语言框架，通过两阶段解析流程增强文档布局理解和内容识别，在复杂文档场景下实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界文档通常具有复杂的布局，包含多级表格、嵌入式图像或公式以及跨页结构，这对现有OCR系统构成挑战。

Method: 采用两阶段解析流程：第一阶段使用大型多模态模型联合预测文档布局和阅读顺序；第二阶段在检测区域内执行文本、公式和表格的局部识别。针对复杂表格结构，提出基于视觉一致性的强化学习方案和两个专门模块：图像解耦表格解析和类型引导表格合并。

Result: 在OmniDocBench v1.5上的综合实验表明，MonkeyOCR v1.5实现了最先进的性能，优于PPOCR-VL和MinerU 2.5，在视觉复杂文档场景中表现出卓越的鲁棒性。

Conclusion: MonkeyOCR v1.5通过统一的视觉语言框架和专门设计的模块，有效解决了复杂文档解析的挑战，在布局理解和内容识别方面都取得了显著改进。

Abstract: Document parsing is a core task in document intelligence, supporting applications such as information extraction, retrieval-augmented generation, and automated document analysis. However, real-world documents often feature complex layouts with multi-level tables, embedded images or formulas, and cross-page structures, which remain challenging for existing OCR systems. We introduce MonkeyOCR v1.5, a unified vision-language framework that enhances both layout understanding and content recognition through a two-stage parsing pipeline. The first stage employs a large multimodal model to jointly predict document layout and reading order, leveraging visual information to ensure structural and sequential consistency. The second stage performs localized recognition of text, formulas, and tables within detected regions, maintaining high visual fidelity while reducing error propagation. To address complex table structures, we propose a visual consistency-based reinforcement learning scheme that evaluates recognition quality via render-and-compare alignment, improving structural accuracy without manual annotations. Additionally, two specialized modules, Image-Decoupled Table Parsing and Type-Guided Table Merging, are introduced to enable reliable parsing of tables containing embedded images and reconstruction of tables crossing pages or columns. Comprehensive experiments on OmniDocBench v1.5 demonstrate that MonkeyOCR v1.5 achieves state-of-the-art performance, outperforming PPOCR-VL and MinerU 2.5 while showing exceptional robustness in visually complex document scenarios.

</details>


### [69] [LLM-YOLOMS: Large Language Model-based Semantic Interpretation and Fault Diagnosis for Wind Turbine Components](https://arxiv.org/abs/2511.10394)
*Yaru Li,Yanxue Wang,Meng Li,Xinming Li,Jianbo Feng*

Main category: cs.CV

TL;DR: 提出了一种结合YOLOMS目标检测和大型语言模型的集成框架，用于风力涡轮机故障的智能分析和诊断，提高故障检测准确性和诊断结果的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有风力涡轮机故障检测方法主要局限于视觉识别，缺乏语义可解释性，无法支持维护决策制定。

Method: 使用YOLOMS进行多尺度检测和滑动窗口裁剪以增强故障特征提取，通过轻量级键值映射模块将视觉输出转换为结构化文本表示，再利用领域调优的LLM进行语义推理生成可解释的故障分析和维护建议。

Result: 在真实数据集上的实验显示，该框架实现了90.6%的故障检测准确率，生成维护报告的平均准确率达到89%。

Conclusion: 该框架显著提高了诊断结果的可解释性，为风力涡轮机的运维提供了实用的决策支持。

Abstract: The health condition of wind turbine (WT) components is crucial for ensuring stable and reliable operation. However, existing fault detection methods are largely limited to visual recognition, producing structured outputs that lack semantic interpretability and fail to support maintenance decision-making. To address these limitations, this study proposes an integrated framework that combines YOLOMS with a large language model (LLM) for intelligent fault analysis and diagnosis. Specifically, YOLOMS employs multi-scale detection and sliding-window cropping to enhance fault feature extraction, while a lightweight key-value (KV) mapping module bridges the gap between visual outputs and textual inputs. This module converts YOLOMS detection results into structured textual representations enriched with both qualitative and quantitative attributes. A domain-tuned LLM then performs semantic reasoning to generate interpretable fault analyses and maintenance recommendations. Experiments on real-world datasets demonstrate that the proposed framework achieves a fault detection accuracy of 90.6\% and generates maintenance reports with an average accuracy of 89\%, thereby improving the interpretability of diagnostic results and providing practical decision support for the operation and maintenance of wind turbines.

</details>


### [70] [3DFETUS: Standardizing Fetal Facial Planes in 3D Ultrasound](https://arxiv.org/abs/2511.10412)
*Alomar Antonia,Rubio Ricardo,Albaiges Gerard,Salort-Benejam Laura,Caminal Julia,Prat Maria,Rueda Carolina,Cortes Berta,Piella Gemma,Sukno Federico*

Main category: cs.CV

TL;DR: 本文提出了GT++算法和3DFETUS深度学习模型，用于从3D胎儿超声体积中自动定位标准面部平面，解决了传统超声检查中因胎儿运动、方向变化和操作者依赖性导致的挑战。


<details>
  <summary>Details</summary>
Motivation: 常规胎儿超声检查中获取标准面部平面具有挑战性，主要由于胎儿运动、方向变化和操作者专业知识差异，这些因素导致不一致性、检查时间增加和潜在诊断偏差。

Method: 1) GT++算法：使用标注的解剖标志从3D超声体积中估计标准面部平面；2) 3DFETUS深度学习模型：在3D胎儿超声体积中自动化和标准化平面定位。

Result: 方法在3D超声体积上实现了平均平移误差4.13毫米和平均旋转误差7.93度每平面，优于其他最先进方法。临床评估证实了GT++和3DFETUS的有效性，在平面估计精度方面显示出统计学显著改进。

Conclusion: 提出的方法通过自动化和标准化面部平面定位，显著提高了3D胎儿超声检查的准确性和效率，为临床实践提供了可靠的技术支持。

Abstract: Acquiring standard facial planes during routine fetal ultrasound (US) examinations is often challenging due to fetal movement, variability in orientation, and operator-dependent expertise. These factors contribute to inconsistencies, increased examination time, and potential diagnostic bias.
  To address these challenges in the context of facial assessment, we present: 1) GT++, a robust algorithm that estimates standard facial planes from 3D US volumes using annotated anatomical landmarks; and 2) 3DFETUS, a deep learning model that automates and standardizes their localization in 3D fetal US volumes.
  We evaluated our methods both qualitatively, through expert clinical review, and quantitatively. The proposed approach achieved a mean translation error of 4.13 mm and a mean rotation error of 7.93 degrees per plane, outperforming other state-of-the-art methods on 3D US volumes. Clinical assessments further confirmed the effectiveness of both GT++ and 3DFETUS, demonstrating statistically significant improvements in plane estimation accuracy.

</details>


### [71] [RodEpil: A Video Dataset of Laboratory Rodents for Seizure Detection and Benchmark Evaluation](https://arxiv.org/abs/2511.10431)
*Daniele Perlo,Vladimir Despotovic,Selma Boudissa,Sang-Yoon Kim,Petr Nazarov,Yanrong Zhang,Max Wintermark,Olivier Keunen*

Main category: cs.CV

TL;DR: 本文介绍了一个用于自动检测实验室啮齿动物惊厥事件的视频数据集，包含10,101个阴性样本和2,952个阳性样本，使用TimeSformer架构实现了97%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 开发非侵入性、基于视频的监测方法，支持临床前癫痫研究的可重复研究。

Method: 使用TimeSformer视频分类器进行五折交叉验证，采用严格的主体分区以防止数据泄漏。

Result: TimeSformer架构能够区分惊厥和正常活动，平均F1分数达到97%。

Conclusion: 该数据集和基线代码已公开发布，为临床前癫痫研究中的非侵入性视频监测提供了支持。

Abstract: We introduce a curated video dataset of laboratory rodents for automatic detection of convulsive events. The dataset contains short (10~s) top-down and side-view video clips of individual rodents, labeled at clip level as normal activity or seizure. It includes 10,101 negative samples and 2,952 positive samples collected from 19 subjects. We describe the data curation, annotation protocol and preprocessing pipeline, and report baseline experiments using a transformer-based video classifier (TimeSformer). Experiments employ five-fold cross-validation with strict subject-wise partitioning to prevent data leakage (no subject appears in more than one fold). Results show that the TimeSformer architecture enables discrimination between seizure and normal activity with an average F1-score of 97%. The dataset and baseline code are publicly released to support reproducible research on non-invasive, video-based monitoring in preclinical epilepsy research. RodEpil Dataset access - DOI: 10.5281/zenodo.17601357

</details>


### [72] [Histology-informed tiling of whole tissue sections improves the interpretability and predictability of cancer relapse and genetic alterations](https://arxiv.org/abs/2511.10432)
*Willem Bonnaffé,Yang Hu,Andrea Chatrian,Mengran Fan,Stefano Malacrino,Sandy Figiel,CRUK ICGC Prostate Group,Srinivasa R. Rao,Richard Colling,Richard J. Bryant,Freddie C. Hamdy,Dan J. Woodcock,Ian G. Mills,Clare Verrill,Jens Rittscher*

Main category: cs.CV

TL;DR: HIT方法通过语义分割从全切片图像中提取腺体作为生物学意义明确的输入补丁，用于多实例学习和表型分析，显著提升了癌症分级和基因变异的检测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统数字病理学管道依赖基于网格的切片方法，忽略了组织结构，引入了不相关信息并限制了可解释性。

Method: 使用语义分割从全切片图像中提取腺体作为生物学意义明确的输入补丁，应用于多实例学习和表型分析。

Result: 在137个样本上训练，腺体水平Dice得分为0.83±0.17；从760个WSI中提取38万个腺体，将MIL模型检测EMT和MYC相关基因CNV的AUC提高了10%；发现15个腺体簇，多个与癌症复发、致癌突变和高Gleason评分相关。

Conclusion: HIT通过聚焦生物学意义明确的结构，提高了MIL预测的准确性和可解释性，同时简化了特征提取的计算过程。

Abstract: Histopathologists establish cancer grade by assessing histological structures, such as glands in prostate cancer. Yet, digital pathology pipelines often rely on grid-based tiling that ignores tissue architecture. This introduces irrelevant information and limits interpretability. We introduce histology-informed tiling (HIT), which uses semantic segmentation to extract glands from whole slide images (WSIs) as biologically meaningful input patches for multiple-instance learning (MIL) and phenotyping. Trained on 137 samples from the ProMPT cohort, HIT achieved a gland-level Dice score of 0.83 +/- 0.17. By extracting 380,000 glands from 760 WSIs across ICGC-C and TCGA-PRAD cohorts, HIT improved MIL models AUCs by 10% for detecting copy number variation (CNVs) in genes related to epithelial-mesenchymal transitions (EMT) and MYC, and revealed 15 gland clusters, several of which were associated with cancer relapse, oncogenic mutations, and high Gleason. Therefore, HIT improved the accuracy and interpretability of MIL predictions, while streamlining computations by focussing on biologically meaningful structures during feature extraction.

</details>


### [73] [OpenSR-SRGAN: A Flexible Super-Resolution Framework for Multispectral Earth Observation Data](https://arxiv.org/abs/2511.10461)
*Simon Donike,Cesar Aybar,Julio Contreras,Luis Gómez-Chova*

Main category: cs.CV

TL;DR: OpenSR-SRGAN是一个用于地球观测中单图像超分辨率的开源模块化框架，提供SRGAN风格模型的统一实现，通过配置文件简化模型配置、扩展和应用。


<details>
  <summary>Details</summary>
Motivation: 降低研究人员和实践者使用SRGAN进行超分辨率实验的门槛，提供可复现的模型比较和部署能力。

Method: 采用配置驱动的工作流程，通过简洁的配置文件暴露生成器、判别器、损失函数和训练计划，支持多光谱卫星数据如Sentinel-2。

Result: 开发了一个实用的工具和基准实现，包含遥感场景的即用配置、对抗训练的合理默认设置，以及日志记录、验证和大场景推理的内置钩子。

Conclusion: OpenSR-SRGAN通过将基于GAN的超分辨率转变为配置驱动的工作流程，为地球观测数据集的超分辨率管道部署提供了便利。

Abstract: We present OpenSR-SRGAN, an open and modular framework for single-image super-resolution in Earth Observation. The software provides a unified implementation of SRGAN-style models that is easy to configure, extend, and apply to multispectral satellite data such as Sentinel-2. Instead of requiring users to modify model code, OpenSR-SRGAN exposes generators, discriminators, loss functions, and training schedules through concise configuration files, making it straightforward to switch between architectures, scale factors, and band setups. The framework is designed as a practical tool and benchmark implementation rather than a state-of-the-art model. It ships with ready-to-use configurations for common remote sensing scenarios, sensible default settings for adversarial training, and built-in hooks for logging, validation, and large-scene inference. By turning GAN-based super-resolution into a configuration-driven workflow, OpenSR-SRGAN lowers the entry barrier for researchers and practitioners who wish to experiment with SRGANs, compare models in a reproducible way, and deploy super-resolution pipelines across diverse Earth-observation datasets.

</details>


### [74] [Utility of Pancreas Surface Lobularity as a CT Biomarker for Opportunistic Screening of Type 2 Diabetes](https://arxiv.org/abs/2511.10484)
*Tejas Sudharshan Mathai,Anisa V. Prasad,Xinya Wang,Praveen T. S. Balamuralikrishna,Yan Zhuang,Abhinav Suri,Jianfei Liu,Perry J. Pickhardt,Ronald M. Summers*

Main category: cs.CV

TL;DR: 本研究开发了一种全自动方法，利用深度学习模型从CT图像中分割胰腺并检测胰腺表面分叶度（PSL），发现PSL在2型糖尿病患者中显著升高，可用于糖尿病筛查和早期预测。


<details>
  <summary>Details</summary>
Motivation: 2型糖尿病早期检测至关重要，但胰腺表面分叶度在糖尿病患者中的作用尚未充分研究。本研究旨在开发自动化方法来探索PSL与T2DM的关联。

Method: 使用四种深度学习模型在584名患者内部数据集上分割胰腺，自动检测PSL，并建立多变量模型利用CT生物标志物预测T2DM。

Result: 糖尿病患者PSL显著高于非糖尿病患者（4.26±8.32 vs 3.19±3.62，p=0.01）。PancAP模型获得最高Dice得分0.79±0.17和最低ASSD误差1.94±2.63mm。多变量模型预测T2DM的AUC为0.90，敏感性66.7%，特异性91.9%。

Conclusion: PSL对2型糖尿病筛查有用，可能有助于预测T2DM的早期发病，为糖尿病早期诊断提供了新的影像学生物标志物。

Abstract: Type 2 Diabetes Mellitus (T2DM) is a chronic metabolic disease that affects millions of people worldwide. Early detection is crucial as it can alter pancreas function through morphological changes and increased deposition of ectopic fat, eventually leading to organ damage. While studies have shown an association between T2DM and pancreas volume and fat content, the role of increased pancreatic surface lobularity (PSL) in patients with T2DM has not been fully investigated. In this pilot work, we propose a fully automated approach to delineate the pancreas and other abdominal structures, derive CT imaging biomarkers, and opportunistically screen for T2DM. Four deep learning-based models were used to segment the pancreas in an internal dataset of 584 patients (297 males, 437 non-diabetic, age: 45$\pm$15 years). PSL was automatically detected and it was higher for diabetic patients (p=0.01) at 4.26 $\pm$ 8.32 compared to 3.19 $\pm$ 3.62 for non-diabetic patients. The PancAP model achieved the highest Dice score of 0.79 $\pm$ 0.17 and lowest ASSD error of 1.94 $\pm$ 2.63 mm (p$<$0.05). For predicting T2DM, a multivariate model trained with CT biomarkers attained 0.90 AUC, 66.7\% sensitivity, and 91.9\% specificity. Our results suggest that PSL is useful for T2DM screening and could potentially help predict the early onset of T2DM.

</details>


### [75] [SPOT: Sparsification with Attention Dynamics via Token Relevance in Vision Transformers](https://arxiv.org/abs/2511.10488)
*Oded Schlesinger,Amirhossein Farzam,J. Matias Di Martino,Guillermo Sapiro*

Main category: cs.CV

TL;DR: SPOT是一个用于Vision Transformers的早期冗余令牌检测框架，通过分析令牌嵌入、交互和注意力动态来识别不重要令牌，实现计算效率提升达40%且不损失性能。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers的计算需求随令牌数量呈二次方增长，需要更高效的方法来减少计算负担。

Method: 利用令牌嵌入、交互和跨层注意力动态来推断令牌重要性，采用轻量级预测器实现输入特定的令牌优先级排序。

Result: 相比标准ViT实现了高达40%的计算效率提升，同时保持甚至提高了准确率。

Conclusion: SPOT提供了一种可适应不同资源约束的高效令牌稀疏化方法，可轻松集成到各种ViT架构中。

Abstract: While Vision Transformers (ViT) have demonstrated remarkable performance across diverse tasks, their computational demands are substantial, scaling quadratically with the number of processed tokens. Compact attention representations, reflecting token interaction distributions, can guide early detection and reduction of less salient tokens prior to attention computation. Motivated by this, we present SParsification with attentiOn dynamics via Token relevance (SPOT), a framework for early detection of redundant tokens within ViTs that leverages token embeddings, interactions, and attention dynamics across layers to infer token importance, resulting in a more context-aware and interpretable relevance detection process. SPOT informs token sparsification and facilitates the elimination of such tokens, improving computational efficiency without sacrificing performance. SPOT employs computationally lightweight predictors that can be plugged into various ViT architectures and learn to derive effective input-specific token prioritization across layers. Its versatile design supports a range of performance levels adaptable to varying resource constraints. Empirical evaluations demonstrate significant efficiency gains of up to 40% compared to standard ViTs, while maintaining or even improving accuracy. Code and models are available at https://github.com/odedsc/SPOT .

</details>


### [76] [Dynamic Avatar-Scene Rendering from Human-centric Context](https://arxiv.org/abs/2511.10539)
*Wenqing Wang,Haosen Yang,Josef Kittler,Xiatian Zhu*

Main category: cs.CV

TL;DR: 提出Separate-then-Map策略，通过专用信息映射机制桥接分别建模的动态人体与场景，解决现有方法在人体-场景边界处的空间不一致和视觉伪影问题。


<details>
  <summary>Details</summary>
Motivation: 现有4D神经渲染方法要么整体建模动态场景忽略人体独特运动特性，要么分别建模场景和背景但缺乏组件间信息交换，导致重建不完整和边界伪影。

Method: 采用Separate-then-Map策略，为每个高斯属性使用共享变换函数统一分别建模的组件，避免详尽成对交互的同时确保人体与环境的空间和视觉一致性。

Result: 在单目视频数据集上的广泛实验表明，该方法在视觉质量和渲染精度上显著优于现有最先进方法，特别是在具有挑战性的人体-场景交互边界处。

Conclusion: 提出的StM策略通过专用映射机制有效解决了分别建模组件间的空间不一致问题，实现了高质量的动态人体与真实环境交互重建。

Abstract: Reconstructing dynamic humans interacting with real-world environments from monocular videos is an important and challenging task. Despite considerable progress in 4D neural rendering, existing approaches either model dynamic scenes holistically or model scenes and backgrounds separately aim to introduce parametric human priors. However, these approaches either neglect distinct motion characteristics of various components in scene especially human, leading to incomplete reconstructions, or ignore the information exchange between the separately modeled components, resulting in spatial inconsistencies and visual artifacts at human-scene boundaries. To address this, we propose {\bf Separate-then-Map} (StM) strategy that introduces a dedicated information mapping mechanism to bridge separately defined and optimized models. Our method employs a shared transformation function for each Gaussian attribute to unify separately modeled components, enhancing computational efficiency by avoiding exhaustive pairwise interactions while ensuring spatial and visual coherence between humans and their surroundings. Extensive experiments on monocular video datasets demonstrate that StM significantly outperforms existing state-of-the-art methods in both visual quality and rendering accuracy, particularly at challenging human-scene interaction boundaries.

</details>


### [77] [Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation](https://arxiv.org/abs/2511.10547)
*Isabela Albuquerque,Ira Ktena,Olivia Wiles,Ivana Kajić,Amal Rannen-Triki,Cristina Vasconcelos,Aida Nematzadeh*

Main category: cs.CV

TL;DR: 该论文提出了一个评估文本到图像模型多样性的框架，通过评估单个概念及其变化因素来系统性地衡量多样性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型在生成质量上有所进步，但往往缺乏多样性，产生同质化输出，需要建立稳健的多样性评估方法。

Method: 开发了包含人类评估模板、精选提示集和通过二项检验比较模型的方法，同时系统比较了各种图像嵌入用于多样性测量。

Result: 该框架能够对文本到图像模型进行多样性排名，并识别出模型在特定类别中表现不佳的情况。

Conclusion: 这项研究为改进文本到图像模型的多样性和度量开发提供了稳健的方法论和见解。

Abstract: Despite advances in generation quality, current text-to-image (T2I) models often lack diversity, generating homogeneous outputs. This work introduces a framework to address the need for robust diversity evaluation in T2I models. Our framework systematically assesses diversity by evaluating individual concepts and their relevant factors of variation. Key contributions include: (1) a novel human evaluation template for nuanced diversity assessment; (2) a curated prompt set covering diverse concepts with their identified factors of variation (e.g. prompt: An image of an apple, factor of variation: color); and (3) a methodology for comparing models in terms of human annotations via binomial tests.
  Furthermore, we rigorously compare various image embeddings for diversity measurement. Notably, our principled approach enables ranking of T2I models by diversity, identifying categories where they particularly struggle. This research offers a robust methodology and insights, paving the way for improvements in T2I model diversity and metric development.

</details>


### [78] [A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space](https://arxiv.org/abs/2511.10555)
*Huijie Liu,Shuhao Cui,Haoxiang Cao,Shuai Ma,Kai Wu,Guoliang Kang*

Main category: cs.CV

TL;DR: 本文提出了一种新的代码到风格图像生成任务，仅通过数值风格代码就能生成具有新颖、一致视觉风格的图像。作者开发了CoTyle方法，通过训练离散风格码本和自回归风格生成器，实现了从数值代码到视觉风格的高效转换。


<details>
  <summary>Details</summary>
Motivation: 现有的生成方法依赖冗长文本提示、参考图像或参数微调，但在风格一致性、创造性和复杂风格表示方面存在困难。本文旨在通过数值代码直接控制视觉风格，简化风格生成过程并提高多样性。

Method: 首先从图像集合中训练离散风格码本提取风格嵌入，作为文本到图像扩散模型的条件。然后训练自回归风格生成器对离散风格嵌入进行建模，实现新颖风格嵌入的合成。推理时，数值风格代码通过风格生成器映射到唯一风格嵌入，指导扩散模型生成对应风格的图像。

Result: 大量实验验证CoTyle能有效将数值代码转化为风格控制器，证明一个风格确实可以用一个代码来表示。该方法在风格一致性和多样性方面表现出色。

Conclusion: 本文证实了"一个风格值得一个数值代码"的理念，提出的CoTyle方法为开源社区提供了首个代码到风格图像生成的解决方案，具有简单性和多样性的优势，能够从最小输入中解锁大量可复现的风格空间。

Abstract: Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.

</details>


### [79] [OmniVGGT: Omni-Modality Driven Visual Geometry Grounded](https://arxiv.org/abs/2511.10560)
*Haosong Peng,Hao Li,Yalun Dai,Yushi Lan,Yihang Luo,Tianyu Qi,Zhengshen Zhang,Yufeng Zhan,Junfei Zhang,Wenchao Xu,Ziwei Liu*

Main category: cs.CV

TL;DR: OmniVGGT是一个新颖的3D基础模型框架，能够有效利用任意数量的几何模态（如深度图、相机内外参）进行训练和推理，通过GeoAdapter和随机多模态融合机制实现稳定优化和鲁棒表示学习。


<details>
  <summary>Details</summary>
Motivation: 现有的3D基础模型大多仅使用RGB输入，忽略了可用的几何线索（相机内参、位姿、深度图等），限制了性能提升。

Method: 提出GeoAdapter使用零初始化卷积渐进注入几何信息，不破坏基础模型表示空间；采用随机多模态融合机制，训练时随机采样模态子集，测试时可接受任意数量模态输入。

Result: 在单目/多视角深度估计、多视角立体视觉和相机位姿估计任务中优于现有方法，即使仅使用RGB输入也能达到最先进水平；集成到视觉-语言-动作模型中在机器人任务上取得一致提升。

Conclusion: OmniVGGT通过有效利用几何模态显著提升了3D视觉任务的性能，同时保持了推理效率，为实际应用提供了实用解决方案。

Abstract: General 3D foundation models have started to lead the trend of unifying diverse vision tasks, yet most assume RGB-only inputs and ignore readily available geometric cues (e.g., camera intrinsics, poses, and depth maps). To address this issue, we introduce OmniVGGT, a novel framework that can effectively benefit from an arbitrary number of auxiliary geometric modalities during both training and inference. In our framework, a GeoAdapter is proposed to encode depth and camera intrinsics/extrinsics into a spatial foundation model. It employs zero-initialized convolutions to progressively inject geometric information without disrupting the foundation model's representation space. This design ensures stable optimization with negligible overhead, maintaining inference speed comparable to VGGT even with multiple additional inputs. Additionally, a stochastic multimodal fusion regimen is proposed, which randomly samples modality subsets per instance during training. This enables an arbitrary number of modality inputs during testing and promotes learning robust spatial representations instead of overfitting to auxiliary cues. Comprehensive experiments on monocular/multi-view depth estimation, multi-view stereo, and camera pose estimation demonstrate that OmniVGGT outperforms prior methods with auxiliary inputs and achieves state-of-the-art results even with RGB-only input. To further highlight its practical utility, we integrated OmniVGGT into vision-language-action (VLA) models. The enhanced VLA model by OmniVGGT not only outperforms the vanilla point-cloud-based baseline on mainstream benchmarks, but also effectively leverages accessible auxiliary inputs to achieve consistent gains on robotic tasks.

</details>


### [80] [Multitask GLocal OBIA-Mamba for Sentinel-2 Landcover Mapping](https://arxiv.org/abs/2511.10604)
*Zack Dewis,Yimin Zhu,Zhengsen Xu,Mabel Heffring,Saeid Taleghanidoozdoozan,Kaylee Xiao,Motasem Alkayid,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 提出了一种新颖的多任务全局-局部OBIA-Mamba（MSOM）方法，用于增强Sentinel-2影像分类，通过结合超像素分割、全局-局部双分支架构和多任务优化，在保持精细细节的同时提高分类精度。


<details>
  <summary>Details</summary>
Motivation: Sentinel-2影像的土地利用和土地覆盖分类面临空间异质性、上下文信息和特征模糊等关键数据挑战，需要开发更有效的分类方法。

Method: 设计了三个核心组件：1）基于对象的图像分析Mamba模型，使用超像素作为Mamba标记减少冗余计算；2）全局-局部双分支CNN-Mamba架构，联合建模局部空间细节和全局上下文信息；3）多任务优化框架，使用双重损失函数平衡局部精度和全局一致性。

Result: 在加拿大阿尔伯塔省的Sentinel-2影像上进行测试，与多种先进分类方法比较，结果表明该方法获得了更高的分类精度和更精细的细节。

Conclusion: 所提出的MSOM方法能够有效解决Sentinel-2影像分类中的关键挑战，在分类精度和细节保持方面优于现有最先进方法。

Abstract: Although Sentinel-2 based land use and land cover (LULC) classification is critical for various environmental monitoring applications, it is a very difficult task due to some key data challenges (e.g., spatial heterogeneity, context information, signature ambiguity). This paper presents a novel Multitask Glocal OBIA-Mamba (MSOM) for enhanced Sentinel-2 classification with the following contributions. First, an object-based image analysis (OBIA) Mamba model (OBIA-Mamba) is designed to reduce redundant computation without compromising fine-grained details by using superpixels as Mamba tokens. Second, a global-local (GLocal) dual-branch convolutional neural network (CNN)-mamba architecture is designed to jointly model local spatial detail and global contextual information. Third, a multitask optimization framework is designed to employ dual loss functions to balance local precision with global consistency. The proposed approach is tested on Sentinel-2 imagery in Alberta, Canada, in comparison with several advanced classification approaches, and the results demonstrate that the proposed approach achieves higher classification accuracy and finer details that the other state-of-the-art methods.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [81] [A Shared-Autonomy Construction Robotic System for Overhead Works](https://arxiv.org/abs/2511.09695)
*David Minkwan Kim,K. M. Brian Lee,Yong Hyeok Seo,Nikola Raicevic,Runfa Blark Li,Kehan Long,Chan Seon Yoon,Dong Min Kang,Byeong Jo Lim,Young Pyoung Kim,Nikolay Atanasov,Truong Nguyen,Se Woong Jun,Young Wook Kim*

Main category: cs.RO

TL;DR: 开发用于天花板钻孔等高空作业的机器人系统，包括移动平台、双机械臂躯干和定制钻孔末端执行器，使用高斯泼溅进行在线3D重建，引入运动参数建模移动物体，开发神经配置空间屏障方法确保动态环境安全操作。


<details>
  <summary>Details</summary>
Motivation: 解决高空作业（如天花板钻孔）中动态环境下有限可见度的远程操作安全问题，开发能够安全处理移动障碍物的机器人系统。

Method: 采用移动平台配合两阶段升降机构，搭载双机械臂躯干和定制钻孔末端执行器；使用高斯泼溅进行在线3D重建，引入运动参数建模移动物体；开发神经配置空间屏障方法进行规划和控制。

Result: 初步可行性研究表明硬件具备钻孔、螺栓固定和锚固能力，软件在动态环境中能够实现安全的远程操作。

Conclusion: 该系统展示了在高空作业场景下处理动态障碍物的可行性和安全性，为复杂环境下的机器人作业提供了有效解决方案。

Abstract: We present the ongoing development of a robotic system for overhead work such as ceiling drilling. The hardware platform comprises a mobile base with a two-stage lift, on which a bimanual torso is mounted with a custom-designed drilling end effector and RGB-D cameras. To support teleoperation in dynamic environments with limited visibility, we use Gaussian splatting for online 3D reconstruction and introduce motion parameters to model moving objects. For safe operation around dynamic obstacles, we developed a neural configuration-space barrier approach for planning and control. Initial feasibility studies demonstrate the capability of the hardware in drilling, bolting, and anchoring, and the software in safe teleoperation in a dynamic environment.

</details>


### [82] [A Robust Task-Level Control Architecture for Learned Dynamical Systems](https://arxiv.org/abs/2511.09790)
*Eshika Pathak,Ahmed Aboudonia,Sandeep Banik,Naira Hovakimyan*

Main category: cs.RO

TL;DR: 提出L1-DS框架，通过L1自适应控制器和动态时间规整目标选择器解决DS-LfD中的任务执行不匹配问题，提高机器人运动轨迹跟踪的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: DS-LfD生成的机器人运动计划在实际执行中会因未建模动态、持续干扰和系统延迟导致任务空间状态与期望轨迹偏离，即任务执行不匹配问题。

Method: 在任意DS-LfD模型基础上增加标称稳定控制器和L1自适应控制器，并引入基于窗口动态时间规整的目标选择器来处理时间错位。

Result: 在LASA和IROS手写数据集上验证了该架构的有效性。

Conclusion: L1-DS框架能够显式处理任务执行不匹配问题，提高DS-LfD生成运动轨迹的跟踪鲁棒性和相位一致性。

Abstract: Dynamical system (DS)-based learning from demonstration (LfD) is a powerful tool for generating motion plans in the operation (`task') space of robotic systems. However, the realization of the generated motion plans is often compromised by a ''task-execution mismatch'', where unmodeled dynamics, persistent disturbances, and system latency cause the robot's actual task-space state to diverge from the desired motion trajectory. We propose a novel task-level robust control architecture, L1-augmented Dynamical Systems (L1-DS), that explicitly handles the task-execution mismatch in tracking a nominal motion plan generated by any DS-based LfD scheme. Our framework augments any DS-based LfD model with a nominal stabilizing controller and an L1 adaptive controller. Furthermore, we introduce a windowed Dynamic Time Warping (DTW)-based target selector, which enables the nominal stabilizing controller to handle temporal misalignment for improved phase-consistent tracking. We demonstrate the efficacy of our architecture on the LASA and IROS handwriting datasets.

</details>


### [83] [Provably Safe Stein Variational Clarity-Aware Informative Planning](https://arxiv.org/abs/2511.09836)
*Kaleb Ben Naveed,Utkrisht Sahai,Anouck Girard,Dimitra Panagou*

Main category: cs.RO

TL;DR: 本文提出了一种结合信息感知规划和安全验证的机器人轨迹优化框架，通过Stein变分推理学习信息丰富的轨迹，同时使用门卫框架确保安全性。


<details>
  <summary>Details</summary>
Motivation: 现有规划器大多将信息建模为静态或均匀衰减，忽略了空间变化的衰减率，且往往将安全作为软约束处理，无法在信息收集任务中同时保证信息性和安全性。

Method: 使用清晰度模型表示环境不确定性，将清晰度动态嵌入轨迹优化，通过Stein变分推理进行贝叶斯推断学习，并使用门卫框架进行安全验证和过滤。

Result: 在具有不同衰减率和障碍物的环境中进行的硬件实验和仿真表明，该方法能持续保证安全性并减少信息赤字。

Conclusion: 提出的框架能够有效处理空间变化的信息衰减，在信息收集任务中同时实现信息性和安全性目标。

Abstract: Autonomous robots are increasingly deployed for information-gathering tasks in environments that vary across space and time. Planning informative and safe trajectories in such settings is challenging because information decays when regions are not revisited. Most existing planners model information as static or uniformly decaying, ignoring environments where the decay rate varies spatially; those that model non-uniform decay often overlook how it evolves along the robot's motion, and almost all treat safety as a soft penalty. In this paper, we address these challenges. We model uncertainty in the environment using clarity, a normalized representation of differential entropy from our earlier work that captures how information improves through new measurements and decays over time when regions are not revisited. Building on this, we present Stein Variational Clarity-Aware Informative Planning, a framework that embeds clarity dynamics within trajectory optimization and enforces safety through a low-level filtering mechanism based on our earlier gatekeeper framework for safety verification. The planner performs Bayesian inference-based learning via Stein variational inference, refining a distribution over informative trajectories while filtering each nominal Stein informative trajectory to ensure safety. Hardware experiments and simulations across environments with varying decay rates and obstacles demonstrate consistent safety and reduced information deficits.

</details>


### [84] [PuffyBot: An Untethered Shape Morphing Robot for Multi-environment Locomotion](https://arxiv.org/abs/2511.09885)
*Shashwat Singh,Zilin Si,Zeynep Temel*

Main category: cs.RO

TL;DR: PuffyBot是一个受两栖动物启发的无缆形状变形机器人，通过剪刀式升降机构实现形态变化，能够在陆地和水下环境中进行爬行和游泳运动。


<details>
  <summary>Details</summary>
Motivation: 受两栖动物在陆地和水生环境中适应形态和运动的生物学特征启发，开发能够通过形态变化来导航多种环境的机器人平台。

Method: 采用剪刀式升降机构作为主要结构，由线性驱动器驱动实现形状变形；集成曲柄连杆机构调整伺服驱动的肢体90度，实现爬行和游泳模式的无缝切换；使用热塑性聚氨酯(TPU)织物确保防水性能。

Result: 机器人能够实现从255.00 cm³到423.75 cm³的体积变化，调节浮力以抵消3.237 N的下向力；可在陆地爬行、水下爬行、水面游泳，并能通过双模浮力调节潜入水下或浮出水面；无缆运行时间达2小时。

Conclusion: 形状变形技术具有创造适用于多种环境的通用且节能机器人平台的潜力。

Abstract: Amphibians adapt their morphologies and motions to accommodate movement in both terrestrial and aquatic environments. Inspired by these biological features, we present PuffyBot, an untethered shape morphing robot capable of changing its body morphology to navigate multiple environments. Our robot design leverages a scissor-lift mechanism driven by a linear actuator as its primary structure to achieve shape morphing. The transformation enables a volume change from 255.00 cm3 to 423.75 cm3, modulating the buoyant force to counteract a downward force of 3.237 N due to 330 g mass of the robot. A bell-crank linkage is integrated with the scissor-lift mechanism, which adjusts the servo-actuated limbs by 90 degrees, allowing a seamless transition between crawling and swimming modes. The robot is fully waterproof, using thermoplastic polyurethane (TPU) fabric to ensure functionality in aquatic environments. The robot can operate untethered for two hours with an onboard battery of 1000 mA h. Our experimental results demonstrate multi-environment locomotion, including crawling on the land, crawling on the underwater floor, swimming on the water surface, and bimodal buoyancy adjustment to submerge underwater or resurface. These findings show the potential of shape morphing to create versatile and energy efficient robotic platforms suitable for diverse environments.

</details>


### [85] [A Study on Enhancing the Generalization Ability of Visuomotor Policies via Data Augmentation](https://arxiv.org/abs/2511.09932)
*Hanwen Wang*

Main category: cs.RO

TL;DR: 本文研究了如何通过自动生成多样化数据来提升视觉运动策略的泛化能力，特别是在零样本仿真到现实迁移中。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成的数据多样性不足，限制了模仿学习策略的泛化能力，特别是在处理不同场景布局时。

Method: 创建了一个更广泛随机化的数据集，涵盖五种机械臂和两种夹爪，包含相机姿态、光照条件、桌面纹理和桌面高度等随机化因素，通过少量人工演示自动生成数据。

Result: 发现所有随机化因素都影响策略泛化能力，任何形式的随机化都能增强泛化，多样化轨迹特别有助于弥合视觉差距。

Conclusion: 在低成本机械臂上验证了场景随机化对提升视觉运动策略零样本仿真到现实迁移泛化能力的有效性。

Abstract: The generalization ability of visuomotor policy is crucial, as a good policy should be deployable across diverse scenarios. Some methods can collect large amounts of trajectory augmentation data to train more generalizable imitation learning policies, aimed at handling the random placement of objects on the scene's horizontal plane. However, the data generated by these methods still lack diversity, which limits the generalization ability of the trained policy. To address this, we investigate the performance of policies trained by existing methods across different scene layout factors via automate the data generation for those factors that significantly impact generalization. We have created a more extensively randomized dataset that can be efficiently and automatically generated with only a small amount of human demonstration. The dataset covers five types of manipulators and two types of grippers, incorporating extensive randomization factors such as camera pose, lighting conditions, tabletop texture, and table height across six manipulation tasks. We found that all of these factors influence the generalization ability of the policy. Applying any form of randomization enhances policy generalization, with diverse trajectories particularly effective in bridging visual gap. Notably, we investigated on low-cost manipulator the effect of the scene randomization proposed in this work on enhancing the generalization capability of visuomotor policies for zero-shot sim-to-real transfer.

</details>


### [86] [Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation](https://arxiv.org/abs/2511.09958)
*Xiangyi Wei,Haotian Zhang,Xinyi Cao,Siyu Xie,Weifeng Ge,Yang Li,Changbo Wang*

Main category: cs.RO

TL;DR: 本文提出Audio-VLA多模态操作策略，利用接触音频感知接触事件和动态过程反馈，克服了仅视觉VLA模型的限制，并引入任务完成率(TCR)指标系统评估动态操作过程。


<details>
  <summary>Details</summary>
Motivation: 仅视觉的视觉-语言-动作模型(VLA)在感知交互和操作动态过程方面存在根本限制，需要引入音频模态来增强对接触事件和动态过程的感知能力。

Method: 使用预训练的DINOv2和SigLIP作为视觉编码器，AudioCLIP作为音频编码器，Llama2作为大语言模型骨干，通过LoRA微调实现跨模态理解，并添加多模态投影层对齐特征。在RLBench和LIBERO环境中添加基于碰撞的音频生成。

Result: 在LIBERO、RLBench和两个真实世界任务上的广泛实验表明，Audio-VLA优于仅视觉的对比方法，TCR指标有效量化了动态过程感知能力。

Conclusion: Audio-VLA通过整合音频模态显著提升了机器人操作性能，TCR指标为动态操作过程提供了系统性评估标准。

Abstract: The Vision-Language-Action models (VLA) have achieved significant advances in robotic manipulation recently. However, vision-only VLA models create fundamental limitations, particularly in perceiving interactive and manipulation dynamic processes. This paper proposes Audio-VLA, a multimodal manipulation policy that leverages contact audio to perceive contact events and dynamic process feedback. Audio-VLA overcomes the vision-only constraints of VLA models. Additionally, this paper introduces the Task Completion Rate (TCR) metric to systematically evaluate dynamic operational processes. Audio-VLA employs pre-trained DINOv2 and SigLIP as visual encoders, AudioCLIP as the audio encoder, and Llama2 as the large language model backbone. We apply LoRA fine-tuning to these pre-trained modules to achieve robust cross-modal understanding of both visual and acoustic inputs. A multimodal projection layer aligns features from different modalities into the same feature space. Moreover RLBench and LIBERO simulation environments are enhanced by adding collision-based audio generation to provide realistic sound feedback during object interactions. Since current robotic manipulation evaluations focus on final outcomes rather than providing systematic assessment of dynamic operational processes, the proposed TCR metric measures how well robots perceive dynamic processes during manipulation, creating a more comprehensive evaluation metric. Extensive experiments on LIBERO, RLBench, and two real-world tasks demonstrate Audio-VLA's superior performance over vision-only comparative methods, while the TCR metric effectively quantifies dynamic process perception capabilities.

</details>


### [87] [Phantom Menace: Exploring and Enhancing the Robustness of VLA Models against Physical Sensor Attacks](https://arxiv.org/abs/2511.10008)
*Xuancun Lu,Jiaxiang Chen,Shilin Xiao,Zizhi Jin,Zhangrui Chen,Hanwen Yu,Bohan Qian,Ruochen Zhou,Xiaoyu Ji,Wenyuan Xu*

Main category: cs.RO

TL;DR: 本文首次系统研究了针对视觉-语言-动作(VLA)模型的物理传感器攻击，提出了"真实-模拟-真实"框架来自动模拟物理传感器攻击向量，并开发了基于对抗训练的防御方法。


<details>
  <summary>Details</summary>
Motivation: VLA模型严重依赖传感器输入，但对其在物理世界传感器攻击下的安全性研究严重不足，需要填补这一安全空白。

Method: 提出"真实-模拟-真实"框架，自动模拟基于物理的传感器攻击向量，包括6种针对摄像头的攻击和2种针对麦克风的攻击，并在真实机器人系统上进行验证。

Result: 通过大规模评估发现VLA模型存在显著漏洞，易感性模式揭示了其对任务类型和模型设计的关键依赖。基于对抗训练的防御方法能增强VLA对传感器攻击引起的分布外物理扰动的鲁棒性。

Conclusion: 研究结果揭示了在安全关键环境中部署VLA系统时，迫切需要标准化鲁棒性基准和缓解策略。

Abstract: Vision-Language-Action (VLA) models revolutionize robotic systems by enabling end-to-end perception-to-action pipelines that integrate multiple sensory modalities, such as visual signals processed by cameras and auditory signals captured by microphones. This multi-modality integration allows VLA models to interpret complex, real-world environments using diverse sensor data streams. Given the fact that VLA-based systems heavily rely on the sensory input, the security of VLA models against physical-world sensor attacks remains critically underexplored.
  To address this gap, we present the first systematic study of physical sensor attacks against VLAs, quantifying the influence of sensor attacks and investigating the defenses for VLA models. We introduce a novel ``Real-Sim-Real'' framework that automatically simulates physics-based sensor attack vectors, including six attacks targeting cameras and two targeting microphones, and validates them on real robotic systems. Through large-scale evaluations across various VLA architectures and tasks under varying attack parameters, we demonstrate significant vulnerabilities, with susceptibility patterns that reveal critical dependencies on task types and model designs. We further develop an adversarial-training-based defense that enhances VLA robustness against out-of-distribution physical perturbations caused by sensor attacks while preserving model performance. Our findings expose an urgent need for standardized robustness benchmarks and mitigation strategies to secure VLA deployments in safety-critical environments.

</details>


### [88] [DecARt Leg: Design and Evaluation of a Novel Humanoid Robot Leg with Decoupled Actuation for Agile Locomotion](https://arxiv.org/abs/2511.10021)
*Egor Davydenko,Andrei Volchenkov,Vladimir Gerasimov,Roman Gorbachev*

Main category: cs.RO

TL;DR: 本文提出了一种新型电动驱动机器人腿设计DecARt Leg，采用准伸缩运动学结构和旋转电机实现解耦驱动，具有近人形外观和创新的踝关节扭矩传输系统，并通过新指标FAST评估其敏捷运动能力。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够执行敏捷运动的电动驱动机器人腿，通过解耦驱动和创新的机械设计来解决传统机器人腿在快速运动时的性能限制。

Method: 采用准伸缩运动学结构配合旋转电机实现解耦驱动，设计近人形外观和向前弯曲的膝关节，开发多连杆系统将踝关节扭矩从膝关节上方的电机传输到脚踝。

Result: 通过FAST指标进行定量评估，显示该设计在敏捷运动能力方面优于其他设计。通过大量仿真和初步硬件实验验证了基于DecARt Leg的机器人性能。

Conclusion: DecARt Leg设计通过创新的机械结构和解耦驱动方法，成功实现了敏捷的机器人运动能力，为电动驱动机器人的高性能运动提供了有效解决方案。

Abstract: In this paper, we propose a novel design of an electrically actuated robotic leg, called the DecARt (Decoupled Actuation Robot) Leg, aimed at performing agile locomotion. This design incorporates several new features, such as the use of a quasi-telescopic kinematic structure with rotational motors for decoupled actuation, a near-anthropomorphic leg appearance with a forward facing knee, and a novel multi-bar system for ankle torque transmission from motors placed above the knee. To analyze the agile locomotion capabilities of the design numerically, we propose a new descriptive metric, called the `Fastest Achievable Swing Time` (FAST), and perform a quantitative evaluation of the proposed design and compare it with other designs. Then we evaluate the performance of the DecARt Leg-based robot via extensive simulation and preliminary hardware experiments.

</details>


### [89] [Physics-informed Machine Learning for Static Friction Modeling in Robotic Manipulators Based on Kolmogorov-Arnold Networks](https://arxiv.org/abs/2511.10079)
*Yizheng Wang,Timon Rabczuk,Yinghua Liu*

Main category: cs.RO

TL;DR: 本文提出了一种基于Kolmogorov Arnold Network (KAN)的物理启发机器学习方法，用于机器人关节的静摩擦建模。该方法结合样条激活函数和符号回归机制，在保持高预测精度和可解释性的同时，能够简化模型并提取物理表达式。


<details>
  <summary>Details</summary>
Motivation: 传统静态摩擦模型（如Stribeck模型）需要预定义函数假设，在处理未知函数结构时面临挑战。本文旨在解决这一问题，开发一种既能准确建模又具有物理可解释性的方法。

Method: 采用基于KAN的物理启发机器学习方法，集成样条激活函数和符号回归机制，通过剪枝和属性评分实现模型简化和物理表达式提取。

Result: 在合成数据和六自由度工业机械臂采集的真实摩擦数据上的实验表明，该方法在各种任务中实现了大于0.95的决定系数，并成功提取了简洁且具有物理意义的摩擦表达式。

Conclusion: 本研究为可解释和数据驱动的机器人摩擦建模提供了新视角，具有很好的工程应用前景。

Abstract: Friction modeling plays a crucial role in achieving high-precision motion control in robotic operating systems. Traditional static friction models (such as the Stribeck model) are widely used due to their simple forms; however, they typically require predefined functional assumptions, which poses significant challenges when dealing with unknown functional structures. To address this issue, this paper proposes a physics-inspired machine learning approach based on the Kolmogorov Arnold Network (KAN) for static friction modeling of robotic joints. The method integrates spline activation functions with a symbolic regression mechanism, enabling model simplification and physical expression extraction through pruning and attribute scoring, while maintaining both high prediction accuracy and interpretability. We first validate the method's capability to accurately identify key parameters under known functional models, and further demonstrate its robustness and generalization ability under conditions with unknown functional structures and noisy data. Experiments conducted on both synthetic data and real friction data collected from a six-degree-of-freedom industrial manipulator show that the proposed method achieves a coefficient of determination greater than 0.95 across various tasks and successfully extracts concise and physically meaningful friction expressions. This study provides a new perspective for interpretable and data-driven robotic friction modeling with promising engineering applicability.

</details>


### [90] [Opinion: Towards Unified Expressive Policy Optimization for Robust Robot Learning](https://arxiv.org/abs/2511.10087)
*Haidong Huang,Haiyue Zhu. Jiayu Song,Xixin Zhao,Yaohua Zhou,Jiayi Zhang,Yuze Zhai,Xiaocong Li*

Main category: cs.RO

TL;DR: UEPO是一个统一的生成框架，通过多种子动态感知扩散策略、动态分歧正则化机制和基于扩散的数据增强模块，解决了离线到在线强化学习中多模态行为覆盖不足和分布偏移的问题。


<details>
  <summary>Details</summary>
Motivation: 解决离线到在线强化学习中多模态行为覆盖有限和在线适应过程中的分布偏移这两个基本挑战。

Method: 提出多种子动态感知扩散策略来高效捕捉多样模态而无需训练多个模型；使用动态分歧正则化机制强制物理上有意义的策略多样性；采用基于扩散的数据增强模块提升动态模型泛化能力。

Result: 在D4RL基准测试中，UEPO在运动任务上比Uni-O4绝对提升5.9%，在灵巧操作任务上提升12.4%，表现出强大的泛化性和可扩展性。

Conclusion: UEPO框架在离线到在线强化学习中取得了显著性能提升，证明了其在多模态行为捕捉和分布偏移处理方面的有效性。

Abstract: Offline-to-online reinforcement learning (O2O-RL) has emerged as a promising paradigm for safe and efficient robotic policy deployment but suffers from two fundamental challenges: limited coverage of multimodal behaviors and distributional shifts during online adaptation. We propose UEPO, a unified generative framework inspired by large language model pretraining and fine-tuning strategies. Our contributions are threefold: (1) a multi-seed dynamics-aware diffusion policy that efficiently captures diverse modalities without training multiple models; (2) a dynamic divergence regularization mechanism that enforces physically meaningful policy diversity; and (3) a diffusion-based data augmentation module that enhances dynamics model generalization. On the D4RL benchmark, UEPO achieves +5.9\% absolute improvement over Uni-O4 on locomotion tasks and +12.4\% on dexterous manipulation, demonstrating strong generalization and scalability.

</details>


### [91] [Learning a Thousand Tasks in a Day](https://arxiv.org/abs/2511.10110)
*Kamil Dreczkowski,Pietro Vitiello,Vitalis Vosylius,Edward Johns*

Main category: cs.RO

TL;DR: 本文提出了一种基于分解和检索的模仿学习方法MT3，通过将操作轨迹分解为对齐和交互两个阶段，在少量演示（<10个）的情况下实现了比单阶段学习高一个数量级的数据效率，能够从每个任务仅一个演示中学习日常操作任务。


<details>
  <summary>Details</summary>
Motivation: 人类从演示中学习任务非常高效，但当前的机器人模仿学习方法通常需要每个任务数百或数千个演示。本文旨在通过分解操作轨迹和基于检索的泛化来提高学习效率。

Method: 提出Multi-Task Trajectory Transfer (MT3)方法，将操作轨迹分解为顺序对齐和交互阶段，采用检索机制替代行为克隆，在少量演示情况下学习任务。

Result: 通过3,450次真实世界测试，在少量演示（<10个）情况下，分解方法比单阶段学习的数据效率提高一个数量级，检索方法在对齐和交互阶段都优于行为克隆。MT3能够从每个任务仅一个演示中学习日常操作任务，并在24小时内教会机器人1,000个不同任务。

Conclusion: 轨迹分解和检索机制显著提高了模仿学习的数据效率，MT3方法能够高效学习大量日常操作任务，为机器人学习提供了新的可能性。

Abstract: Humans are remarkably efficient at learning tasks from demonstrations, but today's imitation learning methods for robot manipulation often require hundreds or thousands of demonstrations per task. We investigate two fundamental priors for improving learning efficiency: decomposing manipulation trajectories into sequential alignment and interaction phases, and retrieval-based generalisation. Through 3,450 real-world rollouts, we systematically study this decomposition. We compare different design choices for the alignment and interaction phases, and examine generalisation and scaling trends relative to today's dominant paradigm of behavioural cloning with a single-phase monolithic policy. In the few-demonstrations-per-task regime (<10 demonstrations), decomposition achieves an order of magnitude improvement in data efficiency over single-phase learning, with retrieval consistently outperforming behavioural cloning for both alignment and interaction. Building on these insights, we develop Multi-Task Trajectory Transfer (MT3), an imitation learning method based on decomposition and retrieval. MT3 learns everyday manipulation tasks from as little as a single demonstration each, whilst also generalising to novel object instances. This efficiency enables us to teach a robot 1,000 distinct everyday tasks in under 24 hours of human demonstrator time. Through 2,200 additional real-world rollouts, we reveal MT3's capabilities and limitations across different task families. Videos of our experiments can be found on at https://www.robot-learning.uk/learning-1000-tasks.

</details>


### [92] [RoboBenchMart: Benchmarking Robots in Retail Environment](https://arxiv.org/abs/2511.10276)
*Konstantin Soshin,Alexander Krapukhin,Andrei Spiridonov,Denis Shepelev,Gregorii Bukhtuev,Andrey Kuznetsov,Vlad Shakhuro*

Main category: cs.RO

TL;DR: 提出了RoboBenchMart基准测试，针对黑暗商店环境中的机器人操作任务，比现有的桌面场景基准更具挑战性和现实性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作基准主要关注简化的桌面场景，缺乏对真实零售环境中复杂操作的评估能力。

Method: 开发了RoboBenchMart套件，包括程序化商店布局生成器、轨迹生成管道、评估工具和微调基线模型。

Result: 当前最先进的通用模型在常见的零售任务上表现不佳，证明了该基准的必要性。

Conclusion: RoboBenchMart为零售领域的机器人操作研究提供了更现实的评估平台，具有近期的自动化应用潜力。

Abstract: Most existing robotic manipulation benchmarks focus on simplified tabletop scenarios, typically involving a stationary robotic arm interacting with various objects on a flat surface. To address this limitation, we introduce RoboBenchMart, a more challenging and realistic benchmark designed for dark store environments, where robots must perform complex manipulation tasks with diverse grocery items. This setting presents significant challenges, including dense object clutter and varied spatial configurations -- with items positioned at different heights, depths, and in close proximity. By targeting the retail domain, our benchmark addresses a setting with strong potential for near-term automation impact. We demonstrate that current state-of-the-art generalist models struggle to solve even common retail tasks. To support further research, we release the RoboBenchMart suite, which includes a procedural store layout generator, a trajectory generation pipeline, evaluation tools and fine-tuned baseline models.

</details>


### [93] [nuPlan-R: A Closed-Loop Planning Benchmark for Autonomous Driving via Reactive Multi-Agent Simulation](https://arxiv.org/abs/2511.10403)
*Mingxing Peng,Ruoyu Yao,Xusen Guo,Jun Ma*

Main category: cs.RO

TL;DR: nuPlan-R是一个新的反应式闭环规划基准，通过基于学习的反应式多智能体模拟取代了传统的基于规则的IDM智能体，提供了更真实、多样和类似人类的交通行为，为自动驾驶规划提供了更公平和现实的评估标准。


<details>
  <summary>Details</summary>
Motivation: 现有基准依赖基于规则的反应式智能体（如IDM），缺乏行为多样性，无法捕捉真实的人类交互，导致交通动态过于简化。

Method: 在nuPlan框架中集成基于学习的反应式多智能体模拟，使用噪声解耦的基于扩散的反应式智能体，并引入交互感知的智能体选择机制以确保真实性和计算效率。

Result: 实验表明，反应式智能体模型产生更真实、多样和类似人类的交通行为，基准环境更好地反映现实世界的交互驾驶。重新实现的规划方法在复杂交互场景中更清晰地反映了规划器性能。

Conclusion: nuPlan-R为公平、反应式和现实的闭环规划评估建立了新标准，更好地突显了基于学习的规划器在处理复杂动态场景中的优势。

Abstract: Recent advances in closed-loop planning benchmarks have significantly improved the evaluation of autonomous vehicles. However, existing benchmarks still rely on rule-based reactive agents such as the Intelligent Driver Model (IDM), which lack behavioral diversity and fail to capture realistic human interactions, leading to oversimplified traffic dynamics. To address these limitations, we present nuPlan-R, a new reactive closed-loop planning benchmark that integrates learning-based reactive multi-agent simulation into the nuPlan framework. Our benchmark replaces the rule-based IDM agents with noise-decoupled diffusion-based reactive agents and introduces an interaction-aware agent selection mechanism to ensure both realism and computational efficiency. Furthermore, we extend the benchmark with two additional metrics to enable a more comprehensive assessment of planning performance. Extensive experiments demonstrate that our reactive agent model produces more realistic, diverse, and human-like traffic behaviors, leading to a benchmark environment that better reflects real-world interactive driving. We further reimplement a collection of rule-based, learning-based, and hybrid planning approaches within our nuPlan-R benchmark, providing a clearer reflection of planner performance in complex interactive scenarios and better highlighting the advantages of learning-based planners in handling complex and dynamic scenarios. These results establish nuPlan-R as a new standard for fair, reactive, and realistic closed-loop planning evaluation. We will open-source the code for the new benchmark.

</details>


### [94] [Improving dependability in robotized bolting operations](https://arxiv.org/abs/2511.10448)
*Lorenzo Pagliara,Violeta Redondo,Enrico Ferrentino,Manuel Ferre,Pasquale Chiacchio*

Main category: cs.RO

TL;DR: 本文提出了一种可靠的机器人螺栓拧紧控制框架，通过精确的扭矩控制、主动柔顺性和多模态人机界面，提高了操作员的情境感知和故障检测能力，在管道法兰连接任务中验证了系统的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前机器人螺栓拧紧系统缺乏可靠的自主性和故障管理能力，需要提高操作安全性和效率。

Method: 设计了包含精确驱动扭矩控制、主动柔顺性、多模态人机界面和高层监督器的控制架构，支持自动与手动控制的无缝切换。

Result: 在管道法兰连接任务中验证，系统提高了故障检测能力、操作员情境感知，并实现了准确柔顺的螺栓拧紧操作，但发现单一摄像头无法实现完全的情境感知。

Conclusion: 提出的控制框架能够有效提升机器人螺栓拧紧任务的可靠性和安全性，但需要改进情境感知系统的完整性。

Abstract: Bolting operations are critical in industrial assembly and in the maintenance of scientific facilities, requiring high precision and robustness to faults. Although robotic solutions have the potential to improve operational safety and effectiveness, current systems still lack reliable autonomy and fault management capabilities. To address this gap, we propose a control framework for dependable robotized bolting tasks and instantiate it on a specific robotic system. The system features a control architecture ensuring accurate driving torque control and active compliance throughout the entire operation, enabling safe interaction even under fault conditions. By designing a multimodal human-robot interface (HRI) providing real-time visualization of relevant system information and supporting seamless transitions between automatic and manual control, we improve operator situation awareness and fault detection capabilities. A high-level supervisor (SV) coordinates the execution and manages transitions between control modes, ensuring consistency with the supervisory control (SVC) paradigm, while preserving the human operator's authority. The system is validated in a representative bolting operation involving pipe flange joining, under several fault conditions. The results demonstrate improved fault detection capabilities, enhanced operator situational awareness, and accurate and compliant execution of the bolting operation. However, they also reveal the limitations of relying on a single camera to achieve full situational awareness.

</details>


### [95] [From Fold to Function: Dynamic Modeling and Simulation-Driven Design of Origami Mechanisms](https://arxiv.org/abs/2511.10580)
*Tianhui Han,Shashwat Singh,Sarvesh Patil,Zeynep Temel*

Main category: cs.RO

TL;DR: 提出基于MuJoCo的折纸机构仿真框架，通过图形界面定义约束和驱动，实现物理一致的仿真，并通过折纸弹射器案例验证了仿真驱动的优化设计方法。


<details>
  <summary>Details</summary>
Motivation: 折纸机构具有轻量化、紧凑和复杂运动能力，但在仿真折叠行为和环境交互方面存在挑战，需要准确的仿真工具来支持设计优化。

Method: 使用MuJoCo的柔性体功能，将折纸片表示为互连的柔性元素图，通过图形界面定义折痕和驱动等约束，生成物理一致的仿真。

Result: 通过折纸弹射器案例研究，使用CMA-ES算法优化设计参数，实验验证优化结构实现了改进的投掷性能。

Conclusion: 该框架支持快速、仿真驱动的折纸设计、优化和分析，为折纸机构开发提供了有效的工具。

Abstract: Origami-inspired mechanisms can transform flat sheets into functional three-dimensional dynamic structures that are lightweight, compact, and capable of complex motion. These properties make origami increasingly valuable in robotic and deployable systems. However, accurately simulating their folding behavior and interactions with the environment remains challenging. To address this, we present a design framework for origami mechanism simulation that utilizes MuJoCo's deformable-body capabilities. In our approach, origami sheets are represented as graphs of interconnected deformable elements with user-specified constraints such as creases and actuation, defined through an intuitive graphical user interface (GUI). This framework allows users to generate physically consistent simulations that capture both the geometric structure of origami mechanisms and their interactions with external objects and surfaces. We demonstrate our method's utility through a case study on an origami catapult, where design parameters are optimized in simulation using the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) and validated experimentally on physical prototypes. The optimized structure achieves improved throwing performance, illustrating how our system enables rapid, simulation-driven origami design, optimization, and analysis.

</details>


### [96] [Optimizing the flight path for a scouting Uncrewed Aerial Vehicle](https://arxiv.org/abs/2511.10598)
*Raghav Adhikari,Sachet Khatiwada,Suman Poudel*

Main category: cs.RO

TL;DR: 本文提出了一种基于优化的无人机路径规划方法，用于灾后环境中的侦察任务，旨在在最优高度上最大化传感器覆盖面积并最小化数据不确定性。


<details>
  <summary>Details</summary>
Motivation: 灾后环境具有非结构化特性，难以规划救援车辆路径，因此需要无人机进行环境侦察。

Method: 采用基于优化的方法规划无人机路径，在最优高度上部署无人机传感器。

Result: 该方法能够在灾后环境中有效规划无人机侦察路径。

Conclusion: 基于优化的无人机路径规划方法适用于灾后非结构化环境的侦察任务。

Abstract: Post-disaster situations pose unique navigation challenges. One of those challenges is the unstructured nature of the environment, which makes it hard to layout paths for rescue vehicles. We propose the use of Uncrewed Aerial Vehicle (UAV) in such scenario to perform reconnaissance across the environment. To accomplish this, we propose an optimization-based approach to plan a path for the UAV at optimal height where the sensors of the UAV can cover the most area and collect data with minimum uncertainty.

</details>
