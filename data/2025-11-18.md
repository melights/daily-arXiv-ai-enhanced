<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding](https://arxiv.org/abs/2511.11552)
*Dawei Zhu,Rui Meng,Jiefeng Chen,Sujian Li,Tomas Pfister,Jinsung Yoon*

Main category: cs.CV

TL;DR: DocLens是一个工具增强的多智能体框架，通过类似镜头的方式放大证据，从完整文档导航到特定页面的视觉元素，然后采用采样-裁决机制生成可靠答案，在MMLongBench-Doc和FinRAGBench-V上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在处理长视觉文档时面临证据定位的根本挑战，难以检索相关页面并忽略视觉元素中的细粒度细节，导致性能有限和模型幻觉。

Method: 提出DocLens框架，首先从完整文档导航到相关页面的特定视觉元素，然后采用采样-裁决机制生成单一可靠答案。

Result: 与Gemini-2.5-Pro配对，DocLens在MMLongBench-Doc和FinRAGBench-V上达到最先进性能，甚至超越人类专家，在视觉中心和不可回答查询上表现尤为突出。

Conclusion: DocLens通过增强的定位能力展示了其优越性，特别是在处理视觉中心和不可回答查询时，证明了其强大的证据定位能力。

Abstract: Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [2] [Volumetric Ergodic Control](https://arxiv.org/abs/2511.11533)
*Jueun Kwon,Max M. Sun,Todd Murphey*

Main category: cs.RO

TL;DR: 本文提出了一种新的遍历控制方法，使用体积状态表示来优化空间覆盖，相比传统方法将机器人视为无体积点的模型，该方法能显著提高覆盖效率。


<details>
  <summary>Details</summary>
Motivation: 现有遍历控制方法将机器人建模为无体积的点，但在实践中机器人通过具有物理体积的身体和传感器与环境交互，因此需要开发考虑机器人体积的遍历控制方法。

Method: 引入新的遍历控制公式，使用体积状态表示来优化空间覆盖，该方法保持遍历控制的渐近覆盖保证，为实时控制增加最小计算开销，并支持任意基于采样的体积模型。

Result: 在搜索和操作任务中评估该方法，使用多种机器人动力学和末端执行器几何形状或传感器模型，结果显示覆盖效率提高超过两倍，同时在所有实验中保持100%的任务完成率，优于标准遍历控制方法。

Conclusion: 该方法在机器人手臂执行机械擦除任务中表现出有效性，为具有物理体积的机器人提供了更高效的遍历控制解决方案。

Abstract: Ergodic control synthesizes optimal coverage behaviors over spatial distributions for nonlinear systems. However, existing formulations model the robot as a non-volumetric point, but in practice a robot interacts with the environment through its body and sensors with physical volume. In this work, we introduce a new ergodic control formulation that optimizes spatial coverage using a volumetric state representation. Our method preserves the asymptotic coverage guarantees of ergodic control, adds minimal computational overhead for real-time control, and supports arbitrary sample-based volumetric models. We evaluate our method across search and manipulation tasks -- with multiple robot dynamics and end-effector geometries or sensor models -- and show that it improves coverage efficiency by more than a factor of two while maintaining a 100% task completion rate across all experiments, outperforming the standard ergodic control method. Finally, we demonstrate the effectiveness of our method on a robot arm performing mechanical erasing tasks.

</details>
