<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 36]
- [cs.RO](#cs.RO) [Total: 39]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Egocentric Bias in Vision-Language Models](https://arxiv.org/abs/2602.15892)
*Maijunxian Wang,Yijiang Li,Bingyang Wang,Tianwei Zhao,Ran Ji,Qingying Gao,Emmy Liu,Hokin Deng,Dezhi Luo*

Main category: cs.CV

TL;DR: FlipSet是一个诊断性基准测试，用于评估视觉语言模型中的二级视觉视角采择能力，发现现有模型存在系统性自我中心偏见，无法将社会认知与空间操作整合。


<details>
  <summary>Details</summary>
Motivation: 视觉视角采择是社会认知的基础能力，但现有视觉语言模型在这方面的能力尚未得到系统评估。研究者希望开发一个诊断性基准来测试模型从他人视角理解世界的能力。

Method: 研究者引入了FlipSet基准测试，要求模型模拟2D字符字符串的180度旋转，从另一个智能体的视角进行推理。该方法将空间变换与3D场景复杂性分离，评估了103个视觉语言模型。

Result: 评估结果显示系统性自我中心偏见：绝大多数模型表现低于随机水平，约四分之三的错误复制了相机视角。控制实验揭示了组合性缺陷：模型在单独任务中表现良好，但在需要整合时完全失败。

Conclusion: 当前视觉语言模型缺乏将社会意识与空间操作绑定的机制，表明基于模型的空间推理存在根本性限制。FlipSet为诊断多模态系统中的视角采择能力提供了认知基础测试平台。

Abstract: Visual perspective taking--inferring how the world appears from another's viewpoint--is foundational to social cognition. We introduce FlipSet, a diagnostic benchmark for Level-2 visual perspective taking (L2 VPT) in vision-language models. The task requires simulating 180-degree rotations of 2D character strings from another agent's perspective, isolating spatial transformation from 3D scene complexity. Evaluating 103 VLMs reveals systematic egocentric bias: the vast majority perform below chance, with roughly three-quarters of errors reproducing the camera viewpoint. Control experiments expose a compositional deficit--models achieve high theory-of-mind accuracy and above-chance mental rotation in isolation, yet fail catastrophically when integration is required. This dissociation indicates that current VLMs lack the mechanisms needed to bind social awareness to spatial operations, suggesting fundamental limitations in model-based spatial reasoning. FlipSet provides a cognitively grounded testbed for diagnosing perspective-taking capabilities in multimodal systems.

</details>


### [2] [Detecting Deepfakes with Multivariate Soft Blending and CLIP-based Image-Text Alignment](https://arxiv.org/abs/2602.15903)
*Jingwei Li,Jiaxin Tong,Pengfei Wu*

Main category: cs.CV

TL;DR: 提出MSBA-CLIP框架，通过多变量软混合增强和CLIP引导的伪造强度估计，提升深度伪造检测的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法存在准确率有限和泛化能力差的问题，主要原因是不同伪造技术生成的样本存在显著分布偏移。

Method: 提出MSBA-CLIP框架：1）利用CLIP的多模态对齐能力捕捉细微伪造痕迹；2）设计多变量软混合增强策略，通过随机权重混合多种伪造方法生成的图像，迫使模型学习可泛化模式；3）设计多变量伪造强度估计模块，显式指导模型学习不同伪造模式和强度的特征。

Result: 在域内测试中，准确率和AUC分别比最佳基线提高3.32%和4.02%；在跨五个数据集的跨域评估中，平均AUC增益达3.27%；消融研究证实了两个组件的有效性。

Conclusion: 虽然依赖大型视觉语言模型带来较高计算成本，但该工作向更可泛化和鲁棒的深度伪造检测迈出了重要一步。

Abstract: The proliferation of highly realistic facial forgeries necessitates robust detection methods. However, existing approaches often suffer from limited accuracy and poor generalization due to significant distribution shifts among samples generated by diverse forgery techniques. To address these challenges, we propose a novel Multivariate and Soft Blending Augmentation with CLIP-guided Forgery Intensity Estimation (MSBA-CLIP) framework. Our method leverages the multimodal alignment capabilities of CLIP to capture subtle forgery traces. We introduce a Multivariate and Soft Blending Augmentation (MSBA) strategy that synthesizes images by blending forgeries from multiple methods with random weights, forcing the model to learn generalizable patterns. Furthermore, a dedicated Multivariate Forgery Intensity Estimation (MFIE) module is designed to explicitly guide the model in learning features related to varied forgery modes and intensities. Extensive experiments demonstrate state-of-the-art performance. On in-domain tests, our method improves Accuracy and AUC by 3.32\% and 4.02\%, respectively, over the best baseline. In cross-domain evaluations across five datasets, it achieves an average AUC gain of 3.27\%. Ablation studies confirm the efficacy of both proposed components. While the reliance on a large vision-language model entails higher computational cost, our work presents a significant step towards more generalizable and robust deepfake detection.

</details>


### [3] [A Comprehensive Survey on Deep Learning-Based LiDAR Super-Resolution for Autonomous Driving](https://arxiv.org/abs/2602.15904)
*June Moh Goo,Zichao Zeng,Jan Boehm*

Main category: cs.CV

TL;DR: 本文首次对自动驾驶中的LiDAR超分辨率方法进行全面综述，将现有方法分为四类，建立了基本概念框架，分析了当前趋势，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 高分辨率LiDAR传感器昂贵，而低成本低分辨率传感器产生的稀疏点云会丢失关键细节。LiDAR超分辨率通过深度学习增强稀疏点云，弥合不同传感器类型之间的差距，实现实际部署中的跨传感器兼容性。

Method: 将现有LiDAR超分辨率方法组织为四类：基于CNN的架构、基于模型的深度展开、隐式表示方法、基于Transformer和Mamba的方法。建立了包括数据表示、问题公式化、基准数据集和评估指标在内的基本概念框架。

Result: 当前趋势包括采用距离图像表示以提高处理效率、极端模型压缩和开发分辨率灵活架构。近期研究优先考虑实时推理和跨传感器泛化以实现实际部署。

Conclusion: 本文填补了LiDAR超分辨率领域缺乏系统性综述的空白，为未来研究提供了概念框架，并指出了开放挑战和未来研究方向，以推进LiDAR超分辨率技术的实际应用。

Abstract: LiDAR sensors are often considered essential for autonomous driving, but high-resolution sensors remain expensive while affordable low-resolution sensors produce sparse point clouds that miss critical details. LiDAR super-resolution addresses this challenge by using deep learning to enhance sparse point clouds, bridging the gap between different sensor types and enabling cross-sensor compatibility in real-world deployments. This paper presents the first comprehensive survey of LiDAR super-resolution methods for autonomous driving. Despite the importance of practical deployment, no systematic review has been conducted until now. We organize existing approaches into four categories: CNN-based architectures, model-based deep unrolling, implicit representation methods, and Transformer and Mamba-based approaches. We establish fundamental concepts including data representations, problem formulation, benchmark datasets and evaluation metrics. Current trends include the adoption of range image representation for efficient processing, extreme model compression and the development of resolution-flexible architectures. Recent research prioritizes real-time inference and cross-sensor generalization for practical deployment. We conclude by identifying open challenges and future research directions for advancing LiDAR super-resolution technology.

</details>


### [4] [EarthSpatialBench: Benchmarking Spatial Reasoning Capabilities of Multimodal LLMs on Earth Imagery](https://arxiv.org/abs/2602.15918)
*Zelin Xu,Yupu Zhang,Saugat Adhikari,Saiful Islam,Tingsong Xiao,Zibo Liu,Shigang Chen,Da Yan,Zhe Jiang*

Main category: cs.CV

TL;DR: EarthSpatialBench：一个用于评估多模态大语言模型在地球影像上空间推理能力的综合基准，包含32.5万个问答对，涵盖距离、方向、拓扑关系等定量推理。


<details>
  <summary>Details</summary>
Motivation: 现有地球影像基准主要关注2D空间定位、图像描述和粗略空间关系，缺乏对定量方向距离推理、系统拓扑关系以及超越边界框的复杂几何对象的支持，而这对具身AI和智能体系统与物理世界的精确交互至关重要。

Method: 提出EarthSpatialBench基准，包含超过32.5万个问答对，涵盖：1）空间距离和方向的定性与定量推理；2）系统拓扑关系；3）单对象查询、对象对查询和组合聚合组查询；4）通过文本描述、视觉叠加和显式几何坐标（包括2D边界框、折线和多边形）的对象引用。

Result: 通过对开源和专有模型进行广泛实验，识别了MLLMs在空间推理方面的局限性。

Conclusion: EarthSpatialBench填补了地球影像空间推理评估的空白，为评估MLLMs在定量空间推理能力方面提供了全面基准，有助于推动具身AI和智能体系统的发展。

Abstract: Benchmarking spatial reasoning in multimodal large language models (MLLMs) has attracted growing interest in computer vision due to its importance for embodied AI and other agentic systems that require precise interaction with the physical world. However, spatial reasoning on Earth imagery has lagged behind, as it uniquely involves grounding objects in georeferenced images and quantitatively reasoning about distances, directions, and topological relations using both visual cues and vector geometry coordinates (e.g., 2D bounding boxes, polylines, and polygons). Existing benchmarks for Earth imagery primarily focus on 2D spatial grounding, image captioning, and coarse spatial relations (e.g., simple directional or proximity cues). They lack support for quantitative direction and distance reasoning, systematic topological relations, and complex object geometries beyond bounding boxes. To fill this gap, we propose \textbf{EarthSpatialBench}, a comprehensive benchmark for evaluating spatial reasoning in MLLMs on Earth imagery. The benchmark contains over 325K question-answer pairs spanning: (1) qualitative and quantitative reasoning about spatial distance and direction; (2) systematic topological relations; (3) single-object queries, object-pair queries, and compositional aggregate group queries; and (4) object references expressed via textual descriptions, visual overlays, and explicit geometry coordinates, including 2D bounding boxes, polylines, and polygons. We conducted extensive experiments on both open-source and proprietary models to identify limitations in the spatial reasoning of MLLMs.

</details>


### [5] [A Study on Real-time Object Detection using Deep Learning](https://arxiv.org/abs/2602.15926)
*Ankita Bose,Jayasravani Bhumireddy,Naveen N*

Main category: cs.CV

TL;DR: 该论文综述了深度学习在实时目标检测中的应用，涵盖主流算法、基准数据集、应用领域比较研究及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 目标检测在多个领域有重要应用价值，包括人机交互、安防监控、交通管理、工业自动化、医疗健康、AR/VR等。实时目标检测能够提供动态视觉信息分析，辅助即时决策。深度学习算法的发展为目标检测提供了更准确高效的解决方案。

Method: 文章详细探讨了深度学习算法在实时目标检测中的应用，包括对Faster R-CNN、Mask R-CNN、Cascade R-CNN、YOLO、SSD、RetinaNet等主流算法的分析。提供了不同目标检测模型的比较、开放基准数据集的研究，以及在各应用领域的使用情况分析。通过对照研究比较各种策略。

Result: 文章提供了对多种目标检测模型的比较分析，揭示了不同策略的优缺点。通过对照研究得出了一些有启发性的发现，为实际应用提供了指导。

Conclusion: 深度学习算法显著提升了实时目标检测的性能和效率。文章最后提出了未来研究的挑战和方向，包括相关深度学习方法和目标识别技术的进一步发展。

Abstract: Object detection has compelling applications over a range of domains, including human-computer interfaces, security and video surveillance, navigation and road traffic monitoring, transportation systems, industrial automation healthcare, the world of Augmented Reality (AR) and Virtual Reality (VR), environment monitoring and activity identification. Applications of real time object detection in all these areas provide dynamic analysis of the visual information that helps in immediate decision making. Furthermore, advanced deep learning algorithms leverage the progress in the field of object detection providing more accurate and efficient solutions. There are some outstanding deep learning algorithms for object detection which includes, Faster R CNN(Region-based Convolutional Neural Network),Mask R-CNN, Cascade R-CNN, YOLO (You Only Look Once), SSD (Single Shot Multibox Detector), RetinaNet etc. This article goes into great detail on how deep learning algorithms are used to enhance real time object recognition. It provides information on the different object detection models available, open benchmark datasets, and studies on the use of object detection models in a range of applications. Additionally, controlled studies are provided to compare various strategies and produce some illuminating findings. Last but not least, a number of encouraging challenges and approaches are offered as suggestions for further investigation in both relevant deep learning approaches and object recognition.

</details>


### [6] [Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families](https://arxiv.org/abs/2602.15950)
*Yuval Levental*

Main category: cs.CV

TL;DR: 视觉语言模型在识别二进制网格中的填充单元格时存在根本性缺陷：当单元格缺乏文本标识时，模型的空间定位能力严重下降。


<details>
  <summary>Details</summary>
Motivation: 揭示视觉语言模型在空间推理方面的基本限制，特别是当视觉元素缺乏文本标识时，模型无法准确识别和定位二进制网格中的填充单元格。

Method: 生成15个15x15的二进制网格，填充密度从10.7%到41.8%，以两种图像形式呈现：文本符号（.和#）和无网格线的填充方块。使用三种前沿视觉语言模型（Claude Opus、ChatGPT 5.2和Gemini 3 Thinking）进行转录测试。

Result: 在文本符号条件下，Claude和ChatGPT达到约91%的单元格准确率和84%的F1分数，Gemini达到84%准确率和63%F1。在填充方块条件下，所有模型性能崩溃至60-73%准确率和29-39%F1。文本与方块的F1差距达34-54个百分点。

Conclusion: 视觉语言模型似乎拥有一个用于空间推理的高保真文本识别通路，其性能远超其原生视觉通路。所有模型在非文本视觉元素的空间定位方面都存在严重缺陷，尽管具体失败模式各不相同。

Abstract: We present a simple experiment that exposes a fundamental limitation in vision-language models (VLMs): the inability to accurately localize filled cells in binary grids when those cells lack textual identity. We generate fifteen 15x15 grids with varying density (10.7%-41.8% filled cells) and render each as two image types -- text symbols (. and #) and filled squares without gridlines -- then ask three frontier VLMs (Claude Opus, ChatGPT 5.2, and Gemini 3 Thinking) to transcribe them. In the text-symbol condition, Claude and ChatGPT achieve approximately 91% cell accuracy and 84% F1, while Gemini achieves 84% accuracy and 63% F1. In the filled-squares condition, all three models collapse to 60-73% accuracy and 29-39% F1. Critically, all conditions pass through the same visual encoder -- the text symbols are images, not tokenized text. The text-vs-squares F1 gap ranges from 34 to 54 points across models, demonstrating that VLMs behave as if they possess a high-fidelity text-recognition pathway for spatial reasoning that dramatically outperforms their native visual pathway. Each model exhibits a distinct failure mode in the squares condition -- systematic under-counting (Claude), massive over-counting (ChatGPT), and template hallucination (Gemini) -- but all share the same underlying deficit: severely degraded spatial localization for non-textual visual elements.

</details>


### [7] [Position-Aware Scene-Appearance Disentanglement for Bidirectional Photoacoustic Microscopy Registration](https://arxiv.org/abs/2602.15959)
*Yiwen Wang,Jiahao Qin*

Main category: cs.CV

TL;DR: GPEReg-Net：一种用于高速光学分辨率光声显微镜的双向扫描配准框架，通过场景-外观解耦和全局位置编码实现高精度图像配准，无需显式形变场估计。


<details>
  <summary>Details</summary>
Motivation: 高速光学分辨率光声显微镜的双向扫描虽然提高成像速度，但会引入域偏移和几何错位问题。现有配准方法受限于亮度恒定假设，而生成方法缺乏跨帧的时间感知能力。

Method: 提出GPEReg-Net框架：1）使用自适应实例归一化分离域不变场景特征和域特定外观编码；2）引入全局位置编码模块，结合可学习位置嵌入、正弦编码和跨帧注意力，利用相邻帧的上下文信息提升时间一致性。

Result: 在OR-PAM-Reg-4K基准测试（432个样本）上，GPEReg-Net达到NCC 0.953、SSIM 0.932、PSNR 34.49dB，SSIM比现有最佳方法提升3.8%，PSNR提升1.99dB，同时保持竞争力的NCC。

Conclusion: GPEReg-Net通过场景-外观解耦和全局时间编码，有效解决了高速OR-PAM双向扫描中的域偏移和几何错位问题，实现了高质量的图像配准，无需显式形变场估计。

Abstract: High-speed optical-resolution photoacoustic microscopy (OR-PAM) with bidirectional raster scanning doubles imaging speed but introduces coupled domain shift and geometric misalignment between forward and backward scan lines. Existing registration methods, constrained by brightness constancy assumptions, achieve limited alignment quality, while recent generative approaches address domain shift through complex architectures that lack temporal awareness across frames. We propose GPEReg-Net, a scene-appearance disentanglement framework that separates domain-invariant scene features from domain-specific appearance codes via Adaptive Instance Normalization (AdaIN), enabling direct image-to-image registration without explicit deformation field estimation. To exploit temporal structure in sequential acquisitions, we introduce a Global Position Encoding (GPE) module that combines learnable position embeddings with sinusoidal encoding and cross-frame attention, allowing the network to leverage context from neighboring frames for improved temporal coherence. On the OR-PAM-Reg-4K benchmark (432 test samples), GPEReg-Net achieves NCC of 0.953, SSIM of 0.932, and PSNR of 34.49dB, surpassing the state-of-the-art by 3.8% in SSIM and 1.99dB in PSNR while maintaining competitive NCC. Code is available at https://github.com/JiahaoQin/GPEReg-Net.

</details>


### [8] [Non-Contact Physiological Monitoring in Pediatric Intensive Care Units via Adaptive Masking and Self-Supervised Learning](https://arxiv.org/abs/2602.15967)
*Mohamed Khalil Ben Salah,Philippe Jouvet,Rita Noumeir*

Main category: cs.CV

TL;DR: 提出基于渐进式课程策略的自监督预训练框架，用于PICU环境下的远程光电容积描记术心率监测，通过VisionMamba架构和自适应掩码机制，在缺乏标注临床数据的情况下实现3.2 bpm的MAE。


<details>
  <summary>Details</summary>
Motivation: PICU中连续监测生命体征对早期发现临床恶化至关重要，但接触式传感器可能导致皮肤刺激、感染风险增加和患者不适。远程光电容积描记术提供无接触替代方案，但在PICU中应用不足，主要受运动伪影、遮挡、光照变化和实验室与临床数据领域偏移的限制。

Method: 采用基于渐进式课程策略的自监督预训练框架，利用VisionMamba架构并集成自适应掩码机制，通过轻量级Mamba控制器分配时空重要性分数来指导概率性补丁采样。采用教师-学生蒸馏设置，由在公共数据集上训练的监督专家模型提供潜在生理指导。课程分为三个阶段：干净的公共视频、合成遮挡场景和来自500名儿科患者的未标记视频。

Result: 框架相对于标准掩码自编码器减少了42%的平均绝对误差，比PhysFormer性能提升31%，最终达到3.2 bpm的MAE。无需显式提取感兴趣区域，模型持续关注脉搏丰富区域，并在临床遮挡和噪声下表现出鲁棒性。

Conclusion: 提出的自监督预训练框架有效解决了PICU环境下rPPG监测的挑战，通过渐进式课程策略和自适应掩码机制，在缺乏标注临床数据的情况下实现了准确的心率估计，为无接触生命体征监测提供了有前景的解决方案。

Abstract: Continuous monitoring of vital signs in Pediatric Intensive Care Units (PICUs) is essential for early detection of clinical deterioration and effective clinical decision-making. However, contact-based sensors such as pulse oximeters may cause skin irritation, increase infection risk, and lead to patient discomfort. Remote photoplethysmography (rPPG) offers a contactless alternative to monitor heart rate using facial video, but remains underutilized in PICUs due to motion artifacts, occlusions, variable lighting, and domain shifts between laboratory and clinical data.
  We introduce a self-supervised pretraining framework for rPPG estimation in the PICU setting, based on a progressive curriculum strategy. The approach leverages the VisionMamba architecture and integrates an adaptive masking mechanism, where a lightweight Mamba-based controller assigns spatiotemporal importance scores to guide probabilistic patch sampling. This strategy dynamically increases reconstruction difficulty while preserving physiological relevance.
  To address the lack of labeled clinical data, we adopt a teacher-student distillation setup. A supervised expert model, trained on public datasets, provides latent physiological guidance to the student. The curriculum progresses through three stages: clean public videos, synthetic occlusion scenarios, and unlabeled videos from 500 pediatric patients.
  Our framework achieves a 42% reduction in mean absolute error relative to standard masked autoencoders and outperforms PhysFormer by 31%, reaching a final MAE of 3.2 bpm. Without explicit region-of-interest extraction, the model consistently attends to pulse-rich areas and demonstrates robustness under clinical occlusions and noise.

</details>


### [9] [LAND: A Longitudinal Analysis of Neuromorphic Datasets](https://arxiv.org/abs/2602.15973)
*Gregory Cohen,Alexandre Marcireau*

Main category: cs.CV

TL;DR: 这篇综述分析了423个神经形态数据集，发现存在数据获取困难、缺乏标准化、数据集规模增长等问题，特别关注合成数据集的潜在风险，并提出元数据集概念作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 神经形态工程面临数据问题：尽管过去十年数据集数量激增，但许多研究仍呼吁需要更多更大数据集。现有数据集存在查找困难、理解其目的和任务性质困难、下载和使用困难等问题，同时合成数据集的兴起带来了新的挑战。

Method: 1. 收集并分析423个现有神经形态数据集的快照；2. 探索这些数据集的任务性质和数据结构；3. 分析数据集规模、标准化程度和访问难度；4. 研究合成数据集（通过模拟或视频转事件方法创建）的增长趋势；5. 引入元数据集概念，从现有数据集创建以减少对新数据的需求并消除潜在偏差。

Result: 分析显示：1. 数据集规模持续增长且复杂度增加；2. 缺乏标准化导致使用困难；3. 数据访问存在实际障碍；4. 合成数据集虽然有助于测试现有算法，但可能限制新应用的探索；5. 元数据集概念为解决数据需求和偏差问题提供了新思路。

Conclusion: 神经形态领域的数据问题不仅在于数量不足，更在于数据集的可访问性、标准化和任务定义的偏差。合成数据集虽有价值但存在局限性，元数据集方法有望减少对新数据的需求并提高现有数据的利用效率，为领域发展提供更可持续的数据解决方案。

Abstract: Neuromorphic engineering has a data problem. Despite the meteoric rise in the number of neuromorphic datasets published over the past ten years, the conclusion of a significant portion of neuromorphic research papers still states that there is a need for yet more data and even larger datasets. Whilst this need is driven in part by the sheer volume of data required by modern deep learning approaches, it is also fuelled by the current state of the available neuromorphic datasets and the difficulties in finding them, understanding their purpose, and determining the nature of their underlying task. This is further compounded by practical difficulties in downloading and using these datasets. This review starts by capturing a snapshot of the existing neuromorphic datasets, covering over 423 datasets, and then explores the nature of their tasks and the underlying structure of the presented data. Analysing these datasets shows the difficulties arising from their size, the lack of standardisation, and difficulties in accessing the actual data. This paper also highlights the growth in the size of individual datasets and the complexities involved in working with the data. However, a more important concern is the rise of synthetic datasets, created by either simulation or video-to-events methods. This review explores the benefits of simulated data for testing existing algorithms and applications, highlighting the potential pitfalls for exploring new applications of neuromorphic technologies. This review also introduces the concepts of meta-datasets, created from existing datasets, as a way of both reducing the need for more data, and to remove potential bias arising from defining both the dataset and the task.

</details>


### [10] [SAM 3D Body: Robust Full-Body Human Mesh Recovery](https://arxiv.org/abs/2602.15989)
*Xitong Yang,Devansh Kukreja,Don Pinkus,Anushka Sagar,Taosha Fan,Jinhyung Park,Soyong Shin,Jinkun Cao,Jiawei Liu,Nicolas Ugrinovic,Matt Feiszli,Jitendra Malik,Piotr Dollar,Kris Kitani*

Main category: cs.CV

TL;DR: SAM 3D Body (3DB) 是一个基于提示的单图像全身3D人体网格恢复模型，使用新的参数化网格表示MHR，在多样化野外条件下表现出最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前3D人体网格恢复方法在多样化野外条件下的泛化能力和准确性有限，需要一种能够处理复杂姿态和罕见成像条件的鲁棒模型。

Method: 采用编码器-解码器架构，使用新的参数化网格表示MHR（分离骨骼结构和表面形状），支持2D关键点和掩码等辅助提示，通过多阶段标注流程获取高质量标注数据，并建立高效的数据引擎确保数据多样性。

Result: 在用户偏好研究和传统定量分析中都优于现有方法，表现出卓越的泛化能力和一致的准确性，特别是在处理罕见姿态和成像条件方面。

Conclusion: SAM 3D Body (3DB) 在单图像3D人体网格恢复任务中实现了最先进的性能，其创新的MHR表示和提示机制为3D人体重建提供了新的方向，模型和数据均已开源。

Abstract: We introduce SAM 3D Body (3DB), a promptable model for single-image full-body 3D human mesh recovery (HMR) that demonstrates state-of-the-art performance, with strong generalization and consistent accuracy in diverse in-the-wild conditions. 3DB estimates the human pose of the body, feet, and hands. It is the first model to use a new parametric mesh representation, Momentum Human Rig (MHR), which decouples skeletal structure and surface shape. 3DB employs an encoder-decoder architecture and supports auxiliary prompts, including 2D keypoints and masks, enabling user-guided inference similar to the SAM family of models. We derive high-quality annotations from a multi-stage annotation pipeline that uses various combinations of manual keypoint annotation, differentiable optimization, multi-view geometry, and dense keypoint detection. Our data engine efficiently selects and processes data to ensure data diversity, collecting unusual poses and rare imaging conditions. We present a new evaluation dataset organized by pose and appearance categories, enabling nuanced analysis of model behavior. Our experiments demonstrate superior generalization and substantial improvements over prior methods in both qualitative user preference studies and traditional quantitative analysis. Both 3DB and MHR are open-source.

</details>


### [11] [MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval](https://arxiv.org/abs/2602.16019)
*Ahmad Elallaf,Yu Zhang,Yuktha Priya Masupalli,Jeong Yang,Young Lee,Zechun Cao,Gongbo Liang*

Main category: cs.CV

TL;DR: MedProbCLIP是一个用于胸部X光和放射学报告的视觉-语言概率学习框架，通过高斯嵌入建模不确定性，在检索和分类任务中优于现有方法，并提高了校准性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言基础模型虽然强大，但其确定性嵌入缺乏可靠性，无法满足高风险生物医学应用的需求。需要一种能够建模不确定性和多对多对应关系的概率框架来提高放射学图像-文本检索系统的可信度和安全性。

Method: 1. 使用高斯嵌入建模图像和文本表示；2. 通过概率对比目标捕获不确定性和多对多对应关系；3. 采用变分信息瓶颈防止过度自信预测；4. 训练时使用多视图X光编码和多部分报告编码提供细粒度监督；5. 推理时仅需单张X光和单个报告。

Result: 在MIMIC-CXR数据集上，MedProbCLIP在检索和零样本分类任务中优于CLIP、CXR-CLIP和PCME++等确定性和概率性基线方法。此外，在校准性、风险覆盖行为、选择性检索可靠性和临床相关损坏的鲁棒性方面表现更优。

Conclusion: 概率视觉-语言建模能够显著提高放射学图像-文本检索系统的可信度和安全性，MedProbCLIP框架为高风险生物医学应用提供了更可靠的解决方案。

Abstract: Vision-language foundation models have emerged as powerful general-purpose representation learners with strong potential for multimodal understanding, but their deterministic embeddings often fail to provide the reliability required for high-stakes biomedical applications. This work introduces MedProbCLIP, a probabilistic vision-language learning framework for chest X-ray and radiology report representation learning and bidirectional retrieval. MedProbCLIP models image and text representations as Gaussian embeddings through a probabilistic contrastive objective that explicitly captures uncertainty and many-to-many correspondences between radiographs and clinical narratives. A variational information bottleneck mitigates overconfident predictions, while MedProbCLIP employs multi-view radiograph encoding and multi-section report encoding during training to provide fine-grained supervision for clinically aligned correspondence, yet requires only a single radiograph and a single report at inference. Evaluated on the MIMIC-CXR dataset, MedProbCLIP outperforms deterministic and probabilistic baselines, including CLIP, CXR-CLIP, and PCME++, in both retrieval and zero-shot classification. Beyond accuracy, MedProbCLIP demonstrates superior calibration, risk-coverage behavior, selective retrieval reliability, and robustness to clinically relevant corruptions, underscoring the value of probabilistic vision-language modeling for improving the trustworthiness and safety of radiology image-text retrieval systems.

</details>


### [12] [LGQ: Learning Discretization Geometry for Scalable and Stable Image Tokenization](https://arxiv.org/abs/2602.16086)
*Idil Bilge Altun,Mert Onur Cakiroglu,Elham Buxton,Mehmet Dalkilic,Hasan Kurban*

Main category: cs.CV

TL;DR: LGQ提出了一种可学习几何量化方法，通过软分配和温度控制实现端到端训练，解决了现有量化器在几何灵活性与代码利用率之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有图像离散化方法面临权衡：向量量化器几何灵活但存在优化偏差、代码利用不足和表示崩溃问题；结构化标量量化器利用率稳定但几何固定，在异构潜在统计下容量分配效率低。

Method: LGQ用温度控制的软分配替代硬最近邻查找，实现完全可微分训练，推理时恢复硬分配。结合token级峰值正则化和全局使用正则化，鼓励自信且平衡的代码利用。

Result: 在ImageNet上，16K代码本大小下，LGQ比FSQ提升rFID 11.88%且减少49.96%活跃代码；比SimVQ提升rFID 6.06%且降低49.45%有效表示率，用更少活跃条目实现相当保真度。

Conclusion: LGQ通过学习离散化几何实现稳定优化和平衡利用，在保持语义结构的同时有效利用离散容量，为可扩展视觉生成提供了更好的图像离散化解决方案。

Abstract: Discrete image tokenization is a key bottleneck for scalable visual generation: a tokenizer must remain compact for efficient latent-space priors while preserving semantic structure and using discrete capacity effectively. Existing quantizers face a trade-off: vector-quantized tokenizers learn flexible geometries but often suffer from biased straight-through optimization, codebook under-utilization, and representation collapse at large vocabularies. Structured scalar or implicit tokenizers ensure stable, near-complete utilization by design, yet rely on fixed discretization geometries that may allocate capacity inefficiently under heterogeneous latent statistics.
  We introduce Learnable Geometric Quantization (LGQ), a discrete image tokenizer that learns discretization geometry end-to-end. LGQ replaces hard nearest-neighbor lookup with temperature-controlled soft assignments, enabling fully differentiable training while recovering hard assignments at inference. The assignments correspond to posterior responsibilities of an isotropic Gaussian mixture and minimize a variational free-energy objective, provably converging to nearest-neighbor quantization in the low-temperature limit. LGQ combines a token-level peakedness regularizer with a global usage regularizer to encourage confident yet balanced code utilization without imposing rigid grids.
  Under a controlled VQGAN-style backbone on ImageNet across multiple vocabulary sizes, LGQ achieves stable optimization and balanced utilization. At 16K codebook size, LGQ improves rFID by 11.88% over FSQ while using 49.96% fewer active codes, and improves rFID by 6.06% over SimVQ with 49.45% lower effective representation rate, achieving comparable fidelity with substantially fewer active entries. Our GitHub repository is available at: https://github.com/KurbanIntelligenceLab/LGQ

</details>


### [13] [OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis](https://arxiv.org/abs/2602.16110)
*Tianwei Lin,Zhongwei Qiu,Wenqiao Zhang,Jiang Liu,Yihan Xie,Mingjian Gao,Zhenxuan Fan,Zhaocheng Li,Sijing Li,Zhongle Xie,Peng LU,Yueting Zhuang,Yingda Xia,Ling Zhang,Beng Chin Ooi*

Main category: cs.CV

TL;DR: OmniCT是一个统一的切片-体积大型视觉语言模型，用于CT场景，通过空间一致性增强和器官级语义增强解决了现有模型在切片与体积理解上的割裂问题。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在CT图像理解上存在割裂：切片驱动模型缺乏跨切片空间一致性，而体积驱动模型粒度粗糙且与切片输入兼容性差。这种缺乏统一建模范式的问题阻碍了医学LVLMs的临床转化。

Method: 提出了OmniCT模型，包含三个核心贡献：1) 空间一致性增强：体积切片组合与三轴位置嵌入引入体积一致性，MoE混合投影实现高效切片-体积适应；2) 器官级语义增强：分割和ROI定位显式对齐解剖区域，强调病灶和器官级语义；3) MedEval-CT：最大的切片-体积CT数据集和混合基准，集成全面评估指标。

Result: OmniCT在各种临床任务中显著优于现有方法，同时满足微观细节敏感性和宏观空间推理需求，为跨模态医学影像理解建立了新范式。

Conclusion: OmniCT解决了CT图像理解中切片与体积建模的割裂问题，通过统一框架实现了空间一致性和语义增强，为医学LVLMs的临床转化提供了有效解决方案。

Abstract: Computed Tomography (CT) is one of the most widely used and diagnostically information-dense imaging modalities, covering critical organs such as the heart, lungs, liver, and colon. Clinical interpretation relies on both slice-driven local features (e.g., sub-centimeter nodules, lesion boundaries) and volume-driven spatial representations (e.g., tumor infiltration, inter-organ anatomical relations). However, existing Large Vision-Language Models (LVLMs) remain fragmented in CT slice versus volumetric understanding: slice-driven LVLMs show strong generalization but lack cross-slice spatial consistency, while volume-driven LVLMs explicitly capture volumetric semantics but suffer from coarse granularity and poor compatibility with slice inputs. The absence of a unified modeling paradigm constitutes a major bottleneck for the clinical translation of medical LVLMs. We present OmniCT, a powerful unified slice-volume LVLM for CT scenarios, which makes three contributions: (i) Spatial Consistency Enhancement (SCE): volumetric slice composition combined with tri-axial positional embedding that introduces volumetric consistency, and an MoE hybrid projection enables efficient slice-volume adaptation; (ii) Organ-level Semantic Enhancement (OSE): segmentation and ROI localization explicitly align anatomical regions, emphasizing lesion- and organ-level semantics; (iii) MedEval-CT: the largest slice-volume CT dataset and hybrid benchmark integrates comprehensive metrics for unified evaluation. OmniCT consistently outperforms existing methods with a substantial margin across diverse clinical tasks and satisfies both micro-level detail sensitivity and macro-level spatial reasoning. More importantly, it establishes a new paradigm for cross-modal medical imaging understanding.

</details>


### [14] [CHAI: CacHe Attention Inference for text2video](https://arxiv.org/abs/2602.16132)
*Joel Mathew Cherian,Ashutosh Muralidhara Bharadwaj,Vima Gupta,Anand Padmanabha Iyer*

Main category: cs.CV

TL;DR: CHAI通过跨推理缓存和Cache Attention机制，在保持视频质量的同时将文本到视频扩散模型的推理速度提升1.65-3.35倍


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频扩散模型因需要顺序去噪3D潜在表示而速度缓慢，现有加速方法要么需要昂贵的模型重训练，要么使用启发式步长跳过但难以在减少去噪步数时保持视频质量

Method: 提出CHAI框架，采用跨推理缓存技术，引入Cache Attention机制，通过选择性注意力机制在语义相关提示之间有效重用缓存的潜在表示

Result: 使用Cache Attention仅需8个去噪步即可生成高质量视频，集成到OpenSora 1.2系统后，推理速度比基线快1.65-3.35倍且保持视频质量

Conclusion: CHAI通过跨推理缓存和Cache Attention机制，在显著加速文本到视频生成的同时保持了视频质量，为解决扩散模型推理延迟问题提供了有效方案

Abstract: Text-to-video diffusion models deliver impressive results but remain slow because of the sequential denoising of 3D latents. Existing approaches to speed up inference either require expensive model retraining or use heuristic-based step skipping, which struggles to maintain video quality as the number of denoising steps decreases. Our work, CHAI, aims to use cross-inference caching to reduce latency while maintaining video quality. We introduce Cache Attention as an effective method for attending to shared objects/scenes across cross-inference latents. This selective attention mechanism enables effective reuse of cached latents across semantically related prompts, yielding high cache hit rates. We show that it is possible to generate high-quality videos using Cache Attention with as few as 8 denoising steps. When integrated into the overall system, CHAI is 1.65x - 3.35x faster than baseline OpenSora 1.2 while maintaining video quality.

</details>


### [15] [IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models](https://arxiv.org/abs/2602.16138)
*Parsa Madinei,Srijita Karmakar,Russell Cohen Hoffing,Felix Gervitz,Miguel P. Eckstein*

Main category: cs.CV

TL;DR: IRIS利用实时眼动追踪数据解决开放视觉问答中的歧义问题，无需额外训练，通过注视点分析将歧义问题的回答准确率从35.2%提升至77.2%


<details>
  <summary>Details</summary>
Motivation: 开放视觉问答中存在大量歧义问题，传统视觉语言模型难以准确理解用户意图。眼动追踪数据包含丰富的认知信息，可用于解决这种歧义性

Method: IRIS通过实时眼动追踪，分析用户开始提问时的注视点（最富信息量的时间窗口），将这些注视数据融入视觉语言模型的推理过程，无需额外训练

Result: 在500个独特图像-问题对的研究中，IRIS将歧义问题的回答准确率从35.2%大幅提升至77.2%，同时在非歧义问题上保持性能，在不同架构的VLMs上均表现一致改进

Conclusion: 眼动追踪数据是解决视觉问答歧义性的有效工具，IRIS提供了一种无需训练的方法，显著提升了模型对用户意图的理解能力，并发布了相关数据集和评估工具

Abstract: We introduce IRIS (Intent Resolution via Inference-time Saccades), a novel training-free approach that uses eye-tracking data in real-time to resolve ambiguity in open-ended VQA. Through a comprehensive user study with 500 unique image-question pairs, we demonstrate that fixations closest to the time participants start verbally asking their questions are the most informative for disambiguation in Large VLMs, more than doubling the accuracy of responses on ambiguous questions (from 35.2% to 77.2%) while maintaining performance on unambiguous queries. We evaluate our approach across state-of-the-art VLMs, showing consistent improvements when gaze data is incorporated in ambiguous image-question pairs, regardless of architectural differences. We release a new benchmark dataset to use eye movement data for disambiguated VQA, a novel real-time interactive protocol, and an evaluation suite.

</details>


### [16] [Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing](https://arxiv.org/abs/2602.16149)
*Huichan Seo,Minki Hong,Sieun Choi,Jihie Kim,Jean Oh*

Main category: cs.CV

TL;DR: 研究发现文本到图像生成中的身份保持失败在图像到图像编辑中普遍存在，且存在人口统计学上的不平等分布，表现为软擦除和刻板印象替换两种失败模式。


<details>
  <summary>Details</summary>
Motivation: 虽然文本到图像生成中的人口统计学偏见已有研究，但指令引导的图像到图像编辑中的人口统计学条件失败尚未得到充分探索。研究者希望探究相同的编辑指令是否会在不同人口统计学主体上产生系统性不同的结果。

Method: 研究者构建了一个受控基准测试，通过生成和编辑基于种族、性别和年龄的人像照片来探测人口统计学条件行为。使用诊断提示集，并通过视觉语言模型评分和人工评估来评估多个编辑器。

Result: 分析显示身份保持失败普遍存在，在人口统计学上分布不均，并受到隐性社会先验的影响，包括职业驱动的性别推断。研究发现，无需模型更新的提示级身份约束可以显著减少少数群体的身份变化，而对多数群体的人像影响较小。

Conclusion: 身份保持是图像到图像编辑中一个核心且人口统计学上分布不均的失败模式。研究结果推动了开发人口统计学鲁棒的编辑系统，揭示了当前编辑器中存在不对称的身份先验。

Abstract: Demographic bias in text-to-image (T2I) generation is well studied, yet demographic-conditioned failures in instruction-guided image-to-image (I2I) editing remain underexplored. We examine whether identical edit instructions yield systematically different outcomes across subject demographics in open-weight I2I editors. We formalize two failure modes: Soft Erasure, where edits are silently weakened or ignored in the output image, and Stereotype Replacement, where edits introduce unrequested, stereotype-consistent attributes. We introduce a controlled benchmark that probes demographic-conditioned behavior by generating and editing portraits conditioned on race, gender, and age using a diagnostic prompt set, and evaluate multiple editors with vision-language model (VLM) scoring and human evaluation. Our analysis shows that identity preservation failures are pervasive, demographically uneven, and shaped by implicit social priors, including occupation-driven gender inference. Finally, we demonstrate that a prompt-level identity constraint, without model updates, can substantially reduce demographic change for minority groups while leaving majority-group portraits largely unchanged, revealing asymmetric identity priors in current editors. Together, our findings establish identity preservation as a central and demographically uneven failure mode in I2I editing and motivate demographic-robust editing systems. Project page: https://seochan99.github.io/i2i-demographic-bias

</details>


### [17] [Uncertainty-Guided Inference-Time Depth Adaptation for Transformer-Based Visual Tracking](https://arxiv.org/abs/2602.16160)
*Patrick Poggi,Divake Kumar,Theja Tulabandhula,Amit Ranjan Trivedi*

Main category: cs.CV

TL;DR: UncL-STARK：一种基于Transformer的跟踪器动态深度自适应方法，通过不确定性估计和反馈策略减少计算成本，保持跟踪精度


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的单目标跟踪器采用固定深度推理，无论视觉复杂度如何都对每一帧执行完整的编码器-解码器堆栈，在时间连贯帧占主导的长视频序列中产生不必要的计算成本

Method: 提出UncL-STARK，一种架构保持方法，通过随机深度训练和知识蒸馏微调模型，使其在多个中间深度保持预测鲁棒性；运行时从角点定位热图推导轻量级不确定性估计，并利用反馈驱动策略基于预测置信度选择下一帧的编码器和解码器深度

Result: 在GOT-10k和LaSOT上的实验显示：计算量减少达12%，延迟降低8.9%，能耗节省10.8%，同时跟踪精度保持在完整深度基线的0.2%范围内

Conclusion: UncL-STARK能够在保持Transformer跟踪器架构不变的情况下，通过动态深度自适应显著减少计算成本，同时维持跟踪精度，特别适用于长视频序列中的高效跟踪

Abstract: Transformer-based single-object trackers achieve state-of-the-art accuracy but rely on fixed-depth inference, executing the full encoder--decoder stack for every frame regardless of visual complexity, thereby incurring unnecessary computational cost in long video sequences dominated by temporally coherent frames. We propose UncL-STARK, an architecture-preserving approach that enables dynamic, uncertainty-aware depth adaptation in transformer-based trackers without modifying the underlying network or adding auxiliary heads. The model is fine-tuned to retain predictive robustness at multiple intermediate depths using random-depth training with knowledge distillation, thus enabling safe inference-time truncation. At runtime, we derive a lightweight uncertainty estimate directly from the model's corner localization heatmaps and use it in a feedback-driven policy that selects the encoder and decoder depth for the next frame based on the prediction confidence by exploiting temporal coherence in video. Extensive experiments on GOT-10k and LaSOT demonstrate up to 12\% GFLOPs reduction, 8.9\% latency reduction, and 10.8\% energy savings while maintaining tracking accuracy within 0.2\% of the full-depth baseline across both short-term and long-term sequences.

</details>


### [18] [DataCube: A Video Retrieval Platform via Natural Language Semantic Profiling](https://arxiv.org/abs/2602.16231)
*Yiming Ju,Hanyu Zhao,Quanyue Ma,Donglin Hao,Chengwei Wu,Ming Li,Songjing Wang,Tengfei Pan*

Main category: cs.CV

TL;DR: DataCube是一个智能视频处理平台，能够自动处理视频、进行多维度分析，并支持查询驱动的检索，帮助用户从大规模视频库中高效构建定制化视频数据集。


<details>
  <summary>Details</summary>
Motivation: 虽然大规模视频库越来越容易获取，但将原始视频转化为高质量、任务特定的数据集仍然成本高昂且效率低下。需要一种智能化的解决方案来简化视频数据的处理和分析流程。

Method: DataCube平台通过构建视频片段的结构化语义表示，支持混合检索（包括神经重排序和深度语义匹配），并提供交互式Web界面，让用户能够高效地从海量视频库中构建定制化视频子集。

Result: DataCube系统已公开可用（https://datacube.baai.ac.cn/），用户可以通过该系统处理自己的私有视频集合，构建可搜索的视频系统，用于训练、分析和评估等任务。

Conclusion: DataCube提供了一个高效的智能平台，能够显著降低视频数据处理成本，提高视频理解和生成任务的数据集构建效率，支持用户从大规模视频库中快速获取所需内容。

Abstract: Large-scale video repositories are increasingly available for modern video understanding and generation tasks. However, transforming raw videos into high-quality, task-specific datasets remains costly and inefficient. We present DataCube, an intelligent platform for automatic video processing, multi-dimensional profiling, and query-driven retrieval. DataCube constructs structured semantic representations of video clips and supports hybrid retrieval with neural re-ranking and deep semantic matching. Through an interactive web interface, users can efficiently construct customized video subsets from massive repositories for training, analysis, and evaluation, and build searchable systems over their own private video collections. The system is publicly accessible at https://datacube.baai.ac.cn/. Demo Video: https://baai-data-cube.ks3-cn-beijing.ksyuncs.com/custom/Adobe%20Express%20-%202%E6%9C%8818%E6%97%A5%20%281%29%281%29%20%281%29.mp4

</details>


### [19] [EasyControlEdge: A Foundation-Model Fine-Tuning for Edge Detection](https://arxiv.org/abs/2602.16238)
*Hiroki Nakamura,Hiroto Iino,Masashi Okada,Tadahiro Taniguchi*

Main category: cs.CV

TL;DR: EasyControlEdge通过适配图像生成基础模型进行边缘检测，在有限训练数据下实现清晰的边缘检测，支持通过引导尺度控制边缘密度


<details>
  <summary>Details</summary>
Motivation: 现实世界边缘检测（如平面图墙壁、卫星道路/建筑、医学器官边界）需要清晰度和数据效率，但在有限训练样本下生成清晰的原始边缘图仍然具有挑战性。图像生成基础模型在下游任务中表现良好，但其预训练先验用于数据高效迁移和迭代细化以保留高频细节的能力在边缘检测中尚未充分利用。

Method: 提出边缘专业化的图像生成基础模型适配方法：1）引入边缘导向目标函数和高效像素空间损失，更好地将基础模型专门化用于边缘检测；2）在推理时引入基于无条件动态的引导，使单个模型能够通过引导尺度控制边缘密度。

Result: 在BSDS500、NYUDv2、BIPED和CubiCasa数据集上的实验表明，该方法相比最先进方法取得了一致的性能提升，特别是在无后处理的清晰度评估和有限训练数据条件下表现优异。

Conclusion: EasyControlEdge成功将图像生成基础模型适配用于边缘检测任务，实现了在有限数据下的清晰边缘检测，并通过引导机制提供了边缘密度控制能力，为现实世界边缘检测应用提供了有效的解决方案。

Abstract: We propose EasyControlEdge, adapting an image-generation foundation model to edge detection. In real-world edge detection (e.g., floor-plan walls, satellite roads/buildings, and medical organ boundaries), crispness and data efficiency are crucial, yet producing crisp raw edge maps with limited training samples remains challenging. Although image-generation foundation models perform well on many downstream tasks, their pretrained priors for data-efficient transfer and iterative refinement for high-frequency detail preservation remain underexploited for edge detection. To enable crisp and data-efficient edge detection using these capabilities, we introduce an edge-specialized adaptation of image-generation foundation models. To better specialize the foundation model for edge detection, we incorporate an edge-oriented objective with an efficient pixel-space loss. At inference, we introduce guidance based on unconditional dynamics, enabling a single model to control the edge density through a guidance scale. Experiments on BSDS500, NYUDv2, BIPED, and CubiCasa compare against state-of-the-art methods and show consistent gains, particularly under no-post-processing crispness evaluation and with limited training data.

</details>


### [20] [AFFMAE: Scalable and Efficient Vision Pretraining for Desktop Graphics Cards](https://arxiv.org/abs/2602.16249)
*David Smerkous,Zian Wang,Behzad Najafian*

Main category: cs.CV

TL;DR: AFFMAE是一种基于自适应非网格标记合并的掩码友好型分层预训练框架，在保持分层可扩展性的同时显著降低计算和内存需求


<details>
  <summary>Details</summary>
Motivation: 自监督预训练虽然改变了计算机视觉，但高分辨率训练通常需要服务器级基础设施，限制了研究实验室开发领域特定基础模型。MAE通过仅编码可见标记来减少计算，但与分层下采样架构结合存在结构挑战

Method: 提出AFFMAE框架，基于自适应非网格标记合并，丢弃掩码标记并仅在可见标记上执行动态合并，移除密集网格假设，开发数值稳定的混合精度Flash风格聚类注意力内核，通过深度监督缓解稀疏阶段表示崩溃

Result: 在高分辨率电子显微镜分割任务中，AFFMAE在相同参数数量下匹配ViT-MAE性能，同时减少FLOPs高达7倍，内存使用减半，在单张RTX 5090上实现更快训练

Conclusion: AFFMAE为资源受限环境下的高分辨率视觉预训练提供了有效的解决方案，通过创新的标记合并策略和优化实现了计算效率的大幅提升

Abstract: Self-supervised pretraining has transformed computer vision by enabling data-efficient fine-tuning, yet high-resolution training typically requires server-scale infrastructure, limiting in-domain foundation model development for many research laboratories. Masked Autoencoders (MAE) reduce computation by encoding only visible tokens, but combining MAE with hierarchical downsampling architectures remains structurally challenging due to dense grid priors and mask-aware design compromises. We introduce AFFMAE, a masking-friendly hierarchical pretraining framework built on adaptive, off-grid token merging. By discarding masked tokens and performing dynamic merging exclusively over visible tokens, AFFMAE removes dense-grid assumptions while preserving hierarchical scalability. We developed numerically stable mixed-precision Flash-style cluster attention kernels, and mitigate sparse-stage representation collapse via deep supervision. On high-resolution electron microscopy segmentation, AFFMAE matches ViT-MAE performance at equal parameter count while reducing FLOPs by up to 7x, halving memory usage, and achieving faster training on a single RTX 5090. Code available at https://github.com/najafian-lab/affmae.

</details>


### [21] [Breaking the Sub-Millimeter Barrier: Eyeframe Acquisition from Color Images](https://arxiv.org/abs/2602.16281)
*Manel Guzmán,Antonio Agudo*

Main category: cs.CV

TL;DR: 提出基于计算机视觉的多视角眼框镜架追踪算法，替代传统机械测量方法，实现亚毫米级精度测量


<details>
  <summary>Details</summary>
Motivation: 传统眼框镜架追踪依赖机械工具，需要精确校准和额外设备，流程耗时且效率低，需要更高效的工作流程

Method: 基于InVision系统采集图像，采用多视角信息处理，包括图像采集、镜架分割、深度估计和多视角处理，将分割的RGB图像与深度数据结合进行精确轮廓测量

Result: 在真实数据上测试不同配置和变体，仅使用彩色静态图像就能获得具有竞争力的测量结果，无需专用追踪设备

Conclusion: 提出的基于人工智能视觉的方法能够替代传统机械测量，减少工作流程复杂度，为光学技术人员提供更高效的解决方案

Abstract: Eyeframe lens tracing is an important process in the optical industry that requires sub-millimeter precision to ensure proper lens fitting and optimal vision correction. Traditional frame tracers rely on mechanical tools that need precise positioning and calibration, which are time-consuming and require additional equipment, creating an inefficient workflow for opticians. This work presents a novel approach based on artificial vision that utilizes multi-view information. The proposed algorithm operates on images captured from an InVision system. The full pipeline includes image acquisition, frame segmentation to isolate the eyeframe from background, depth estimation to obtain 3D spatial information, and multi-view processing that integrates segmented RGB images with depth data for precise frame contour measurement. To this end, different configurations and variants are proposed and analyzed on real data, providing competitive measurements from still color images with respect to other solutions, while eliminating the need for specialized tracing equipment and reducing workflow complexity for optical technicians.

</details>


### [22] [A Self-Supervised Approach for Enhanced Feature Representations in Object Detection Tasks](https://arxiv.org/abs/2602.16322)
*Santiago C. Vilabella,Pablo Pérez-Núñez,Beatriz Remeseiro*

Main category: cs.CV

TL;DR: 该研究提出了一种基于自监督学习的特征提取器增强方法，能够在较少标注数据的情况下提升目标检测任务的性能，超越基于ImageNet预训练的现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型复杂度增加，获取高质量标注数据成为重大挑战，特别是对于目标检测等复杂任务需要大量时间和资源进行数据标注。企业需要投入大量资金聘请专业人员或外包标注工作，成本高昂。

Method: 采用自监督学习策略，在无标注数据上训练模型，增强特征提取器的能力，使模型能够从更少的标注数据中学习更有效的特征表示。

Result: 提出的方法超越了基于ImageNet预训练的最先进特征提取器，特别是在目标检测任务上表现更优。模型能够更关注对象的关键特征，获得更好的特征表示，从而提高了可靠性和鲁棒性。

Conclusion: 通过增强特征提取器可以有效缓解标注数据不足的挑战，使模型能够用更少的标注数据学习更有效的表示，为实际应用提供了更经济高效的解决方案。

Abstract: In the fast-evolving field of artificial intelligence, where models are increasingly growing in complexity and size, the availability of labeled data for training deep learning models has become a significant challenge. Addressing complex problems like object detection demands considerable time and resources for data labeling to achieve meaningful results. For companies developing such applications, this entails extensive investment in highly skilled personnel or costly outsourcing. This research work aims to demonstrate that enhancing feature extractors can substantially alleviate this challenge, enabling models to learn more effective representations with less labeled data. Utilizing a self-supervised learning strategy, we present a model trained on unlabeled data that outperforms state-of-the-art feature extractors pre-trained on ImageNet and particularly designed for object detection tasks. Moreover, the results demonstrate that our approach encourages the model to focus on the most relevant aspects of an object, thus achieving better feature representations and, therefore, reinforcing its reliability and robustness.

</details>


### [23] [SCAR: Satellite Imagery-Based Calibration for Aerial Recordings](https://arxiv.org/abs/2602.16349)
*Henry Hölzemann,Michael Schleiss*

Main category: cs.CV

TL;DR: SCAR是一种利用卫星图像作为全局参考的航空视觉惯性系统长期自动标定方法，通过将航拍图像与公开正射影像和数字高程模型对齐来估计内外参数，无需人工干预或专用标定动作。


<details>
  <summary>Details</summary>
Motivation: 现有航空视觉惯性系统标定方法依赖专用标定动作或人工地面控制点，难以在长期野外部署条件下维持标定精度。需要一种能利用外部地理空间数据自动检测和修正标定退化的方法。

Method: SCAR利用公开可用的正射影像和数字高程模型作为持久全局参考，通过航拍图像与这些地理空间数据之间的2D-3D对应关系对齐，同时估计相机内参和外参，实现长期自动标定细化。

Result: 在两年内六个大规模航空任务数据集上评估，涵盖不同季节和环境条件。SCAR在所有序列中均优于现有基线方法（Kalibr、COLMAP、VINS-Mono），大幅降低重投影误差中值，并将标定增益转化为显著降低的视觉定位旋转误差和更高位姿精度。

Conclusion: SCAR能够在长期航空操作中提供准确、鲁棒且可重复的标定，无需人工干预，为航空视觉惯性系统的长期部署提供了实用的自动标定解决方案。

Abstract: We introduce SCAR, a method for long-term auto-calibration refinement of aerial visual-inertial systems that exploits georeferenced satellite imagery as a persistent global reference. SCAR estimates both intrinsic and extrinsic parameters by aligning aerial images with 2D--3D correspondences derived from publicly available orthophotos and elevation models. In contrast to existing approaches that rely on dedicated calibration maneuvers or manually surveyed ground control points, our method leverages external geospatial data to detect and correct calibration degradation under field deployment conditions. We evaluate our approach on six large-scale aerial campaigns conducted over two years under diverse seasonal and environmental conditions. Across all sequences, SCAR consistently outperforms established baselines (Kalibr, COLMAP, VINS-Mono), reducing median reprojection error by a large margin, and translating these calibration gains into substantially lower visual localization rotation errors and higher pose accuracy. These results demonstrate that SCAR provides accurate, robust, and reproducible calibration over long-term aerial operations without the need for manual intervention.

</details>


### [24] [Parameter-Free Adaptive Multi-Scale Channel-Spatial Attention Aggregation framework for 3D Indoor Semantic Scene Completion Toward Assisting Visually Impaired](https://arxiv.org/abs/2602.16385)
*Qi He,XiangXiang Wang,Jingtao Zhang,Yongbin Yu,Hongxiang Chu,Manping Fan,JingYe Cai,Zhenglin Yang*

Main category: cs.CV

TL;DR: AMAA框架通过自适应多尺度注意力聚合提升单目3D语义场景补全的性能，在NYUv2基准上取得SSC mIoU 27.25%和SC IoU 43.10%的改进，同时保持系统复杂度可控，可在嵌入式硬件上稳定运行。


<details>
  <summary>Details</summary>
Motivation: 针对室内辅助视觉障碍用户的3D语义场景补全任务，现有单目SSC方法在体素特征可靠性和跨尺度信息传播方面存在不足，容易受到投影扩散和特征纠缠的影响，限制了结构稳定性。

Method: 提出自适应多尺度注意力聚合框架，基于MonoScene管道，通过并行通道-空间注意力聚合联合校准语义和空间维度的体素特征，采用分层自适应特征门控策略稳定多尺度编码器-解码器融合，调节跨尺度信息注入。

Result: 在NYUv2基准测试中，AMAA相比MonoScene取得显著改进：SSC mIoU达到27.25%（提升0.31），SC IoU达到43.10%（提升0.59），系统复杂度未显著增加，可在NVIDIA Jetson嵌入式平台上稳定运行。

Conclusion: AMAA框架提升了单目SSC的质量，为面向视觉障碍用户的室内辅助系统提供了一个可靠且可部署的感知框架，在保持系统轻量化的同时改善了特征可靠性和结构稳定性。

Abstract: In indoor assistive perception for visually impaired users, 3D Semantic Scene Completion (SSC) is expected to provide structurally coherent and semantically consistent occupancy under strictly monocular vision for safety-critical scene understanding. However, existing monocular SSC approaches often lack explicit modeling of voxel-feature reliability and regulated cross-scale information propagation during 2D-3D projection and multi-scale fusion, making them vulnerable to projection diffusion and feature entanglement and thus limiting structural stability.To address these challenges, this paper presents an Adaptive Multi-scale Attention Aggregation (AMAA) framework built upon the MonoScene pipeline. Rather than introducing a heavier backbone, AMAA focuses on reliability-oriented feature regulation within a monocular SSC framework. Specifically, lifted voxel features are jointly calibrated in semantic and spatial dimensions through parallel channel-spatial attention aggregation, while multi-scale encoder-decoder fusion is stabilized via a hierarchical adaptive feature-gating strategy that regulates information injection across scales.Experiments on the NYUv2 benchmark demonstrate consistent improvements over MonoScene without significantly increasing system complexity: AMAA achieves 27.25% SSC mIoU (+0.31) and 43.10% SC IoU (+0.59). In addition, system-level deployment on an NVIDIA Jetson platform verifies that the complete AMAA framework can be executed stably on embedded hardware. Overall, AMAA improves monocular SSC quality and provides a reliable and deployable perception framework for indoor assistive systems targeting visually impaired users.

</details>


### [25] [Visual Self-Refine: A Pixel-Guided Paradigm for Accurate Chart Parsing](https://arxiv.org/abs/2602.16455)
*Jinsong Li,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jiaqi Wang,Dahua Lin*

Main category: cs.CV

TL;DR: 该论文提出了视觉自我精炼（VSR）范式，通过让模型生成像素级定位输出并可视化，然后反馈给自己进行视觉感知错误的检查和修正，特别应用于图表解析任务中。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型在文本层面的推理和自我修正能力很强，但在视觉感知为中心的任务（如图表解析）中表现不佳，容易出现数据遗漏、错位和幻觉等问题。受人类使用手指作为"视觉锚点"确保阅读复杂图表准确性的启发，需要一种新的方法来提升模型在视觉密集任务中的准确性。

Method: 提出了视觉自我精炼（VSR）范式，让模型能够生成像素级定位输出，将其可视化，然后将这些可视化反馈给模型自身，使其能够直观地检查和修正潜在的视觉感知错误。在图表解析领域具体实现为ChartVSR模型，将解析过程分解为两个阶段：精炼阶段（迭代使用视觉反馈确保所有数据点的像素级定位准确性）和解码阶段（使用这些经过验证的定位作为精确的视觉锚点来解析最终的结构化数据）。

Result: 构建了ChartP-Bench，这是一个新的且极具挑战性的图表解析基准测试。VSR作为一种通用的视觉反馈机制，为提升各种视觉中心任务的准确性提供了有前景的新方向。

Conclusion: VSR范式通过让模型生成并可视化像素级定位输出，然后进行自我反馈和修正，显著提升了模型在视觉密集任务（如图表解析）中的准确性。该方法不仅解决了现有模型在图表解析中的局限性，还为广泛的视觉中心任务提供了通用的视觉反馈机制。

Abstract: While Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities for reasoning and self-correction at the textual level, these strengths provide minimal benefits for complex tasks centered on visual perception, such as Chart Parsing. Existing models often struggle with visually dense charts, leading to errors like data omission, misalignment, and hallucination. Inspired by the human strategy of using a finger as a ``visual anchor'' to ensure accuracy when reading complex charts, we propose a new paradigm named Visual Self-Refine (VSR). The core idea of VSR is to enable a model to generate pixel-level localization outputs, visualize them, and then feed these visualizations back to itself, allowing it to intuitively inspect and correct its own potential visual perception errors. We instantiate the VSR paradigm in the domain of Chart Parsing by proposing ChartVSR. This model decomposes the parsing process into two stages: a Refine Stage, where it iteratively uses visual feedback to ensure the accuracy of all data points' Pixel-level Localizations, and a Decode Stage, where it uses these verified localizations as precise visual anchors to parse the final structured data. To address the limitations of existing benchmarks, we also construct ChartP-Bench, a new and highly challenging benchmark for chart parsing. Our work also highlights VSR as a general-purpose visual feedback mechanism, offering a promising new direction for enhancing accuracy on a wide range of vision-centric tasks.

</details>


### [26] [Benchmarking Adversarial Robustness and Adversarial Training Strategies for Object Detection](https://arxiv.org/abs/2602.16494)
*Alexis Winter,Jean-Vincent Martini,Romaric Audigier,Angelique Loesch,Bertrand Luvison*

Main category: cs.CV

TL;DR: 该论文针对目标检测模型对抗攻击防御评估缺乏标准化的问题，提出了统一的基准框架，研究了攻击的跨架构可迁移性和最有效的对抗训练策略。


<details>
  <summary>Details</summary>
Motivation: 目标检测模型在自动驾驶等系统中至关重要，但其对抗攻击敏感性带来严重安全风险。当前防御研究落后于分类任务，主要原因是缺乏标准化评估，现有工作使用不同数据集、不一致的效率指标和不同的扰动成本度量，导致难以全面比较攻击或防御方法。

Method: 首先提出一个专注于数字、非基于补丁攻击的统一基准框架，引入特定指标来分离定位和分类错误，并使用多种感知度量评估攻击成本。基于该基准，对最先进的攻击和多种检测器进行了广泛实验。

Result: 研究发现两个主要结论：1）现代对抗攻击对目标检测模型在基于transformer的架构上表现出显著缺乏可迁移性；2）最鲁棒的对抗训练策略是利用由不同目标（如空间和语义）的高扰动攻击组成的混合数据集，这优于在任何单一攻击上的训练。

Conclusion: 该研究填补了目标检测对抗攻击评估的标准化空白，提出的基准框架为公平比较攻击方法提供了工具，同时揭示了攻击跨架构可迁移性的局限性和混合攻击对抗训练的有效性，为目标检测模型的鲁棒性研究提供了重要指导。

Abstract: Object detection models are critical components of automated systems, such as autonomous vehicles and perception-based robots, but their sensitivity to adversarial attacks poses a serious security risk. Progress in defending these models lags behind classification, hindered by a lack of standardized evaluation. It is nearly impossible to thoroughly compare attack or defense methods, as existing work uses different datasets, inconsistent efficiency metrics, and varied measures of perturbation cost. This paper addresses this gap by investigating three key questions: (1) How can we create a fair benchmark to impartially compare attacks? (2) How well do modern attacks transfer across different architectures, especially from Convolutional Neural Networks to Vision Transformers? (3) What is the most effective adversarial training strategy for robust defense? To answer these, we first propose a unified benchmark framework focused on digital, non-patch-based attacks. This framework introduces specific metrics to disentangle localization and classification errors and evaluates attack cost using multiple perceptual metrics. Using this benchmark, we conduct extensive experiments on state-of-the-art attacks and a wide range of detectors. Our findings reveal two major conclusions: first, modern adversarial attacks against object detection models show a significant lack of transferability to transformer-based architectures. Second, we demonstrate that the most robust adversarial training strategy leverages a dataset composed of a mix of high-perturbation attacks with different objectives (e.g., spatial and semantic), which outperforms training on any single attack.

</details>


### [27] [DressWild: Feed-Forward Pose-Agnostic Garment Sewing Pattern Generation from In-the-Wild Images](https://arxiv.org/abs/2602.16502)
*Zeng Tao,Ying Jiang,Yunuo Chen,Tianyi Xie,Huamin Wang,Yingnian Wu,Yin Yang,Abishek Sampath Kumar,Kenji Tashiro,Chenfanfu Jiang*

Main category: cs.CV

TL;DR: DressWild：从单张野外图像重建物理一致的2D缝纫图案和3D服装的端到端前馈方法


<details>
  <summary>Details</summary>
Motivation: 现有服装图案生成方法存在局限性：前馈方法难以处理多样姿态和视角，基于优化的方法计算成本高且难以扩展。需要一种能够生成可编辑、可分离、模拟就绪的服装图案的方法，适用于服装建模和制造应用。

Method: 提出DressWild前馈流程：1）利用视觉语言模型在图像层面归一化姿态变化；2）提取姿态感知、3D感知的服装特征；3）通过基于Transformer的编码器融合特征；4）预测可直接用于物理模拟、纹理合成和虚拟试穿的缝纫图案参数。

Result: 实验表明，该方法能够从单张野外图像稳健地恢复多样缝纫图案和对应的3D服装，无需多视角输入或迭代优化，为真实服装模拟和动画提供了高效可扩展的解决方案。

Conclusion: DressWild通过结合视觉语言模型和Transformer架构，实现了从单张野外图像到物理一致缝纫图案和3D服装的端到端重建，解决了现有方法在姿态多样性、计算效率和可扩展性方面的限制。

Abstract: Recent advances in garment pattern generation have shown promising progress. However, existing feed-forward methods struggle with diverse poses and viewpoints, while optimization-based approaches are computationally expensive and difficult to scale. This paper focuses on sewing pattern generation for garment modeling and fabrication applications that demand editable, separable, and simulation-ready garments. We propose DressWild, a novel feed-forward pipeline that reconstructs physics-consistent 2D sewing patterns and the corresponding 3D garments from a single in-the-wild image. Given an input image, our method leverages vision-language models (VLMs) to normalize pose variations at the image level, then extract pose-aware, 3D-informed garment features. These features are fused through a transformer-based encoder and subsequently used to predict sewing pattern parameters, which can be directly applied to physical simulation, texture synthesis, and multi-layer virtual try-on. Extensive experiments demonstrate that our approach robustly recovers diverse sewing patterns and the corresponding 3D garments from in-the-wild images without requiring multi-view inputs or iterative optimization, offering an efficient and scalable solution for realistic garment simulation and animation.

</details>


### [28] [Let's Split Up: Zero-Shot Classifier Edits for Fine-Grained Video Understanding](https://arxiv.org/abs/2602.16545)
*Kaiting Liu,Hazel Doughty*

Main category: cs.CV

TL;DR: 提出视频分类中的类别分割任务，将粗粒度类别细分为更精细的子类别，无需重新训练整个模型


<details>
  <summary>Details</summary>
Motivation: 现有视频识别模型通常在固定分类体系上训练，分类过于粗糙，将不同对象、方式或结果合并到单一标签下。当任务和定义演变时，这些模型无法适应新出现的区分，而收集新标注并重新训练成本高昂

Method: 提出零样本编辑方法，利用视频分类器的潜在组合结构来暴露细粒度区分，无需额外数据。同时展示低样本微调虽然简单但非常有效，并能从零样本初始化中受益

Result: 在新建立的视频类别分割基准测试中，该方法显著优于视觉语言基线方法，在新增分割类别上提高准确性，同时不牺牲其他类别的性能

Conclusion: 类别分割任务能够有效解决视频分类中粗粒度标签的问题，提出的零样本编辑方法结合低样本微调，可以在不重新训练整个模型的情况下实现细粒度分类改进

Abstract: Video recognition models are typically trained on fixed taxonomies which are often too coarse, collapsing distinctions in object, manner or outcome under a single label. As tasks and definitions evolve, such models cannot accommodate emerging distinctions and collecting new annotations and retraining to accommodate such changes is costly. To address these challenges, we introduce category splitting, a new task where an existing classifier is edited to refine a coarse category into finer subcategories, while preserving accuracy elsewhere. We propose a zero-shot editing method that leverages the latent compositional structure of video classifiers to expose fine-grained distinctions without additional data. We further show that low-shot fine-tuning, while simple, is highly effective and benefits from our zero-shot initialization. Experiments on our new video benchmarks for category splitting demonstrate that our method substantially outperforms vision-language baselines, improving accuracy on the newly split categories without sacrificing performance on the rest. Project page: https://kaitingliu.github.io/Category-Splitting/.

</details>


### [29] [Arc2Morph: Identity-Preserving Facial Morphing with Arc2Face](https://arxiv.org/abs/2602.16569)
*Nicolò Di Domenico,Annalisa Franco,Matteo Ferrara,Davide Maltoni*

Main category: cs.CV

TL;DR: 本文提出了一种基于Arc2Face的新型人脸融合攻击技术，该技术利用身份条件化的人脸基础模型从紧凑的身份表示中合成逼真的人脸图像，在多个数据集上展示了与传统地标点方法相当的融合攻击潜力。


<details>
  <summary>Details</summary>
Motivation: 人脸融合攻击被认为是电子身份文档中人脸识别系统面临的最严峻威胁之一。这些攻击利用了护照登记过程中的关键漏洞，即许多国家在采集面部图像时缺乏监督的实时捕获过程。因此需要开发更有效的融合攻击技术来评估和提升防御系统的能力。

Method: 提出了一种基于Arc2Face的新型人脸融合技术。Arc2Face是一个身份条件化的人脸基础模型，能够从紧凑的身份表示中合成逼真的人脸图像。该方法利用深度学习模型在融合生成过程中有效保持和管理身份信息。

Result: 在两个大规模隔离的人脸融合攻击检测数据集以及从FEI和ONOT衍生的两个新型融合人脸数据集上进行了实验。结果显示，所提出的深度学习方法在融合攻击潜力指标上与传统的地标点技术相当，而后者通常被认为是最具挑战性的方法。

Conclusion: 所提出的方法在融合生成过程中能够有效保持和管理身份信息，证明了基于深度学习的人脸融合技术可以达到与传统地标点方法相当的攻击潜力，为评估人脸识别系统的安全性提供了新的有效工具。

Abstract: Face morphing attacks are widely recognized as one of the most challenging threats to face recognition systems used in electronic identity documents. These attacks exploit a critical vulnerability in passport enrollment procedures adopted by many countries, where the facial image is often acquired without a supervised live capture process. In this paper, we propose a novel face morphing technique based on Arc2Face, an identity-conditioned face foundation model capable of synthesizing photorealistic facial images from compact identity representations. We demonstrate the effectiveness of the proposed approach by comparing the morphing attack potential metric on two large-scale sequestered face morphing attack detection datasets against several state-of-the-art morphing methods, as well as on two novel morphed face datasets derived from FEI and ONOT. Experimental results show that the proposed deep learning-based approach achieves a morphing attack potential comparable to that of landmark-based techniques, which have traditionally been regarded as the most challenging. These findings confirm the ability of the proposed method to effectively preserve and manage identity information during the morph generation process.

</details>


### [30] [A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification](https://arxiv.org/abs/2602.16590)
*Qi You,Yitai Cheng,Zichao Zeng,James Haworth*

Main category: cs.CV

TL;DR: CLIP-MHAdapter：一种轻量级CLIP适配器，通过多头自注意力机制增强对街景图像细粒度属性的分类能力


<details>
  <summary>Details</summary>
Motivation: 街景图像属性分类是自动驾驶、城市分析等高德地图构建的重要任务。现有基于CLIP的适配方法主要依赖全局图像嵌入，难以捕捉复杂街景中细粒度的局部属性特征，且计算成本较高。

Method: 提出CLIP-MHAdapter，在轻量级CLIP适配范式基础上，添加配备多头自注意力的瓶颈MLP，作用于patch tokens以建模patch间依赖关系，仅需约140万可训练参数。

Result: 在Global StreetScapes数据集的八个属性分类任务上，CLIP-MHAdapter取得了优越或具有竞争力的准确率，创造了新的SOTA结果，同时保持了较低的计算成本。

Conclusion: CLIP-MHAdapter通过多头自注意力机制有效提升了CLIP模型对街景图像细粒度属性的分类能力，在保持轻量化的同时实现了优异的性能。

Abstract: Street-view image attribute classification is a vital downstream task of image classification, enabling applications such as autonomous driving, urban analytics, and high-definition map construction. It remains computationally demanding whether training from scratch, initialising from pre-trained weights, or fine-tuning large models. Although pre-trained vision-language models such as CLIP offer rich image representations, existing adaptation or fine-tuning methods often rely on their global image embeddings, limiting their ability to capture fine-grained, localised attributes essential in complex, cluttered street scenes. To address this, we propose CLIP-MHAdapter, a variant of the current lightweight CLIP adaptation paradigm that appends a bottleneck MLP equipped with multi-head self-attention operating on patch tokens to model inter-patch dependencies. With approximately 1.4 million trainable parameters, CLIP-MHAdapter achieves superior or competitive accuracy across eight attribute classification tasks on the Global StreetScapes dataset, attaining new state-of-the-art results while maintaining low computational cost. The code is available at https://github.com/SpaceTimeLab/CLIP-MHAdapter.

</details>


### [31] [Unpaired Image-to-Image Translation via a Self-Supervised Semantic Bridge](https://arxiv.org/abs/2602.16664)
*Jiaming Liu,Felix Petersen,Yunhe Gao,Yabin Zhang,Hyojin Kim,Akshay S. Chaudhari,Yu Sun,Stefano Ermon,Sergios Gatidis*

Main category: cs.CV

TL;DR: SSB框架利用自监督视觉编码器学习外观不变但保留几何结构的表示，构建共享潜在空间来引导扩散桥，实现无需跨域监督的空间保真图像翻译


<details>
  <summary>Details</summary>
Motivation: 现有对抗扩散和扩散反演方法在无配对图像翻译中存在局限性：对抗方法需要目标域对抗损失训练，限制了泛化能力；扩散反演方法由于噪声潜在表示不完美导致翻译保真度低

Method: 提出自监督语义桥（SSB）框架，整合外部语义先验到扩散桥模型中。利用自监督视觉编码器学习外观不变但捕获几何结构的表示，形成共享潜在空间来条件化扩散桥

Result: 在具有挑战性的医学图像合成任务中，SSB在域内和域外设置下均优于现有方法，并能轻松扩展到高质量文本引导编辑

Conclusion: SSB框架通过整合语义先验和自监督表示学习，实现了无需跨域监督的空间保真图像翻译，在医学图像合成和文本引导编辑中表现出色

Abstract: Adversarial diffusion and diffusion-inversion methods have advanced unpaired image-to-image translation, but each faces key limitations. Adversarial approaches require target-domain adversarial loss during training, which can limit generalization to unseen data, while diffusion-inversion methods often produce low-fidelity translations due to imperfect inversion into noise-latent representations. In this work, we propose the Self-Supervised Semantic Bridge (SSB), a versatile framework that integrates external semantic priors into diffusion bridge models to enable spatially faithful translation without cross-domain supervision. Our key idea is to leverage self-supervised visual encoders to learn representations that are invariant to appearance changes but capture geometric structure, forming a shared latent space that conditions the diffusion bridges. Extensive experiments show that SSB outperforms strong prior methods for challenging medical image synthesis in both in-domain and out-of-domain settings, and extends easily to high-quality text-guided editing.

</details>


### [32] [PredMapNet: Future and Historical Reasoning for Consistent Online HD Vectorized Map Construction](https://arxiv.org/abs/2602.16669)
*Bo Lang,Nirav Savaliya,Zhihao Zheng,Jinglun Feng,Zheng-Hang Yeh,Mooi Choo Chuah*

Main category: cs.CV

TL;DR: 提出了一种用于在线高精地图构建的端到端框架，通过语义感知查询生成、历史栅格地图记忆和短期未来预测来提升时间一致性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于查询的方法存在随机查询初始化和隐式时间建模问题，导致构建全局地图时出现时间不一致和不稳定。需要解决在线高精地图构建中的时间连续性问题。

Method: 1. 语义感知查询生成器：用空间对齐的语义掩码初始化查询，全局捕捉场景级上下文；2. 历史栅格地图记忆：存储每个跟踪实例的细粒度实例级地图，提供显式历史先验；3. 历史地图引导模块：将栅格地图信息整合到跟踪查询中；4. 短期未来引导模块：基于存储的历史轨迹预测地图实例的即时运动。

Result: 在nuScenes和Argoverse2数据集上的广泛实验表明，该方法在保持良好效率的同时，优于当前最先进的方法。

Conclusion: 提出的端到端框架通过联合执行地图实例跟踪和短期预测，有效解决了在线高精地图构建中的时间一致性问题，为自动驾驶提供了更稳定可靠的地图表示。

Abstract: High-definition (HD) maps are crucial to autonomous driving, providing structured representations of road elements to support navigation and planning. However, existing query-based methods often employ random query initialization and depend on implicit temporal modeling, which lead to temporal inconsistencies and instabilities during the construction of a global map. To overcome these challenges, we introduce a novel end-to-end framework for consistent online HD vectorized map construction, which jointly performs map instance tracking and short-term prediction. First, we propose a Semantic-Aware Query Generator that initializes queries with spatially aligned semantic masks to capture scene-level context globally. Next, we design a History Rasterized Map Memory to store fine-grained instance-level maps for each tracked instance, enabling explicit historical priors. A History-Map Guidance Module then integrates rasterized map information into track queries, improving temporal continuity. Finally, we propose a Short-Term Future Guidance module to forecast the immediate motion of map instances based on the stored history trajectories. These predicted future locations serve as hints for tracked instances to further avoid implausible predictions and keep temporal consistency. Extensive experiments on the nuScenes and Argoverse2 datasets demonstrate that our proposed method outperforms state-of-the-art (SOTA) methods with good efficiency.

</details>


### [33] [VETime: Vision Enhanced Zero-Shot Time Series Anomaly Detection](https://arxiv.org/abs/2602.16681)
*Yingyuan Yang,Tian Lan,Yifei Gao,Yimeng Lu,Wenjun He,Meng Wang,Chenghao Liu,Chen Zhang*

Main category: cs.CV

TL;DR: VETime是一个统一时间和视觉模态的时间序列异常检测框架，通过细粒度视觉-时间对齐和动态融合解决现有模型在点异常和上下文异常检测中的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在时间序列异常检测中存在根本性权衡：1D时间模型能提供细粒度点定位但缺乏全局上下文视角，而2D视觉模型能捕捉全局模式但存在时间对齐缺失和信息瓶颈问题，导致点检测粗糙。

Method: VETime提出可逆图像转换和补丁级时间对齐模块建立共享视觉-时间时间线，保持判别细节同时维持时间敏感性；设计异常窗口对比学习机制和任务自适应多模态融合，自适应整合两种模态的互补感知优势。

Result: 大量实验表明，VETime在零样本场景中显著优于最先进模型，在比当前视觉方法更低计算开销的情况下实现了更优的定位精度。

Conclusion: VETime通过统一时间和视觉模态，解决了时间序列异常检测中细粒度点定位与全局上下文感知的权衡问题，为TSAD提供了更有效的解决方案。

Abstract: Time-series anomaly detection (TSAD) requires identifying both immediate Point Anomalies and long-range Context Anomalies. However, existing foundation models face a fundamental trade-off: 1D temporal models provide fine-grained pointwise localization but lack a global contextual perspective, while 2D vision-based models capture global patterns but suffer from information bottlenecks due to a lack of temporal alignment and coarse-grained pointwise detection. To resolve this dilemma, we propose VETime, the first TSAD framework that unifies temporal and visual modalities through fine-grained visual-temporal alignment and dynamic fusion. VETime introduces a Reversible Image Conversion and a Patch-Level Temporal Alignment module to establish a shared visual-temporal timeline, preserving discriminative details while maintaining temporal sensitivity. Furthermore, we design an Anomaly Window Contrastive Learning mechanism and a Task-Adaptive Multi-Modal Fusion to adaptively integrate the complementary perceptual strengths of both modalities. Extensive experiments demonstrate that VETime significantly outperforms state-of-the-art models in zero-shot scenarios, achieving superior localization precision with lower computational overhead than current vision-based approaches. Code available at: https://github.com/yyyangcoder/VETime.

</details>


### [34] [Learning Situated Awareness in the Real World](https://arxiv.org/abs/2602.16682)
*Chuhan Li,Ruilin Han,Joy Hsu,Yongyuan Liang,Rajiv Dhawan,Jiajun Wu,Ming-Hsuan Yang,Xin Eric Wang*

Main category: cs.CV

TL;DR: SAW-Bench是一个新的基准测试，用于评估多模态基础模型在真实世界视频中的自我中心情境感知能力，包含786个智能眼镜拍摄的视频和2071个问答对，揭示了37.66%的人机性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注环境中心的空间关系（场景中物体间的关系），而忽视了需要从观察者视角、姿态和运动角度进行推理的观察者中心关系。为了弥补这一空白，需要开发能够评估自我中心情境感知能力的基准测试。

Method: 研究者使用Ray-Ban Meta (Gen 2)智能眼镜录制了786个涵盖室内外环境的真实世界视频，并创建了2071个人工标注的问答对。该基准测试包含六个不同的感知任务，用于探测模型的观察者中心理解能力。

Result: 评估显示，即使是表现最好的多模态基础模型Gemini 3 Flash，也存在37.66%的人机性能差距。深入分析发现，虽然模型能够利用自我中心视频中的部分几何线索，但常常无法推断出连贯的相机几何结构，导致系统性空间推理错误。

Conclusion: SAW-Bench作为情境空间智能的基准测试，超越了被动观察，转向理解物理基础、观察者中心的动态关系，为评估多模态基础模型的自我中心情境感知能力提供了重要工具。

Abstract: A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.

</details>


### [35] [Are Object-Centric Representations Better At Compositional Generalization?](https://arxiv.org/abs/2602.16689)
*Ferdinand Kapl,Amir Mohammad Karimi Mamaghan,Maximilian Seitzer,Karl Henrik Johansson,Carsten Marr,Stefan Bauer,Andrea Dittadi*

Main category: cs.CV

TL;DR: 研究比较了面向对象表示与密集表示在视觉问答任务中的组合泛化能力，发现面向对象方法在更难的组合泛化场景中表现更好，而密集表示仅在较简单场景中占优且需要更多计算资源。


<details>
  <summary>Details</summary>
Motivation: 组合泛化是人类认知的基础能力，也是机器学习的重大挑战。面向对象表示被认为能支持这种泛化，但在视觉丰富环境中的系统证据有限。研究旨在评估不同视觉编码器在未见过的物体属性组合上的泛化能力。

Method: 在三个受控视觉世界（CLEVRTex、Super-CLEVR、MOVi-C）中创建视觉问答基准测试，公平比较有/无面向对象偏置的视觉编码器。使用DINOv2和SigLIP2作为基础模型及其面向对象版本，控制训练数据多样性、样本量、表示大小、下游模型容量和计算资源。

Result: 1) 面向对象方法在更难的组合泛化场景中表现更优；2) 原始密集表示仅在较简单场景中超越面向对象方法，且通常需要更多下游计算；3) 面向对象模型样本效率更高，用更少图像实现更强泛化，而密集编码器只有在足够数据和多样性时才能赶上或超越。

Conclusion: 当数据集大小、训练数据多样性或下游计算资源中任一因素受限时，面向对象表示能提供更强的组合泛化能力。这表明面向对象表示在资源受限场景中具有实际优势。

Abstract: Compositional generalization, the ability to reason about novel combinations of familiar concepts, is fundamental to human cognition and a critical challenge for machine learning. Object-centric (OC) representations, which encode a scene as a set of objects, are often argued to support such generalization, but systematic evidence in visually rich settings is limited. We introduce a Visual Question Answering benchmark across three controlled visual worlds (CLEVRTex, Super-CLEVR, and MOVi-C) to measure how well vision encoders, with and without object-centric biases, generalize to unseen combinations of object properties. To ensure a fair and comprehensive comparison, we carefully account for training data diversity, sample size, representation size, downstream model capacity, and compute. We use DINOv2 and SigLIP2, two widely used vision encoders, as the foundation models and their OC counterparts. Our key findings reveal that (1) OC approaches are superior in harder compositional generalization settings; (2) original dense representations surpass OC only on easier settings and typically require substantially more downstream compute; and (3) OC models are more sample efficient, achieving stronger generalization with fewer images, whereas dense encoders catch up or surpass them only with sufficient data and diversity. Overall, object-centric representations offer stronger compositional generalization when any one of dataset size, training data diversity, or downstream compute is constrained.

</details>


### [36] [Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning](https://arxiv.org/abs/2602.16702)
*Mingjia Shi,Yinhan He,Yaochen Zhu,Jundong Li*

Main category: cs.CV

TL;DR: 该论文提出了SAP（显著性感知原则选择）方法，通过在高层次推理原则上操作而非token级轨迹，解决视觉语言模型中视觉输入仅在推理开始时提供一次的问题，使后续推理步骤能够重新参考视觉证据，减少物体幻觉并提高推理稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型面临的主要挑战是：视觉输入通常在推理开始时仅提供一次，而文本推理是自回归生成的，导致推理过程逐渐被文本主导，早期视觉基础错误会累积。此外，推理过程中的视觉基础引导通常是粗糙和嘈杂的，难以在长文本推理中进行有效控制。

Method: 提出了SAP（显著性感知原则选择）方法，该方法在高层次推理原则上操作而非token级轨迹，能够在嘈杂反馈下稳定控制离散生成，并允许后续推理步骤在需要时重新参考视觉证据。SAP还支持多路径推理，能够并行探索不同的推理行为。该方法与模型无关且无需数据，不需要额外训练。

Result: 实验结果表明，SAP在可比较的token生成预算下实现了有竞争力的性能，特别是在减少物体幻觉方面表现突出。与CoT式长序列推理相比，SAP产生了更稳定的推理过程和更低的响应延迟。

Conclusion: SAP方法通过在高层次推理原则上操作，有效解决了视觉语言模型中视觉输入仅在推理开始时提供一次的问题，使模型能够在需要时重新参考视觉证据，从而减少物体幻觉并提高推理稳定性，同时支持多路径推理，为视觉语言模型的推理时间计算分配提供了有效解决方案。

Abstract: Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominated and allowing early visual grounding errors to accumulate. Moreover, vanilla guidance for visual grounding during inference is often coarse and noisy, making it difficult to steer reasoning over long texts. To address these challenges, we propose \emph{Saliency-Aware Principle} (SAP) selection. SAP operates on high-level reasoning principles rather than token-level trajectories, which enable stable control over discrete generation under noisy feedback while allowing later reasoning steps to re-consult visual evidence when renewed grounding is required. In addition, SAP supports multi-route inference, enabling parallel exploration of diverse reasoning behaviors. SAP is model-agnostic and data-free, requiring no additional training. Empirical results show that SAP achieves competitive performance, especially in reducing object hallucination, under comparable token-generation budgets while yielding more stable reasoning and lower response latency than CoT-style long sequential reasoning.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [37] [From Conflicts to Collisions: A Two-Stage Collision Scenario-Testing Approach for Autonomous Driving Systems](https://arxiv.org/abs/2602.15837)
*Siyuan Chen,Fuyuan Zhang,Hua Qi,Lei Ma,Tomoyuki Tsuchiya,Michio Hayashi,Manabu Okada*

Main category: cs.RO

TL;DR: 论文提出了一种基于冲突的两阶段自动驾驶场景测试框架，通过先搜索冲突场景再将其突变为碰撞场景，显著提高了测试效率和碰撞类型多样性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统测试方法主要关注已接近碰撞的场景，忽略了其他潜在危险情况。需要一种更全面、高效的测试方法来发现更多类型的碰撞风险。

Method: 提出两阶段测试框架：第一阶段搜索冲突场景（碰撞相关但未实际发生碰撞），第二阶段将这些冲突场景突变为实际碰撞场景。使用冲突作为中间搜索目标来扩大搜索范围。

Result: 在百度Apollo上评估，单次运行可发现多达12种不同的碰撞类型，是现有最佳基准方法的两倍，同时由于冲突导向的突变，所需模拟次数更少。

Conclusion: 使用冲突作为中间目标能够扩大搜索视野，显著提高自动驾驶系统安全评估的效率和效果，为更全面的安全测试提供了新思路。

Abstract: Autonomous driving systems (ADS) are safety-critical and require rigorous testing before public deployment. Simulation-based scenario testing provides a safe and cost-effective alternative to extensive on-road trials, enabling efficient evaluation of ADS under diverse and high-risk conditions. However, existing approaches mainly evaluates the scenarios based on their proximity to collisions and focus on scenarios already close to collision, leaving many other hazardous situations unexplored. To bridge this, we introduce a collision-related concept of conflict as an intermediate search target and propose a two-stage scenario testing framework that first searches for conflicts and then mutates these conflict scenarios to induce actual collisions. Evaluated on Baidu Apollo, our approach reveals up to 12 distinct collision types in a single run, doubling the diversity discovered by state-of-the-art baselines while requiring fewer simulations thanks to conflict-targeted mutations. These results show that using conflicts as intermediate objectives broadens the search horizon and significantly improves the efficiency and effectiveness of ADS safety evaluation.

</details>


### [38] [TurboADMM: A Structure-Exploiting Parallel Solver for Multi-Agent Trajectory Optimization](https://arxiv.org/abs/2602.15838)
*Yucheng Chen*

Main category: cs.RO

TL;DR: TurboADMM是一种专门的多智能体轨迹优化QP求解器，通过ADMM分解、Riccati预热和参数化QP热启动三种技术协同设计，实现了在智能体数量上的近似线性复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体轨迹优化求解器无法同时利用时间结构、智能体分解和迭代相似性。通用QP求解器（如OSQP、MOSEK）在处理多智能体问题时遇到可扩展性问题，而结构利用型求解器（如HPIPM）虽然能利用时间结构，但对密集耦合约束脆弱。

Method: TurboADMM通过三个互补组件的协同设计：1) ADMM分解创建可并行求解的每个智能体子问题，在密集耦合下保持块三对角结构；2) Riccati预热利用时间结构为每个智能体的QP提供高质量的原对偶初始化；3) 参数化QP热启动在qpOASES中跨ADMM迭代重用相似的KKT系统分解。

Result: TurboADMM在智能体数量上实现了经验上的近似线性复杂度，能够高效处理多智能体轨迹优化问题，解决了现有求解器的可扩展性问题。

Conclusion: 通过ADMM分解、Riccati预热和参数化QP热启动的协同设计，TurboADMM为多智能体轨迹优化提供了一种高效的专用QP求解器，能够同时利用时间结构、智能体分解和迭代相似性，实现近似线性的复杂度扩展。

Abstract: Multi-agent trajectory optimization with dense interaction networks require solving large coupled QPs at control rates, yet existing solvers fail to simultaneously exploit temporal structure, agent decomposition, and iteration similarity. One usually treats multi-agent problems monolithically when using general-purpose QP solvers (OSQP, MOSEK), which encounter scalability difficulties with agent count. Structure-exploiting solvers (HPIPM) leverage temporal structure through Riccati recursion but can be vulnerable to dense coupling constraints. We introduce TurboADMM, a specialized single-machine QP solver that achieves empirically near linear complexity in agent count through systematic co-design of three complementary components: (1) ADMM decomposition creates per-agent subproblems solvable in parallel, preserving block-tridiagonal structure under dense coupling; (2) Riccati warmstart exploits temporal structure to provide high-quality primal-dual initialization for each agent's QP; (3) parametric QP hotstart \footnote{In the paper, we refer warmstart as the technique that uses the Riccati equation results as auxiliary QP initialization for a single QP solve, while hotstart as reusing the QR factorization across QP solve iterations.}in qpOASES reuses similar KKT system factorizations across ADMM iterations.

</details>


### [39] [A Decade of Human-Robot Interaction through Immersive Lenses: A Literature Review on Extended Reality as a Research Instrument in Social Robotics](https://arxiv.org/abs/2602.15840)
*André Helgert,Carolin Straßmann,Sabrina C. Eimler*

Main category: cs.RO

TL;DR: 对2015-2025年XR在社交机器人交互研究中的系统综述，发现该领域仍处于实验室模拟阶段，存在硬件软件报告不全、机器人被动、样本同质化等问题，提出了五阶段发展路线图。


<details>
  <summary>Details</summary>
Motivation: 尽管XR技术在人类-机器人交互研究中受到关注，但在社交机器人实证研究中仍未被充分探索。研究者希望通过系统综述了解XR在社交机器人交互研究中的现状、方法和挑战。

Method: 对2015-2025年的6527篇同行评审文章进行系统综述，筛选出33篇符合严格纳入标准的研究。从四个维度进行分析：XR和虚拟社交机器人的使用方式和场景、数据收集分析方法、研究者和参与者人口统计学特征、以及面临的挑战和未来议程。

Result: 研究发现：社交XR-HRI研究仍以实验室模拟为主；关键硬件、软件和机器人规格经常未报告；机器人通常作为被动视觉刺激；现代头戴设备的生物信号和日志功能未被充分利用；研究团队和样本以技术中心、西方、年轻、男性为主；存在硬件延迟、样本同质化、研究周期短浅等局限。

Conclusion: 提出了五阶段路线图来建立可靠的社交XR-HRI研究媒介：促进方法创新、通过应用场景增强生态效度、提高机器人交互质量、促进样本多样性、发展社交XR-HRI分类学。这些方向对于XR从实验室原型发展为社交机器人研究的生态有效工具至关重要。

Abstract: Over the past decade, extended reality (XR), including virtual, augmented, and mixed reality, gained attention as a research instrument in human-robot interaction studies, but remains underexplored in empirical investigations of social robotics. To map the field, we systematically reviewed empirical studies from 2015 to 2025. Of 6,527 peer-reviewed articles, only 33 met strict inclusion criteria. We examined (1) how XR and virtual social robots are used and in which contexts, (2) data collection and analysis methods, (3) demographics of the researchers and participants, and (4) the stated challenges and future agendas. Our findings show that social XR-HRI research is still dominated by laboratory simulations, while crucial specifications like used hardware, software, and robots are often not reported. Robots typically act as passive and less interactive visual stimuli, while the rich biosignal (e.g., eye-tracking) and logging functions of modern head-mounted displays remain largely untapped. The research teams and samples are predominantly tech-centric, Western, young, and male, with frequent gaps in demographic reporting. Key limitations include hardware delays, small homogeneous samples, and short, shallow study cycles. We propose a five-phase roadmap to establish social XR-HRI as a reliable research medium, which includes fostering methodological innovation, a reinforced ecological validity by, e.g., using application contexts, the improvement of the robot's interaction quality, promoting diversity in the sample and the development of a social XR-HRI taxonomy. Advancing in these directions is essential for XR to mature from a lab prototype into an ecologically valid research instrument for social robotics.

</details>


### [40] [ReasonNavi: Human-Inspired Global Map Reasoning for Zero-Shot Embodied Navigation](https://arxiv.org/abs/2602.15864)
*Yuzhuo Ao,Anbang Wang,Yu-Wing Tai,Chi-Keung Tang*

Main category: cs.RO

TL;DR: ReasonNavi是一个受人类启发的导航框架，通过将多模态大语言模型与确定性规划器结合，实现"先推理后行动"的范式，在零样本设置下显著提升导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有具身智能体主要依赖局部自我中心观察，缺乏全局预见性，导致探索效率低下。而人类使用地图进行规划：先全局推理，再局部行动。

Method: 将俯视地图转换为离散推理空间（房间分割和候选目标节点采样），通过多阶段查询MLLM识别与指令最一致的候选目标，利用确定性动作规划器将选定路径点转换为可执行轨迹，结合预训练的目标检测和分割器确保目标识别鲁棒性。

Result: 在三个导航任务中，ReasonNavi始终优于需要大量训练或复杂场景建模的先前方法，提供了可扩展、可解释且全局基础的导航解决方案。

Conclusion: ReasonNavi提供了一个统一的零样本导航框架，无需MLLM微调，避免了基于RL策略的脆弱性，并能自然随着基础模型改进而扩展，为具身导航提供了可扩展、可解释的解决方案。

Abstract: Embodied agents often struggle with efficient navigation because they rely primarily on partial egocentric observations, which restrict global foresight and lead to inefficient exploration. In contrast, humans plan using maps: we reason globally first, then act locally. We introduce ReasonNavi, a human-inspired framework that operationalizes this reason-then-act paradigm by coupling Multimodal Large Language Models (MLLMs) with deterministic planners. ReasonNavi converts a top-down map into a discrete reasoning space by room segmentation and candidate target nodes sampling. An MLLM is then queried in a multi-stage process to identify the candidate most consistent with the instruction (object, image, or text goal), effectively leveraging the model's semantic reasoning ability while sidestepping its weakness in continuous coordinate prediction. The selected waypoint is grounded into executable trajectories using a deterministic action planner over an online-built occupancy map, while pretrained object detectors and segmenters ensure robust recognition at the goal. This yields a unified zero-shot navigation framework that requires no MLLM fine-tuning, circumvents the brittleness of RL-based policies and scales naturally with foundation model improvements. Across three navigation tasks, ReasonNavi consistently outperforms prior methods that demand extensive training or heavy scene modeling, offering a scalable, interpretable, and globally grounded solution to embodied navigation. Project page: https://reasonnavi.github.io/

</details>


### [41] [MARVL: Multi-Stage Guidance for Robotic Manipulation via Vision-Language Models](https://arxiv.org/abs/2602.15872)
*Xunlan Zhou,Xuanlin Chen,Shaowei Zhang,Xiangkun Li,ShengHua Wan,Xiaohai Hu,Yuan Lei,Le Gan,De-chuan Zhan*

Main category: cs.RO

TL;DR: MARVL通过微调视觉语言模型实现空间语义一致性，将任务分解为多阶段子任务，显著提升机器人强化学习的样本效率和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 传统密集奖励函数依赖人工设计，限制了强化学习的可扩展性和自动化；现有视觉语言模型奖励存在与任务进展不对齐、空间定位困难、任务语义理解有限等问题

Method: MARVL方法：1）微调视觉语言模型以获得空间和语义一致性；2）将任务分解为多阶段子任务；3）通过任务方向投影实现轨迹敏感性

Result: 在Meta-World基准测试中，MARVL显著优于现有的VLM奖励方法，在稀疏奖励操作任务上表现出更优的样本效率和鲁棒性

Conclusion: MARVL通过改进视觉语言模型的奖励设计，解决了传统密集奖励的人工工程限制，为机器人强化学习的自动化提供了有效解决方案

Abstract: Designing dense reward functions is pivotal for efficient robotic Reinforcement Learning (RL). However, most dense rewards rely on manual engineering, which fundamentally limits the scalability and automation of reinforcement learning. While Vision-Language Models (VLMs) offer a promising path to reward design, naive VLM rewards often misalign with task progress, struggle with spatial grounding, and show limited understanding of task semantics. To address these issues, we propose MARVL-Multi-stAge guidance for Robotic manipulation via Vision-Language models. MARVL fine-tunes a VLM for spatial and semantic consistency and decomposes tasks into multi-stage subtasks with task direction projection for trajectory sensitivity. Empirically, MARVL significantly outperforms existing VLM-reward methods on the Meta-World benchmark, demonstrating superior sample efficiency and robustness on sparse-reward manipulation tasks.

</details>


### [42] [Test-Time Adaptation for Tactile-Vision-Language Models](https://arxiv.org/abs/2602.15873)
*Chuyang Ye,Haoxian Jing,Qinting Jiang,Yixi Lin,Qiang Li,Xing Tang,Jingyan Jiang*

Main category: cs.RO

TL;DR: 提出了一种针对触觉-视觉-语言多模态模型在测试时分布偏移下的可靠性感知自适应框架，通过估计各模态可靠性来过滤不可靠样本、自适应融合特征并正则化优化过程。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多模态感知任务（如机器人应用）不可避免地面临测试时分布偏移，现有测试时自适应方法在单模态设置中提供过滤，但缺乏对异步跨模态偏移下模态可靠性的显式处理，当某些模态变得不可靠时表现脆弱。

Method: 提出可靠性感知框架：1) 通过预测不确定性和基于扰动的响应估计每个模态的可靠性；2) 利用共享可靠性信号：a) 过滤不可靠测试样本，b) 自适应融合触觉、视觉和语言特征，c) 使用可靠性引导目标正则化测试时优化。

Result: 在TAG-C基准测试和额外TVL场景中，该方法持续优于强基线，在严重模态损坏下实现高达49.9%的准确率提升，证明了显式模态可靠性建模对鲁棒测试时自适应的重要性。

Conclusion: 显式建模模态可靠性对于触觉-视觉-语言模型在测试时分布偏移下的鲁棒自适应至关重要，所提可靠性感知框架能有效处理异步跨模态偏移，提升模型在现实世界多模态感知任务中的性能。

Abstract: Tactile-vision-language (TVL) models are increasingly deployed in real-world robotic and multimodal perception tasks, where test-time distribution shifts are unavoidable. Existing test-time adaptation (TTA) methods provide filtering in unimodal settings but lack explicit treatment of modality-wise reliability under asynchronous cross-modal shifts, leaving them brittle when some modalities become unreliable. We study TTA for TVL models under such shifts and propose a reliability-aware framework that estimates per-modality reliability from prediction uncertainty and perturbation-based responses. This shared reliability signal is used to (i) filter unreliable test samples, (ii) adaptively fuse tactile, visual, and language features, and (iii) regularize test-time optimization with a reliability-guided objective. On the TAG-C benchmark and additional TVL scenarios, our approach consistently outperforms strong TTA baselines, achieving accuracy gains of up to 49.9\% under severe modality corruptions, underscoring the importance of explicit modality-wise reliability modeling for robust test-time adaptation.

</details>


### [43] [Fly0: Decoupling Semantic Grounding from Geometric Planning for Zero-Shot Aerial Navigation](https://arxiv.org/abs/2602.15875)
*Zhenxing Xu,Brikit Lu,Weidong Bao,Zhengqiu Zhu,Junsong Zhang,Hui Yan,Wenhao Lu,Ji Wang*

Main category: cs.RO

TL;DR: Fly0框架通过解耦语义推理与几何规划，解决了视觉语言导航中语义理解与控制精度之间的权衡问题，显著提升了导航成功率并降低了导航误差。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言导航方法面临语义理解与控制精度之间的权衡问题。多模态大语言模型虽然推理能力强，但作为低级控制器会导致高延迟、轨迹振荡和泛化能力差，主要原因是几何基础薄弱。

Method: Fly0采用三阶段流水线：1) MLLM驱动模块将自然语言指令接地到2D像素坐标；2) 几何投影模块利用深度数据将目标定位到3D空间；3) 几何规划器生成无碰撞轨迹。这种机制即使在视觉接触丢失时也能实现鲁棒导航。

Result: 在仿真和真实环境中的大量实验表明，Fly0优于最先进的基线方法，在非结构化环境中将成功率提高了20%以上，导航误差降低了约50%。

Conclusion: Fly0通过解耦语义推理和几何规划，消除了持续推理的需求，降低了计算开销并提高了系统稳定性，为视觉语言导航提供了更有效的解决方案。

Abstract: Current Visual-Language Navigation (VLN) methodologies face a trade-off between semantic understanding and control precision. While Multimodal Large Language Models (MLLMs) offer superior reasoning, deploying them as low-level controllers leads to high latency, trajectory oscillations, and poor generalization due to weak geometric grounding. To address these limitations, we propose Fly0, a framework that decouples semantic reasoning from geometric planning. The proposed method operates through a three-stage pipeline: (1) an MLLM-driven module for grounding natural language instructions into 2D pixel coordinates; (2) a geometric projection module that utilizes depth data to localize targets in 3D space; and (3) a geometric planner that generates collision-free trajectories. This mechanism enables robust navigation even when visual contact is lost. By eliminating the need for continuous inference, Fly0 reduces computational overhead and improves system stability. Extensive experiments in simulation and real-world environments demonstrate that Fly0 outperforms state-of-the-art baselines, improving the Success Rate by over 20\% and reducing Navigation Error (NE) by approximately 50\% in unstructured environments. Our code is available at https://github.com/xuzhenxing1/Fly0.

</details>


### [44] [FUTURE-VLA: Forecasting Unified Trajectories Under Real-time Execution](https://arxiv.org/abs/2602.15882)
*Jingjing Fan,Yushan Liu,Shoujie Li,Botao Ren,Siyuan Li,Xiao-Ping Zhang,Wenbo Ding,Zhidong Deng*

Main category: cs.RO

TL;DR: FUTURE-VLA：一种统一架构，将长时程控制和未来预测重构为单一序列生成任务，通过双端效率范式实现实时推理，在保持单帧基线延迟的同时扩展16倍时空窗口


<details>
  <summary>Details</summary>
Motivation: 当前通用视觉语言模型虽然支持长视频流的时空推理，但在机器人部署中面临长时程历史处理和高维未来预测的延迟瓶颈，限制了实时控制能力

Method: 采用双端效率范式：1）时间自适应压缩策略最大化时空信息密度，处理多视角历史；2）潜在空间自回归在单次前向传播中同时生成可执行动态和可预览视觉前瞻；3）预测引导的人机交互机制通过交互执行门控实现动态验证

Result: 在LIBERO上达到99.2%成功率，RoboTwin上75.4%，真实世界Piper平台78.0%，所有实验在保持单帧基线推理延迟的同时扩展16倍时空窗口

Conclusion: FUTURE-VLA通过统一序列生成框架和双端效率设计，成功解决了机器人长时程控制中的延迟瓶颈，实现了实时预测和交互验证，为机器人部署提供了高效解决方案

Abstract: General vision-language models increasingly support unified spatiotemporal reasoning over long video streams, yet deploying such capabilities on robots remains constrained by the prohibitive latency of processing long-horizon histories and generating high-dimensional future predictions. To bridge this gap, we present FUTURE-VLA, a unified architecture that reformulates long-horizon control and future forecasting as a monolithic sequence-generation task. Adopting a dual-sided efficiency paradigm, FUTURE-VLA leverages a temporally adaptive compression strategy to maximize spatiotemporal information density, enabling the ingestion of extensive multi-view histories while maintaining constant inference latency. Simultaneously, it performs latent-space autoregression to align actionable dynamics with reviewable visual look-aheads in a single forward pass. These real-time predictive capabilities further enable a prediction-guided Human-In-the-Loop mechanism via interactive execution gating, allowing operators to dynamically validate behaviors based on interpretable future previews. Extensive evaluations demonstrate that FUTURE-VLA establishes new state-of-the-art performance, attaining success rates of 99.2% on LIBERO, 75.4% on RoboTwin, and 78.0% on a real-world Piper platform, all with a $16\times$ extended spatiotemporal window while maintaining the inference latency of a single-frame baseline.

</details>


### [45] [The SLAM Confidence Trap](https://arxiv.org/abs/2602.15884)
*Sebastian Sansoni,Santiago Ramón Tosetti Sanz*

Main category: cs.RO

TL;DR: 论文批评SLAM领域陷入"置信度陷阱"，过度追求基准测试分数而忽视不确定性估计，导致系统几何精度高但概率不一致且脆弱，主张将不确定性计算作为主要成功指标


<details>
  <summary>Details</summary>
Motivation: SLAM社区过度关注基准测试分数，忽视了不确定性估计的重要性，导致系统虽然几何精度高，但在概率上不一致且脆弱，需要范式转变

Method: 提出将一致、实时的不确定性计算作为主要成功指标，强调从单纯追求几何精度转向概率一致性

Result: 识别了SLAM领域的"置信度陷阱"问题，指出当前系统在概率上不一致且脆弱，需要重新定义成功标准

Conclusion: SLAM领域需要进行范式转变，将一致、实时的不确定性计算作为主要成功指标，而不仅仅是追求几何精度和基准测试分数

Abstract: The SLAM community has fallen into a "Confidence Trap" by prioritizing benchmark scores over principled uncertainty estimation. This yields systems that are geometrically accurate but probabilitistically inconsistent and brittle. We advocate for a paradigm shift where the consistent, real-time computation of uncertainty becomes a primary metric of success.

</details>


### [46] [A novel Integrated Motion Tracking Device (IMTD) for Objective Laparoscopic Training Assessment: Development and Validation](https://arxiv.org/abs/2602.15885)
*Siwar Bouzid,Abdelbadia Chaker,Marc Arsicault,Sami Bennour,Med Amine Laribi*

Main category: cs.RO

TL;DR: 本文介绍了一种用于腹腔镜手术训练的新型四自由度运动跟踪设备（IMTD），该设备具有紧凑设计、低成本特点，能够精确跟踪手术器械运动，为手术技能训练提供客观实时反馈。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜手术训练需要客观评估工具来跟踪手术器械运动，现有系统成本高且集成复杂，需要开发一种紧凑、低成本、易于集成到标准训练箱中的运动跟踪设备。

Method: 开发了四自由度运动跟踪设备（IMTD），包括运动学设计、机械结构、仪器配置和原型制作。系统专门针对腹腔镜训练需求设计，支持固定运动中心跟踪。通过与运动捕捉系统（MoCap）对比验证跟踪精度和可靠性，评估角度和直线运动捕捉能力。

Result: IMTD系统在跟踪手术手势方面表现出有效性，能够准确捕捉手术器械运动。系统具有高精度、流畅性、速度和整体运动效率。低成本集成设计使其易于在训练室中部署实施。

Conclusion: IMTD系统为腹腔镜手术训练提供了实用且可访问的解决方案，通过提供客观实时反馈，能够显著改善手术技能并缩短新手学习曲线，同时为未来手势评分算法和标准化训练协议开发奠定基础。

Abstract: This paper presents a novel, compact four-degree-of-freedom motion-tracking device (IMTD) designed for training and evaluation in laparoscopic surgery. The device's kinematics, mechanical design, instrumentation, and prototypes are developed and presented to meet the specific requirements of laparoscopic training context, including movement around a fixed center of motion and seamless integration into standard box trainers. The system IMTD's tracking accuracy and reliability are compared to a motion capture system (MoCap), assessing its ability to capture both angular and translational motions of surgical instruments. The study then focuses on key performance parameters including precision, fluidity, speed, and overall motion efficiency. The results highlight the system's effectiveness in tracking surgical gestures, providing valuable insights into its potential as a tool for training and performance evaluation in minimally invasive surgery. Additionally, IMTD's low cost and integrated design allow for easy integration and implementation in training rooms, offering a practical and accessible solution for general use. By offering objective, real-time feedback, the system can significantly contribute to improving surgical skills and shortening the learning curve for novice students, while also providing a foundation for future development of gesture scoring algorithms and standardized training protocols.

</details>


### [47] [Optimization of an Augmented R-CUBE mechanism for Cervical Surgery](https://arxiv.org/abs/2602.15886)
*Terence Essomba,Yu-Wen Wu,Abdelbadia Chaker,Med Amine Laribi*

Main category: cs.RO

TL;DR: 提出一种基于增强版全平移R-CUBE机构的新型机械架构，用于脊柱手术中椎骨钻孔，实现3T2R运动以操纵手术钻头


<details>
  <summary>Details</summary>
Motivation: 在脊柱手术中，需要在椎骨上钻孔以植入椎弓根螺钉，需要一种能够实现特定运动轨迹的机械机构来精确操纵手术钻头

Method: 基于增强版全平移R-CUBE机构，通过改进连杆实现附加旋转运动，构建包含平移、传动和旋转三个阶段的3T2R运动机构，分别推导各阶段的运动学和速度模型并组合，基于真实患者病例的钻孔轨迹优化机构以获得最佳运动性能

Result: 设计出一种能够实现3T2R运动的机械机构，适用于脊柱手术钻孔操作，通过优化获得了最高的运动学性能

Conclusion: 提出的新型机械架构能够满足脊柱手术钻孔的特定运动需求，通过多阶段设计和优化实现了高性能的3T2R运动控制

Abstract: In some surgical operations targeting the spine, it is required to drill cavities in the vertebrae for the insertion of pedicle screws. A new mechanical architecture is proposed for this application. It is based on an augmented version of the full translational R-CUBE mechanism, with improved linkages to implement additional rotational motion. Using this concept, a mechanism presented with a 3T2R motion that is required for the manipulation of the surgical drill. It is mainly composed three stages: one translational, one transmitting and one rotational. Their respective kinematic and velocity models are separately derived, then combined. Based on the drilling trajectories obtained from a real patient case, the mechanism is optimized for generating the highest kinematic performances.

</details>


### [48] [Learning to Drive in New Cities Without Human Demonstrations](https://arxiv.org/abs/2602.15891)
*Zilin Wang,Saeed Rahmani,Daphne Cornelisse,Bidipta Sarkar,Alexander David Goldie,Jakob Nicolaus Foerster,Shimon Whiteson*

Main category: cs.RO

TL;DR: NOMAD使用自博弈多智能体强化学习，仅通过目标城市地图和元信息就能让自动驾驶策略适应新城市，无需人类演示数据


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在特定区域表现可靠，但部署到新城市成本高、速度慢，主要瓶颈是需要收集大量人类演示轨迹来适应新城市的不同道路几何、交通规则和交互模式

Method: 提出NOMAD方法，基于自博弈多智能体强化学习，仅使用目标城市地图和元信息，在基于目标城市地图构建的模拟器中进行策略适应，使用简单的奖励函数

Result: NOMAD显著提高了目标城市的任务成功率和轨迹真实性，展示了比数据密集型城市迁移方法更有效和可扩展的替代方案

Conclusion: 自博弈多智能体强化学习能够仅通过地图和元信息就让驾驶策略适应显著不同的目标城市，无需该城市的人类演示数据，为自动驾驶城市迁移提供了有效解决方案

Abstract: While autonomous vehicles have achieved reliable performance within specific operating regions, their deployment to new cities remains costly and slow. A key bottleneck is the need to collect many human demonstration trajectories when adapting driving policies to new cities that differ from those seen in training in terms of road geometry, traffic rules, and interaction patterns. In this paper, we show that self-play multi-agent reinforcement learning can adapt a driving policy to a substantially different target city using only the map and meta-information, without requiring any human demonstrations from that city. We introduce NO data Map-based self-play for Autonomous Driving (NOMAD), which enables policy adaptation in a simulator constructed based on the target-city map. Using a simple reward function, NOMAD substantially improves both task success rate and trajectory realism in target cities, demonstrating an effective and scalable alternative to data-intensive city-transfer methods. Project Page: https://nomaddrive.github.io/

</details>


### [49] [Statistical-Geometric Degeneracy in UAV Search: A Physics-Aware Asymmetric Filtering Approach](https://arxiv.org/abs/2602.15893)
*Zhiyuan Ren,Yudong Fang,Tao Zhang,Wenchi Cheng,Ben Lan*

Main category: cs.RO

TL;DR: 提出AsymmetricHuberEKF方法，通过非对称损失函数处理灾后无人机定位中的非视距传播问题，解决了对称滤波器在非负偏差环境中的理论失配问题。


<details>
  <summary>Details</summary>
Motivation: 灾后无人机定位面临非视距传播的物理挑战，信号反射导致严格非负的测距偏差。现有对称损失函数的鲁棒估计器（如Huber或Tukey）假设误差对称，在这种非对称偏差环境下存在理论失配，导致统计几何退化现象。

Method: 提出AsymmetricHuberEKF方法，通过推导的非对称损失函数显式地纳入NLOS偏差的非负物理先验。该方法还设计了协同主动感知策略以获取必要的双边信息。在2D天底视图扫描场景中进行验证。

Result: 相比对称基线方法，该方法显著加速了收敛速度，为数据稀缺且几何受限的搜索操作提供了弹性构建模块。

Conclusion: 通过物理基础的非对称损失函数方法有效解决了灾后无人机定位中的统计几何退化问题，标准对称滤波器只是该方法在物理约束放松时的退化特例。

Abstract: Post-disaster survivor localization using Unmanned Aerial Vehicles (UAVs) faces a fundamental physical challenge: the prevalence of Non-Line-of-Sight (NLOS) propagation in collapsed structures. Unlike standard Gaussian noise, signal reflection from debris introduces strictly non-negative ranging biases. Existing robust estimators, typically designed with symmetric loss functions (e.g., Huber or Tukey), implicitly rely on the assumption of error symmetry. Consequently, they experience a theoretical mismatch in this regime, leading to a phenomenon we formally identify as Statistical-Geometric Degeneracy (SGD)-a state where the estimator stagnates due to the coupling of persistent asymmetric bias and limited observation geometry. While emerging data-driven approaches offer alternatives, they often struggle with the scarcity of training data and the sim-to-real gap inherent in unstructured disaster zones. In this work, we propose a physically-grounded solution, the AsymmetricHuberEKF, which explicitly incorporates the non-negative physical prior of NLOS biases via a derived asymmetric loss function. Theoretically, we show that standard symmetric filters correspond to a degenerate case of our framework where the physical constraint is relaxed. Furthermore, we demonstrate that resolving SGD requires not just a robust filter, but specific bilateral information, which we achieve through a co-designed active sensing strategy. Validated in a 2D nadir-view scanning scenario, our approach significantly accelerates convergence compared to symmetric baselines, offering a resilient building block for search operations where data is scarce and geometry is constrained.

</details>


### [50] [VGGT-based online 3D semantic SLAM for indoor scene understanding and navigation](https://arxiv.org/abs/2602.15899)
*Anna Gelencsér-Horváth,Gergely Dinya,Dorka Boglárka Erős,Péter Halász,Islam Muhammad Muqsit,Kristóf Karacs*

Main category: cs.RO

TL;DR: SceneVGGT：结合SLAM与语义映射的时空3D场景理解框架，用于自主和辅助导航，通过滑动窗口处理长视频流，保持几何一致性，实现高效内存使用和实时交互


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理长视频流的3D场景理解系统，用于自主和辅助导航，需要同时解决几何重建、语义理解和实时性能的问题

Method: 基于VGGT构建，采用滑动窗口管道处理长视频流；通过相机位姿变换对齐局部子地图；使用VGGT跟踪头将2D实例掩码提升到3D对象；将对象位置投影到估计的地板平面上用于辅助导航

Result: GPU内存使用保持在17GB以下，不受输入序列长度影响；在ScanNet++基准测试中达到有竞争力的点云性能；支持实时交互辅助导航与音频反馈

Conclusion: SceneVGGT提供了一个稳健的语义识别框架，能够高效处理长视频序列，支持实时辅助导航应用，在内存效率和性能方面表现优异

Abstract: We present SceneVGGT, a spatio-temporal 3D scene understanding framework that combines SLAM with semantic mapping for autonomous and assistive navigation. Built on VGGT, our method scales to long video streams via a sliding-window pipeline. We align local submaps using camera-pose transformations, enabling memory- and speed-efficient mapping while preserving geometric consistency. Semantics are lifted from 2D instance masks to 3D objects using the VGGT tracking head, maintaining temporally coherent identities for change detection. As a proof of concept, object locations are projected onto an estimated floor plane for assistive navigation. The pipeline's GPU memory usage remains under 17 GB, irrespectively of the length of the input sequence and achieves competitive point-cloud performance on the ScanNet++ benchmark. Overall, SceneVGGT ensures robust semantic identification and is fast enough to support interactive assistive navigation with audio feedback.

</details>


### [51] [Coverage Path Planning for Autonomous Sailboats in Inhomogeneous and Time-Varying Oceans: A Spatiotemporal Optimization Approach](https://arxiv.org/abs/2602.15901)
*Yang An,Zhikang Ge,Taiyu Zhang,Jean-Baptiste R. G. Souppez,Gaofei Xu,Zhengru Ren*

Main category: cs.RO

TL;DR: 提出面向自主帆船的非均匀时变海洋环境覆盖路径规划框架，结合空间拓扑约束和时间预测规划，解决传统方法失效的问题


<details>
  <summary>Details</summary>
Motivation: 自主帆船适合长期海洋观测，但其性能具有高度各向异性，受非均匀时变风场和海流场影响，现有覆盖方法（如往复扫描）效果有限，环境约束下的规划研究不足

Method: 提出时空覆盖路径规划框架：1) 空间域基于拓扑的形态约束，促进紧凑连续覆盖；2) 时间域基于预测的前瞻规划，预判环境演化实现远见决策

Result: 在随机非均匀时变海洋环境（包括部分方向可达场景）的仿真中，该方法能生成高效可行的覆盖路径，而传统策略常常失败

Conclusion: 这是首个专门针对自主帆船在非均匀时变海洋环境中覆盖路径规划问题的解决方案，为未来多帆船协同覆盖奠定了基础

Abstract: Autonomous sailboats are well suited for long-duration ocean observation due to their wind-driven endurance. However, their performance is highly anisotropic and strongly influenced by inhomogeneous and time-varying wind and current fields, limiting the effectiveness of existing coverage methods such as boustrophedon sweeping. Planning under these environmental and maneuvering constraints remains underexplored. This paper presents a spatiotemporal coverage path planning framework tailored to autonomous sailboats, combining (1) topology-based morphological constraints in the spatial domain to promote compact and continuous coverage, and (2) forecast-aware look-ahead planning in the temporal domain to anticipate environmental evolution and enable foresighted decision-making. Simulations conducted under stochastic inhomogeneous and time-varying ocean environments, including scenarios with partial directional accessibility, demonstrate that the proposed method generates efficient and feasible coverage paths where traditional strategies often fail. To the best of our knowledge, this study provides the first dedicated solution to the coverage path planning problem for autonomous sailboats operating in inhomogeneous and time-varying ocean environments, establishing a foundation for future cooperative multi-sailboat coverage.

</details>


### [52] [World Action Models are Zero-shot Policies](https://arxiv.org/abs/2602.15922)
*Seonghyeon Ye,Yunhao Ge,Kaiyuan Zheng,Shenyuan Gao,Sihyun Yu,George Kurian,Suneel Indupuru,You Liang Tan,Chuning Zhu,Jiannan Xiang,Ayaan Malik,Kyungmin Lee,William Liang,Nadun Ranawaka,Jiasheng Gu,Yinzhen Xu,Guanzhi Wang,Fengyuan Hu,Avnish Narayan,Johan Bjorck,Jing Wang,Gwanghyun Kim,Dantong Niu,Ruijie Zheng,Yuqi Xie,Jimmy Wu,Qi Wang,Ryan Julian,Danfei Xu,Yilun Du,Yevgen Chebotar,Scott Reed,Jan Kautz,Yuke Zhu,Linxi "Jim" Fan,Joel Jang*

Main category: cs.RO

TL;DR: DreamZero是一个基于预训练视频扩散模型的世界动作模型，通过联合建模视频和动作来学习物理动态，相比现有VLA模型在任务和环境泛化能力上提升2倍以上，并能实现实时闭环控制和跨具身迁移。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型在语义泛化方面表现良好，但在新环境中面对未见过的物理运动时泛化能力不足。需要一种能够学习物理动态并实现更好泛化的模型。

Method: 基于预训练视频扩散主干构建世界动作模型，通过预测未来世界状态和动作来学习物理动态，使用视频作为世界演变的密集表示。联合建模视频和动作，从异构机器人数据中有效学习多样化技能。

Result: 在真实机器人实验中，相比最先进的VLA模型，在新任务和环境上的泛化能力提升超过2倍。通过模型和系统优化，使140亿参数的自回归视频扩散模型能够以7Hz频率进行实时闭环控制。跨具身迁移方面，仅使用其他机器人或人类的视频演示就能在未见任务上获得超过42%的相对改进，仅需10-20分钟数据。更令人惊讶的是，DreamZero能够实现少样本具身适应，仅用30分钟游戏数据就能迁移到新具身，同时保持零样本泛化能力。

Conclusion: DreamZero作为世界动作模型，通过联合建模视频和动作学习物理动态，显著提升了机器人控制的泛化能力，实现了实时控制和跨具身迁移，为机器人学习提供了新的有效方法。

Abstract: State-of-the-art Vision-Language-Action (VLA) models excel at semantic generalization but struggle to generalize to unseen physical motions in novel environments. We introduce DreamZero, a World Action Model (WAM) built upon a pretrained video diffusion backbone. Unlike VLAs, WAMs learn physical dynamics by predicting future world states and actions, using video as a dense representation of how the world evolves. By jointly modeling video and action, DreamZero learns diverse skills effectively from heterogeneous robot data without relying on repetitive demonstrations. This results in over 2x improvement in generalization to new tasks and environments compared to state-of-the-art VLAs in real robot experiments. Crucially, through model and system optimizations, we enable a 14B autoregressive video diffusion model to perform real-time closed-loop control at 7Hz. Finally, we demonstrate two forms of cross-embodiment transfer: video-only demonstrations from other robots or humans yield a relative improvement of over 42% on unseen task performance with just 10-20 minutes of data. More surprisingly, DreamZero enables few-shot embodiment adaptation, transferring to a new embodiment with only 30 minutes of play data while retaining zero-shot generalization.

</details>


### [53] [The human intention. A taxonomy attempt and its applications to robotics](https://arxiv.org/abs/2602.15963)
*J. E. Domínguez-Vidal,Alberto Sanfeliu*

Main category: cs.RO

TL;DR: 本文通过心理学视角重新定义机器人学中的人类意图概念，将其从单一任务目标扩展为多维结构，为机器人研究提供更人性化的框架。


<details>
  <summary>Details</summary>
Motivation: 当前机器人研究中缺乏对"人类意图"的统一明确定义，现有研究常将意图简化为特定任务目标，忽略了意图的多维复杂性。本文旨在填补这一空白，从心理学角度建立更全面的意图理解框架。

Method: 1. 借鉴心理学和传播学理论，将意图整合为可理解框架；2. 基于心理学和传播学研究对意图进行分类；3. 将现有机器人研究映射到这些意图类别；4. 通过协作搜索和物体运输两个用例进行深入分析。

Result: 建立了人类意图的多维分类框架，展示了如何将现有机器人研究与该框架对齐，并通过具体用例证明了考虑意图多样性的重要性。

Conclusion: 人类意图是多维复杂的，不能简化为单一任务目标。机器人研究需要从纯技术改进转向更人性化的视角，考虑意图的多样性对实现有效人机协作至关重要。

Abstract: Despite a surge in robotics research dedicated to inferring and understanding human intent, a universally accepted definition remains elusive since existing works often equate human intention with specific task-related goals. This article seeks to address this gap by examining the multifaceted nature of intention. Drawing on insights from psychology, it attempts to consolidate a definition of intention into a comprehensible framework for a broader audience. The article classifies different types of intention based on psychological and communication studies, offering guidance to researchers shifting from pure technical enhancements to a more human-centric perspective in robotics. It then demonstrates how various robotics studies can be aligned with these intention categories. Finally, through in-depth analyses of collaborative search and object transport use cases, the article underscores the significance of considering the diverse facets of human intention.

</details>


### [54] [ODYN: An All-Shifted Non-Interior-Point Method for Quadratic Programming in Robotics and AI](https://arxiv.org/abs/2602.16005)
*Jose Rojas,Aristotelis Papatheodorou,Sergi Martinez,Ioannis Havoutis,Carlos Mastalli*

Main category: cs.RO

TL;DR: ODYN是一个新型的全移位原始-对偶非内点二次规划求解器，结合了全移位非线性互补问题函数和近端乘子法，能高效处理稠密和稀疏QP问题，具有优异的预热启动性能。


<details>
  <summary>Details</summary>
Motivation: 针对现有QP求解器在处理病态、退化问题时的局限性，以及在线性约束不独立情况下的困难，需要开发一个鲁棒、高效且适用于机器人和AI实时应用的优化求解器。

Method: ODYN结合了全移位非线性互补问题函数和近端乘子法，采用非内点方法，不需要约束的线性独立性，能够鲁棒地处理病态和退化问题。

Result: 在Maros-Mészáros测试集上展示了最先进的收敛性能，在从小规模到大规模问题上都表现优异，特别突出了其卓越的预热启动能力，这在机器人和AI的序列和实时应用中至关重要。

Conclusion: ODYN是一个高效、鲁棒的QP求解器，特别适合机器学习和机器人应用，已成功应用于预测控制框架、深度学习可微优化层和接触动力学仿真等多个实际场景。

Abstract: We introduce ODYN, a novel all-shifted primal-dual non-interior-point quadratic programming (QP) solver designed to efficiently handle challenging dense and sparse QPs. ODYN combines all-shifted nonlinear complementarity problem (NCP) functions with proximal method of multipliers to robustly address ill-conditioned and degenerate problems, without requiring linear independence of the constraints. It exhibits strong warm-start performance and is well suited to both general-purpose optimization, and robotics and AI applications, including model-based control, estimation, and kernel-based learning methods. We provide an open-source implementation and benchmark ODYN on the Maros-Mészáros test set, demonstrating state-of-the-art convergence performance in small-to-high-scale problems. The results highlight ODYN's superior warm-starting capabilities, which are critical in sequential and real-time settings common in robotics and AI. These advantages are further demonstrated by deploying ODYN as the backend of an SQP-based predictive control framework (OdynSQP), as the implicitly differentiable optimization layer for deep learning (ODYNLayer), and the optimizer of a contact-dynamics simulation (ODYNSim).

</details>


### [55] [The Impact of Class Uncertainty Propagation in Perception-Based Motion Planning](https://arxiv.org/abs/2602.16035)
*Jibran Iqbal Shah,Andrei Ivanovic,Kelly Zhu,Masha Itkina,Rowan McAllister,Igor Gilitschenski,Florian Shkurti*

Main category: cs.RO

TL;DR: 该研究分析了感知不确定性校准对自动驾驶运动规划的影响，发现在nuPlan基准测试中，包含上游不确定性传播的方法在复杂闭环场景中表现出更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需要处理传感器数据带来的感知不确定性，现有不确定性感知规划器可能对预测不确定性校准错误敏感，但这一影响的严重程度尚未被量化。

Method: 通过比较两种具有不同不确定性传播水平的新型预测-规划管道，在nuPlan规划基准上进行详细分析，使用闭环评估研究上游不确定性校准的影响。

Result: 研究发现，包含上游不确定性传播的方法在复杂闭环场景中表现出更优越的泛化能力。

Conclusion: 不确定性传播对自动驾驶运动规划至关重要，特别是在处理复杂闭环场景时，正确校准上游不确定性能够显著提升规划性能。

Abstract: Autonomous vehicles (AVs) are being increasingly deployed in urban environments. In order to operate safely and reliably, AVs need to account for the inherent uncertainty associated with perceiving the world through sensor data and incorporate that into their decision-making process. Uncertainty-aware planners have recently been developed to account for upstream perception and prediction uncertainty. However, such planners may be sensitive to prediction uncertainty miscalibration, the magnitude of which has not yet been characterized. Towards this end, we perform a detailed analysis on the impact that perceptual uncertainty propagation and calibration has on perception-based motion planning. We do so by comparing two novel prediction-planning pipelines with varying levels of uncertainty propagation on the recently-released nuPlan planning benchmark. We study the impact of upstream uncertainty calibration using closed-loop evaluation on the nuPlan challenge scenarios. We find that the method incorporating upstream uncertainty propagation demonstrates superior generalization to complex closed-loop scenarios.

</details>


### [56] [ScenicRules: An Autonomous Driving Benchmark with Multi-Objective Specifications and Abstract Scenarios](https://arxiv.org/abs/2602.16073)
*Kevin Kai-Chun Chang,Ekin Beyazit,Alberto Sangiovanni-Vincentelli,Tichakorn Wongpiromsarn,Sanjit A. Seshia*

Main category: cs.RO

TL;DR: ScenicRules是一个用于评估自动驾驶系统在随机环境下基于优先级多目标规范的基准测试框架，包含形式化目标、分层规则书和场景集合。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶评估基准缺乏结合多目标优先级规则和形式化环境模型的特性，而复杂交通环境中多个目标（如避免碰撞、遵守交规、高效行驶）往往存在冲突且需要明确的优先级关系。

Method: 1) 形式化多样化的目标作为量化评估指标；2) 设计可解释、可适应的分层规则书框架，编码多个目标及其优先级关系；3) 使用Scenic语言构建紧凑但具有代表性的场景集合，涵盖不同驾驶上下文和近事故情况。

Result: 实验结果表明，形式化的目标和分层规则书与人类驾驶判断高度一致，且该基准能有效暴露智能体在优先级目标方面的失败情况。

Conclusion: ScenicRules为自动驾驶系统在随机环境下的多目标优先级规范评估提供了有效的基准测试框架，填补了现有评估方法的空白。

Abstract: Developing autonomous driving systems for complex traffic environments requires balancing multiple objectives, such as avoiding collisions, obeying traffic rules, and making efficient progress. In many situations, these objectives cannot be satisfied simultaneously, and explicit priority relations naturally arise. Also, driving rules require context, so it is important to formally model the environment scenarios within which such rules apply. Existing benchmarks for evaluating autonomous vehicles lack such combinations of multi-objective prioritized rules and formal environment models. In this work, we introduce ScenicRules, a benchmark for evaluating autonomous driving systems in stochastic environments under prioritized multi-objective specifications. We first formalize a diverse set of objectives to serve as quantitative evaluation metrics. Next, we design a Hierarchical Rulebook framework that encodes multiple objectives and their priority relations in an interpretable and adaptable manner. We then construct a compact yet representative collection of scenarios spanning diverse driving contexts and near-accident situations, formally modeled in the Scenic language. Experimental results show that our formalized objectives and Hierarchical Rulebooks align well with human driving judgments and that our benchmark effectively exposes agent failures with respect to the prioritized objectives. Our benchmark can be accessed at https://github.com/BerkeleyLearnVerify/ScenicRules/.

</details>


### [57] [Reactive Slip Control in Multifingered Grasping: Hybrid Tactile Sensing and Internal-Force Optimization](https://arxiv.org/abs/2602.16127)
*Théo Ayral,Saifeddine Aloui,Mathieu Grossard*

Main category: cs.RO

TL;DR: 提出了一种结合学习和模型的方法，通过自适应调整抓取力来防止多指机器人手抓取物体时的滑动，使用多模态触觉传感实现快速响应。


<details>
  <summary>Details</summary>
Motivation: 在多指机器人抓取中，如何快速检测并防止物体滑动是一个关键挑战。传统方法要么依赖模型计算但响应慢，要么依赖学习但缺乏理论保证。需要一种既能快速响应又能保证稳定性的混合方法。

Method: 采用多模态触觉堆栈：压电传感器(PzE)用于快速检测滑动信号，压阻阵列(PzR)用于接触定位和几何信息。在线构建抓取矩阵，当检测到滑动时，通过二次规划在抓取零空间中更新内力，同时保持物体受力平衡和执行器限制。

Result: 实现了理论上的传感到命令延迟为35-40毫秒，其中PzR接触和几何更新需要5毫秒，二次规划求解约4毫秒。在控制试验中，滑动起始检测时间为20毫秒。展示了在多指抓取中对外部扰动的闭环稳定能力。

Conclusion: 将高效的分析力控制与学习的触觉线索相结合，既实现了鲁棒性又获得了快速反应能力。测量延迟主要来自实验数据路径而非实际计算，分析表明有清晰路径实现亚50毫秒的闭环稳定。

Abstract: We present a hybrid learning and model-based approach that adapts internal grasp forces to halt in-hand slip on a multifingered robotic gripper. A multimodal tactile stack combines piezoelectric (PzE) sensing for fast slip cues with piezoresistive (PzR) arrays for contact localization, enabling online construction of the grasp matrix. Upon slip, we update internal forces computed in the null space of the grasp via a quadratic program that preserves the object wrench while enforcing actuation limits. The pipeline yields a theoretical sensing-to-command latency of 35-40 ms, with 5 ms for PzR-based contact and geometry updates and about 4 ms for the quadratic program solve. In controlled trials, slip onset is detected at 20ms. We demonstrate closed-loop stabilization on multifingered grasps under external perturbations. Augmenting efficient analytic force control with learned tactile cues yields both robustness and rapid reactions, as confirmed in our end-to-end evaluation. Measured delays are dominated by the experimental data path rather than actual computation. The analysis outlines a clear route to sub-50 ms closed-loop stabilization.

</details>


### [58] [Image Measurement Method for Automatic Insertion of Forks into Inclined Pallet](https://arxiv.org/abs/2602.16178)
*Nobuyuki Kita,Takuro Kato*

Main category: cs.RO

TL;DR: 提出使用广角相机测量托盘倾斜角度的方法，以及相机坐标系与叉车坐标系之间的标定方法，实现AGF自动叉入托盘孔洞


<details>
  <summary>Details</summary>
Motivation: 为了实现AGF（自动导引叉车）自动将叉子插入托盘孔洞，需要控制叉子的高度、伸出位置和倾斜角度以匹配托盘孔洞的位置和方向。传统方法需要测量托盘在相机坐标系中的倾斜角度，并建立相机与叉车坐标系之间的标定关系。

Method: 1. 使用广角相机测量托盘在相机坐标系中的俯仰倾斜角度；2. 提出图像测量方法，简化获取相机坐标系与叉车坐标系之间标定信息的过程；3. 在实验环境中，将广角相机固定在伸缩式叉车的靠背上，对前方放置的托盘进行图像处理。

Result: 通过比较图像测量值与手动测量值，评估了托盘俯仰倾斜角度、托盘与叉车相对高度以及托盘是否装载货物等情况下的误差。结果显示，所有误差都在安全插入叉子所需的允许范围内。

Conclusion: 提出的图像测量方法能够准确测量托盘倾斜角度，并且相机与叉车坐标系的标定方法有效，误差控制在安全范围内，为实现AGF自动叉入托盘孔洞提供了可行的技术方案。

Abstract: In order to insert a fork into a hole of a pallet by a forklift located in front of a pallet, it is necessary to control the height position, reach position, and tilt angle of the fork to match the position and orientation of the hole of the pallet. In order to make AGF (Autonomous Guided Forklift) do this automatically, we propose an image measurement method to measure the pitch inclination of the pallet in the camera coordinate system from an image obtained by using a wide-angle camera. In addition, we propose an image measurement method to easily acquire the calibration information between the camera coordinate system and the fork coordinate system necessary to apply the measurements in the camera coordinate system to the fork control. In the experiment space, a wide-angle camera was fixed at the backrest of a reach type forklift. The wide-angle images taken by placing a pallet in front of the camera were processed. As a result of evaluating the error by comparing the image measurement value with the hand measurement value when changing the pitch inclination angle of the pallet, the relative height of the pallet and the fork, and whether the pallet is loaded or not, it was confirmed that the error was within the allowable range for safely inserting the fork.

</details>


### [59] [SIT-LMPC: Safe Information-Theoretic Learning Model Predictive Control for Iterative Tasks](https://arxiv.org/abs/2602.16187)
*Zirui Zang,Ahmad Amine,Nick-Marios T. Kokolakis,Truong X. Nghiem,Ugo Rosolia,Rahul Mangharam*

Main category: cs.RO

TL;DR: 提出SIT-LMPC算法，结合信息论模型预测控制与归一化流学习价值函数，用于迭代任务中的机器人控制，在保证安全的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 机器人在复杂不确定环境中执行迭代任务时，需要平衡鲁棒性、安全性和高性能的控制策略。现有方法在处理约束、不确定性和实时优化方面存在挑战。

Method: 基于信息论模型预测控制框架，设计自适应惩罚方法确保安全性；利用归一化流从历史轨迹学习价值函数，实现更丰富的不确定性建模；优化GPU并行执行以实现实时优化。

Result: 基准仿真和硬件实验表明，SIT-LMPC能够迭代提升系统性能，同时鲁棒地满足系统约束，在复杂不确定环境中表现优异。

Conclusion: SIT-LMPC为迭代任务提供了一种安全、高效的控制框架，结合信息论优化与深度学习，在保证安全约束的同时实现性能的持续改进。

Abstract: Robots executing iterative tasks in complex, uncertain environments require control strategies that balance robustness, safety, and high performance. This paper introduces a safe information-theoretic learning model predictive control (SIT-LMPC) algorithm for iterative tasks. Specifically, we design an iterative control framework based on an information-theoretic model predictive control algorithm to address a constrained infinite-horizon optimal control problem for discrete-time nonlinear stochastic systems. An adaptive penalty method is developed to ensure safety while balancing optimality. Trajectories from previous iterations are utilized to learn a value function using normalizing flows, which enables richer uncertainty modeling compared to Gaussian priors. SIT-LMPC is designed for highly parallel execution on graphics processing units, allowing efficient real-time optimization. Benchmark simulations and hardware experiments demonstrate that SIT-LMPC iteratively improves system performance while robustly satisfying system constraints.

</details>


### [60] [Nonplanar Model Predictive Control for Autonomous Vehicles with Recursive Sparse Gaussian Process Dynamics](https://arxiv.org/abs/2602.16206)
*Ahmad Amine,Kabir Puri,Viet-Anh Le,Rahul Mangharam*

Main category: cs.RO

TL;DR: 该论文提出了一种用于非平面地形上自动驾驶车辆的非平面模型预测控制框架，通过几何感知建模方法学习残差高斯过程来近似复杂车辆动力学，并利用递归稀疏高斯过程实现实时地形几何适应。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在非平面地形上运行时面临复杂动力学挑战，传统平面模型无法准确描述三维地形下的车辆行为，需要开发能够适应地形几何变化的控制框架。

Method: 提出非平面模型预测控制框架，采用几何感知建模方法学习残差高斯过程来近似复杂车辆动力学，使用递归稀疏高斯过程实现实时适应，并结合模型预测路径积分控制器进行参考轨迹跟踪。

Result: 在自定义Isaac Sim环境中的验证表明，该框架能够在具有挑战性的三维表面上保持高跟踪精度，证明了学习模型的有效性。

Conclusion: 提出的非平面MPC框架通过几何感知建模和递归稀疏高斯过程，成功实现了自动驾驶车辆在非平面地形上的实时适应和高精度控制。

Abstract: This paper proposes a nonplanar model predictive control (MPC) framework for autonomous vehicles operating on nonplanar terrain. To approximate complex vehicle dynamics in such environments, we develop a geometry-aware modeling approach that learns a residual Gaussian Process (GP). By utilizing a recursive sparse GP, the framework enables real-time adaptation to varying terrain geometry. The effectiveness of the learned model is demonstrated in a reference-tracking task using a Model Predictive Path Integral (MPPI) controller. Validation within a custom Isaac Sim environment confirms the framework's capability to maintain high tracking accuracy on challenging 3D surfaces.

</details>


### [61] [Markerless Robot Detection and 6D Pose Estimation for Multi-Agent SLAM](https://arxiv.org/abs/2602.16308)
*Markus Rueggeberg,Maximilian Ulmer,Maximilian Durner,Wout Boerdijk,Marcus Gerhard Mueller,Rudolph Triebel,Riccardo Giubilato*

Main category: cs.RO

TL;DR: 提出基于深度学习6D姿态估计的无标记多机器人SLAM系统，解决多机器人数据关联和相对定位问题


<details>
  <summary>Details</summary>
Motivation: 多机器人SLAM中数据关联困难，传统基于标记的方法（如AprilTag阵列）存在观测范围有限、光照条件敏感等限制

Method: 利用深度学习6D姿态估计技术实现无标记姿态估计，集成到去中心化多机器人SLAM系统中

Result: 实验验证表明该方法能提高机器人团队间的相对定位精度，在行星类似环境中测试有效

Conclusion: 深度学习6D姿态估计为多机器人SLAM提供有效的无标记解决方案，改善了相对定位性能

Abstract: The capability of multi-robot SLAM approaches to merge localization history and maps from different observers is often challenged by the difficulty in establishing data association. Loop closure detection between perceptual inputs of different robotic agents is easily compromised in the context of perceptual aliasing, or when perspectives differ significantly. For this reason, direct mutual observation among robots is a powerful way to connect partial SLAM graphs, but often relies on the presence of calibrated arrays of fiducial markers (e.g., AprilTag arrays), which severely limits the range of observations and frequently fails under sharp lighting conditions, e.g., reflections or overexposure. In this work, we propose a novel solution to this problem leveraging recent advances in Deep-Learning-based 6D pose estimation. We feature markerless pose estimation as part of a decentralized multi-robot SLAM system and demonstrate the benefit to the relative localization accuracy among the robotic team. The solution is validated experimentally on data recorded in a test field campaign on a planetary analogous environment.

</details>


### [62] [Dual-Quadruped Collaborative Transportation in Narrow Environments via Safe Reinforcement Learning](https://arxiv.org/abs/2602.16353)
*Zhezhi Lei,Zhihai Bi,Wenxin Wang,Jun Ma*

Main category: cs.RO

TL;DR: 该论文提出了一种基于安全强化学习的双四足机器人协作运输方法，通过约束马尔可夫博弈建模，引入成本优势分解和约束分配机制，在狭窄环境中实现安全高效的协同运输。


<details>
  <summary>Details</summary>
Motivation: 多机器人协作运输在狭窄环境中面临挑战，因为可行区域极其有限，难以同时保证安全性和高性能。现有方法在确保机器人间安全协作方面存在困难，特别是在空间受限的场景中。

Method: 1. 将任务建模为完全合作的约束马尔可夫博弈，将碰撞避免作为约束条件；2. 提出成本优势分解方法，确保团队约束总和低于上限，在强化学习框架内保证任务安全性；3. 设计约束分配方法，将共享约束分配给个体机器人以最大化整体任务奖励，促进机器人间的自主任务分配。

Result: 仿真和实时实验结果表明，与现有方法相比，所提方法在双四足机器人协作运输任务中实现了更优的性能和更高的成功率。

Conclusion: 该研究提出的安全强化学习方法有效解决了狭窄环境中多机器人协作运输的安全性和性能平衡问题，通过约束分配机制促进了机器人间的自主协作，为复杂环境下的多机器人系统提供了可行的解决方案。

Abstract: Collaborative transportation, where multiple robots collaboratively transport a payload, has garnered significant attention in recent years. While ensuring safe and high-performance inter-robot collaboration is critical for effective task execution, it is difficult to pursue in narrow environments where the feasible region is extremely limited. To address this challenge, we propose a novel approach for dual-quadruped collaborative transportation via safe reinforcement learning (RL). Specifically, we model the task as a fully cooperative constrained Markov game, where collision avoidance is formulated as constraints. We introduce a cost-advantage decomposition method that enforces the sum of team constraints to remain below an upper bound, thereby guaranteeing task safety within an RL framework. Furthermore, we propose a constraint allocation method that assigns shared constraints to individual robots to maximize the overall task reward, encouraging autonomous task-assignment among robots, thereby improving collaborative task performance. Simulation and real-time experimental results demonstrate that the proposed approach achieves superior performance and a higher success rate in dual-quadruped collaborative transportation compared to existing methods.

</details>


### [63] [Articulated 3D Scene Graphs for Open-World Mobile Manipulation](https://arxiv.org/abs/2602.16356)
*Martin Büchner,Adrian Röfer,Tim Engelbracht,Tim Welschehold,Zuria Bauer,Hermann Blum,Marc Pollefeys,Abhinav Valada*

Main category: cs.RO

TL;DR: MoMa-SG是一个构建语义-运动学3D场景图的新框架，能够从RGB-D序列中重建包含大量可交互物体的铰接场景，估计物体运动参数，并支持机器人对日常环境中铰接物体的鲁棒操作。


<details>
  <summary>Details</summary>
Motivation: 机器人在真实环境中操作时面临关键限制：无法预测物体如何运动。长时程移动操作需要在语义、几何和运动学之间建立桥梁，以理解铰接场景中的物体交互。

Method: 提出MoMa-SG框架：1) 从包含多个物体铰接的RGB-D序列中，通过时间分割识别物体交互；2) 使用抗遮挡点跟踪推断物体运动；3) 将点轨迹提升到3D空间；4) 通过新颖的统一扭转估计公式，在单次优化中鲁棒估计旋转和平移关节参数；5) 将物体与估计的铰接关联，并通过父子关系推理检测包含的物体。

Result: 在两个数据集上进行了广泛评估，并消融了关键设计选择。真实世界实验表明，该语义-运动学场景图能够使四足机器人和移动操作机器人在日常家庭环境中对铰接物体进行鲁棒操作。还引入了Arti4D-Semantic数据集，包含62个真实世界RGB-D序列、600个物体交互和三种不同观察范式。

Conclusion: MoMa-SG框架成功构建了语义-运动学3D场景图，填补了语义、几何和运动学之间的鸿沟，使机器人能够理解和操作日常环境中的铰接物体，为长时程移动操作提供了重要基础。

Abstract: Semantics has enabled 3D scene understanding and affordance-driven object interaction. However, robots operating in real-world environments face a critical limitation: they cannot anticipate how objects move. Long-horizon mobile manipulation requires closing the gap between semantics, geometry, and kinematics. In this work, we present MoMa-SG, a novel framework for building semantic-kinematic 3D scene graphs of articulated scenes containing a myriad of interactable objects. Given RGB-D sequences containing multiple object articulations, we temporally segment object interactions and infer object motion using occlusion-robust point tracking. We then lift point trajectories into 3D and estimate articulation models using a novel unified twist estimation formulation that robustly estimates revolute and prismatic joint parameters in a single optimization pass. Next, we associate objects with estimated articulations and detect contained objects by reasoning over parent-child relations at identified opening states. We also introduce the novel Arti4D-Semantic dataset, which uniquely combines hierarchical object semantics including parent-child relation labels with object axis annotations across 62 in-the-wild RGB-D sequences containing 600 object interactions and three distinct observation paradigms. We extensively evaluate the performance of MoMa-SG on two datasets and ablate key design choices of our approach. In addition, real-world experiments on both a quadruped and a mobile manipulator demonstrate that our semantic-kinematic scene graphs enable robust manipulation of articulated objects in everyday home environments. We provide code and data at: https://momasg.cs.uni-freiburg.de.

</details>


### [64] [System Identification under Constraints and Disturbance: A Bayesian Estimation Approach](https://arxiv.org/abs/2602.16358)
*Sergi Martinez,Steve Tonneau,Carlos Mastalli*

Main category: cs.RO

TL;DR: 提出贝叶斯系统辨识框架，联合估计机器人状态轨迹和物理参数，嵌入物理约束和摩擦模型，实现线性时间复杂度的可扩展算法


<details>
  <summary>Details</summary>
Motivation: 现有机器人系统辨识方法在联合估计状态轨迹和物理参数时精度不足，特别是在处理非线性摩擦效应和物理一致性约束方面存在局限，需要更准确且可扩展的辨识框架

Method: 提出贝叶斯系统辨识框架，嵌入物理一致的逆动力学、接触和闭环约束作为硬等式约束；使用能量回归器增强参数可观测性；支持惯性和驱动参数的等式和不等式先验；推导参数化等式约束Riccati递归保持带状结构，实现线性时间复杂度

Result: 在代表性机器人系统仿真和Unitree B1机器人硬件实验中，相比前向动力学和解耦辨识基线，该方法收敛更快，惯性和摩擦估计误差更低，接触一致性更好；在模型预测控制中部署时，在挑战性环境中的轨迹跟踪性能显著提升

Conclusion: 该贝叶斯系统辨识框架能够高精度联合估计机器人状态轨迹和物理参数，通过物理约束嵌入和可扩展算法设计，在仿真和硬件实验中均表现出优越性能，为机器人模型预测控制提供了更准确的模型基础

Abstract: We introduce a Bayesian system identification (SysID) framework for jointly estimating robot's state trajectories and physical parameters with high accuracy. It embeds physically consistent inverse dynamics, contact and loop-closure constraints, and fully featured joint friction models as hard, stage-wise equality constraints. It relies on energy-based regressors to enhance parameter observability, supports both equality and inequality priors on inertial and actuation parameters, enforces dynamically consistent disturbance projections, and augments proprioceptive measurements with energy observations to disambiguate nonlinear friction effects. To ensure scalability, we derive a parameterized equality-constrained Riccati recursion that preserves the banded structure of the problem, achieving linear complexity in the time horizon, and develop computationally efficient derivatives. Simulation studies on representative robotic systems, together with hardware experiments on a Unitree B1 equipped with a Z1 arm, demonstrate faster convergence, lower inertial and friction estimation errors, and improved contact consistency compared to forward-dynamics and decoupled identification baselines. When deployed within model predictive control frameworks, the resulting models yield measurable improvements in tracking performance during locomotion over challenging environments.

</details>


### [65] [Docking and Persistent Operations for a Resident Underwater Vehicle](https://arxiv.org/abs/2602.16360)
*Leonard Günzel,Gabrielė Kasparavičiūtė,Ambjørn Grimsrud Waldum,Bjørn-Magnus Moslått,Abubakar Aliyu Badawi,Celil Yılmaz,Md Shamin Yeasher Yousha,Robert Staven,Martin Ludvigsen*

Main category: cs.RO

TL;DR: 开发并部署了90米深度的驻留式ROV基础设施系统，通过声学和视觉融合导航实现自主对接与检查任务，验证了无缆水下长期监测的可行性。


<details>
  <summary>Details</summary>
Motivation: 当前海洋观测方法受限于高成本和后勤困难，主要依赖稀疏的广域调查或固定点的长期测量。需要能够实现持续自主运行而无需水面支持的监测系统。尽管有进展，但由于自主性、机器人韧性和机械鲁棒性等挑战，驻留式水下车辆仍然不常见。

Method: 开发并部署了90米深度的驻留式基础设施，包括对接站和迷你级ROV。ROV配备增强的机载处理和感知能力，能够使用USBL信号自主导航，通过ArUco标记视觉定位（通过扩展卡尔曼滤波器融合）进行对接，并执行本地检查任务。

Result: 系统实现了90%的自主对接成功率，在四分钟内完成完整的检查任务，验证了声学和视觉导航在真实环境中的集成效果。

Conclusion: 结果表明在深度进行可靠的无缆操作是可行的，突显了驻留式ROV系统在可扩展、经济高效的水下监测方面的潜力。

Abstract: Our understanding of the oceans remains limited by sparse and infrequent observations, primarily because current methods are constrained by the high cost and logistical effort of underwater monitoring, relying either on sporadic surveys across broad areas or on long-term measurements at fixed locations. To overcome these limitations, monitoring systems must enable persistent and autonomous operations without the need for continuous surface support. Despite recent advances, resident underwater vehicles remain uncommon due to persistent challenges in autonomy, robotic resilience, and mechanical robustness, particularly under long-term deployment in harsh and remote environments. This work addresses these problems by presenting the development, deployment, and operation of a resident infrastructure using a docking station with a mini-class Remotely Operated Vehicle (ROV) at 90m depth. The ROVis equipped with enhanced onboard processing and perception, allowing it to autonomously navigate using USBL signals, dock via ArUco marker-based visual localisation fused through an Extended Kalman Filter, and carry out local inspection routines. The system demonstrated a 90% autonomous docking success rate and completed full inspection missions within four minutes, validating the integration of acoustic and visual navigation in real-world conditions. These results show that reliable, untethered operations at depth are feasible, highlighting the potential of resident ROV systems for scalable, cost-effective underwater monitoring.

</details>


### [66] [Dynamic Modeling and MPC for Locomotion of Tendon-Driven Soft Quadruped](https://arxiv.org/abs/2602.16371)
*Saumya Karan,Neerav Maram,Suraj Borate,Madhu Vadali*

Main category: cs.RO

TL;DR: SLOT是一种肌腱驱动的软体四足机器人，使用3D打印TPU腿，仅用4个执行器研究顺应性腿式运动的物理建模与控制


<details>
  <summary>Details</summary>
Motivation: 研究如何将连续体软腿集成到基于模型的运动控制中，开发可扩展、可重用的软体四足机器人建模与控制方法

Method: 使用离散Cosserat杆理论建模腿部为可变形连续体，引入模块化全身建模框架，将柔顺腿部动力学表示为作用在刚性躯干上的物理一致反作用力，并嵌入凸模型预测控制框架

Result: 控制器在各种扰动下实现渐近稳定性，在物理原型上验证爬行和行走步态，质心轨迹RMSE小于5毫米，达到高精度

Conclusion: 该框架为将连续体软腿集成到基于模型的运动控制提供了通用方法，推进了软体四足机器人的可扩展和可重用建模与控制方法

Abstract: SLOT (Soft Legged Omnidirectional Tetrapod), a tendon-driven soft quadruped robot with 3D-printed TPU legs, is presented to study physics-informed modeling and control of compliant legged locomotion using only four actuators. Each leg is modeled as a deformable continuum using discrete Cosserat rod theory, enabling the capture of large bending deformations, distributed elasticity, tendon actuation, and ground contact interactions. A modular whole-body modeling framework is introduced, in which compliant leg dynamics are represented through physically consistent reaction forces applied to a rigid torso, providing a scalable interface between continuum soft limbs and rigid-body locomotion dynamics. This formulation allows efficient whole-body simulation and real-time control without sacrificing physical fidelity. The proposed model is embedded into a convex model predictive control framework that optimizes ground reaction forces over a 0.495 s prediction horizon and maps them to tendon actuation through a physics-informed force-angle relationship. The resulting controller achieves asymptotic stability under diverse perturbations. The framework is experimentally validated on a physical prototype during crawling and walking gaits, achieving high accuracy with less than 5 mm RMSE in center of mass trajectories. These results demonstrate a generalizable approach for integrating continuum soft legs into model-based locomotion control, advancing scalable and reusable modeling and control methods for soft quadruped robots.

</details>


### [67] [RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation](https://arxiv.org/abs/2602.16444)
*Yixue Zhang,Kun Wu,Zhi Gao,Zhen Zhao,Pei Ren,Zhiyuan Xu,Fei Liao,Xinhua Wang,Shichao Fan,Di Wu,Qiuxuan Feng,Meng Li,Zhengping Che,Chang Liu,Jian Tang*

Main category: cs.RO

TL;DR: RoboGene是一个自动化生成多样化、物理可行的机器人操作任务的智能框架，通过多样性采样、自我反思和人工反馈机制，显著优于现有基础模型，并提升视觉语言动作模型的预训练效果。


<details>
  <summary>Details</summary>
Motivation: 通用机器人操作面临真实交互数据稀缺的挑战，数据收集成本高昂且现有方法难以自动生成多样化、物理可行的任务。手动方法不可扩展且偏向常见任务，而现有基础模型常产生物理不可行的指令。

Method: RoboGene框架包含三个核心组件：1) 多样性驱动采样实现广泛任务覆盖；2) 自我反思机制强制执行物理约束；3) 人机协同循环实现持续改进。支持单臂、双臂和移动机器人。

Result: 收集了18k条轨迹数据集，引入新指标评估任务质量、可行性和多样性。RoboGene显著优于GPT-4o、Gemini 2.5 Pro等最先进基础模型。使用RoboGene预训练的VLA模型在真实实验中表现出更高的成功率和更好的泛化能力。

Conclusion: RoboGene通过自动化生成高质量、多样化的物理可行任务，解决了机器人数据收集的关键瓶颈，为通用机器人操作提供了有效的数据生成解决方案，并显著提升了视觉语言动作模型的性能。

Abstract: The pursuit of general-purpose robotic manipulation is hindered by the scarcity of diverse, real-world interaction data. Unlike data collection from web in vision or language, robotic data collection is an active process incurring prohibitive physical costs. Consequently, automated task curation to maximize data value remains a critical yet under-explored challenge. Existing manual methods are unscalable and biased toward common tasks, while off-the-shelf foundation models often hallucinate physically infeasible instructions. To address this, we introduce RoboGene, an agentic framework designed to automate the generation of diverse, physically plausible manipulation tasks across single-arm, dual-arm, and mobile robots. RoboGene integrates three core components: diversity-driven sampling for broad task coverage, self-reflection mechanisms to enforce physical constraints, and human-in-the-loop refinement for continuous improvement. We conduct extensive quantitative analysis and large-scale real-world experiments, collecting datasets of 18k trajectories and introducing novel metrics to assess task quality, feasibility, and diversity. Results demonstrate that RoboGene significantly outperforms state-of-the-art foundation models (e.g., GPT-4o, Gemini 2.5 Pro). Furthermore, real-world experiments show that VLA models pre-trained with RoboGene achieve higher success rates and superior generalization, underscoring the importance of high-quality task generation. Our project is available at https://robogene-boost-vla.github.io.

</details>


### [68] [Reactive Motion Generation With Particle-Based Perception in Dynamic Environments](https://arxiv.org/abs/2602.16462)
*Xiyuan Zhao,Huijun Li,Lifeng Zhu,Zhikai Wei,Xianyi Zhu,Aiguo Song*

Main category: cs.RO

TL;DR: 该论文提出了一种结合动态感知与反应式规划的机械臂运动生成框架，通过张量化粒子权重更新方案显式建模障碍物动态特性，并基于此开发了障碍物感知的MPPI规划方法，在动态环境中显著提升了安全性和反应能力。


<details>
  <summary>Details</summary>
Motivation: 在动态和非结构化场景中，反应式运动生成通常受限于静态感知和系统动力学模型。可靠地建模动态障碍物并在感知和控制不确定性下优化无碰撞轨迹具有挑战性。本文旨在从模型驱动视角揭示机械臂反应式规划与动态映射之间的紧密联系。

Method: 1) 提出张量化粒子权重更新方案，实现高效粒子感知并显式维护障碍物速度和协方差；2) 基于此动态表示，提出障碍物感知的MPPI规划公式，联合传播机器人-障碍物动力学，在不确定性下预测和评估未来系统运动。

Result: 模型预测方法显著提高了动态环境中的安全性和反应能力。在模拟和噪声真实环境中的完整框架应用表明，显式建模机器人-障碍物动力学相比现有MPPI感知规划基线，在避免多个静态和动态障碍物方面持续提升性能。

Conclusion: 通过显式建模机器人-障碍物动力学，提出的框架在动态环境中实现了更安全、更反应灵敏的运动规划，验证了动态感知与反应式规划紧密集成的有效性。

Abstract: Reactive motion generation in dynamic and unstructured scenarios is typically subject to essentially static perception and system dynamics. Reliably modeling dynamic obstacles and optimizing collision-free trajectories under perceptive and control uncertainty are challenging. This article focuses on revealing tight connection between reactive planning and dynamic mapping for manipulators from a model-based perspective. To enable efficient particle-based perception with expressively dynamic property, we present a tensorized particle weight update scheme that explicitly maintains obstacle velocities and covariance meanwhile. Building upon this dynamic representation, we propose an obstacle-aware MPPI-based planning formulation that jointly propagates robot-obstacle dynamics, allowing future system motion to be predicted and evaluated under uncertainty. The model predictive method is shown to significantly improve safety and reactivity with dynamic surroundings. By applying our complete framework in simulated and noisy real-world environments, we demonstrate that explicit modeling of robot-obstacle dynamics consistently enhances performance over state-of-the-art MPPI-based perception-planning baselines avoiding multiple static and dynamic obstacles.

</details>


### [69] [VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety](https://arxiv.org/abs/2602.16511)
*Osher Azulay,Zhengjie Xu,Andrew Scheffer,Stella X. Yu*

Main category: cs.RO

TL;DR: 提出统一跌倒安全方法，通过特权教师-学生蒸馏框架实现跨复杂地形的零样本跌倒恢复，无需真实世界微调


<details>
  <summary>Details</summary>
Motivation: 人形机器人在复杂环境中需要可靠的跌倒恢复能力，现有方法将跌倒安全分解为多个独立问题，或依赖端到端策略但缺乏视觉输入，在复杂地形上泛化能力有限

Method: 基于两个关键洞察：1）自然人类跌倒和恢复姿势具有高度约束性，可通过对齐从平坦地形迁移到复杂地形；2）快速全身反应需要整合感知-运动表征。使用稀疏人类演示训练特权教师，然后蒸馏到仅依赖自我中心深度和本体感知的可部署学生模型

Result: 在仿真和真实Unitree G1人形机器人上展示了对多种非平坦环境的鲁棒、零样本跌倒安全能力，无需真实世界微调

Conclusion: 提出的统一跌倒安全方法通过目标在上下文中的潜在表征学习，实现了跨复杂地形的泛化能力，为人形机器人在杂乱环境中的可靠操作提供了解决方案

Abstract: Reliable fall recovery is critical for humanoids operating in cluttered environments. Unlike quadrupeds or wheeled robots, humanoids experience high-energy impacts, complex whole-body contact, and large viewpoint changes during a fall, making recovery essential for continued operation. Existing methods fragment fall safety into separate problems such as fall avoidance, impact mitigation, and stand-up recovery, or rely on end-to-end policies trained without vision through reinforcement learning or imitation learning, often on flat terrain. At a deeper level, fall safety is treated as monolithic data complexity, coupling pose, dynamics, and terrain and requiring exhaustive coverage, limiting scalability and generalization. We present a unified fall safety approach that spans all phases of fall recovery. It builds on two insights: 1) Natural human fall and recovery poses are highly constrained and transferable from flat to complex terrain through alignment, and 2) Fast whole-body reactions require integrated perceptual-motor representations. We train a privileged teacher using sparse human demonstrations on flat terrain and simulated complex terrains, and distill it into a deployable student that relies only on egocentric depth and proprioception. The student learns how to react by matching the teacher's goal-in-context latent representation, which combines the next target pose with the local terrain, rather than separately encoding what it must perceive and how it must act. Results in simulation and on a real Unitree G1 humanoid demonstrate robust, zero-shot fall safety across diverse non-flat environments without real-world fine-tuning. The project page is available at https://vigor2026.github.io/

</details>


### [70] [Decentralized and Fully Onboard: Range-Aided Cooperative Localization and Navigation on Micro Aerial Vehicles](https://arxiv.org/abs/2602.16594)
*Abhishek Goudar,Angela P. Schoellig*

Main category: cs.RO

TL;DR: 本文提出了一种完全去中心化的机器人团队定位与编队控制方法，仅使用车载里程计和机器人间距离测量，无需全局定位系统或集中式计算，实现了分米级精度。


<details>
  <summary>Details</summary>
Motivation: 集中式方法扩展性差，全局定位系统可能不可用，需要一种去中心化的解决方案来实现机器人团队的协调定位和编队控制。

Method: 采用块坐标下降方法进行定位，无需机器人间严格协调；提出基于因子图推理的编队控制新公式，考虑状态估计不确定性，可高效求解。

Result: 方法完全去中心化，无需特殊轨迹维持编队，在多种室内外环境中实现了分米级定位和编队控制精度。

Conclusion: 提出的范围辅助去中心化定位和基于编队的导航方法有效解决了机器人团队协调控制问题，具有实际应用价值。

Abstract: Controlling a team of robots in a coordinated manner is challenging because centralized approaches (where all computation is performed on a central machine) scale poorly, and globally referenced external localization systems may not always be available. In this work, we consider the problem of range-aided decentralized localization and formation control. In such a setting, each robot estimates its relative pose by combining data only from onboard odometry sensors and distance measurements to other robots in the team. Additionally, each robot calculates the control inputs necessary to collaboratively navigate an environment to accomplish a specific task, for example, moving in a desired formation while monitoring an area. We present a block coordinate descent approach to localization that does not require strict coordination between the robots. We present a novel formulation for formation control as inference on factor graphs that takes into account the state estimation uncertainty and can be solved efficiently. Our approach to range-aided localization and formation-based navigation is completely decentralized, does not require specialized trajectories to maintain formation, and achieves decimeter-level positioning and formation control accuracy. We demonstrate our approach through multiple real experiments involving formation flights in diverse indoor and outdoor environments.

</details>


### [71] [Sensor Query Schedule and Sensor Noise Covariances for Accuracy-constrained Trajectory Estimation](https://arxiv.org/abs/2602.16598)
*Abhishek Goudar,Angela P. Schoellig*

Main category: cs.RO

TL;DR: 该论文提出了一种通过半定规划方法估计传感器参数（测量频率和噪声协方差）以实现特定轨迹估计精度的新方法。


<details>
  <summary>Details</summary>
Motivation: 移动机器人轨迹估计的精度受限于传感器精度和测量频率，但成本约束限制了这些参数。需要一种系统方法来估计达到特定估计精度所需的传感器参数。

Method: 将传感器参数估计问题建模为半定规划问题：1) 已知传感器协方差时估计所需测量频率/调度；2) 给定测量频率时估计所需传感器协方差。使用现成求解器解决这些优化问题。

Result: 仿真和真实实验验证了所提方法计算出的传感器调度和协方差能够达到期望的轨迹估计精度，并能识别出在给定系统和传感器特性下无法达到某些估计精度的情况。

Conclusion: 该方法为传感器参数选择提供了系统化的数学框架，能够在成本约束下优化轨迹估计性能，并识别不可行的精度要求。

Abstract: Trajectory estimation involves determining the trajectory of a mobile robot by combining prior knowledge about its dynamic model with noisy observations of its state obtained using sensors. The accuracy of such a procedure is dictated by the system model fidelity and the sensor parameters, such as the accuracy of the sensor (as represented by its noise covariance) and the rate at which it can generate observations, referred to as the sensor query schedule. Intuitively, high-rate measurements from accurate sensors lead to accurate trajectory estimation. However, cost and resource constraints limit the sensor accuracy and its measurement rate. Our work's novel contribution is the estimation of sensor schedules and sensor covariances necessary to achieve a specific estimation accuracy. Concretely, we focus on estimating: (i) the rate or schedule with which a sensor of known covariance must generate measurements to achieve specific estimation accuracy, and alternatively, (ii) the sensor covariance necessary to achieve specific estimation accuracy for a given sensor update rate. We formulate the problem of estimating these sensor parameters as semidefinite programs, which can be solved by off-the-shelf solvers. We validate our approach in simulation and real experiments by showing that the sensor schedules and the sensor covariances calculated using our proposed method achieve the desired trajectory estimation accuracy. Our method also identifies scenarios where certain estimation accuracy is unachievable with the given system and sensor characteristics.

</details>


### [72] [Towards Autonomous Robotic Kidney Ultrasound: Spatial-Efficient Volumetric Imaging via Template Guided Optimal Pivoting](https://arxiv.org/abs/2602.16641)
*Xihan Ma,Haichong Zhang*

Main category: cs.RO

TL;DR: 提出基于模板引导最优枢轴扫描的自主肾脏超声成像工作流，通过探索性成像定位肾脏后执行固定点枢轴扫描，最小化探头移动，提高成像效率。


<details>
  <summary>Details</summary>
Motivation: 传统手持超声成像存在操作者依赖性、缺乏3D定位信息、易导致工作相关肌肉骨骼疾病等问题。现有机器人超声系统缺乏确定最佳成像窗口的能力，导致扫描盲目、探头覆盖过大，造成声影和器官覆盖不全。

Method: 1. 执行探索性成像获取肾脏部分观测数据；2. 将数据配准到肾脏模板以估计器官姿态；3. 机器人执行固定点枢轴扫描，使成像平面与肾脏长轴对齐，最小化探头平移。

Result: 仿真表明60%探索比例在肾脏定位精度和扫描效率间达到最佳平衡。在体评估显示肾脏定位精度达7.36毫米和13.84度，最优枢轴方法比基线缩短探头覆盖约75毫米。

Conclusion: 该方法验证了利用解剖模板优化探头对齐进行容积扫描的可行性，实现了更高效的肾脏超声成像。

Abstract: Medical ultrasound (US) imaging is a frontline tool for the diagnosis of kidney diseases. However, traditional freehand imaging procedure suffers from inconsistent, operator-dependent outcomes, lack of 3D localization information, and risks of work-related musculoskeletal disorders. While robotic ultrasound (RUS) systems offer the potential for standardized, operator-independent 3D kidney data acquisition, the existing scanning methods lack the ability to determine the optimal imaging window for efficient imaging. As a result, the scan is often blindly performed with excessive probe footprint, which frequently leads to acoustic shadowing and incomplete organ coverage. Consequently, there is a critical need for a spatially efficient imaging technique that can maximize the kidney coverage through minimum probe footprint. Here, we propose an autonomous workflow to achieve efficient kidney imaging via template-guided optimal pivoting. The system first performs an explorative imaging to generate partial observations of the kidney. This data is then registered to a kidney template to estimate the organ pose. With the kidney localized, the robot executes a fixed-point pivoting sweep where the imaging plane is aligned with the kidney long axis to minimize the probe translation. The proposed method was validated in simulation and in-vivo. Simulation results indicate that a 60% exploration ratio provides optimal balance between kidney localization accuracy and scanning efficiency. In-vivo evaluation on two male subjects demonstrates a kidney localization accuracy up to 7.36 mm and 13.84 degrees. Moreover, the optimal pivoting approach shortened the probe footprint by around 75 mm when compared with the baselines. These results valid our approach of leveraging anatomical templates to align the probe optimally for volumetric sweep.

</details>


### [73] [Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation](https://arxiv.org/abs/2602.16705)
*Runpei Dong,Ziyan Li,Xialin He,Saurabh Gupta*

Main category: cs.RO

TL;DR: HERO：结合大视觉模型泛化能力与仿真训练控制性能的人形机器人物体定位操作新范式


<details>
  <summary>Details</summary>
Motivation: 现有基于真实世界模仿学习的方法由于大规模训练数据收集困难，泛化能力有限。需要一种能在多样化真实环境中可靠操作日常物体的人形机器人系统。

Method: 提出HERO范式，结合大视觉模型的开放词汇理解和仿真训练的强控制性能。设计残差感知末端执行器跟踪策略，融合逆运动学、学习型神经前向模型、目标调整和重规划技术。

Result: 末端执行器跟踪误差降低3.2倍，系统能在办公室、咖啡店等多样化真实环境中可靠操作各种日常物体（如杯子、苹果、玩具），操作表面高度范围43cm至92cm。

Conclusion: HERO范式为人形机器人训练与日常物体交互开辟了新途径，通过结合大视觉模型泛化能力和仿真训练控制性能，实现了在多样化真实环境中的可靠操作。

Abstract: Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.

</details>


### [74] [EgoScale: Scaling Dexterous Manipulation with Diverse Egocentric Human Data](https://arxiv.org/abs/2602.16710)
*Ruijie Zheng,Dantong Niu,Yuqi Xie,Jing Wang,Mengda Xu,Yunfan Jiang,Fernando Castañeda,Fengyuan Hu,You Liang Tan,Letian Fu,Trevor Darrell,Furong Huang,Yuke Zhu,Danfei Xu,Linxi Fan*

Main category: cs.RO

TL;DR: EgoScale框架利用大规模人类数据训练VLA模型，通过两阶段迁移方法实现灵巧操作，性能提升54%


<details>
  <summary>Details</summary>
Motivation: 人类行为是学习物理智能的可扩展数据源，但如何有效利用它进行灵巧操作尚不明确。现有方法在受限环境中实现了人机迁移，但大规模人类数据是否能支持精细、高自由度的灵巧操作仍有疑问。

Method: 提出EgoScale框架：1）在20,854小时动作标注的自我中心人类视频上训练视觉语言动作模型，数据规模是先前工作的20倍以上；2）发现人类数据规模与验证损失之间的对数线性缩放定律；3）采用两阶段迁移方法：大规模人类预训练+轻量级对齐的人机中间训练。

Result: 验证损失与下游真实机器人性能强相关，证明大规模人类数据是可预测的监督源。最终策略在22自由度灵巧机械手上比无预训练基线平均成功率提高54%，并能有效迁移到低自由度机械手。

Conclusion: 大规模人类运动提供了可重用、与具体实现无关的运动先验，支持强长时程灵巧操作和一次性任务适应，仅需最小机器人监督。

Abstract: Human behavior is among the most scalable sources of data for learning physical intelligence, yet how to effectively leverage it for dexterous manipulation remains unclear. While prior work demonstrates human to robot transfer in constrained settings, it is unclear whether large scale human data can support fine grained, high degree of freedom dexterous manipulation. We present EgoScale, a human to dexterous manipulation transfer framework built on large scale egocentric human data. We train a Vision Language Action (VLA) model on over 20,854 hours of action labeled egocentric human video, more than 20 times larger than prior efforts, and uncover a log linear scaling law between human data scale and validation loss. This validation loss strongly correlates with downstream real robot performance, establishing large scale human data as a predictable supervision source. Beyond scale, we introduce a simple two stage transfer recipe: large scale human pretraining followed by lightweight aligned human robot mid training. This enables strong long horizon dexterous manipulation and one shot task adaptation with minimal robot supervision. Our final policy improves average success rate by 54% over a no pretraining baseline using a 22 DoF dexterous robotic hand, and transfers effectively to robots with lower DoF hands, indicating that large scale human motion provides a reusable, embodiment agnostic motor prior.

</details>


### [75] [One Hand to Rule Them All: Canonical Representations for Unified Dexterous Manipulation](https://arxiv.org/abs/2602.16712)
*Zhenyu Wei,Yunchao Yao,Mingyu Ding*

Main category: cs.RO

TL;DR: 提出了一种参数化规范表示方法，统一了多种灵巧手架构，实现了跨不同手部形态的零样本策略迁移


<details>
  <summary>Details</summary>
Motivation: 当前灵巧操作策略大多假设固定的手部设计，限制了其向具有不同运动学和结构布局的新手部形态的泛化能力

Method: 引入参数化规范表示，包括统一的参数空间和规范URDF格式，通过变分自编码器学习结构化潜在流形，并开发基于规范表示的条件化抓取策略

Result: 在未见过的形态上实现了81.9%的零样本成功率（3指LEAP手），验证了框架在统一表示空间和动作空间方面的有效性

Conclusion: 该框架为跨手部学习提供了可扩展的基础，为实现通用灵巧操作迈出了重要一步

Abstract: Dexterous manipulation policies today largely assume fixed hand designs, severely restricting their generalization to new embodiments with varied kinematic and structural layouts. To overcome this limitation, we introduce a parameterized canonical representation that unifies a broad spectrum of dexterous hand architectures. It comprises a unified parameter space and a canonical URDF format, offering three key advantages. 1) The parameter space captures essential morphological and kinematic variations for effective conditioning in learning algorithms. 2) A structured latent manifold can be learned over our space, where interpolations between embodiments yield smooth and physically meaningful morphology transitions. 3) The canonical URDF standardizes the action space while preserving dynamic and functional properties of the original URDFs, enabling efficient and reliable cross-embodiment policy learning. We validate these advantages through extensive analysis and experiments, including grasp policy replay, VAE latent encoding, and cross-embodiment zero-shot transfer. Specifically, we train a VAE on the unified representation to obtain a compact, semantically rich latent embedding, and develop a grasping policy conditioned on the canonical representation that generalizes across dexterous hands. We demonstrate, through simulation and real-world tasks on unseen morphologies (e.g., 81.9% zero-shot success rate on 3-finger LEAP Hand), that our framework unifies both the representational and action spaces of structurally diverse hands, providing a scalable foundation for cross-hand learning toward universal dexterous manipulation.

</details>
