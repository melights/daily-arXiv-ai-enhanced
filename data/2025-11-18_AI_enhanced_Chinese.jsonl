{"id": "2511.11533", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11533", "abs": "https://arxiv.org/abs/2511.11533", "authors": ["Jueun Kwon", "Max M. Sun", "Todd Murphey"], "title": "Volumetric Ergodic Control", "comment": "8 pages, 8 figures", "summary": "Ergodic control synthesizes optimal coverage behaviors over spatial distributions for nonlinear systems. However, existing formulations model the robot as a non-volumetric point, but in practice a robot interacts with the environment through its body and sensors with physical volume. In this work, we introduce a new ergodic control formulation that optimizes spatial coverage using a volumetric state representation. Our method preserves the asymptotic coverage guarantees of ergodic control, adds minimal computational overhead for real-time control, and supports arbitrary sample-based volumetric models. We evaluate our method across search and manipulation tasks -- with multiple robot dynamics and end-effector geometries or sensor models -- and show that it improves coverage efficiency by more than a factor of two while maintaining a 100% task completion rate across all experiments, outperforming the standard ergodic control method. Finally, we demonstrate the effectiveness of our method on a robot arm performing mechanical erasing tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u904d\u5386\u63a7\u5236\u65b9\u6cd5\uff0c\u4f7f\u7528\u4f53\u79ef\u72b6\u6001\u8868\u793a\u6765\u4f18\u5316\u7a7a\u95f4\u8986\u76d6\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5c06\u673a\u5668\u4eba\u89c6\u4e3a\u65e0\u4f53\u79ef\u70b9\u7684\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u8986\u76d6\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u904d\u5386\u63a7\u5236\u65b9\u6cd5\u5c06\u673a\u5668\u4eba\u5efa\u6a21\u4e3a\u65e0\u4f53\u79ef\u7684\u70b9\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u673a\u5668\u4eba\u901a\u8fc7\u5177\u6709\u7269\u7406\u4f53\u79ef\u7684\u8eab\u4f53\u548c\u4f20\u611f\u5668\u4e0e\u73af\u5883\u4ea4\u4e92\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u8003\u8651\u673a\u5668\u4eba\u4f53\u79ef\u7684\u904d\u5386\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u65b0\u7684\u904d\u5386\u63a7\u5236\u516c\u5f0f\uff0c\u4f7f\u7528\u4f53\u79ef\u72b6\u6001\u8868\u793a\u6765\u4f18\u5316\u7a7a\u95f4\u8986\u76d6\uff0c\u8be5\u65b9\u6cd5\u4fdd\u6301\u904d\u5386\u63a7\u5236\u7684\u6e10\u8fd1\u8986\u76d6\u4fdd\u8bc1\uff0c\u4e3a\u5b9e\u65f6\u63a7\u5236\u589e\u52a0\u6700\u5c0f\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u652f\u6301\u4efb\u610f\u57fa\u4e8e\u91c7\u6837\u7684\u4f53\u79ef\u6a21\u578b\u3002", "result": "\u5728\u641c\u7d22\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8bc4\u4f30\u8be5\u65b9\u6cd5\uff0c\u4f7f\u7528\u591a\u79cd\u673a\u5668\u4eba\u52a8\u529b\u5b66\u548c\u672b\u7aef\u6267\u884c\u5668\u51e0\u4f55\u5f62\u72b6\u6216\u4f20\u611f\u5668\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793a\u8986\u76d6\u6548\u7387\u63d0\u9ad8\u8d85\u8fc7\u4e24\u500d\uff0c\u540c\u65f6\u5728\u6240\u6709\u5b9e\u9a8c\u4e2d\u4fdd\u6301100%\u7684\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u4f18\u4e8e\u6807\u51c6\u904d\u5386\u63a7\u5236\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u624b\u81c2\u6267\u884c\u673a\u68b0\u64e6\u9664\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u4e3a\u5177\u6709\u7269\u7406\u4f53\u79ef\u7684\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u904d\u5386\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11552", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11552", "abs": "https://arxiv.org/abs/2511.11552", "authors": ["Dawei Zhu", "Rui Meng", "Jiefeng Chen", "Sujian Li", "Tomas Pfister", "Jinsung Yoon"], "title": "DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding", "comment": null, "summary": "Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.", "AI": {"tldr": "DocLens\u662f\u4e00\u4e2a\u5de5\u5177\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u7c7b\u4f3c\u955c\u5934\u7684\u65b9\u5f0f\u653e\u5927\u8bc1\u636e\uff0c\u4ece\u5b8c\u6574\u6587\u6863\u5bfc\u822a\u5230\u7279\u5b9a\u9875\u9762\u7684\u89c6\u89c9\u5143\u7d20\uff0c\u7136\u540e\u91c7\u7528\u91c7\u6837-\u88c1\u51b3\u673a\u5236\u751f\u6210\u53ef\u9760\u7b54\u6848\uff0c\u5728MMLongBench-Doc\u548cFinRAGBench-V\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u89c6\u89c9\u6587\u6863\u65f6\u9762\u4e34\u8bc1\u636e\u5b9a\u4f4d\u7684\u6839\u672c\u6311\u6218\uff0c\u96be\u4ee5\u68c0\u7d22\u76f8\u5173\u9875\u9762\u5e76\u5ffd\u7565\u89c6\u89c9\u5143\u7d20\u4e2d\u7684\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u5bfc\u81f4\u6027\u80fd\u6709\u9650\u548c\u6a21\u578b\u5e7b\u89c9\u3002", "method": "\u63d0\u51faDocLens\u6846\u67b6\uff0c\u9996\u5148\u4ece\u5b8c\u6574\u6587\u6863\u5bfc\u822a\u5230\u76f8\u5173\u9875\u9762\u7684\u7279\u5b9a\u89c6\u89c9\u5143\u7d20\uff0c\u7136\u540e\u91c7\u7528\u91c7\u6837-\u88c1\u51b3\u673a\u5236\u751f\u6210\u5355\u4e00\u53ef\u9760\u7b54\u6848\u3002", "result": "\u4e0eGemini-2.5-Pro\u914d\u5bf9\uff0cDocLens\u5728MMLongBench-Doc\u548cFinRAGBench-V\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\uff0c\u5728\u89c6\u89c9\u4e2d\u5fc3\u548c\u4e0d\u53ef\u56de\u7b54\u67e5\u8be2\u4e0a\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "DocLens\u901a\u8fc7\u589e\u5f3a\u7684\u5b9a\u4f4d\u80fd\u529b\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u89c6\u89c9\u4e2d\u5fc3\u548c\u4e0d\u53ef\u56de\u7b54\u67e5\u8be2\u65f6\uff0c\u8bc1\u660e\u4e86\u5176\u5f3a\u5927\u7684\u8bc1\u636e\u5b9a\u4f4d\u80fd\u529b\u3002"}}
