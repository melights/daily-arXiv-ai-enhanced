<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 170]
- [cs.RO](#cs.RO) [Total: 51]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Randomized-MLP Regularization Improves Domain Adaptation and Interpretability in DINOv2](https://arxiv.org/abs/2511.05509)
*Joel Valdivia Ortega,Lorenz Lamm,Franziska Eckardt,Benedikt Schworm,Marion Jasnin,Tingying Peng*

Main category: cs.CV

TL;DR: 本文提出RMLP正则化方法，通过对比学习改进Vision Transformers在医学图像中的可解释性，同时保持或提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers（如DINOv2）在跨领域任务中表现优异，但其patch tokens的重用方式降低了注意力和特征图的可解释性，特别是在医学成像领域，领域偏移会同时影响性能和透明度。

Method: 引入基于对比学习的随机化MLP（RMLP）正则化方法，在微调DINOv2时使用RMLP，鼓励生成更具语义对齐的表征。

Result: RMLP在医学和自然图像模态上都能改善或保持下游性能，同时产生更可解释的注意力图。

Conclusion: RMLP正则化能有效增强ViT模型的可解释性，并为理解对比学习提供了数学分析基础。

Abstract: Vision Transformers (ViTs), such as DINOv2, achieve strong performance across
domains but often repurpose low-informative patch tokens in ways that reduce
the interpretability of attention and feature maps. This challenge is
especially evident in medical imaging, where domain shifts can degrade both
performance and transparency. In this paper, we introduce Randomized-MLP (RMLP)
regularization, a contrastive learning-based method that encourages more
semantically aligned representations. We use RMLPs when fine-tuning DINOv2 to
both medical and natural image modalities, showing that it improves or
maintains downstream performance while producing more interpretable attention
maps. We also provide a mathematical analysis of RMLPs, offering insights into
its role in enhancing ViT-based models and advancing our understanding of
contrastive learning.

</details>


### [2] [Token Is All You Need: Cognitive Planning through Sparse Intent Alignment](https://arxiv.org/abs/2511.05540)
*Shiyao Sang*

Main category: cs.CV

TL;DR: 该论文挑战了端到端自动驾驶需要详尽场景建模的传统假设，提出仅需少量语义丰富的token即可实现有效规划，在nuPlan基准测试中取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 挑战传统端到端自动驾驶需要详尽场景建模的假设，探索更简洁高效的规划方法，避免计算密集的未来场景生成和受马尔可夫假设约束的视觉-语言-动作系统。

Method: 使用感知信息化的BEV表示，基于稀疏语义token进行轨迹规划，通过预测未来token来改善轨迹解码，并验证了显式重建损失在可靠感知输入下的效果。

Result: 在nuPlan基准测试中：1）无未来预测的稀疏表示达到0.548m ADE，优于nuScenes上约0.75m的现有方法；2）基于预测未来token的轨迹解码将ADE降至0.479m，比当前状态基线提升12.6%；3）显式重建损失在可靠感知下无益且可能损害性能。

Conclusion: 提出了"token即所需"原则，标志着从重建世界到理解世界的范式转变，为基于认知启发的系统奠定了基础，这些系统通过想象而非反应进行规划，并观察到时间模糊性的出现，模型自适应关注任务相关语义而非严格对齐固定时间戳。

Abstract: We challenge the long-standing assumption that exhaustive scene modeling is
required for high-performance end-to-end autonomous driving (E2EAD). Unlike
world-model approaches that rely on computationally intensive future scene
generation or vision-language-action (VLA) systems constrained by Markov
assumptions, we show that a minimal set of semantically rich tokens is
sufficient for effective planning. Experiments on the nuPlan benchmark (720
scenarios, over 11,000 samples) using perception-informed BEV representations
yield three key findings: (1) even without future prediction, our sparse
representation achieves 0.548 m ADE, comparable to or surpassing prior methods
reporting around 0.75 m on nuScenes; (2) conditioning trajectory decoding on
predicted future tokens reduces ADE to 0.479 m, a 12.6% improvement over
current-state baselines; and (3) explicit reconstruction loss offers no benefit
and may degrade performance under reliable perception inputs. Notably, we
observe the emergence of temporal fuzziness, where the model adaptively attends
to task-relevant semantics rather than aligning rigidly to fixed timestamps,
providing a cognitive advantage for planning under uncertainty. Our "token is
all you need" principle marks a paradigm shift from reconstructing the world to
understanding it, laying a foundation for cognitively inspired systems that
plan through imagination rather than reaction.

</details>


### [3] [Automated Invoice Data Extraction: Using LLM and OCR](https://arxiv.org/abs/2511.05547)
*Advait Thakur,Khushi Khanchandani,Akshita Shetty,Chaitravi Reddy,Ritisa Behera*

Main category: cs.CV

TL;DR: 本文提出了一种结合OCR、深度学习、大语言模型和图分析的全方位AI平台，以解决传统OCR系统在发票布局变化、手写文本和低质量扫描方面的局限性，实现前所未有的提取质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统OCR系统在处理变体发票布局、手写文本和低质量扫描时面临挑战，主要受限于强模板依赖性，限制了其在不同文档结构和布局中的灵活性。

Method: 采用混合架构，结合OCR技术、深度学习模型（如CNN和Transformer）、大语言模型（LLMs）以及图分析技术，构建全方位的AI平台。

Result: 通过视觉命名实体识别（NER）能力，从发票图像中提取信息具有更高的上下文敏感性和准确性，优于传统方法。

Conclusion: 该混合架构实现了最大可扩展性和最小人工干预，在发票信息提取方面达到了前所未有的质量和一致性水平。

Abstract: Conventional Optical Character Recognition (OCR) systems are challenged by
variant invoice layouts, handwritten text, and low- quality scans, which are
often caused by strong template dependencies that restrict their flexibility
across different document structures and layouts. Newer solutions utilize
advanced deep learning models such as Convolutional Neural Networks (CNN) as
well as Transformers, and domain-specific models for better layout analysis and
accuracy across various sections over varied document types. Large Language
Models (LLMs) have revolutionized extraction pipelines at their core with
sophisticated entity recognition and semantic comprehension to support complex
contextual relationship mapping without direct programming specification.
Visual Named Entity Recognition (NER) capabilities permit extraction from
invoice images with greater contextual sensitivity and much higher accuracy
rates than older approaches. Existing industry best practices utilize hybrid
architectures that blend OCR technology and LLM for maximum scalability and
minimal human intervention. This work introduces a holistic Artificial
Intelligence (AI) platform combining OCR, deep learning, LLMs, and graph
analytics to achieve unprecedented extraction quality and consistency.

</details>


### [4] [In-Context-Learning-Assisted Quality Assessment Vision-Language Models for Metal Additive Manufacturing](https://arxiv.org/abs/2511.05551)
*Qiaojie Zheng,Jiucai Zhang,Xiaoli Zhang*

Main category: cs.CV

TL;DR: 利用视觉语言模型（VLMs）的推理能力进行增材制造质量评估，通过上下文学习（ICL）提供应用特定知识，无需大量训练数据即可达到传统机器学习模型的分类准确率，并提供可解释的决策依据。


<details>
  <summary>Details</summary>
Motivation: 传统基于视觉的质量评估需要专用机器学习模型和大量应用特定数据集，数据收集和模型训练成本高昂且耗时。

Method: 引入上下文学习（ICL）为VLMs提供必要的应用特定知识和示范样本，探索不同的ICL采样策略寻找最优配置，在两个VLMs（Gemini-2.5-flash和Gemma3:27b）上评估线激光直接能量沉积过程的质量评估任务。

Result: ICL辅助的VLMs仅需少量样本即可达到与传统机器学习模型相似的分类准确率，同时能生成人类可解释的决策依据。提出了知识相关性和理由有效性两个指标来评估VLMs解释质量。

Conclusion: ICL辅助的VLMs能够用有限数据解决应用特定任务，在实现较高准确率的同时提供有效的支持理由，提高决策透明度。

Abstract: Vision-based quality assessment in additive manufacturing often requires
dedicated machine learning models and application-specific datasets. However,
data collection and model training can be expensive and time-consuming. In this
paper, we leverage vision-language models' (VLMs') reasoning capabilities to
assess the quality of printed parts and introduce in-context learning (ICL) to
provide VLMs with necessary application-specific knowledge and demonstration
samples. This method eliminates the requirement for large application-specific
datasets for training models. We explored different sampling strategies for ICL
to search for the optimal configuration that makes use of limited samples. We
evaluated these strategies on two VLMs, Gemini-2.5-flash and Gemma3:27b, with
quality assessment tasks in wire-laser direct energy deposition processes. The
results show that ICL-assisted VLMs can reach quality classification accuracies
similar to those of traditional machine learning models while requiring only a
minimal number of samples. In addition, unlike traditional classification
models that lack transparency, VLMs can generate human-interpretable rationales
to enhance trust. Since there are no metrics to evaluate their interpretability
in manufacturing applications, we propose two metrics, knowledge relevance and
rationale validity, to evaluate the quality of VLMs' supporting rationales. Our
results show that ICL-assisted VLMs can address application-specific tasks with
limited data, achieving relatively high accuracy while also providing valid
supporting rationales for improved decision transparency.

</details>


### [5] [EVLP:Learning Unified Embodied Vision-Language Planner with Reinforced Supervised Fine-Tuning](https://arxiv.org/abs/2511.05553)
*Xinyan Cai,Shiguang Wu,Dafeng Chi,Yuzheng Zhuang,Xingyue Quan,Jianye Hao,Qiang Guan*

Main category: cs.CV

TL;DR: EVLP是一个创新的多模态统一生成框架，通过动态预训练和强化对齐联合建模语言推理和视觉生成，解决长时程操作任务中的多模态规划不一致问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法缺乏多模态规划的统一生成框架，导致在复杂长时程操作任务中多模态规划不一致。需要整合文本逻辑推理和视觉空间想象来实现高效准确的操作。

Method: 1) 统一多模态生成框架：整合语义信息和空间特征进行视觉感知，通过可学习跨模态注意力机制直接学习离散图像的联合分布进行一步视觉合成；2) 动态感知预训练：使用逆向动力学任务和正向动力学任务的双向动态对齐策略；3) 强化监督微调：在统一生成空间中进行基于指令的微调，构建强化损失来对齐文本动作和生成图像的空间逻辑。

Result: 该方法实现了长时程任务的多模态规划，通过协调的语言-视觉建模获得了空间感知的多模态规划能力。

Conclusion: EVLP框架通过统一的多模态生成、动态预训练和强化对齐，有效解决了复杂长时程操作任务中的多模态规划挑战，实现了语言推理和视觉生成的协同整合。

Abstract: In complex embodied long-horizon manipulation tasks, effective task
decomposition and execution require synergistic integration of textual logical
reasoning and visual-spatial imagination to ensure efficient and accurate
operation. Current methods fail to adopt a unified generation framework for
multimodal planning, lead to inconsistent in multimodal planning. To address
this challenge, we present \textbf{EVLP (Embodied Vision-Language Planner)}, an
innovative multimodal unified generation framework that jointly models
linguistic reasoning and visual generation. Our approach achieves multimodal
planning for long-horizon tasks through a novel training pipeline incorporating
dynamic pretraining and reinforced alignment. Our core innovations consist of
three key components: \textbf{1) Unified Multimodal Generation Framework}: For
understanding, We integrate semantic information with spatial features to
provide comprehensive visual perception. For generation, we directly learn the
joint distribution of discrete images for one-step visual synthesis, enabling
coordinated language-visual modeling through learnable cross-modal attention
mechanisms. \textbf{2) Dynamic Perception Pretraining}: We propose a
bidirectional dynamic alignment strategy employing inverse dynamics tasks and
forward dynamics tasks, effectively strengthening multimodal correlations
within a unified feature space. \textbf{3) Reinforced Supervised Fine-Tuning}:
While conducting instruction-based fine-tuning in the unified generation space,
we construct a reinforce loss to align the spatial logic between textual
actions and generated images, enabling the model to acquire spatio-awared
multimodal planning capabilities.

</details>


### [6] [Compressing Multi-Task Model for Autonomous Driving via Pruning and Knowledge Distillation](https://arxiv.org/abs/2511.05557)
*Jiayuan Wang,Q. M. Jonathan Wu,Ning Zhang,Katsuya Suto,Lei Zhong*

Main category: cs.CV

TL;DR: 提出了一种多任务模型压缩框架，结合任务感知安全剪枝和特征级知识蒸馏，用于自动驾驶全景感知任务，在保持性能的同时显著减少模型参数。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统依赖全景感知来同时处理目标检测、可行驶区域分割和车道线分割，但多任务学习模型参数和复杂度不断增加，难以在车载设备上部署。

Method: 结合任务感知安全剪枝（集成泰勒基通道重要性和梯度冲突惩罚）和任务头无关蒸馏（从教师模型向学生模型传递中间骨干和编码器特征）。

Result: 在BDD100K数据集上，压缩模型参数减少32.7%，分割性能几乎没有精度损失，检测性能仅有轻微下降（召回率-1.2%，mAP50 -1.8%），仍能以32.7 FPS实时运行。

Conclusion: 结合剪枝和知识蒸馏为多任务全景感知提供了有效的压缩解决方案。

Abstract: Autonomous driving systems rely on panoptic perception to jointly handle
object detection, drivable area segmentation, and lane line segmentation.
Although multi-task learning is an effective way to integrate these tasks, its
increasing model parameters and complexity make deployment on on-board devices
difficult. To address this challenge, we propose a multi-task model compression
framework that combines task-aware safe pruning with feature-level knowledge
distillation. Our safe pruning strategy integrates Taylor-based channel
importance with gradient conflict penalty to keep important channels while
removing redundant and conflicting channels. To mitigate performance
degradation after pruning, we further design a task head-agnostic distillation
method that transfers intermediate backbone and encoder features from a teacher
to a student model as guidance. Experiments on the BDD100K dataset demonstrate
that our compressed model achieves a 32.7% reduction in parameters while
segmentation performance shows negligible accuracy loss and only a minor
decrease in detection (-1.2% for Recall and -1.8% for mAP50) compared to the
teacher. The compressed model still runs at 32.7 FPS in real-time. These
results show that combining pruning and knowledge distillation provides an
effective compression solution for multi-task panoptic perception.

</details>


### [7] [M2S2L: Mamba-based Multi-Scale Spatial-temporal Learning for Video Anomaly Detection](https://arxiv.org/abs/2511.05564)
*Yang Liu,Boan Chen,Xiaoguang Zhu,Jing Liu,Peng Sun,Wei Zhou*

Main category: cs.CV

TL;DR: 提出基于Mamba的多尺度时空学习框架M2S2L，用于视频异常检测，在保持计算效率的同时提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 传统视频异常检测方法难以平衡检测精度与计算效率，无法满足现代监控系统对复杂视频内容的鲁棒性要求。现有方法要么缺乏全面的时空建模，要么计算资源需求过高。

Method: 采用分层空间编码器处理多粒度空间信息，多时间编码器捕捉不同时间尺度的运动动态，引入特征分解机制进行外观和运动重建的特定任务优化。

Result: 在三个基准数据集上取得优异性能：UCSD Ped2帧级AUC 98.5%、CUHK Avenue 92.1%、ShanghaiTech 77.9%，同时保持20.1G FLOPs和45 FPS的推理速度。

Conclusion: M2S2L框架在检测精度和计算效率之间取得了良好平衡，适用于实际监控部署。

Abstract: Video anomaly detection (VAD) is an essential task in the image processing
community with prospects in video surveillance, which faces fundamental
challenges in balancing detection accuracy with computational efficiency. As
video content becomes increasingly complex with diverse behavioral patterns and
contextual scenarios, traditional VAD approaches struggle to provide robust
assessment for modern surveillance systems. Existing methods either lack
comprehensive spatial-temporal modeling or require excessive computational
resources for real-time applications. In this regard, we present a Mamba-based
multi-scale spatial-temporal learning (M2S2L) framework in this paper. The
proposed method employs hierarchical spatial encoders operating at multiple
granularities and multi-temporal encoders capturing motion dynamics across
different time scales. We also introduce a feature decomposition mechanism to
enable task-specific optimization for appearance and motion reconstruction,
facilitating more nuanced behavioral modeling and quality-aware anomaly
assessment. Experiments on three benchmark datasets demonstrate that M2S2L
framework achieves 98.5%, 92.1%, and 77.9% frame-level AUCs on UCSD Ped2, CUHK
Avenue, and ShanghaiTech respectively, while maintaining efficiency with 20.1G
FLOPs and 45 FPS inference speed, making it suitable for practical surveillance
deployment.

</details>


### [8] [In-Context Adaptation of VLMs for Few-Shot Cell Detection in Optical Microscopy](https://arxiv.org/abs/2511.05565)
*Shreyan Ganguly,Angona Biswas,Jaydeep Rade,Md Hasibul Hasan Hasib,Nabila Masud,Nitish Singla,Abhipsa Dash,Ushashi Bhattacharjee,Aditya Balu,Anwesha Sarkar,Adarsh Krishnamurthy,Soumik Sarkar*

Main category: cs.CV

TL;DR: 该论文研究了基础视觉语言模型在生物医学显微镜图像上的应用，通过上下文学习实现少样本目标检测，并提出了Micro-OD基准测试。


<details>
  <summary>Details</summary>
Motivation: 基础视觉语言模型在自然图像上表现出色，但在生物医学显微镜领域的应用尚未充分探索，特别是在缺乏大规模标注数据集的情况下。

Method: 引入Micro-OD基准测试，包含252张图像和11种细胞类型的边界框标注；系统评估8种视觉语言模型在少样本条件下的表现；实现结合检测头和VLM少样本分类器的混合少样本目标检测流水线。

Result: 零样本性能较弱，但少样本支持能持续改善检测性能，6个样本后增益趋于稳定；具有推理标记的模型更适合端到端定位，而简单变体更适合分类预定位裁剪区域。

Conclusion: 上下文适应是显微镜图像处理的实用路径，该基准测试为推进生物医学成像中的开放词汇检测提供了可复现的测试平台。

Abstract: Foundation vision-language models (VLMs) excel on natural images, but their
utility for biomedical microscopy remains underexplored. In this paper, we
investigate how in-context learning enables state-of-the-art VLMs to perform
few-shot object detection when large annotated datasets are unavailable, as is
often the case with microscopic images. We introduce the Micro-OD benchmark, a
curated collection of 252 images specifically curated for in-context learning,
with bounding-box annotations spanning 11 cell types across four sources,
including two in-lab expert-annotated sets. We systematically evaluate eight
VLMs under few-shot conditions and compare variants with and without implicit
test-time reasoning tokens. We further implement a hybrid Few-Shot Object
Detection (FSOD) pipeline that combines a detection head with a VLM-based
few-shot classifier, which enhances the few-shot performance of recent VLMs on
our benchmark. Across datasets, we observe that zero-shot performance is weak
due to the domain gap; however, few-shot support consistently improves
detection, with marginal gains achieved after six shots. We observe that models
with reasoning tokens are more effective for end-to-end localization, whereas
simpler variants are more suitable for classifying pre-localized crops. Our
results highlight in-context adaptation as a practical path for microscopy, and
our benchmark provides a reproducible testbed for advancing open-vocabulary
detection in biomedical imaging.

</details>


### [9] [C3-Diff: Super-resolving Spatial Transcriptomics via Cross-modal Cross-content Contrastive Diffusion Modelling](https://arxiv.org/abs/2511.05571)
*Xiaofei Wang,Stephen Price,Chao Li*

Main category: cs.CV

TL;DR: C3-Diff是一个用于空间转录组学增强的跨模态对比扩散框架，通过整合组织学图像和基因表达数据来提高ST图谱的分辨率。


<details>
  <summary>Details</summary>
Motivation: 当前空间转录组学平台分辨率较低，限制了空间基因表达的深入理解。需要开发有效的方法来整合组织学图像和基因表达数据以增强ST图谱。

Method: 提出跨模态跨内容对比扩散框架C3-Diff：1）改进传统对比学习范式提取模态不变和内容不变特征；2）在特征单元超球面上进行噪声信息增强；3）动态跨模态插补训练策略缓解数据稀缺问题。

Result: 在四个公共数据集上测试，C3-Diff相比竞争方法有显著提升。在下游任务（细胞类型定位、基因表达相关性、单细胞级基因表达预测）中表现优异。

Conclusion: C3-Diff通过有效的跨模态整合方法显著提升了空间转录组学增强性能，推动了AI增强生物技术在生物医学研究和临床应用中的发展。

Abstract: The rapid advancement of spatial transcriptomics (ST), i.e., spatial gene
expressions, has made it possible to measure gene expression within original
tissue, enabling us to discover molecular mechanisms. However, current ST
platforms frequently suffer from low resolution, limiting the in-depth
understanding of spatial gene expression. Super-resolution approaches promise
to enhance ST maps by integrating histology images with gene expressions of
profiled tissue spots. However, it remains a challenge to model the
interactions between histology images and gene expressions for effective ST
enhancement. This study presents a cross-modal cross-content contrastive
diffusion framework, called C3-Diff, for ST enhancement with histology images
as guidance. In C3-Diff, we firstly analyze the deficiency of traditional
contrastive learning paradigm, which is then refined to extract both
modal-invariant and content-invariant features of ST maps and histology images.
Further, to overcome the problem of low sequencing sensitivity in ST maps, we
perform nosing-based information augmentation on the surface of feature unit
hypersphere. Finally, we propose a dynamic cross-modal imputation-based
training strategy to mitigate ST data scarcity. We tested C3-Diff by
benchmarking its performance on four public datasets, where it achieves
significant improvements over competing methods. Moreover, we evaluate C3-Diff
on downstream tasks of cell type localization, gene expression correlation and
single-cell-level gene expression prediction, promoting AI-enhanced
biotechnology for biomedical research and clinical applications. Codes are
available at https://github.com/XiaofeiWang2018/C3-Diff.

</details>


### [10] [Video Text Preservation with Synthetic Text-Rich Videos](https://arxiv.org/abs/2511.05573)
*Ziyang Liu,Kevin Valencia,Justin Cui*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级方法来改进文本到视频(T2V)扩散模型，通过使用合成监督数据来提升视频中文本的清晰度和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的T2V模型在生成包含清晰、连贯文本的视频方面存在困难，特别是对于短短语或单词的渲染效果不佳，而之前的解决方案计算成本高昂且不适合视频生成。

Method: 首先使用文本到图像(T2I)扩散模型生成富含文本的图像，然后通过文本无关的图像到视频(I2V)模型将这些图像动画化为短视频，利用这些合成的视频-提示对来微调预训练的T2V模型Wan2.1，无需架构更改。

Result: 结果显示在短文本清晰度和时间一致性方面有所改善，并为长文本提供了新兴的结构先验。

Conclusion: 精心策划的合成数据和弱监督为提高T2V生成中的文本保真度提供了一条实用路径。

Abstract: While Text-To-Video (T2V) models have advanced rapidly, they continue to
struggle with generating legible and coherent text within videos. In
particular, existing models often fail to render correctly even short phrases
or words and previous attempts to address this problem are computationally
expensive and not suitable for video generation. In this work, we investigate a
lightweight approach to improve T2V diffusion models using synthetic
supervision. We first generate text-rich images using a text-to-image (T2I)
diffusion model, then animate them into short videos using a text-agnostic
image-to-video (I2v) model. These synthetic video-prompt pairs are used to
fine-tune Wan2.1, a pre-trained T2V model, without any architectural changes.
Our results show improvement in short-text legibility and temporal consistency
with emerging structural priors for longer text. These findings suggest that
curated synthetic data and weak supervision offer a practical path toward
improving textual fidelity in T2V generation.

</details>


### [11] [DiffSwap++: 3D Latent-Controlled Diffusion for Identity-Preserving Face Swapping](https://arxiv.org/abs/2511.05575)
*Weston Bondurant,Arkaprava Sinha,Hieu Le,Srijan Das,Stephanie Schuckers*

Main category: cs.CV

TL;DR: DiffSwap++是一种基于扩散模型的人脸交换方法，通过整合3D面部潜在特征来提升几何一致性和身份保持能力，在CelebA、FFHQ和CelebV-Text数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的人脸交换方法在处理挑战性姿态和表情时存在细粒度伪影和身份保持不佳的问题，主要原因是未能有效利用3D面部结构来解耦身份与姿态表情。

Method: 提出DiffSwap++方法，在训练过程中整合3D面部潜在特征，设计扩散架构在去噪过程中同时条件化身份嵌入和面部关键点，实现高保真和身份保持的人脸交换。

Result: 在CelebA、FFHQ和CelebV-Text数据集上的广泛实验表明，DiffSwap++在保持源身份同时维持目标姿态和表情方面优于先前方法，并通过生物特征风格评估和用户研究验证了方法的真实性和有效性。

Conclusion: DiffSwap++通过整合3D面部特征有效提升了人脸交换的几何一致性和身份保持能力，为扩散模型在人脸交换任务中的应用提供了新的解决方案。

Abstract: Diffusion-based approaches have recently achieved strong results in face
swapping, offering improved visual quality over traditional GAN-based methods.
However, even state-of-the-art models often suffer from fine-grained artifacts
and poor identity preservation, particularly under challenging poses and
expressions. A key limitation of existing approaches is their failure to
meaningfully leverage 3D facial structure, which is crucial for disentangling
identity from pose and expression. In this work, we propose DiffSwap++, a novel
diffusion-based face-swapping pipeline that incorporates 3D facial latent
features during training. By guiding the generation process with 3D-aware
representations, our method enhances geometric consistency and improves the
disentanglement of facial identity from appearance attributes. We further
design a diffusion architecture that conditions the denoising process on both
identity embeddings and facial landmarks, enabling high-fidelity and
identity-preserving face swaps. Extensive experiments on CelebA, FFHQ, and
CelebV-Text demonstrate that DiffSwap++ outperforms prior methods in preserving
source identity while maintaining target pose and expression. Additionally, we
introduce a biometric-style evaluation and conduct a user study to further
validate the realism and effectiveness of our approach. Code will be made
publicly available at https://github.com/WestonBond/DiffSwapPP

</details>


### [12] [Beyond Softmax: Dual-Branch Sigmoid Architecture for Accurate Class Activation Maps](https://arxiv.org/abs/2511.05590)
*Yoojin Oh,Junhyug Noh*

Main category: cs.CV

TL;DR: 本文提出了一种双分支sigmoid头方法，解决了传统CAM方法因依赖softmax分类器而导致的logit偏移和符号崩溃问题，在保持分类精度的同时提高了定位性能。


<details>
  <summary>Details</summary>
Motivation: 传统CAM方法及其扩展依赖最终的softmax分类器，存在两个基本失真问题：加性logit偏移会任意偏置重要性分数，符号崩溃会混淆兴奋性和抑制性特征。

Method: 提出一种简单的、架构无关的双分支sigmoid头，将定位与分类解耦。克隆预训练模型的分类头到并行分支，使用逐类sigmoid输出，冻结原始softmax头，仅用类别平衡的二元监督微调sigmoid分支。

Result: 在细粒度任务（CUB-200-2011、Stanford Cars）和WSOL基准（ImageNet-1K、OpenImages30K）上的广泛评估显示，解释保真度得到改善，Top-1定位性能一致提升，且分类准确率无下降。

Conclusion: 该方法能与大多数CAM变体无缝集成，带来可忽略的开销，在保持识别准确性的同时，从sigmoid分支生成类别证据图，保留了特征贡献的大小和符号。

Abstract: Class Activation Mapping (CAM) and its extensions have become indispensable
tools for visualizing the evidence behind deep network predictions. However, by
relying on a final softmax classifier, these methods suffer from two
fundamental distortions: additive logit shifts that arbitrarily bias importance
scores, and sign collapse that conflates excitatory and inhibitory features. We
propose a simple, architecture-agnostic dual-branch sigmoid head that decouples
localization from classification. Given any pretrained model, we clone its
classification head into a parallel branch ending in per-class sigmoid outputs,
freeze the original softmax head, and fine-tune only the sigmoid branch with
class-balanced binary supervision. At inference, softmax retains recognition
accuracy, while class evidence maps are generated from the sigmoid branch --
preserving both magnitude and sign of feature contributions. Our method
integrates seamlessly with most CAM variants and incurs negligible overhead.
Extensive evaluations on fine-grained tasks (CUB-200-2011, Stanford Cars) and
WSOL benchmarks (ImageNet-1K, OpenImages30K) show improved explanation fidelity
and consistent Top-1 Localization gains -- without any drop in classification
accuracy. Code is available at https://github.com/finallyupper/beyond-softmax.

</details>


### [13] [In-process 3D Deviation Mapping and Defect Monitoring (3D-DM2) in High Production-rate Robotic Additive Manufacturing](https://arxiv.org/abs/2511.05604)
*Subash Gautam,Alejandro Vargas-Uscategui,Peter King,Hans Lohr,Alireza Bab-Hadiashar,Ivan Cole,Ehsan Asadi*

Main category: cs.CV

TL;DR: 本文提出了一种实时监测系统，用于在高速机器人增材制造过程中检测和重建生长部件，通过与近净参考模型直接比较来识别形状偏差，实现及时干预和补偿。


<details>
  <summary>Details</summary>
Motivation: 高速机器人增材制造（如冷喷涂增材制造）虽然沉积率高，但当前开环系统中的过程不稳定性导致形状精度难以保持，需要实时检测偏差以防止误差传播。

Method: 开发实时监测系统，采集和重建生长部件，与近净参考模型直接比较，检测制造过程中的形状偏差，并对每个偏差区域进行分割和跟踪。

Result: 实现了对形状不一致性的早期识别，能够及时检测制造过程中的偏差。

Conclusion: 该监测系统为及时干预和补偿提供了途径，有助于实现一致的零件质量，减少后处理需求。

Abstract: Additive manufacturing (AM) is an emerging digital manufacturing technology
to produce complex and freeform objects through a layer-wise deposition. High
deposition rate robotic AM (HDRRAM) processes, such as cold spray additive
manufacturing (CSAM), offer significantly increased build speeds by delivering
large volumes of material per unit time. However, maintaining shape accuracy
remains a critical challenge, particularly due to process instabilities in
current open-loop systems. Detecting these deviations as they occur is
essential to prevent error propagation, ensure part quality, and minimize
post-processing requirements. This study presents a real-time monitoring system
to acquire and reconstruct the growing part and directly compares it with a
near-net reference model to detect the shape deviation during the manufacturing
process. The early identification of shape inconsistencies, followed by
segmenting and tracking each deviation region, paves the way for timely
intervention and compensation to achieve consistent part quality.

</details>


### [14] [Walking the Schrödinger Bridge: A Direct Trajectory for Text-to-3D Generation](https://arxiv.org/abs/2511.05609)
*Ziying Li,Xuequan Lu,Xinkui Zhao,Guanjie Cheng,Shuiguang Deng,Jianwei Yin*

Main category: cs.CV

TL;DR: 本文提出TraCe框架，通过将文本到3D生成过程建模为学习渲染分布到目标分布的最优传输轨迹，解决了传统SDS方法导致的过饱和和过平滑问题。


<details>
  <summary>Details</summary>
Motivation: 基于优化的文本到3D生成方法依赖从预训练文本到图像扩散模型蒸馏知识，但SDS等技术会引入过饱和和过平滑等伪影，需要解决这一关键问题。

Method: 首先理论证明SDS是Schrödinger Bridge框架的简化实例，然后提出TraCe框架，将Schrödinger Bridge重新表述为从当前渲染到文本条件去噪目标的扩散桥，并在该轨迹的分数动态上训练LoRA适应模型进行稳健3D优化。

Result: 综合实验表明，TraCe在质量和保真度上始终优于现有最先进技术。

Conclusion: TraCe通过构建最优传输轨迹实现了高质量的文本到3D生成，能够使用更小的CFG值获得更好的生成效果。

Abstract: Recent advancements in optimization-based text-to-3D generation heavily rely
on distilling knowledge from pre-trained text-to-image diffusion models using
techniques like Score Distillation Sampling (SDS), which often introduce
artifacts such as over-saturation and over-smoothing into the generated 3D
assets. In this paper, we address this essential problem by formulating the
generation process as learning an optimal, direct transport trajectory between
the distribution of the current rendering and the desired target distribution,
thereby enabling high-quality generation with smaller Classifier-free Guidance
(CFG) values. At first, we theoretically establish SDS as a simplified instance
of the Schr\"odinger Bridge framework. We prove that SDS employs the reverse
process of an Schr\"odinger Bridge, which, under specific conditions (e.g., a
Gaussian noise as one end), collapses to SDS's score function of the
pre-trained diffusion model. Based upon this, we introduce Trajectory-Centric
Distillation (TraCe), a novel text-to-3D generation framework, which
reformulates the mathematically trackable framework of Schr\"odinger Bridge to
explicitly construct a diffusion bridge from the current rendering to its
text-conditioned, denoised target, and trains a LoRA-adapted model on this
trajectory's score dynamics for robust 3D optimization. Comprehensive
experiments demonstrate that TraCe consistently achieves superior quality and
fidelity to state-of-the-art techniques.

</details>


### [15] [Pose-Aware Multi-Level Motion Parsing for Action Quality Assessment](https://arxiv.org/abs/2511.05611)
*Shuaikang Zhu,Yang Yang,Chen Sun*

Main category: cs.CV

TL;DR: 提出了一种基于增强时空姿态特征的多层次运动解析框架，用于动作质量评估。该框架包含动作单元解析器、运动解析器、条件解析器和权重调整评分模块，在大规模跳水运动数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 人体姿态是动作质量评估的核心，细微的时空姿态变化往往决定了动作的优劣。在高级别比赛中，这些细微差异成为评分的关键因素。

Method: 设计多层次运动解析框架：第一层动作单元解析器通过姿态提取实现精确动作分割和局部-全局姿态表示；第二层运动解析器通过时空特征学习捕捉每个动作单元的姿势变化和外观细节；额外设计条件解析器处理身体相关之外的特殊条件（如跳水中的水花）；引入权重调整评分模块以适应不同动作类型的多样化需求。

Result: 在大规模跳水运动数据集上的广泛评估表明，该多层次运动解析框架在动作分割和动作评分任务中都达到了最先进的性能。

Conclusion: 提出的多层次运动解析框架能够有效捕捉细微的姿态变化，为动作质量评估提供了灵活且高性能的解决方案。

Abstract: Human pose serves as a cornerstone of action quality assessment (AQA), where
subtle spatial-temporal variations in pose often distinguish excellence from
mediocrity. In high-level competitions, these nuanced differences become
decisive factors in scoring. In this paper, we propose a novel multi-level
motion parsing framework for AQA based on enhanced spatial-temporal pose
features. On the first level, the Action-Unit Parser is designed with the help
of pose extraction to achieve precise action segmentation and comprehensive
local-global pose representations. On the second level, Motion Parser is used
by spatial-temporal feature learning to capture pose changes and appearance
details for each action-unit. Meanwhile, some special conditions other than
body-related will impact action scoring, like water splash in diving. In this
work, we design an additional Condition Parser to offer users more flexibility
in their choices. Finally, Weight-Adjust Scoring Module is introduced to better
accommodate the diverse requirements of various action types and the
multi-scale nature of action-units. Extensive evaluations on large-scale diving
sports datasets demonstrate that our multi-level motion parsing framework
achieves state-of-the-art performance in both action segmentation and action
scoring tasks.

</details>


### [16] [Grounding Foundational Vision Models with 3D Human Poses for Robust Action Recognition](https://arxiv.org/abs/2511.05622)
*Nicholas Babey,Tiffany Gu,Yiheng Li,Cristian Meo,Kevin Zhu*

Main category: cs.CV

TL;DR: 提出了一种融合V-JEPA 2的世界动态预测能力和CoMotion的遮挡容忍人体姿态数据的动作识别模型，在复杂遮挡场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于RGB视频的动作识别模型只能学习表面模式与标签的关联，难以捕捉底层物理交互动态和人体姿态，特别是在复杂场景中。

Method: 融合V-JEPA 2的上下文预测世界动态和CoMotion的显式遮挡容忍人体姿态数据，构建物理空间基础的动作识别模型。

Result: 在InHARD和UCF-19-Y-OCC基准测试中优于三个基线模型，特别是在复杂遮挡场景下表现突出。

Conclusion: 动作识别需要基于空间理解而非统计模式识别，物理空间的建模对理解人类动作至关重要。

Abstract: For embodied agents to effectively understand and interact within the world
around them, they require a nuanced comprehension of human actions grounded in
physical space. Current action recognition models, often relying on RGB video,
learn superficial correlations between patterns and action labels, so they
struggle to capture underlying physical interaction dynamics and human poses in
complex scenes. We propose a model architecture that grounds action recognition
in physical space by fusing two powerful, complementary representations: V-JEPA
2's contextual, predictive world dynamics and CoMotion's explicit,
occlusion-tolerant human pose data. Our model is validated on both the InHARD
and UCF-19-Y-OCC benchmarks for general action recognition and high-occlusion
action recognition, respectively. Our model outperforms three other baselines,
especially within complex, occlusive scenes. Our findings emphasize a need for
action recognition to be supported by spatial understanding instead of
statistical pattern recognition.

</details>


### [17] [Registration-Free Monitoring of Unstructured Point Cloud Data via Intrinsic Geometrical Properties](https://arxiv.org/abs/2511.05623)
*Mariafrancesca Patalano,Giovanna Capizzi,Kamran Paynabar*

Main category: cs.CV

TL;DR: 提出了一种无需配准和网格重建的点云数据监控新方法，利用拉普拉斯和测地距离提取内在几何特征，通过阈值技术选择最能指示异常状态的特征进行监控。


<details>
  <summary>Details</summary>
Motivation: 传统点云数据监控需要配准和网格重建等预处理步骤，但这些步骤容易出错、耗时且可能引入伪影，影响监控结果。

Method: 开发了两种基于内在几何特性的特征学习方法（拉普拉斯和测地距离），以及一个通用的监控方案，使用阈值技术选择最相关的内在特征。

Result: 数值实验和案例研究表明，该方法能有效识别各种类型的缺陷。

Conclusion: 该方法为复杂形状的点云数据监控提供了一种无需配准和网格重建的有效解决方案。

Abstract: Modern sensing technologies have enabled the collection of unstructured point
cloud data (PCD) of varying sizes, which are used to monitor the geometric
accuracy of 3D objects. PCD are widely applied in advanced manufacturing
processes, including additive, subtractive, and hybrid manufacturing. To ensure
the consistency of analysis and avoid false alarms, preprocessing steps such as
registration and mesh reconstruction are commonly applied prior to monitoring.
However, these steps are error-prone, time-consuming and may introduce
artifacts, potentially affecting monitoring outcomes. In this paper, we present
a novel registration-free approach for monitoring PCD of complex shapes,
eliminating the need for both registration and mesh reconstruction. Our
proposal consists of two alternative feature learning methods and a common
monitoring scheme. Feature learning methods leverage intrinsic geometric
properties of the shape, captured via the Laplacian and geodesic distances. In
the monitoring scheme, thresholding techniques are used to further select
intrinsic features most indicative of potential out-of-control conditions.
Numerical experiments and case studies highlight the effectiveness of the
proposed approach in identifying different types of defects.

</details>


### [18] [Culture in Action: Evaluating Text-to-Image Models through Social Activities](https://arxiv.org/abs/2511.05681)
*Sina Malakouti,Boqing Gong,Adriana Kovashka*

Main category: cs.CV

TL;DR: CULTIVate是一个评估文本到图像模型在跨文化活动表现的新基准，涵盖16个国家、576个提示和19000多张图像，通过可解释的描述符框架评估文化维度，并提出四个衡量文化对齐、幻觉、夸张元素和多样性的指标。


<details>
  <summary>Details</summary>
Motivation: 现有文化基准主要关注以对象为中心的类别，忽视了更能反映文化规范的社会和日常活动，且缺乏衡量文化忠实度的指标。

Method: 构建包含16个国家跨文化活动的基准数据集，采用可解释的描述符评估框架，从背景、服饰、对象和互动等多个文化维度进行分析，并提出了四个新的评估指标。

Result: 研究发现系统性的差异：模型对全球北方国家的表现优于全球南方国家，不同T2I系统存在不同的失败模式。人类研究证实所提指标与人类判断的相关性优于现有文本-图像指标。

Conclusion: CULTIVate基准揭示了T2I模型在文化表现上的系统性偏见，所提出的评估框架和指标能更准确地衡量模型的文化忠实度，为改进模型的文化包容性提供了重要工具。

Abstract: Text-to-image (T2I) diffusion models achieve impressive photorealism by
training on large-scale web data, but models inherit cultural biases and fail
to depict underrepresented regions faithfully. Existing cultural benchmarks
focus mainly on object-centric categories (e.g., food, attire, and
architecture), overlooking the social and daily activities that more clearly
reflect cultural norms. Few metrics exist for measuring cultural faithfulness.
We introduce CULTIVate, a benchmark for evaluating T2I models on cross-cultural
activities (e.g., greetings, dining, games, traditional dances, and cultural
celebrations). CULTIVate spans 16 countries with 576 prompts and more than
19,000 images, and provides an explainable descriptor-based evaluation
framework across multiple cultural dimensions, including background, attire,
objects, and interactions. We propose four metrics to measure cultural
alignment, hallucination, exaggerated elements, and diversity. Our findings
reveal systematic disparities: models perform better for global north countries
than for the global south, with distinct failure modes across T2I systems.
Human studies confirm that our metrics correlate more strongly with human
judgments than existing text-image metrics.

</details>


### [19] [VMDT: Decoding the Trustworthiness of Video Foundation Models](https://arxiv.org/abs/2511.05682)
*Yujin Potter,Zhun Wang,Nicholas Crispino,Kyle Montgomery,Alexander Xiong,Ethan Y. Chang,Francesco Pinto,Yuqi Chen,Rahul Gupta,Morteza Ziyadi,Christos Christodoulopoulos,Bo Li,Chenguang Wang,Dawn Song*

Main category: cs.CV

TL;DR: VMDT是首个评估文本到视频(T2V)和视频到文本(V2T)模型在安全性、幻觉、公平性、隐私和对抗鲁棒性五个关键可信度维度的统一平台。研究发现开源T2V模型普遍无法识别有害查询并生成有害视频，V2T模型的公平性和隐私风险随规模增加而上升，而安全性则与模型规模无关。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型变得越来越复杂，确保其可信度变得至关重要，但视频模态仍缺乏全面的可信度基准。

Method: 开发了VMDT统一评估平台，对7个T2V模型和19个V2T模型在五个可信度维度进行系统评估。

Result: 所有开源T2V模型都未能识别有害查询并经常生成有害视频，表现出比图像模态模型更高的不公平性。V2T模型中，不公平性和隐私风险随规模增加，幻觉和对抗鲁棒性有所改善但整体性能仍低。安全性不随模型规模变化。

Conclusion: 研究结果强调了开发更鲁棒和可信视频基础模型的迫切需求，VMDT为衡量和跟踪这一目标的进展提供了系统框架。

Abstract: As foundation models become more sophisticated, ensuring their
trustworthiness becomes increasingly critical; yet, unlike text and image, the
video modality still lacks comprehensive trustworthiness benchmarks. We
introduce VMDT (Video-Modal DecodingTrust), the first unified platform for
evaluating text-to-video (T2V) and video-to-text (V2T) models across five key
trustworthiness dimensions: safety, hallucination, fairness, privacy, and
adversarial robustness. Through our extensive evaluation of 7 T2V models and 19
V2T models using VMDT, we uncover several significant insights. For instance,
all open-source T2V models evaluated fail to recognize harmful queries and
often generate harmful videos, while exhibiting higher levels of unfairness
compared to image modality models. In V2T models, unfairness and privacy risks
rise with scale, whereas hallucination and adversarial robustness improve --
though overall performance remains low. Uniquely, safety shows no correlation
with model size, implying that factors other than scale govern current safety
levels. Our findings highlight the urgent need for developing more robust and
trustworthy video foundation models, and VMDT provides a systematic framework
for measuring and tracking progress toward this goal. The code is available at
https://sunblaze-ucb.github.io/VMDT-page/.

</details>


### [20] [Pedicle Screw Pairing and Registration for Screw Pose Estimation from Dual C-arm Images Using CAD Models](https://arxiv.org/abs/2511.05702)
*Yehyun Suh,Lin Li,Aric Plumley,Chaochao Zhou,Daniel Moyer,Kongbin Kang*

Main category: cs.CV

TL;DR: 本文提出了一种从双C臂图像中解决椎弓根螺钉对应关系和姿态估计的方法，通过比较螺钉组合和2D-3D配准，在配对和配准任务中表现出稳定准确性。


<details>
  <summary>Details</summary>
Motivation: 在脊柱手术中，准确匹配前后位和侧位图像中的椎弓根螺钉对于成功的手术至关重要，但特别是在侧位视图中建立螺钉对应关系仍然是一个显著的临床挑战。

Method: 通过比较螺钉组合，并采用螺钉CAD 3D模型进行2D-3D配准，从双视图中准确配对和估计螺钉姿态。

Result: 结果显示正确的螺钉组合在所有测试案例中始终优于错误配对，即使在配准前也是如此。配准后，正确组合进一步增强了投影与图像之间的对齐，显著减少了投影误差。

Conclusion: 该方法通过提供可靠的螺钉定位反馈，有望改善脊柱手术的手术效果。

Abstract: Accurate matching of pedicle screws in both anteroposterior (AP) and lateral
(LAT) images is critical for successful spinal decompression and stabilization
during surgery. However, establishing screw correspondence, especially in LAT
views, remains a significant clinical challenge. This paper introduces a method
to address pedicle screw correspondence and pose estimation from dual C-arm
images. By comparing screw combinations, the approach demonstrates consistent
accuracy in both pairing and registration tasks. The method also employs 2D-3D
alignment with screw CAD 3D models to accurately pair and estimate screw pose
from dual views. Our results show that the correct screw combination
consistently outperforms incorrect pairings across all test cases, even prior
to registration. After registration, the correct combination further enhances
alignment between projections and images, significantly reducing projection
error. This approach shows promise for improving surgical outcomes in spinal
procedures by providing reliable feedback on screw positioning.

</details>


### [21] [Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale](https://arxiv.org/abs/2511.05705)
*David Acuna,Chao-Han Huck Yang,Yuntian Deng,Jaehun Jung,Ximing Lu,Prithviraj Ammanabrolu,Hyunwoo Kim,Yuan-Hong Liao,Yejin Choi*

Main category: cs.CV

TL;DR: 本文提出了一个大规模视觉中心推理数据生成框架，包含超过100万个高质量合成视觉问题，支持离线和在线强化学习。通过在Qwen2.5-VL-7B上进行微调，在多个视觉基准测试中超越了所有开源基线模型，甚至超过了某些闭源模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理进展主要依赖于未公开数据集和专有数据合成方法，缺乏系统构建大规模视觉中心推理数据集的方法，特别是针对超越视觉数学的任务。

Method: 采用两阶段合成框架：规模和复杂性。通过视觉语言模型和推理大语言模型的两阶段过程合成推理轨迹，生成包含丰富认知行为的思维链轨迹。

Result: 在Qwen2.5-VL-7B上微调后，在V* Bench、CV-Bench和MMStar-V等视觉基准测试中表现优异，超越了所有开源基线模型和某些闭源模型。数据还显示出对纯文本推理和音频推理的正向迁移效果。

Conclusion: 高质量数据上的监督微调对于有效在线强化学习至关重要；分阶段离线强化学习可以达到在线强化学习的性能同时降低计算需求；精心设计的监督微调可以显著改善跨领域、跨模态的迁移能力。

Abstract: Recent progress in multimodal reasoning has been driven largely by
undisclosed datasets and proprietary data synthesis recipes, leaving open
questions about how to systematically build large-scale, vision-centric
reasoning datasets, particularly for tasks that go beyond visual math. In this
work, we introduce a new reasoning data generation framework spanning diverse
skills and levels of complexity with over 1M high-quality synthetic
vision-centric questions. The dataset also includes preference data and
instruction prompts supporting both offline and online RL. Our synthesis
framework proceeds in two stages: (1) scale; and (2) complexity. Reasoning
traces are then synthesized through a two-stage process that leverages VLMs and
reasoning LLMs, producing CoT traces for VLMs that capture the richness and
diverse cognitive behaviors found in frontier reasoning models. Remarkably, we
show that finetuning Qwen2.5-VL-7B on our data outperforms all open-data
baselines across all evaluated vision-centric benchmarks, and even surpasses
strong closed-data models such as MiMo-VL-7B-RL on V* Bench, CV-Bench and
MMStar-V. Perhaps most surprising, despite being entirely vision-centric, our
data transfers positively to text-only reasoning (MMLU-Pro) and audio reasoning
(MMAU), demonstrating its effectiveness. Similarly, despite not containing
videos or embodied visual data, we observe notable gains when evaluating on a
single-evidence embodied QA benchmark (NiEH). Finally, we use our data to
analyze the entire VLM post-training pipeline. Our empirical analysis
highlights that (i) SFT on high-quality data with non-linear reasoning traces
is essential for effective online RL, (ii) staged offline RL matches online
RL's performance while reducing compute demands, and (iii) careful SFT on high
quality data can substantially improve out-of-domain, cross-modality transfer.

</details>


### [22] [Towards Better Ultrasound Video Segmentation Foundation Model: An Empirical study on SAM2 Finetuning from Data Perspective](https://arxiv.org/abs/2511.05731)
*Xing Yao,Ahana Gangopadhyay,Hsi-Ming Chang,Ravi Soni*

Main category: cs.CV

TL;DR: 本文系统研究了SAM2模型在超声视频分割中的适应性问题，发现数据规模和时间上下文比模型架构或初始化更重要，联合训练在模态对齐和任务专业化之间提供了有效平衡。


<details>
  <summary>Details</summary>
Motivation: 超声视频分割面临数据集变异性大、运动伪影和标注数据有限等挑战。虽然SAM2等基础模型在零样本和提示引导分割方面表现出色，但在医学影像领域性能显著下降。现有研究主要关注架构修改，而数据特性和训练机制的影响尚未得到系统研究。

Method: 通过三种范式（任务特定微调、中间适应、多任务联合训练）在五个SAM2变体和多种提示模式下，分析训练集大小、视频时长和增强方案对适应性能的影响。设计了六种超声特定增强方法，并与通用策略进行比较。

Result: 在三个代表性超声数据集上的实验表明，数据规模和时间上下文比模型架构或初始化发挥更决定性作用。联合训练在模态对齐和任务专业化之间提供了有效的折衷方案。

Conclusion: 本研究为开发高效、数据感知的SAM2在超声视频分析中的适应流程提供了实证见解，强调了数据特性在模型适应中的关键作用。

Abstract: Ultrasound (US) video segmentation remains a challenging problem due to
strong inter- and intra-dataset variability, motion artifacts, and limited
annotated data. Although foundation models such as Segment Anything Model 2
(SAM2) demonstrate strong zero-shot and prompt-guided segmentation
capabilities, their performance deteriorates substantially when transferred to
medical imaging domains. Current adaptation studies mainly emphasize
architectural modifications, while the influence of data characteristics and
training regimes has not been systematically examined. In this study, we
present a comprehensive, data-centric investigation of SAM2 adaptation for
ultrasound video segmentation. We analyze how training-set size, video
duration, and augmentation schemes affect adaptation performance under three
paradigms: task-specific fine-tuning, intermediate adaptation, and multi-task
joint training, across five SAM2 variants and multiple prompting modes. We
further design six ultrasound-specific augmentations, assessing their effect
relative to generic strategies. Experiments on three representative ultrasound
datasets reveal that data scale and temporal context play a more decisive role
than model architecture or initialization. Moreover, joint training offers an
efficient compromise between modality alignment and task specialization. This
work aims to provide empirical insights for developing efficient, data-aware
adaptation pipelines for SAM2 in ultrasound video analysis.

</details>


### [23] [TCSA-UDA: Text-Driven Cross-Semantic Alignment for Unsupervised Domain Adaptation in Medical Image Segmentation](https://arxiv.org/abs/2511.05782)
*Lalit Maurya,Honghai Liu,Reyer Zwiggelaar*

Main category: cs.CV

TL;DR: 提出TCSA-UDA框架，利用文本驱动的跨语义对齐方法解决医学图像分割中的无监督域适应问题，通过视觉语言协方差余弦损失和原型对齐模块显著减少域偏移。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中的无监督域适应面临成像模态（如CT和MRI）间显著域偏移的挑战，现有视觉语言表示学习方法在此任务中潜力未被充分探索。

Method: 提出文本驱动的跨语义对齐框架，包括视觉语言协方差余弦损失直接对齐图像编码器特征与类间文本语义关系，以及原型对齐模块使用高级语义原型跨域对齐类级像素级特征分布。

Result: 在心脏、腹部和脑肿瘤分割基准测试中，TCSA-UDA框架显著减少域偏移，持续优于最先进的无监督域适应方法。

Conclusion: TCSA-UDA为将语言驱动语义整合到域自适应医学图像分析建立了新范式。

Abstract: Unsupervised domain adaptation for medical image segmentation remains a
significant challenge due to substantial domain shifts across imaging
modalities, such as CT and MRI. While recent vision-language representation
learning methods have shown promise, their potential in UDA segmentation tasks
remains underexplored. To address this gap, we propose TCSA-UDA, a Text-driven
Cross-Semantic Alignment framework that leverages domain-invariant textual
class descriptions to guide visual representation learning. Our approach
introduces a vision-language covariance cosine loss to directly align image
encoder features with inter-class textual semantic relations, encouraging
semantically meaningful and modality-invariant feature representations.
Additionally, we incorporate a prototype alignment module that aligns
class-wise pixel-level feature distributions across domains using high-level
semantic prototypes. This mitigates residual category-level discrepancies and
enhances cross-modal consistency. Extensive experiments on challenging
cross-modality cardiac, abdominal, and brain tumor segmentation benchmarks
demonstrate that our TCSA-UDA framework significantly reduces domain shift and
consistently outperforms state-of-the-art UDA methods, establishing a new
paradigm for integrating language-driven semantics into domain-adaptive medical
image analysis.

</details>


### [24] [MACMD: Multi-dilated Contextual Attention and Channel Mixer Decoding for Medical Image Segmentation](https://arxiv.org/abs/2511.05803)
*Lalit Maurya,Honghai Liu,Reyer Zwiggelaar*

Main category: cs.CV

TL;DR: 提出MACMD解码器，通过增强注意力机制和通道混合来解决医学图像分割中局部细节丢失和全局上下文整合不足的问题，在二元和多器官分割任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中CNN难以建模长距离依赖关系，而Transformer缺乏保留局部上下文信息能力的问题，同时解决编码器-解码器架构中浅层细节丢失和局部-全局信息整合效率低下的限制。

Method: 提出MACMD解码器，采用分层扩张卷积、注意力驱动调制和跨通道混合模块，通过跳跃连接促进编码器和解码器阶段之间的通道混合，在保留局部上下文细节的同时捕获长距离依赖关系。

Result: 在二元和多器官分割任务上的评估显示，该方法在Dice分数和计算效率方面均优于现有最先进方法，实现了准确且鲁棒的分割性能。

Conclusion: MACMD解码器通过增强的注意力机制和通道混合设计，有效解决了医学图像分割中的关键挑战，在保持计算效率的同时显著提升了分割精度。

Abstract: Medical image segmentation faces challenges due to variations in anatomical
structures. While convolutional neural networks (CNNs) effectively capture
local features, they struggle with modeling long-range dependencies.
Transformers mitigate this issue with self-attention mechanisms but lack the
ability to preserve local contextual information. State-of-the-art models
primarily follow an encoder-decoder architecture, achieving notable success.
However, two key limitations remain: (1) Shallow layers, which are closer to
the input, capture fine-grained details but suffer from information loss as
data propagates through deeper layers. (2) Inefficient integration of local
details and global context between the encoder and decoder stages. To address
these challenges, we propose the MACMD-based decoder, which enhances attention
mechanisms and facilitates channel mixing between encoder and decoder stages
via skip connections. This design leverages hierarchical dilated convolutions,
attention-driven modulation, and a cross channel-mixing module to capture
long-range dependencies while preserving local contextual details, essential
for precise medical image segmentation. We evaluated our approach using
multiple transformer encoders on both binary and multi-organ segmentation
tasks. The results demonstrate that our method outperforms state-of-the-art
approaches in terms of Dice score and computational efficiency, highlighting
its effectiveness in achieving accurate and robust segmentation performance.
The code available at https://github.com/lalitmaurya47/MACMD

</details>


### [25] [LRANet++: Low-Rank Approximation Network for Accurate and Efficient Text Spotting](https://arxiv.org/abs/2511.05818)
*Yuchen Su,Zhineng Chen,Yongkun Du,Zuxuan Wu,Hongtao Xie,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: LRANet++是一个端到端文本识别框架，通过低秩逼近的参数化文本形状方法和三重分配检测头，实现了对任意形状文本的精确高效检测和识别。


<details>
  <summary>Details</summary>
Motivation: 现有端到端文本识别方法在任意形状文本检测方面存在瓶颈，主要问题是缺乏可靠高效的文本检测方法。

Method: 提出基于低秩逼近的数据驱动参数化文本形状方法，使用ℓ1-范数重构文本形状；采用三重分配检测头架构，包含深度稀疏分支、超轻量稀疏分支和密集分支；将增强的检测模块与轻量识别分支集成。

Result: 在多个挑战性基准测试上的广泛实验表明，LRANet++相比最先进方法具有优越性。

Conclusion: LRANet++能够准确高效地识别任意形状文本，解决了端到端文本识别中的关键瓶颈问题。

Abstract: End-to-end text spotting aims to jointly optimize text detection and
recognition within a unified framework. Despite significant progress, designing
an accurate and efficient end-to-end text spotter for arbitrary-shaped text
remains largely unsolved. We identify the primary bottleneck as the lack of a
reliable and efficient text detection method. To address this, we propose a
novel parameterized text shape method based on low-rank approximation for
precise detection and a triple assignment detection head to enable fast
inference. Specifically, unlike other shape representation methods that employ
data-irrelevant parameterization, our data-driven approach derives a low-rank
subspace directly from labeled text boundaries. To ensure this process is
robust against the inherent annotation noise in this data, we utilize a
specialized recovery method based on an $\ell_1$-norm formulation, which
accurately reconstructs the text shape with only a few key orthogonal vectors.
By exploiting the inherent shape correlation among different text contours, our
method achieves consistency and compactness in shape representation. Next, the
triple assignment scheme introduces a novel architecture where a deep sparse
branch (for stabilized training) is used to guide the learning of an
ultra-lightweight sparse branch (for accelerated inference), while a dense
branch provides rich parallel supervision. Building upon these advancements, we
integrate the enhanced detection module with a lightweight recognition branch
to form an end-to-end text spotting framework, termed LRANet++, capable of
accurately and efficiently spotting arbitrary-shaped text. Extensive
experiments on several challenging benchmarks demonstrate the superiority of
LRANet++ compared to state-of-the-art methods. Code will be available at:
https://github.com/ychensu/LRANet-PP.git

</details>


### [26] [Hilbert-Guided Block-Sparse Local Attention](https://arxiv.org/abs/2511.05832)
*Yunge Li,Lanyu Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于希尔伯特曲线的局部注意力方法，通过重新排列图像token来增加块稀疏性，结合块稀疏核显著提升2D局部注意力的效率。


<details>
  <summary>Details</summary>
Motivation: 全局自注意力的二次计算和内存成本限制了其在高分辨率图像中的应用。局部注意力虽然降低了复杂度，但传统模式在块稀疏视角下效率提升有限，因为窗口内的token在1D序列中不连续。

Method: 提出基于希尔伯特曲线的窗口构建方法：首先将图像token沿希尔伯特曲线重新排序，然后在重排序的1D序列上形成窗口和邻域，结合现有块稀疏核提高2D局部注意力效率。

Result: 实验表明，提出的希尔伯特窗口注意力和希尔伯特滑动注意力分别能加速窗口注意力约4倍和滑动注意力约18倍。希尔伯特窗口变换器和希尔伯特邻域变换器都实现了端到端加速且精度损失最小。

Conclusion: 结合希尔伯特引导的局部注意力和块稀疏核为提升图像2D局部注意力效率提供了一种通用且实用的方法。

Abstract: The quadratic compute and memory costs of global self-attention severely
limit its use in high-resolution images. Local attention reduces complexity by
restricting attention to neighborhoods. Block-sparse kernels can further
improve the efficiency of local attention, but conventional local attention
patterns often fail to deliver significant speedups because tokens within a
window are not contiguous in the 1D sequence. This work proposes a novel method
for constructing windows and neighborhoods based on the Hilbert curve. Image
tokens are first reordered along a Hilbert curve, and windows and neighborhoods
are then formed on the reordered 1D sequence. From a block-sparse perspective,
this strategy significantly increases block sparsity and can be combined with
existing block-sparse kernels to improve the efficiency of 2D local attention.
Experiments show that the proposed Hilbert Window Attention and Hilbert Slide
Attention can accelerate window attention and slide attention by about
$4\times$ and $18\times$, respectively. To assess practicality, the strategy is
instantiated as the Hilbert Window Transformer and the Hilbert Neighborhood
Transformer, both of which achieve end-to-end speedups with minimal accuracy
loss. Overall, combining Hilbert-guided local attention with block-sparse
kernels offers a general and practical approach to enhancing the efficiency of
2D local attention for images. The code is available at
https://github.com/Yunge6666/Hilbert-Local-Attention.

</details>


### [27] [Understanding Cross Task Generalization in Handwriting-Based Alzheimer's Screening via Vision Language Adaptation](https://arxiv.org/abs/2511.05841)
*Changqing Gong,Huafeng Qin,Mounim A. El-Yacoubi*

Main category: cs.CV

TL;DR: 本文提出了一个轻量级的跨层融合适配器框架，利用CLIP模型进行基于笔迹的阿尔茨海默病筛查，系统研究了不同任务类型对诊断性能和跨任务泛化的影响。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病早期检测至关重要，笔迹提供了非侵入性且经济有效的窗口来观察细微运动和认知衰退。现有研究主要依赖在线轨迹和手工特征，未系统研究任务类型对诊断性能和跨任务泛化的影响。

Method: 引入轻量级跨层融合适配器框架，在视觉编码器中植入多级融合适配器，逐步对齐表示以适应笔迹特定的医学线索，实现无需提示的高效零样本推理。

Result: 系统研究了跨任务泛化能力，揭示了哪些任务类型和书写模式最有效地区分阿尔茨海默病，并突出了有助于早期识别的特征性笔画模式和任务级因素。

Conclusion: 该框架为基于笔迹的认知评估提供了诊断见解和基准，展示了在笔迹疾病检测领域应用大规模视觉语言模型的潜力。

Abstract: Alzheimer's disease is a prevalent neurodegenerative disorder for which early
detection is critical. Handwriting-often disrupted in prodromal AD-provides a
non-invasive and cost-effective window into subtle motor and cognitive decline.
Existing handwriting-based AD studies, mostly relying on online trajectories
and hand-crafted features, have not systematically examined how task type
influences diagnostic performance and cross-task generalization. Meanwhile,
large-scale vision language models have demonstrated remarkable zero or
few-shot anomaly detection in natural images and strong adaptability across
medical modalities such as chest X-ray and brain MRI. However,
handwriting-based disease detection remains largely unexplored within this
paradigm. To close this gap, we introduce a lightweight Cross-Layer Fusion
Adapter framework that repurposes CLIP for handwriting-based AD screening. CLFA
implants multi-level fusion adapters within the visual encoder to progressively
align representations toward handwriting-specific medical cues, enabling
prompt-free and efficient zero-shot inference. Using this framework, we
systematically investigate cross-task generalization-training on a specific
handwriting task and evaluating on unseen ones-to reveal which task types and
writing patterns most effectively discriminate AD. Extensive analyses further
highlight characteristic stroke patterns and task-level factors that contribute
to early AD identification, offering both diagnostic insights and a benchmark
for handwriting-based cognitive assessment.

</details>


### [28] [Enhancing Diffusion Model Guidance through Calibration and Regularization](https://arxiv.org/abs/2511.05844)
*Seyed Alireza Javid,Amirhossein Bagheri,Nuria González-Prelcic*

Main category: cs.CV

TL;DR: 本文针对分类器引导扩散模型中早期去噪步骤预测过于自信导致梯度消失的问题，提出了两种互补的解决方案：基于平滑期望校准误差的可微校准目标，以及无需重新训练即可在现成分类器上运行的增强采样引导方法。


<details>
  <summary>Details</summary>
Motivation: 分类器引导扩散模型在条件图像生成中表现出色，但在早期去噪步骤中分类器预测过于自信，导致引导梯度消失，影响生成质量。

Method: 1) 提出基于平滑期望校准误差的可微校准目标，通过最小化微调改善分类器校准；2) 开发增强采样引导方法，包括批量重加权倾斜采样、自适应熵正则化采样和基于f-散度的采样策略。

Result: 在ImageNet 128x128数据集上的实验表明，使用ResNet-101分类器的散度正则化引导方法实现了2.13的FID分数，优于现有分类器引导扩散方法，且无需扩散模型重新训练。

Conclusion: 原则性校准和散度感知采样为分类器引导扩散提供了实用且有效的改进，显著提升了生成质量。

Abstract: Classifier-guided diffusion models have emerged as a powerful approach for
conditional image generation, but they suffer from overconfident predictions
during early denoising steps, causing the guidance gradient to vanish. This
paper introduces two complementary contributions to address this issue. First,
we propose a differentiable calibration objective based on the Smooth Expected
Calibration Error (Smooth ECE), which improves classifier calibration with
minimal fine-tuning and yields measurable improvements in Frechet Inception
Distance (FID). Second, we develop enhanced sampling guidance methods that
operate on off-the-shelf classifiers without requiring retraining. These
include tilted sampling with batch-level reweighting, adaptive
entropy-regularized sampling to preserve diversity, and a novel
f-divergence-based sampling strategy that strengthens class-consistent guidance
while maintaining mode coverage. Experiments on ImageNet 128x128 demonstrate
that our divergence-regularized guidance achieves an FID of 2.13 using a
ResNet-101 classifier, improving upon existing classifier-guided diffusion
methods while requiring no diffusion model retraining. The results show that
principled calibration and divergence-aware sampling provide practical and
effective improvements for classifier-guided diffusion.

</details>


### [29] [Point Cloud Segmentation of Integrated Circuits Package Substrates Surface Defects Using Causal Inference: Dataset Construction and Methodology](https://arxiv.org/abs/2511.05853)
*Bingyang Guo,Qiang Zuo,Ruiyun Yu*

Main category: cs.CV

TL;DR: 本文构建了首个高质量陶瓷封装基板表面缺陷3D分割数据集CPS3D-Seg，并提出基于因果推理的新型分割方法CINet，显著优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 陶瓷封装基板在集成电路中至关重要，但其复杂结构和微小缺陷检测面临挑战，缺乏公开数据集阻碍了相关研究发展。

Method: 构建CPS3D-Seg数据集（1300个点云样本，20个产品类别），并提出CINet方法，通过结构精炼和质量评估模块量化点云中的潜在混淆因素。

Result: CPS3D-Seg在点分辨率和精度上优于现有工业数据集，CINet在mIoU和准确率上显著超越现有算法。

Conclusion: CPS3D-Seg数据集填补了领域空白，CINet方法为3D缺陷检测提供了有效解决方案，推动了陶瓷封装基板表面缺陷检测的发展。

Abstract: The effective segmentation of 3D data is crucial for a wide range of
industrial applications, especially for detecting subtle defects in the field
of integrated circuits (IC). Ceramic package substrates (CPS), as an important
electronic material, are essential in IC packaging owing to their superior
physical and chemical properties. However, the complex structure and minor
defects of CPS, along with the absence of a publically available dataset,
significantly hinder the development of CPS surface defect detection. In this
study, we construct a high-quality point cloud dataset for 3D segmentation of
surface defects in CPS, i.e., CPS3D-Seg, which has the best point resolution
and precision compared to existing 3D industrial datasets. CPS3D-Seg consists
of 1300 point cloud samples under 20 product categories, and each sample
provides accurate point-level annotations. Meanwhile, we conduct a
comprehensive benchmark based on SOTA point cloud segmentation algorithms to
validate the effectiveness of CPS3D-Seg. Additionally, we propose a novel 3D
segmentation method based on causal inference (CINet), which quantifies
potential confounders in point clouds through Structural Refine (SR) and
Quality Assessment (QA) Modules. Extensive experiments demonstrate that CINet
significantly outperforms existing algorithms in both mIoU and accuracy.

</details>


### [30] [CGCE: Classifier-Guided Concept Erasure in Generative Models](https://arxiv.org/abs/2511.05865)
*Viet Nguyen,Vishal M. Patel*

Main category: cs.CV

TL;DR: 提出Classifier-Guided Concept Erasure (CGCE)框架，通过轻量级分类器检测和优化包含不良概念的文本嵌入，实现无需修改模型权重的鲁棒概念擦除，在保持生成质量的同时有效防止有害内容生成。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法容易受到对抗攻击，且鲁棒擦除往往会降低模型对安全无关概念的生成质量，需要在安全性和性能之间进行权衡。

Method: 使用在文本嵌入上操作的轻量级分类器，首先检测然后优化包含不良概念的提示词。该方法仅在不安全的嵌入上进行推理时修改，支持通过聚合多个分类器实现多概念擦除。

Result: CGCE在广泛的红队攻击下实现了最先进的鲁棒性，同时保持了高生成效用，在各种现代T2I和T2V模型上展示了其多功能性。

Conclusion: CGCE为安全生成AI提供了一个实用有效的解决方案，在安全性和性能之间实现了优越的平衡。

Abstract: Recent advancements in large-scale generative models have enabled the
creation of high-quality images and videos, but have also raised significant
safety concerns regarding the generation of unsafe content. To mitigate this,
concept erasure methods have been developed to remove undesirable concepts from
pre-trained models. However, existing methods remain vulnerable to adversarial
attacks that can regenerate the erased content. Moreover, achieving robust
erasure often degrades the model's generative quality for safe, unrelated
concepts, creating a difficult trade-off between safety and performance. To
address this challenge, we introduce Classifier-Guided Concept Erasure (CGCE),
an efficient plug-and-play framework that provides robust concept erasure for
diverse generative models without altering their original weights. CGCE uses a
lightweight classifier operating on text embeddings to first detect and then
refine prompts containing undesired concepts. This approach is highly scalable,
allowing for multi-concept erasure by aggregating guidance from several
classifiers. By modifying only unsafe embeddings at inference time, our method
prevents harmful content generation while preserving the model's original
quality on benign prompts. Extensive experiments show that CGCE achieves
state-of-the-art robustness against a wide range of red-teaming attacks. Our
approach also maintains high generative utility, demonstrating a superior
balance between safety and performance. We showcase the versatility of CGCE
through its successful application to various modern T2I and T2V models,
establishing it as a practical and effective solution for safe generative AI.

</details>


### [31] [Light-Field Dataset for Disparity Based Depth Estimation](https://arxiv.org/abs/2511.05866)
*Suresh Nehra,Aupendu Kar,Jayanta Mukhopadhyay,Prabir Kumar Biswas*

Main category: cs.CV

TL;DR: 本文介绍了一个公开可用的光场图像数据集，包含285张使用Lytro Illum相机拍摄的真实光场图像和13张合成光场图像，用于开发基于视差的光场深度估计算法。


<details>
  <summary>Details</summary>
Motivation: 光场相机通过微透镜阵列捕获空间和角度信息，可用于深度估计，但现有数据集不足，且焦距位置对深度估计有重要影响，需要更合适的数据集来支持算法开发。

Method: 使用Lytro Illum光场相机采集285张真实光场图像，生成13张合成光场图像，并创建真实和合成的立体光场数据集，通过机械龙门系统和Blender软件实现。

Result: 提出了包含285张真实和13张合成光场图像的公开数据集，展示了焦距位置对3D点视差的影响，并指出了现有数据集的局限性。

Conclusion: 该数据集为光场深度估计算法的开发提供了重要资源，解决了现有数据集不足的问题，并考虑了焦距位置对深度估计的关键影响。

Abstract: A Light Field (LF) camera consists of an additional two-dimensional array of
micro-lenses placed between the main lens and sensor, compared to a
conventional camera. The sensor pixels under each micro-lens receive light from
a sub-aperture of the main lens. This enables the image sensor to capture both
spatial information and the angular resolution of a scene point. This
additional angular information is used to estimate the depth of a 3-D scene.
The continuum of virtual viewpoints in light field data enables efficient depth
estimation using Epipolar Line Images (EPIs) with robust occlusion handling.
However, the trade-off between angular information and spatial information is
very critical and depends on the focal position of the camera. To design,
develop, implement, and test novel disparity-based light field depth estimation
algorithms, the availability of suitable light field image datasets is
essential. In this paper, a publicly available light field image dataset is
introduced and thoroughly described. We have also demonstrated the effect of
focal position on the disparity of a 3-D point as well as the shortcomings of
the currently available light field dataset. The proposed dataset contains 285
light field images captured using a Lytro Illum LF camera and 13 synthetic LF
images. The proposed dataset also comprises a synthetic dataset with similar
disparity characteristics to those of a real light field camera. A real and
synthetic stereo light field dataset is also created by using a mechanical
gantry system and Blender. The dataset is available at
https://github.com/aupendu/light-field-dataset.

</details>


### [32] [Hybrid second-order gradient histogram based global low-rank sparse regression for robust face recognition](https://arxiv.org/abs/2511.05893)
*Hongxia Li,Ying Ji,Yongxin Dong,Yuehua Feng*

Main category: cs.CV

TL;DR: 本文提出了一种基于混合二阶梯度直方图的全局低秩稀疏回归模型（H2H-GLRSR），用于解决人脸识别中复杂遮挡和光照变化带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 为了解决人脸识别中复杂遮挡和光照变化带来的挑战，需要开发更有效的特征描述符和回归模型来提升识别性能。

Method: 首先设计了混合二阶梯度直方图（H2H）特征描述符来更有效地表征面部图像的局部结构特征，然后将其与基于稀疏正则化核范数的矩阵回归（SR_NMR）集成，并在残差矩阵上施加全局低秩约束以更好地捕捉结构化噪声中的全局相关性。

Result: 实验结果表明，在涉及遮挡、光照变化和非约束环境的挑战性场景下，所提出的方法显著优于现有的基于回归的分类方法。

Conclusion: H2H-GLRSR模型通过结合新颖的H2H特征描述符和全局低秩约束，能够有效应对人脸识别中的复杂遮挡和光照变化问题，展现出优越的性能。

Abstract: Low-rank sparse regression models have been widely applied in the field of
face recognition. To further address the challenges caused by complex
occlusions and illumination variations, this paper proposes a Hybrid
Second-Order Gradient Histogram based Global Low-Rank Sparse Regression
(H2H-GLRSR) model. Specifically, a novel feature descriptor called the Hybrid
Second-Order Gradient Histogram (H2H) is first designed to more effectively
characterize the local structural features of facial images. Then, this
descriptor is integrated with the Sparse Regularized Nuclear Norm based Matrix
Regression (SR$\_$NMR). Moreover, a global low-rank constraint is imposed on
the residual matrix, enabling the model to better capture the global
correlations inherent in structured noise. Experimental results demonstrate
that the proposed method significantly outperforms existing regression-based
classification approaches under challenging scenarios involving occlusions,
illumination changes, and unconstrained environments.

</details>


### [33] [Open-World 3D Scene Graph Generation for Retrieval-Augmented Reasoning](https://arxiv.org/abs/2511.05894)
*Fei Yu,Quan Deng,Shengeng Tang,Yuehua Li,Lechao Cheng*

Main category: cs.CV

TL;DR: 提出了一个开放世界3D场景图生成的统一框架，结合检索增强推理，实现可泛化的交互式3D场景理解。


<details>
  <summary>Details</summary>
Motivation: 解决开放世界设置下3D场景理解的根本挑战，特别是封闭词汇监督和静态标注的限制。

Method: 将视觉语言模型与基于检索的推理相结合，包含动态场景图生成模块和检索增强推理管道两个关键组件。

Result: 在3DSSG和Replica基准测试的四个任务上评估，展示了在多样化环境中的稳健泛化和优越性能。

Conclusion: 结合开放词汇感知与基于检索的推理对于可扩展的3D场景理解是有效的。

Abstract: Understanding 3D scenes in open-world settings poses fundamental challenges
for vision and robotics, particularly due to the limitations of
closed-vocabulary supervision and static annotations. To address this, we
propose a unified framework for Open-World 3D Scene Graph Generation with
Retrieval-Augmented Reasoning, which enables generalizable and interactive 3D
scene understanding. Our method integrates Vision-Language Models (VLMs) with
retrieval-based reasoning to support multimodal exploration and language-guided
interaction. The framework comprises two key components: (1) a dynamic scene
graph generation module that detects objects and infers semantic relationships
without fixed label sets, and (2) a retrieval-augmented reasoning pipeline that
encodes scene graphs into a vector database to support text/image-conditioned
queries. We evaluate our method on 3DSSG and Replica benchmarks across four
tasks-scene question answering, visual grounding, instance retrieval, and task
planning-demonstrating robust generalization and superior performance in
diverse environments. Our results highlight the effectiveness of combining
open-vocabulary perception with retrieval-based reasoning for scalable 3D scene
understanding.

</details>


### [34] [GABFusion: Rethinking Feature Fusion for Low-Bit Quantization of Multi-Task Networks](https://arxiv.org/abs/2511.05898)
*Zhaoyang Wang,Dong Wang*

Main category: cs.CV

TL;DR: 提出GABFusion和ADA方法解决多任务架构量化感知训练中的特征差异和梯度冲突问题，显著提升量化模型性能


<details>
  <summary>Details</summary>
Motivation: 多任务架构在量化感知训练中由于任务特定特征差异和梯度冲突导致性能显著下降

Method: 提出梯度感知平衡特征融合(GABFusion)动态平衡梯度幅度和融合任务特定特征，以及注意力分布对齐(ADA)特征级蒸馏策略

Result: 在PASCAL VOC和COCO数据集上平均mAP分别提升约3.3%和1.6%，YOLOv5在4位量化下与全精度模型差距缩小至1.7%

Conclusion: 该方法具有强泛化性，模块化设计易于集成，兼容现有QAT技术，有效保持低比特约束下的性能

Abstract: Despite the effectiveness of quantization-aware training (QAT) in compressing
deep neural networks, its performance on multi-task architectures often
degrades significantly due to task-specific feature discrepancies and gradient
conflicts. To address these challenges, we propose Gradient-Aware Balanced
Feature Fusion (GABFusion), which dynamically balances gradient magnitudes and
fuses task-specific features in a quantization-friendly manner. We further
introduce Attention Distribution Alignment (ADA), a feature-level distillation
strategy tailored for quantized models. Our method demonstrates strong
generalization across network architectures and QAT algorithms, with
theoretical guarantees on gradient bias reduction. Extensive experiments
demonstrate that our strategy consistently enhances a variety of QAT methods
across different network architectures and bit-widths. On PASCAL VOC and COCO
datasets, the proposed approach achieves average mAP improvements of
approximately 3.3% and 1.6%, respectively. When applied to YOLOv5 under 4-bit
quantization, our method narrows the accuracy gap with the full-precision model
to only 1.7% on VOC, showcasing its effectiveness in preserving performance
under low-bit constraints. Notably, the proposed framework is modular, easy to
integrate, and compatible with any existing QAT technique-enhancing the
performance of quantized models without requiring modifications to the original
network architecture.

</details>


### [35] [CoMA: Complementary Masking and Hierarchical Dynamic Multi-Window Self-Attention in a Unified Pre-training Framework](https://arxiv.org/abs/2511.05929)
*Jiaxuan Li,Qing Xu,Xiangjian He,Ziyu Liu,Chang Xing,Zhen Chen,Daokun Zhang,Rong Qu,Chang Wen Chen*

Main category: cs.CV

TL;DR: CoMA采用互补掩码策略确保所有像素均匀采样，结合DyViT的动态多窗口自注意力机制，在仅需12%预训练轮次的情况下达到MAE性能，同时减少10%每轮训练时间。


<details>
  <summary>Details</summary>
Motivation: MAE及其变体采用随机掩码需要更多预训练轮次来保持适应性，且ViT在MAE中因固定空间分辨率导致参数使用效率低下。

Method: 提出互补掩码自动编码器(CoMA)确保均匀像素采样，以及分层视觉变换器DyViT采用动态多窗口自注意力机制。

Result: 在ImageNet-1K上预训练，DyViT仅用12%预训练轮次即达到MAE下游性能，每轮训练时间减少10%。

Conclusion: CoMA和DyViT的组合实现了更有效的学习，显著提升了预训练效率。

Abstract: Masked Autoencoders (MAE) achieve self-supervised learning of image
representations by randomly removing a portion of visual tokens and
reconstructing the original image as a pretext task, thereby significantly
enhancing pretraining efficiency and yielding excellent adaptability across
downstream tasks. However, MAE and other MAE-style paradigms that adopt random
masking generally require more pre-training epochs to maintain adaptability.
Meanwhile, ViT in MAE suffers from inefficient parameter use due to fixed
spatial resolution across layers. To overcome these limitations, we propose the
Complementary Masked Autoencoders (CoMA), which employ a complementary masking
strategy to ensure uniform sampling across all pixels, thereby improving
effective learning of all features and enhancing the model's adaptability.
Furthermore, we introduce DyViT, a hierarchical vision transformer that employs
a Dynamic Multi-Window Self-Attention (DM-MSA), significantly reducing the
parameters and FLOPs while improving fine-grained feature learning. Pre-trained
on ImageNet-1K with CoMA, DyViT matches the downstream performance of MAE using
only 12% of the pre-training epochs, demonstrating more effective learning. It
also attains a 10% reduction in pre-training time per epoch, further
underscoring its superior pre-training efficiency.

</details>


### [36] [AD-DAE: Unsupervised Modeling of Longitudinal Alzheimer's Disease Progression with Diffusion Auto-Encoder](https://arxiv.org/abs/2511.05934)
*Ayantika Das,Arunima Sarkar,Keerthi Ram,Mohanasankar Sivaprakasam*

Main category: cs.CV

TL;DR: 提出了一种条件化扩散自编码器框架，用于从基线图像生成疾病进展图像，无需特定受试者的纵向监督。


<details>
  <summary>Details</summary>
Motivation: 现有生成建模方法在潜在空间控制性有限，需要明确的纵向图像监督，限制了在无监督条件下生成随访图像的能力。

Method: 采用条件化扩散自编码器框架，通过显式编码机制形成紧凑的潜在空间，将进展相关因素与身份保持组件分离，在子空间中应用受控偏移来生成随访图像。

Result: 在阿尔茨海默病数据集上验证了生成图像的质量、体积进展分析和下游分类任务的有效性。

Conclusion: 该方法在阿尔茨海默病进展建模和纵向图像生成方面表现出有效性，无需受试者特定的纵向监督。

Abstract: Generative modeling frameworks have emerged as an effective approach to
capture high-dimensional image distributions from large datasets without
requiring domain-specific knowledge, a capability essential for longitudinal
disease progression modeling. Recent generative modeling approaches have
attempted to capture progression by mapping images into a latent
representational space and then controlling and guiding the representations to
generate follow-up images from a baseline image. However, existing approaches
impose constraints on distribution learning, leading to latent spaces with
limited controllability to generate follow-up images without explicit
supervision from subject-specific longitudinal images. In order to enable
controlled movements in the latent representational space and generate
progression images from a baseline image in an unsupervised manner, we
introduce a conditionable Diffusion Auto-encoder framework. The explicit
encoding mechanism of image-diffusion auto-encoders forms a compact latent
space capturing high-level semantics, providing means to disentangle
information relevant for progression. Our approach leverages this latent space
to condition and apply controlled shifts to baseline representations for
generating follow-up. Controllability is induced by restricting these shifts to
a subspace, thereby isolating progression-related factors from subject
identity-preserving components. The shifts are implicitly guided by correlating
with progression attributes, without requiring subject-specific longitudinal
supervision. We validate the generations through image quality metrics,
volumetric progression analysis, and downstream classification in Alzheimer's
disease datasets from two different sources and disease categories. This
demonstrates the effectiveness of our approach for Alzheimer's progression
modeling and longitudinal image generation.

</details>


### [37] [Interaction-Centric Knowledge Infusion and Transfer for Open-Vocabulary Scene Graph Generation](https://arxiv.org/abs/2511.05935)
*Lin Li,Chuhan Zhang,Dong Zhang,Chong Sun,Chen Li,Long Chen*

Main category: cs.CV

TL;DR: 本文提出了一种以交互为中心的开放词汇场景图生成框架ACC，通过双向交互提示和交互引导的查询选择来解决现有方法中交互建模不足的问题，在三个基准测试中达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的开放词汇场景图生成方法采用两阶段流程，但由于缺乏显式的交互建模，难以区分同一对象类别的交互和非交互实例，导致在知识注入阶段产生噪声伪监督，在知识转移阶段产生模糊查询匹配。

Method: 提出ACC框架：1）交互中心知识注入使用双向交互提示生成鲁棒的伪监督；2）交互中心知识转移采用交互引导的查询选择优先配对交互对象，并集成交互一致的知识蒸馏来增强鲁棒性。

Result: 在三个基准测试上的广泛实验结果表明，ACC实现了最先进的性能。

Conclusion: 交互中心范式在开放词汇场景图生成中具有潜力，能够有效减少对象不匹配问题，提升模型性能。

Abstract: Open-vocabulary scene graph generation (OVSGG) extends traditional SGG by
recognizing novel objects and relationships beyond predefined categories,
leveraging the knowledge from pre-trained large-scale models. Existing OVSGG
methods always adopt a two-stage pipeline: 1) \textit{Infusing knowledge} into
large-scale models via pre-training on large datasets; 2) \textit{Transferring
knowledge} from pre-trained models with fully annotated scene graphs during
supervised fine-tuning. However, due to a lack of explicit interaction
modeling, these methods struggle to distinguish between interacting and
non-interacting instances of the same object category. This limitation induces
critical issues in both stages of OVSGG: it generates noisy pseudo-supervision
from mismatched objects during knowledge infusion, and causes ambiguous query
matching during knowledge transfer. To this end, in this paper, we propose an
inter\textbf{AC}tion-\textbf{C}entric end-to-end OVSGG framework (\textbf{ACC})
in an interaction-driven paradigm to minimize these mismatches. For
\textit{interaction-centric knowledge infusion}, ACC employs a bidirectional
interaction prompt for robust pseudo-supervision generation to enhance the
model's interaction knowledge. For \textit{interaction-centric knowledge
transfer}, ACC first adopts interaction-guided query selection that prioritizes
pairing interacting objects to reduce interference from non-interacting ones.
Then, it integrates interaction-consistent knowledge distillation to bolster
robustness by pushing relational foreground away from the background while
retaining general knowledge. Extensive experimental results on three benchmarks
show that ACC achieves state-of-the-art performance, demonstrating the
potential of interaction-centric paradigms for real-world applications.

</details>


### [38] [Polymap: generating high definition map based on rasterized polygons](https://arxiv.org/abs/2511.05944)
*Shiyu Gao,Hao Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种基于实例分割的高精地图感知方法，将道路元素重新解释为栅格化多边形，通过端到端的实例分割和Potrace后处理模块生成矢量化地图元素，旨在提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于检测的高精地图构建方法虽然能够实时构建，但缺乏鲁棒的泛化能力，限制了在自动标注系统中的应用。因此需要改进泛化性能。

Method: 将道路元素重新解释为栅格化多边形，设计基于实例分割的简洁框架：首先使用基于分割的transformer端到端输出实例掩码，然后通过Potrace后处理模块最终生成矢量化地图元素。

Result: 在Nuscene数据集上的定量结果证实了该方法的有效性和泛化能力。

Conclusion: 提出的基于实例分割的方法相比检测方法具有更好的泛化性能，能够有效生成矢量化高精地图元素。

Abstract: The perception of high-definition maps is an integral component of
environmental perception in autonomous driving systems. Existing research have
often focused on online construction of high-definition maps. For instance, the
Maptr[9] series employ a detection-based method to output vectorized map
instances parallelly in an end-to-end manner. However, despite their capability
for real-time construction, detection-based methods are observed to lack robust
generalizability[19], which hampers their applicability in auto-labeling
systems. Therefore, aiming to improve the generalizability, we reinterpret road
elements as rasterized polygons and design a concise framework based on
instance segmentation. Initially, a segmentation-based transformer is employed
to deliver instance masks in an end-to-end manner; succeeding this step, a
Potrace-based[17] post-processing module is used to ultimately yield vectorized
map elements. Quantitative results attained on the Nuscene[1] dataset
substantiate the effectiveness and generaliz-ability of our method.

</details>


### [39] [On Accurate and Robust Estimation of 3D and 2D Circular Center: Method and Application to Camera-Lidar Calibration](https://arxiv.org/abs/2511.06611)
*Jiajun Jiang,Xiao Hu,Wancheng Liu,Wei Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于几何原理的LiDAR-相机外参标定框架，通过鲁棒的3D圆中心估计和2D投影中心恢复方法，解决了现有方法在3D-2D圆中心对应中的挑战。


<details>
  <summary>Details</summary>
Motivation: 圆形目标在LiDAR-相机外参标定中被广泛使用，但实现准确的3D-2D圆中心对应仍然具有挑战性。现有方法由于解耦的3D拟合和错误的2D椭圆中心估计而经常失败。

Method: 提出几何原理框架，包括：(i) 基于共形几何代数和RANSAC的鲁棒3D圆中心估计器；(ii) 通过弦长方差最小化方法恢复真实2D投影中心，通过单应性验证或准RANSAC备用方案解决双最小值模糊问题。

Result: 在合成和真实世界数据集上的评估表明，该框架显著优于最先进的方法，减少了外参估计误差，并在不同传感器和目标类型（包括自然圆形物体）上实现了鲁棒标定。

Conclusion: 提出的框架通过几何原理方法有效解决了LiDAR-相机外参标定中的3D-2D圆中心对应问题，代码将公开发布以确保可复现性。

Abstract: Circular targets are widely used in LiDAR-camera extrinsic calibration due to
their geometric consistency and ease of detection. However, achieving accurate
3D-2D circular center correspondence remains challenging. Existing methods
often fail due to decoupled 3D fitting and erroneous 2D ellipse-center
estimation. To address this, we propose a geometrically principled framework
featuring two innovations: (i) a robust 3D circle center estimator based on
conformal geometric algebra and RANSAC; and (ii) a chord-length variance
minimization method to recover the true 2D projected center, resolving its
dual-minima ambi- guity via homography validation or a quasi-RANSAC fallback.
Evaluated on synthetic and real-world datasets, our framework significantly
outperforms state-of-the-art approaches. It reduces extrinsic estimation error
and enables robust calibration across diverse sensors and target types,
including natural circular objects. Our code will be publicly released for
reproducibility.

</details>


### [40] [U(PM)$^2$:Unsupervised polygon matching with pre-trained models for challenging stereo images](https://arxiv.org/abs/2511.05949)
*Chang Li,Xingtao Peng*

Main category: cs.CV

TL;DR: 本文提出了一种名为U(PM)^2的低成本无监督多边形匹配方法，通过结合预训练模型自动学习特征和手工特征，解决了立体图像多边形匹配中的视差不连续、尺度变化、训练需求和泛化等挑战。


<details>
  <summary>Details</summary>
Motivation: 立体图像匹配是计算机视觉、摄影测量和遥感中的基础任务，但多边形匹配这一领域几乎未被探索，面临视差不连续、尺度变化、训练需求和泛化等挑战。

Method: 首先使用预训练的Segment Anything模型获取掩码，然后将掩码转换为多边形和图形结构；全局匹配器基于双向金字塔策略和预训练的LoFTR处理全局视角变化和尺度变化；局部匹配器通过局部联合几何和多特征匹配策略结合匈牙利算法解决局部视差不连续和拓扑不一致问题。

Result: 在ScanNet和SceneFlow数据集上使用新提出的度量标准进行基准测试，实现了最先进的精度、竞争性速度、满意的泛化性能，且无需任何训练需求。

Conclusion: U(PM)^2方法在低成本下实现了高效的多边形匹配，解决了该领域的关键挑战，具有优异的性能和泛化能力。

Abstract: Stereo image matching is a fundamental task in computer vision,
photogrammetry and remote sensing, but there is an almost unexplored field,
i.e., polygon matching, which faces the following challenges: disparity
discontinuity, scale variation, training requirement, and generalization. To
address the above-mentioned issues, this paper proposes a novel U(PM)$^2$:
low-cost unsupervised polygon matching with pre-trained models by uniting
automatically learned and handcrafted features, of which pipeline is as
follows: firstly, the detector leverages the pre-trained segment anything model
to obtain masks; then, the vectorizer converts the masks to polygons and
graphic structure; secondly, the global matcher addresses challenges from
global viewpoint changes and scale variation based on bidirectional-pyramid
strategy with pre-trained LoFTR; finally, the local matcher further overcomes
local disparity discontinuity and topology inconsistency of polygon matching by
local-joint geometry and multi-feature matching strategy with Hungarian
algorithm. We benchmark our U(PM)$^2$ on the ScanNet and SceneFlow datasets
using our proposed new metric, which achieved state-of-the-art accuracy at a
competitive speed and satisfactory generalization performance at low cost
without any training requirement.

</details>


### [41] [PanoNav: Mapless Zero-Shot Object Navigation with Panoramic Scene Parsing and Dynamic Memory](https://arxiv.org/abs/2511.06840)
*Qunchao Jin,Yilin Wu,Changhao Chen*

Main category: cs.CV

TL;DR: PanoNav是一个基于全景RGB输入的零样本目标导航框架，通过全景场景解析模块和记忆引导决策机制，在无地图导航中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决零样本目标导航中现有方法依赖深度传感器或预建地图的问题，以及无地图方法因缺乏历史上下文而导致的短视决策和局部死锁问题。

Method: 提出PanoNav框架，包含全景场景解析模块解锁MLLMs的空间解析能力，以及采用动态有界内存队列的记忆引导决策机制来整合探索历史。

Result: 在公开导航基准测试中，PanoNav在SR和SPL指标上显著优于代表性基线方法。

Conclusion: PanoNav证明了仅使用RGB输入的无地图零样本目标导航的可行性，通过全景解析和记忆机制有效提升了导航性能。

Abstract: Zero-shot object navigation (ZSON) in unseen environments remains a
challenging problem for household robots, requiring strong perceptual
understanding and decision-making capabilities. While recent methods leverage
metric maps and Large Language Models (LLMs), they often depend on depth
sensors or prebuilt maps, limiting the spatial reasoning ability of Multimodal
Large Language Models (MLLMs). Mapless ZSON approaches have emerged to address
this, but they typically make short-sighted decisions, leading to local
deadlocks due to a lack of historical context. We propose PanoNav, a fully
RGB-only, mapless ZSON framework that integrates a Panoramic Scene Parsing
module to unlock the spatial parsing potential of MLLMs from panoramic RGB
inputs, and a Memory-guided Decision-Making mechanism enhanced by a Dynamic
Bounded Memory Queue to incorporate exploration history and avoid local
deadlocks. Experiments on the public navigation benchmark show that PanoNav
significantly outperforms representative baselines in both SR and SPL metrics.

</details>


### [42] [CSGaze: Context-aware Social Gaze Prediction](https://arxiv.org/abs/2511.05955)
*Surbhi Madan,Shreya Ghosh,Ramanathan Subramanian,Abhinav Dhall,Tom Gedeon*

Main category: cs.CV

TL;DR: CSGaze是一个基于上下文感知的多模态方法，利用面部和场景信息来预测多人物图像中的社交注视模式，通过注意力机制提升预测性能，并在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用上下文线索结合视觉场景和面部信息来有效预测和解释对话互动中的社交注视模式，因为注视能反映人的注意力焦点、社交参与度和自信心。

Method: 提出CSGaze模型，采用多模态方法整合面部和场景信息作为互补输入，并引入以主要说话者为中心的细粒度注意力机制来更好地建模社交注视动态。

Result: 实验结果显示CSGaze在GP-Static、UCO-LAEO和AVA-LAEO数据集上表现与最先进方法相当，生成的注意力分数提供了模型决策过程的可解释性，并在开放数据集上验证了模型的泛化能力。

Conclusion: 研究强调了上下文线索在改善社交注视预测中的重要作用，提出的模型具有良好的泛化性和可解释性，能够适应多样化的场景。

Abstract: A person's gaze offers valuable insights into their focus of attention, level
of social engagement, and confidence. In this work, we investigate how
contextual cues combined with visual scene and facial information can be
effectively utilized to predict and interpret social gaze patterns during
conversational interactions. We introduce CSGaze, a context aware multimodal
approach that leverages facial, scene information as complementary inputs to
enhance social gaze pattern prediction from multi-person images. The model also
incorporates a fine-grained attention mechanism centered on the principal
speaker, which helps in better modeling social gaze dynamics. Experimental
results show that CSGaze performs competitively with state-of-the-art methods
on GP-Static, UCO-LAEO and AVA-LAEO. Our findings highlight the role of
contextual cues in improving social gaze prediction. Additionally, we provide
initial explainability through generated attention scores, offering insights
into the model's decision-making process. We also demonstrate our model's
generalizability by testing our model on open set datasets that demonstrating
its robustness across diverse scenarios.

</details>


### [43] [Aerial Image Stitching Using IMU Data from a UAV](https://arxiv.org/abs/2511.06841)
*Selim Ahmet Iz,Mustafa Unel*

Main category: cs.CV

TL;DR: 提出了一种结合IMU数据和计算机视觉技术的无人机图像拼接新方法，通过估计位移和旋转、校正透视畸变、计算单应性矩阵等步骤，提高了图像拼接的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决无人机图像拼接中特征检测和匹配存在的错误和模糊性问题，特别是在大位移、旋转和相机姿态变化等挑战性场景下。

Method: 使用IMU数据和计算机视觉技术相结合的方法，包括估计无人机在连续图像间的位移和旋转、校正透视畸变、计算单应性矩阵，然后使用标准图像拼接算法进行对齐和融合。

Result: 实验证明该方法在准确性和可靠性方面优于现有的基于特征的图像拼接算法，特别是在挑战性场景下表现更加稳健。

Conclusion: 该方法利用IMU数据提供的额外信息，校正多种畸变源，可以轻松集成到现有的无人机工作流程中，为无人机图像拼接提供了一种有效且鲁棒的解决方案。

Abstract: Unmanned Aerial Vehicles (UAVs) are widely used for aerial photography and
remote sensing applications. One of the main challenges is to stitch together
multiple images into a single high-resolution image that covers a large area.
Featurebased image stitching algorithms are commonly used but can suffer from
errors and ambiguities in feature detection and matching. To address this,
several approaches have been proposed, including using bundle adjustment
techniques or direct image alignment. In this paper, we present a novel method
that uses a combination of IMU data and computer vision techniques for
stitching images captured by a UAV. Our method involves several steps such as
estimating the displacement and rotation of the UAV between consecutive images,
correcting for perspective distortion, and computing a homography matrix. We
then use a standard image stitching algorithm to align and blend the images
together. Our proposed method leverages the additional information provided by
the IMU data, corrects for various sources of distortion, and can be easily
integrated into existing UAV workflows. Our experiments demonstrate the
effectiveness and robustness of our method, outperforming some of the existing
feature-based image stitching algorithms in terms of accuracy and reliability,
particularly in challenging scenarios such as large displacements, rotations,
and variations in camera pose.

</details>


### [44] [Real-Time LiDAR Super-Resolution via Frequency-Aware Multi-Scale Fusion](https://arxiv.org/abs/2511.07377)
*June Moh Goo,Zichao Zeng,Jan Boehm*

Main category: cs.CV

TL;DR: FLASH是一种新颖的LiDAR超分辨率框架，通过双域处理结合空间和频域分析，实现了在单次前向传播中处理不确定性的高效方法，在KITTI数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于transformer的方法如TULIP仅限于空间域处理且感受野受限的问题，以及需要多次前向传播的随机推理方法计算成本高的问题。

Method: 提出双域处理框架：1) 频率感知窗口注意力，结合局部空间注意力和通过FFT的全局频域分析；2) 自适应多尺度融合，用学习的位置特定特征聚合替换传统跳跃连接，并通过CBAM注意力增强动态特征选择。

Result: 在KITTI数据集上的广泛实验表明，FLASH在所有评估指标上都达到了最先进的性能，超越了包括需要多次前向传播的不确定性增强基线在内的所有方法。

Conclusion: FLASH通过架构设计而非计算昂贵的随机推理有效处理不确定性，在保持单次传播效率的同时实现了实时部署，为自动驾驶系统提供了实用的解决方案。

Abstract: LiDAR super-resolution addresses the challenge of achieving high-quality 3D
perception from cost-effective, low-resolution sensors. While recent
transformer-based approaches like TULIP show promise, they remain limited to
spatial-domain processing with restricted receptive fields. We introduce FLASH
(Frequency-aware LiDAR Adaptive Super-resolution with Hierarchical fusion), a
novel framework that overcomes these limitations through dual-domain
processing. FLASH integrates two key innovations: (i) Frequency-Aware Window
Attention that combines local spatial attention with global frequency-domain
analysis via FFT, capturing both fine-grained geometry and periodic scanning
patterns at log-linear complexity. (ii) Adaptive Multi-Scale Fusion that
replaces conventional skip connections with learned position-specific feature
aggregation, enhanced by CBAM attention for dynamic feature selection.
Extensive experiments on KITTI demonstrate that FLASH achieves state-of-the-art
performance across all evaluation metrics, surpassing even uncertainty-enhanced
baselines that require multiple forward passes. Notably, FLASH outperforms
TULIP with Monte Carlo Dropout while maintaining single-pass efficiency, which
enables real-time deployment. The consistent superiority across all distance
ranges validates that our dual-domain approach effectively handles uncertainty
through architectural design rather than computationally expensive stochastic
inference, making it practical for autonomous systems.

</details>


### [45] [Commonality in Few: Few-Shot Multimodal Anomaly Detection via Hypergraph-Enhanced Memory](https://arxiv.org/abs/2511.05966)
*Yuxuan Lin,Hanjing Yan,Xuan Tong,Yang Chang,Huanzhen Wang,Ziheng Zhou,Shuyong Gao,Yan Wang,Wenqiang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于结构共性的少样本多模态工业异常检测方法CIF，通过超图提取训练样本中的结构共性，使用记忆库存储结构先验，在少样本设置下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 少样本多模态工业异常检测是一个重要但研究不足的任务，由于训练样本不足无法覆盖测试样本的多样模式，需要从少量训练样本中提取结构共性来缓解这一挑战。

Method: 提出CIF方法：1) 为单语义工业图像设计语义感知超图构建模块提取共同结构；2) 使用免训练超图消息传递模块更新测试样本特征；3) 提出超边引导记忆搜索模块利用结构信息辅助记忆搜索。

Result: 在MVTec 3D-AD和Eyecandies数据集上的实验结果表明，该方法在少样本设置下优于最先进的方法。

Conclusion: CIF方法通过提取结构共性有效解决了少样本工业异常检测问题，在多个数据集上验证了其优越性能。

Abstract: Few-shot multimodal industrial anomaly detection is a critical yet
underexplored task, offering the ability to quickly adapt to complex industrial
scenarios. In few-shot settings, insufficient training samples often fail to
cover the diverse patterns present in test samples. This challenge can be
mitigated by extracting structural commonality from a small number of training
samples. In this paper, we propose a novel few-shot unsupervised multimodal
industrial anomaly detection method based on structural commonality, CIF
(Commonality In Few). To extract intra-class structural information, we employ
hypergraphs, which are capable of modeling higher-order correlations, to
capture the structural commonality within training samples, and use a memory
bank to store this intra-class structural prior. Firstly, we design a
semantic-aware hypergraph construction module tailored for single-semantic
industrial images, from which we extract common structures to guide the
construction of the memory bank. Secondly, we use a training-free hypergraph
message passing module to update the visual features of test samples, reducing
the distribution gap between test features and features in the memory bank. We
further propose a hyperedge-guided memory search module, which utilizes
structural information to assist the memory search process and reduce the false
positive rate. Experimental results on the MVTec 3D-AD dataset and the
Eyecandies dataset show that our method outperforms the state-of-the-art (SOTA)
methods in few-shot settings. Code is available at
https://github.com/Sunny5250/CIF.

</details>


### [46] [TwinOR: Photorealistic Digital Twins of Dynamic Operating Rooms for Embodied AI Research](https://arxiv.org/abs/2511.07412)
*Han Zhang,Yiqing Shen,Roger D. Soberanis-Mukul,Ankita Ghosh,Hao Ding,Lalithkumar Seenivasan,Jose L. Porras,Zhekai Mao,Chenjia Li,Wenjie Xiao,Lonny Yarmus,Angela Christine Argento,Masaru Ishii,Mathias Unberath*

Main category: cs.CV

TL;DR: TwinOR框架用于构建手术室的光真实感动态数字孪生，支持具身AI研究，通过多视角感知重建静态几何和动态运动，提供可控仿真环境。


<details>
  <summary>Details</summary>
Motivation: 手术室的安全限制阻碍了具身AI的感知和交互，需要安全可控的数字环境进行持续学习和评估。

Method: 从预扫描视频重建静态几何，通过多视角感知持续建模人和设备运动，将静态和动态组件融合到沉浸式3D环境中。

Result: 重建的几何精度达到厘米级，保持动态交互，合成数据上的模型性能接近真实室内数据集报告精度。

Conclusion: TwinOR建立了从真实到仿真的流程，为具身AI提供安全、可扩展、数据高效的发展和基准测试环境，加速从仿真到真实的部署。

Abstract: Developing embodied AI for intelligent surgical systems requires safe,
controllable environments for continual learning and evaluation. However,
safety regulations and operational constraints in operating rooms (ORs) limit
embodied agents from freely perceiving and interacting in realistic settings.
Digital twins provide high-fidelity, risk-free environments for exploration and
training. How we may create photorealistic and dynamic digital representations
of ORs that capture relevant spatial, visual, and behavioral complexity remains
unclear. We introduce TwinOR, a framework for constructing photorealistic,
dynamic digital twins of ORs for embodied AI research. The system reconstructs
static geometry from pre-scan videos and continuously models human and
equipment motion through multi-view perception of OR activities. The static and
dynamic components are fused into an immersive 3D environment that supports
controllable simulation and embodied exploration. The proposed framework
reconstructs complete OR geometry with centimeter level accuracy while
preserving dynamic interaction across surgical workflows, enabling realistic
renderings and a virtual playground for embodied AI systems. In our
experiments, TwinOR simulates stereo and monocular sensor streams for geometry
understanding and visual localization tasks. Models such as FoundationStereo
and ORB-SLAM3 on TwinOR-synthesized data achieve performance within their
reported accuracy on real indoor datasets, demonstrating that TwinOR provides
sensor-level realism sufficient for perception and localization challenges. By
establishing a real-to-sim pipeline for constructing dynamic, photorealistic
digital twins of OR environments, TwinOR enables the safe, scalable, and
data-efficient development and benchmarking of embodied AI, ultimately
accelerating the deployment of embodied AI from sim-to-real.

</details>


### [47] [Adapted Foundation Models for Breast MRI Triaging in Contrast-Enhanced and Non-Contrast Enhanced Protocols](https://arxiv.org/abs/2511.05967)
*Tri-Thien Nguyen,Lorenz A. Kapsner,Tobias Hepp,Shirin Heidarikahkesh,Hannes Schreiter,Luise Brock,Dominika Skwierawska,Dominique Hadler,Julian Hossbach,Evelyn Wenkel,Sabine Ohlmeyer,Frederik B. Laun,Andrzej Liebert,Andreas Maier,Michael Uder,Sebastian Bickelhaupt*

Main category: cs.CV

TL;DR: 本研究评估了基于DINOv2的医学切片变换器在乳腺MRI中排除BI-RADS≥4病变的性能，在97.5%灵敏度下，对比增强和非对比增强MRI分别达到19%和17%的特异性。


<details>
  <summary>Details</summary>
Motivation: 乳腺MRI解读耗时，人工智能可能有助于预筛查，提高诊断效率。

Method: 采用回顾性研究，包括1847例内部数据集和924例外部队列，测试四种简化协议：T1加权早期减影、扩散加权成像、DWI+T2加权、T1sub+T2加权，使用五折交叉验证和AUC分析评估性能。

Result: T1sub+T2w协议AUC为0.77±0.04，在97.5%灵敏度下特异性最高（19%±7%）。漏诊病变平均直径<10mm，主要为非肿块强化。外部验证AUC为0.77，88%注意力图评级良好或中等。

Conclusion: MST框架在97.5%灵敏度下能正确排除无BI-RADS≥4的病例，但在临床应用前需要进一步研究。

Abstract: Background: Magnetic resonance imaging (MRI) has high sensitivity for breast
cancer detection, but interpretation is time-consuming. Artificial intelligence
may aid in pre-screening. Purpose: To evaluate the DINOv2-based Medical Slice
Transformer (MST) for ruling out significant findings (Breast Imaging Reporting
and Data System [BI-RADS] >=4) in contrast-enhanced and non-contrast-enhanced
abbreviated breast MRI. Materials and Methods: This institutional review board
approved retrospective study included 1,847 single-breast MRI examinations (377
BI-RADS >=4) from an in-house dataset and 924 from an external validation
dataset (Duke). Four abbreviated protocols were tested: T1-weighted early
subtraction (T1sub), diffusion-weighted imaging with b=1500 s/mm2 (DWI1500),
DWI1500+T2-weighted (T2w), and T1sub+T2w. Performance was assessed at 90%, 95%,
and 97.5% sensitivity using five-fold cross-validation and area under the
receiver operating characteristic curve (AUC) analysis. AUC differences were
compared with the DeLong test. False negatives were characterized, and
attention maps of true positives were rated in the external dataset. Results: A
total of 1,448 female patients (mean age, 49 +/- 12 years) were included.
T1sub+T2w achieved an AUC of 0.77 +/- 0.04; DWI1500+T2w, 0.74 +/- 0.04
(p=0.15). At 97.5% sensitivity, T1sub+T2w had the highest specificity (19% +/-
7%), followed by DWI1500+T2w (17% +/- 11%). Missed lesions had a mean diameter
<10 mm at 95% and 97.5% thresholds for both T1sub and DWI1500, predominantly
non-mass enhancements. External validation yielded an AUC of 0.77, with 88% of
attention maps rated good or moderate. Conclusion: At 97.5% sensitivity, the
MST framework correctly triaged cases without BI-RADS >=4, achieving 19%
specificity for contrast-enhanced and 17% for non-contrast-enhanced MRI.
Further research is warranted before clinical implementation.

</details>


### [48] [DiA-gnostic VLVAE: Disentangled Alignment-Constrained Vision Language Variational AutoEncoder for Robust Radiology Reporting with Missing Modalities](https://arxiv.org/abs/2511.05968)
*Nagur Shareef Shaik,Teja Krishna Cherukuri,Adnan Masood,Dong Hye Ye*

Main category: cs.CV

TL;DR: 本文提出DiA-gnostic VLVAE框架，通过解耦对齐方法解决医学影像与临床上下文融合中的模态缺失和特征纠缠问题，实现稳健的放射学报告生成。


<details>
  <summary>Details</summary>
Motivation: 当前自动化方法依赖资源密集型大语言模型或静态知识图谱，难以处理真实临床数据中的模态缺失和特征纠缠问题，导致次优融合和临床不忠实的幻觉发现。

Method: 使用基于专家混合的视觉语言变分自编码器解耦共享和模态特定特征，通过约束优化目标强制潜在表示的正交性和对齐，然后使用紧凑的LLaMA-X解码器生成报告。

Result: 在IU X-Ray和MIMIC-CXR数据集上分别获得0.266和0.134的BLEU@4分数，显著优于现有最先进模型。

Conclusion: DiA框架通过解耦对齐方法有效解决了医学影像与临床上下文融合中的关键挑战，实现了稳健且高效的放射学报告生成。

Abstract: The integration of medical images with clinical context is essential for
generating accurate and clinically interpretable radiology reports. However,
current automated methods often rely on resource-heavy Large Language Models
(LLMs) or static knowledge graphs and struggle with two fundamental challenges
in real-world clinical data: (1) missing modalities, such as incomplete
clinical context , and (2) feature entanglement, where mixed modality-specific
and shared information leads to suboptimal fusion and clinically unfaithful
hallucinated findings. To address these challenges, we propose the DiA-gnostic
VLVAE, which achieves robust radiology reporting through Disentangled
Alignment. Our framework is designed to be resilient to missing modalities by
disentangling shared and modality-specific features using a Mixture-of-Experts
(MoE) based Vision-Language Variational Autoencoder (VLVAE). A constrained
optimization objective enforces orthogonality and alignment between these
latent representations to prevent suboptimal fusion. A compact LLaMA-X decoder
then uses these disentangled representations to generate reports efficiently.
On the IU X-Ray and MIMIC-CXR datasets, DiA has achieved competetive BLEU@4
scores of 0.266 and 0.134, respectively. Experimental results show that the
proposed method significantly outperforms state-of-the-art models.

</details>


### [49] [A Dual-Mode ViT-Conditioned Diffusion Framework with an Adaptive Conditioning Bridge for Breast Cancer Segmentation](https://arxiv.org/abs/2511.05989)
*Prateek Singh,Moumita Dholey,P. K. Vinod*

Main category: cs.CV

TL;DR: 提出了一种结合ViT编码器和增强UNet解码器的条件去噪扩散模型，用于解决乳腺超声图像中病灶分割的挑战，通过自适应条件桥、拓扑去噪一致性损失和双头架构等创新，在多个公开数据集上达到了新的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺超声图像中病灶分割面临低对比度、斑点噪声和边界模糊等挑战，传统卷积架构难以捕获足够的全局上下文信息，导致分割结果在解剖学上不一致。

Method: 使用Vision Transformer编码器提取全局特征，结合增强UNet生成解码器；引入自适应条件桥实现多尺度语义特征融合；提出拓扑去噪一致性损失来惩罚去噪过程中的结构不一致性；采用双头架构，利用去噪目标作为正则化器。

Result: 在公开乳腺超声数据集上达到新的最佳性能：BUSI数据集Dice分数0.96，BrEaST数据集0.90，BUS-UCLM数据集0.97。消融研究验证了各组件对结果的关键作用。

Conclusion: 该框架不仅实现了高精度的分割结果，而且产生了在解剖学上合理的分割，为解决乳腺超声图像分割中的挑战提供了有效解决方案。

Abstract: In breast ultrasound images, precise lesion segmentation is essential for
early diagnosis; however, low contrast, speckle noise, and unclear boundaries
make this difficult. Even though deep learning models have demonstrated
potential, standard convolutional architectures frequently fall short in
capturing enough global context, resulting in segmentations that are
anatomically inconsistent. To overcome these drawbacks, we suggest a flexible,
conditional Denoising Diffusion Model that combines an enhanced UNet-based
generative decoder with a Vision Transformer (ViT) encoder for global feature
extraction. We introduce three primary innovations: 1) an Adaptive Conditioning
Bridge (ACB) for efficient, multi-scale fusion of semantic features; 2) a novel
Topological Denoising Consistency (TDC) loss component that regularizes
training by penalizing structural inconsistencies during denoising; and 3) a
dual-head architecture that leverages the denoising objective as a powerful
regularizer, enabling a lightweight auxiliary head to perform rapid and
accurate inference on smaller datasets and a noise prediction head. Our
framework establishes a new state-of-the-art on public breast ultrasound
datasets, achieving Dice scores of 0.96 on BUSI, 0.90 on BrEaST and 0.97 on
BUS-UCLM. Comprehensive ablation studies empirically validate that the model
components are critical for achieving these results and for producing
segmentations that are not only accurate but also anatomically plausible.

</details>


### [50] [Exploring Category-level Articulated Object Pose Tracking on SE(3) Manifolds](https://arxiv.org/abs/2511.05996)
*Xianhui Meng,Yukang Huo,Li Zhang,Liu Liu,Haonan Jiang,Yan Zhong,Pingrui Zhang,Cewu Lu,Jun Liu*

Main category: cs.CV

TL;DR: PPF-Tracker是一个基于点对特征的铰接物体姿态跟踪框架，通过SE(3)李群空间的准正则化和点对特征建模，结合关节轴语义信息施加运动学约束，在合成和真实场景中表现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 铰接物体在日常生活中和机器人操作任务中普遍存在，但与刚性物体相比，由于其固有的运动学约束，铰接物体的姿态跟踪问题仍未得到充分探索。

Method: 提出PPF-Tracker框架：首先在SE(3)李群空间进行点云的准正则化，然后使用点对特征(PPF)建模铰接物体，利用SE(3)的不变性预测姿态投票参数，最后结合关节轴语义信息对所有部件施加统一的运动学约束。

Result: 在合成数据集和真实场景中系统评估，PPF-Tracker在多样化和挑战性环境中表现出强大的泛化能力，实验结果表明其在铰接物体多帧姿态跟踪中的有效性和鲁棒性。

Conclusion: PPF-Tracker为铰接物体姿态跟踪提供了有效的解决方案，相信这项工作能够推动机器人学、具身智能和增强现实领域的进展。

Abstract: Articulated objects are prevalent in daily life and robotic manipulation
tasks. However, compared to rigid objects, pose tracking for articulated
objects remains an underexplored problem due to their inherent kinematic
constraints. To address these challenges, this work proposes a novel
point-pair-based pose tracking framework, termed \textbf{PPF-Tracker}. The
proposed framework first performs quasi-canonicalization of point clouds in the
SE(3) Lie group space, and then models articulated objects using Point Pair
Features (PPF) to predict pose voting parameters by leveraging the invariance
properties of SE(3). Finally, semantic information of joint axes is
incorporated to impose unified kinematic constraints across all parts of the
articulated object. PPF-Tracker is systematically evaluated on both synthetic
datasets and real-world scenarios, demonstrating strong generalization across
diverse and challenging environments. Experimental results highlight the
effectiveness and robustness of PPF-Tracker in multi-frame pose tracking of
articulated objects. We believe this work can foster advances in robotics,
embodied intelligence, and augmented reality. Codes are available at
https://github.com/mengxh20/PPFTracker.

</details>


### [51] [MALeR: Improving Compositional Fidelity in Layout-Guided Generation](https://arxiv.org/abs/2511.06002)
*Shivank Saxena,Dhruv Srivastava,Makarand Tapaswi*

Main category: cs.CV

TL;DR: MALeR是一种解决文本到图像生成中多主体组合场景挑战的方法，通过布局引导和属性绑定机制，防止主体出现在布局外、生成分布外图像以及属性跨主体泄漏的问题。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型在生成包含多个主体和属性的组合场景时面临挑战，包括主体出现在布局外、生成分布外图像包含不自然伪影、以及属性跨主体泄漏导致视觉输出错误。

Method: 提出MALeR方法，给定文本提示和对应布局，防止主体出现在给定布局外同时保持分布内生成；提出掩码属性感知绑定机制防止属性泄漏，在复杂组合场景中准确渲染具有多个属性的主体。

Result: 定性和定量评估表明，该方法在组合准确性、生成一致性和属性绑定方面优于先前工作，特别擅长生成具有多个主体且每个主体具有多个属性的场景图像。

Conclusion: MALeR有效解决了文本到图像生成中组合场景的关键挑战，在复杂多主体多属性场景中表现出优越性能。

Abstract: Recent advances in text-to-image models have enabled a new era of creative
and controllable image generation. However, generating compositional scenes
with multiple subjects and attributes remains a significant challenge. To
enhance user control over subject placement, several layout-guided methods have
been proposed. However, these methods face numerous challenges, particularly in
compositional scenes. Unintended subjects often appear outside the layouts,
generated images can be out-of-distribution and contain unnatural artifacts, or
attributes bleed across subjects, leading to incorrect visual outputs. In this
work, we propose MALeR, a method that addresses each of these challenges. Given
a text prompt and corresponding layouts, our method prevents subjects from
appearing outside the given layouts while being in-distribution. Additionally,
we propose a masked, attribute-aware binding mechanism that prevents attribute
leakage, enabling accurate rendering of subjects with multiple attributes, even
in complex compositional scenes. Qualitative and quantitative evaluation
demonstrates that our method achieves superior performance in compositional
accuracy, generation consistency, and attribute binding compared to previous
work. MALeR is particularly adept at generating images of scenes with multiple
subjects and multiple attributes per subject.

</details>


### [52] [How Reasoning Influences Intersectional Biases in Vision Language Models](https://arxiv.org/abs/2511.06005)
*Adit Desai,Sudipta Roy,Mohna Chakraborty*

Main category: cs.CV

TL;DR: 本文分析了五个开源视觉语言模型在职业预测任务中的社会偏见，发现其推理模式存在系统性偏见，导致交叉性差异，强调在部署前需要将VLM推理与人类价值观对齐。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在训练数据中编码了社会偏见，这些偏见会在输出中显现。与人类通过上下文和社会线索理解图像不同，VLM通过统计关联处理图像，导致推理与人类推理存在差异。

Method: 在FairFace数据集上，对五个开源VLM进行职业预测任务的系统性分析，涵盖32种职业和三种不同的提示风格，获取预测结果和推理过程。

Result: 研究发现，有偏见的推理模式系统地导致了交叉性差异，表明VLM的推理过程存在系统性偏见。

Conclusion: 在视觉语言模型部署到下游任务之前，需要将其推理过程与人类价值观对齐，以减少社会偏见的影响。

Abstract: Vision Language Models (VLMs) are increasingly deployed across downstream
tasks, yet their training data often encode social biases that surface in
outputs. Unlike humans, who interpret images through contextual and social
cues, VLMs process them through statistical associations, often leading to
reasoning that diverges from human reasoning. By analyzing how a VLM reasons,
we can understand how inherent biases are perpetuated and can adversely affect
downstream performance. To examine this gap, we systematically analyze social
biases in five open-source VLMs for an occupation prediction task, on the
FairFace dataset. Across 32 occupations and three different prompting styles,
we elicit both predictions and reasoning. Our findings reveal that the biased
reasoning patterns systematically underlie intersectional disparities,
highlighting the need to align VLM reasoning with human values prior to its
downstream deployment.

</details>


### [53] [Distributed Deep Learning for Medical Image Denoising with Data Obfuscation](https://arxiv.org/abs/2511.06006)
*Sulaimon Oyeniyi Adebayo,Ayaz H. Khan*

Main category: cs.CV

TL;DR: 本研究探索了使用分布式深度学习对胸部X光图像进行去噪，采用U-Net和U-Net++架构，在单GPU、标准多GPU和优化多GPU配置下进行训练。U-Net++在去噪性能上表现更优，而优化训练管道将训练时间减少了60%以上。


<details>
  <summary>Details</summary>
Motivation: 医学图像去噪对于提高图像质量同时最小化敏感信息暴露至关重要，特别是在处理大规模临床数据集时。研究旨在探索分布式深度学习在医学图像去噪中的应用。

Method: 使用NIH Chest X-ray14数据集，采用加性高斯噪声作为轻量级模糊技术。实现并评估了U-Net和U-Net++架构，在单GPU、标准多GPU（DataParallel）和优化多GPU训练配置（使用PyTorch的DistributedDataParallel和自动混合精度）下进行训练。

Result: U-Net++在去噪性能上持续表现更优，在PSNR和SSIM指标上获得竞争性分数，但在低和中度噪声水平下LPIPS表现不如U-Net。优化训练管道将训练时间减少了60%以上，比标准DataParallel快40%以上，两个模型仅出现轻微精度下降。

Conclusion: 研究证明了结合架构设计、轻量级模糊和先进分布式训练策略在医学图像处理中的实际可行性，突显了软件级优化在医学成像分布式学习中的有效性。

Abstract: Medical image denoising is essential for improving image quality while
minimizing the exposure of sensitive information, particularly when working
with large-scale clinical datasets. This study explores distributed deep
learning for denoising chest X-ray images from the NIH Chest X-ray14 dataset,
using additive Gaussian noise as a lightweight obfuscation technique. We
implement and evaluate U-Net and U-Net++ architectures under single-GPU,
standard multi-GPU (DataParallel), and optimized multi-GPU training
configurations using PyTorch's DistributedDataParallel (DDP) and Automatic
Mixed Precision (AMP). Our results show that U-Net++ consistently delivers
superior denoising performance, achieving competitive Peak Signal to Noise
Ratio (PSNR) and Structured Similarity Index Method (SSIM) scores, though with
less performance in Learned Perceptual Image Patch Similarity (LPIPS) compared
to U-Net under low and moderate noise levels. This indicates U-Net++'s enhanced
structural fidelity and low perceptual similarity. Meanwhile, our optimized
training pipeline reduces training time by over 60% for both models compared to
single-GPU training, and outperforms standard DataParallel by over 40%, with
only a minor accuracy drop for both models (trading some accuracy for speed).
These findings highlight the effectiveness of software-level optimization in
distributed learning for medical imaging. This work demonstrates the practical
viability of combining architectural design, lightweight obfuscation, and
advanced distributed training strategies to accelerate and enhance medical
image processing pipelines in real-world clinical and research environments.
The full implementation is publicly available at:
https://github.com/Suadey/medical-image-denoising-ddp.

</details>


### [54] [One-Shot Knowledge Transfer for Scalable Person Re-Identification](https://arxiv.org/abs/2511.06016)
*Longhua Li,Lei Qi,Xin Geng*

Main category: cs.CV

TL;DR: 本文提出了一种名为OSKT（一次性知识转移）的新方法，通过权重链将教师模型的知识整合到中间载体中，从而在需要不同尺寸模型时无需重复计算。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中的人员重识别需要不同尺寸的模型来适应不同资源条件，传统压缩方法需要为每个学生模型单独计算，导致重复和繁琐的计算。

Method: 提出OSKT方法，将教师模型的知识整合到权重链中，当需要特定资源约束的模型时，权重链可以扩展到目标模型尺寸而无需额外计算。

Result: OSKT显著优于最先进的压缩方法，并具有一次性知识转移的优势，无需为每个目标模型频繁计算。

Conclusion: OSKT方法有效解决了边缘计算中人员重识别模型压缩的重复计算问题，提供了一种高效的一次性知识转移解决方案。

Abstract: Edge computing in person re-identification (ReID) is crucial for reducing the
load on central cloud servers and ensuring user privacy. Conventional
compression methods for obtaining compact models require computations for each
individual student model. When multiple models of varying sizes are needed to
accommodate different resource conditions, this leads to repetitive and
cumbersome computations. To address this challenge, we propose a novel
knowledge inheritance approach named OSKT (One-Shot Knowledge Transfer), which
consolidates the knowledge of the teacher model into an intermediate carrier
called a weight chain. When a downstream scenario demands a model that meets
specific resource constraints, this weight chain can be expanded to the target
model size without additional computation. OSKT significantly outperforms
state-of-the-art compression methods, with the added advantage of one-time
knowledge transfer that eliminates the need for frequent computations for each
target model.

</details>


### [55] [S2ML: Spatio-Spectral Mutual Learning for Depth Completion](https://arxiv.org/abs/2511.06033)
*Zihui Zhao,Yifei Zhang,Zheng Wang,Yang Li,Kui Jiang,Zihan Geng,Chia-Wen Lin*

Main category: cs.CV

TL;DR: 提出了S2ML框架，通过空间域和频域的相互学习来解决深度图像补全问题，在NYU-Depth V2和SUN RGB-D数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: RGB-D相机捕获的原始深度图像常因弱反射、边界阴影和伪影导致深度值不完整，现有方法在图像域进行深度补全但忽略了物理特性，发现无效深度区域会改变频率分布模式。

Method: 提出空间-频谱相互学习框架(S2ML)，考虑幅度和相位谱的不同特性设计专用频谱融合模块，在统一嵌入空间中计算空间域和频域特征的局部和全局相关性，通过渐进式相互表示和细化充分探索互补物理特性和先验知识。

Result: 在NYU-Depth V2和SUN RGB-D数据集上分别比最先进方法CFormer高出0.828 dB和0.834 dB。

Conclusion: S2ML框架通过协调空间域和频域的优势，能够更准确地进行深度补全，实验证明了该方法的有效性。

Abstract: The raw depth images captured by RGB-D cameras using Time-of-Flight (TOF) or
structured light often suffer from incomplete depth values due to weak
reflections, boundary shadows, and artifacts, which limit their applications in
downstream vision tasks. Existing methods address this problem through depth
completion in the image domain, but they overlook the physical characteristics
of raw depth images. It has been observed that the presence of invalid depth
areas alters the frequency distribution pattern. In this work, we propose a
Spatio-Spectral Mutual Learning framework (S2ML) to harmonize the advantages of
both spatial and frequency domains for depth completion. Specifically, we
consider the distinct properties of amplitude and phase spectra and devise a
dedicated spectral fusion module. Meanwhile, the local and global correlations
between spatial-domain and frequency-domain features are calculated in a
unified embedding space. The gradual mutual representation and refinement
encourage the network to fully explore complementary physical characteristics
and priors for more accurate depth completion. Extensive experiments
demonstrate the effectiveness of our proposed S2ML method, outperforming the
state-of-the-art method CFormer by 0.828 dB and 0.834 dB on the NYU-Depth V2
and SUN RGB-D datasets, respectively.

</details>


### [56] [StreamSTGS: Streaming Spatial and Temporal Gaussian Grids for Real-Time Free-Viewpoint Video](https://arxiv.org/abs/2511.06046)
*Zhihui Ke,Yuyang Liu,Xiaobo Zhou,Tie Qiu*

Main category: cs.CV

TL;DR: StreamSTGS是一种用于实时流式自由视点视频的新表示方法，通过将规范3D高斯属性编码为2D图像、时间特征编码为视频，显著压缩了存储需求，支持自适应码率控制，在保持竞争力的同时将平均帧大小降至170KB。


<details>
  <summary>Details</summary>
Motivation: 解决基于3D高斯泼溅的自由视点视频方法存储需求过高（每帧可达10MB）的问题，使其能够实现实时流式传输。

Method: 使用规范3D高斯、时间特征和变形场表示动态场景；将高斯属性编码为2D图像，时间特征编码为视频；采用滑动窗口方案聚合相邻时间特征学习局部运动，引入transformer引导的辅助训练模块学习全局运动。

Result: 在多样化FVV基准测试中，相比最先进方法在所有指标上都具有竞争力表现，平均PSNR提高1dB，同时平均帧大小降至170KB。

Conclusion: StreamSTGS成功解决了3DGS-based FVV方法的存储瓶颈，实现了实时流式传输，支持自适应码率控制，且无需额外训练。

Abstract: Streaming free-viewpoint video~(FVV) in real-time still faces significant
challenges, particularly in training, rendering, and transmission efficiency.
Harnessing superior performance of 3D Gaussian Splatting~(3DGS), recent
3DGS-based FVV methods have achieved notable breakthroughs in both training and
rendering. However, the storage requirements of these methods can reach up to
$10$MB per frame, making stream FVV in real-time impossible. To address this
problem, we propose a novel FVV representation, dubbed StreamSTGS, designed for
real-time streaming. StreamSTGS represents a dynamic scene using canonical 3D
Gaussians, temporal features, and a deformation field. For high compression
efficiency, we encode canonical Gaussian attributes as 2D images and temporal
features as a video. This design not only enables real-time streaming, but also
inherently supports adaptive bitrate control based on network condition without
any extra training. Moreover, we propose a sliding window scheme to aggregate
adjacent temporal features to learn local motions, and then introduce a
transformer-guided auxiliary training module to learn global motions. On
diverse FVV benchmarks, StreamSTGS demonstrates competitive performance on all
metrics compared to state-of-the-art methods. Notably, StreamSTGS increases the
PSNR by an average of $1$dB while reducing the average frame size to just
$170$KB. The code is publicly available on https://github.com/kkkzh/StreamSTGS.

</details>


### [57] [Neodragon: Mobile Video Generation using Diffusion Transformer](https://arxiv.org/abs/2511.06055)
*Animesh Karnewar,Denis Korzhenkov,Ioannis Lelekas,Adil Karjauv,Noor Fathima,Hanwen Xiong,Vancheeswaran Vaidyanathan,Will Zeng,Rafael Esteves,Tushar Singhal,Fatih Porikli,Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

TL;DR: Neodragon是一个专为移动硬件优化的文本到视频生成系统，能在高通Hexagon NPU上6.7秒内生成2秒640x1024分辨率视频，实现高效、高保真的移动端视频合成。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的文本到视频生成模型主要针对离线场景，缺乏针对移动硬件优化的高效解决方案。Neodragon旨在实现低成本、私密、设备端的视频生成，使创作者无需依赖云服务即可生成高质量视频。

Method: 采用四项关键技术：(1)文本编码器蒸馏，用0.2B DT5替换4.762B T5xxl；(2)非对称解码器蒸馏，替换编解码器潜在VAE解码器；(3)基于重要性的MMDiT块剪枝和两阶段蒸馏恢复性能；(4)使用DMD进行步长蒸馏，减少去噪器NFE需求。

Result: Neodragon系统参数量4.945B，峰值内存使用3.5GB，端到端延迟6.7秒，VBench总分81.61，成为高度参数、内存和运行时效率的移动友好模型。

Conclusion: Neodragon通过移动硬件优化实现了高效、高保真的设备端文本到视频合成，为AI视频内容创作提供了低成本、私密的解决方案，推动了视频创作的民主化。

Abstract: We introduce Neodragon, a text-to-video system capable of generating 2s (49
frames @24 fps) videos at the 640x1024 resolution directly on a Qualcomm
Hexagon NPU in a record 6.7s (7 FPS). Differing from existing transformer-based
offline text-to-video generation models, Neodragon is the first to have been
specifically optimised for mobile hardware to achieve efficient and
high-fidelity video synthesis. We achieve this through four key technical
contributions: (1) Replacing the original large 4.762B T5xxl Text-Encoder with
a much smaller 0.2B DT5 (DistilT5) with minimal quality loss, enabled through a
novel Text-Encoder Distillation procedure. (2) Proposing an Asymmetric Decoder
Distillation approach allowing us to replace the native codec-latent-VAE
decoder with a more efficient one, without disturbing the generative
latent-space of the generation pipeline. (3) Pruning of MMDiT blocks within the
denoiser backbone based on their relative importance, with recovery of original
performance through a two-stage distillation process. (4) Reducing the NFE
(Neural Functional Evaluation) requirement of the denoiser by performing step
distillation using DMD adapted for pyramidal flow-matching, thereby
substantially accelerating video generation. When paired with an optimised
SSD1B first-frame image generator and QuickSRNet for 2x super-resolution, our
end-to-end Neodragon system becomes a highly parameter (4.945B full model),
memory (3.5GB peak RAM usage), and runtime (6.7s E2E latency) efficient
mobile-friendly model, while achieving a VBench total score of 81.61. By
enabling low-cost, private, and on-device text-to-video synthesis, Neodragon
democratizes AI-based video content creation, empowering creators to generate
high-quality videos without reliance on cloud services. Code and model will be
made publicly available at our website:
https://qualcomm-ai-research.github.io/neodragon

</details>


### [58] [LoopExpose: An Unsupervised Framework for Arbitrary-Length Exposure Correction](https://arxiv.org/abs/2511.06066)
*Ao Li,Chen Chen,Zhenyu Wang,Tao Huang,Fangfang Wu,Weisheng Dong*

Main category: cs.CV

TL;DR: 提出了一种基于伪标签的无监督曝光校正方法LoopExpose，采用嵌套循环优化策略，通过多曝光融合生成伪标签来训练校正模型，并引入反馈机制和亮度排序损失来提升性能。


<details>
  <summary>Details</summary>
Motivation: 监督学习方法在曝光校正领域取得了显著进展，但严重依赖大规模标注数据集，这在现实场景中难以获取。为了解决这一限制，需要开发无监督方法。

Method: 提出嵌套循环优化策略，上层训练校正模型使用下层多曝光融合生成的伪标签。引入反馈机制，将校正后的图像反馈到融合过程中以优化伪标签，形成自增强学习循环。考虑到亮度校准在曝光校正中的主导作用，引入了亮度排序损失作为自监督约束。

Result: 在不同基准数据集上的大量实验表明，LoopExpose在曝光校正和融合性能方面优于现有的最先进无监督方法。

Conclusion: LoopExpose通过伪标签和嵌套循环优化策略，有效解决了无监督曝光校正问题，在多个数据集上表现出优越性能，为实际应用提供了可行的解决方案。

Abstract: Exposure correction is essential for enhancing image quality under
challenging lighting conditions. While supervised learning has achieved
significant progress in this area, it relies heavily on large-scale labeled
datasets, which are difficult to obtain in practical scenarios. To address this
limitation, we propose a pseudo label-based unsupervised method called
LoopExpose for arbitrary-length exposure correction. A nested loop optimization
strategy is proposed to address the exposure correction problem, where the
correction model and pseudo-supervised information are jointly optimized in a
two-level framework. Specifically, the upper-level trains a correction model
using pseudo-labels generated through multi-exposure fusion at the lower level.
A feedback mechanism is introduced where corrected images are fed back into the
fusion process to refine the pseudo-labels, creating a self-reinforcing
learning loop. Considering the dominant role of luminance calibration in
exposure correction, a Luminance Ranking Loss is introduced to leverage the
relative luminance ordering across the input sequence as a self-supervised
constraint. Extensive experiments on different benchmark datasets demonstrate
that LoopExpose achieves superior exposure correction and fusion performance,
outperforming existing state-of-the-art unsupervised methods. Code is available
at https://github.com/FALALAS/LoopExpose.

</details>


### [59] [An Artificial Intelligence-based Assistant for the Visually Impaired](https://arxiv.org/abs/2511.06080)
*Luis Marquez-Carpintero,Francisco Gomez-Donoso,Zuria Bauer,Bessie Dominguez-Dager,Alvaro Belmonte-Baeza,Mónica Pina-Navarro,Francisco Morillas-Espejo,Felix Escalona,Miguel Cazorla*

Main category: cs.CV

TL;DR: AIDEN是一个基于人工智能的辅助应用，旨在通过先进的机器学习算法帮助视障人士识别物体、阅读文本和回答环境问题，从而提高他们的生活质量。


<details>
  <summary>Details</summary>
Motivation: 视障人士在识别物体、阅读文本和导航陌生环境方面面临挑战，现有解决方案如盲文、有声读物和屏幕阅读器在某些情况下效果有限，需要更智能的辅助工具来提升他们的独立性和生活质量。

Method: 应用采用最先进的机器学习算法，包括YOLO架构和大型语言视觉助手，识别和描述物体、阅读文本、回答环境问题，并提供多种交互方式让用户以适当方式获取文本和视觉信息。

Result: AIDEN系统增强了用户的自主性和信息获取能力，用户反馈表明该应用在日常使用中感知效果良好。

Conclusion: AIDEN人工智能助手通过整合先进的计算机视觉和自然语言处理技术，有效帮助视障人士克服日常生活中的障碍，提升他们的独立性和生活质量。

Abstract: This paper describes an artificial intelligence-based assistant application,
AIDEN, developed during 2023 and 2024, aimed at improving the quality of life
for visually impaired individuals. Visually impaired individuals face
challenges in identifying objects, reading text, and navigating unfamiliar
environments, which can limit their independence and reduce their quality of
life. Although solutions such as Braille, audio books, and screen readers
exist, they may not be effective in all situations. This application leverages
state-of-the-art machine learning algorithms to identify and describe objects,
read text, and answer questions about the environment. Specifically, it uses
You Only Look Once architectures and a Large Language and Vision Assistant. The
system incorporates several methods to facilitate the user's interaction with
the system and access to textual and visual information in an appropriate
manner. AIDEN aims to enhance user autonomy and access to information,
contributing to an improved perception of daily usability, as supported by user
feedback.

</details>


### [60] [Hybrid CNN-ViT Framework for Motion-Blurred Scene Text Restoration](https://arxiv.org/abs/2511.06087)
*Umar Rashid,Muhammad Arslan Arshad,Ghulam Ahmad,Muhammad Zeeshan Anjum,Rizwan Khan,Muhammad Akmal*

Main category: cs.CV

TL;DR: 提出了一种结合CNN和ViT的混合深度学习框架，用于恢复运动模糊场景文本图像的可读性，在PSNR和SSIM指标上表现优异，同时保持轻量级和快速推理。


<details>
  <summary>Details</summary>
Motivation: 运动模糊严重影响场景文本图像的可读性，传统去模糊方法在处理空间变化模糊和长距离依赖建模方面存在不足，限制了在自动驾驶、文档数字化等应用中的可靠性。

Method: 采用CNN编码器-解码器结构保留局部结构细节，结合Transformer模块通过自注意力机制增强全局上下文理解，使用合成模糊数据集训练，采用包含MAE、MSE、感知相似性和SSIM的复合损失函数进行优化。

Result: 在定量评估中达到32.20 dB的PSNR和0.934的SSIM，模型仅含283万个参数，平均推理时间为61毫秒，展现了优异的性能和计算效率。

Conclusion: CNN-ViT混合设计在运动模糊场景文本恢复中表现出色，兼具有效性和实用性，适合实际应用部署。

Abstract: Motion blur in scene text images severely impairs readability and hinders the
reliability of computer vision tasks, including autonomous driving, document
digitization, and visual information retrieval. Conventional deblurring
approaches are often inadequate in handling spatially varying blur and
typically fall short in modeling the long-range dependencies necessary for
restoring textual clarity. To overcome these limitations, we introduce a hybrid
deep learning framework that combines convolutional neural networks (CNNs) with
vision transformers (ViTs), thereby leveraging both local feature extraction
and global contextual reasoning. The architecture employs a CNN-based
encoder-decoder to preserve structural details, while a transformer module
enhances global awareness through self-attention. Training is conducted on a
curated dataset derived from TextOCR, where sharp scene-text samples are paired
with synthetically blurred versions generated using realistic motion-blur
kernels of multiple sizes and orientations. Model optimization is guided by a
composite loss that incorporates mean absolute error (MAE), squared error
(MSE), perceptual similarity, and structural similarity (SSIM). Quantitative
eval- uations show that the proposed method attains 32.20 dB in PSNR and 0.934
in SSIM, while remaining lightweight with 2.83 million parameters and an
average inference time of 61 ms. These results highlight the effectiveness and
computational efficiency of the CNN-ViT hybrid design, establishing its
practicality for real-world motion-blurred scene-text restoration.

</details>


### [61] [Latent Refinement via Flow Matching for Training-free Linear Inverse Problem Solving](https://arxiv.org/abs/2511.06138)
*Hossein Askari,Yadan Luo,Hongfu Sun,Fred Roosta*

Main category: cs.CV

TL;DR: LFlow是一个基于预训练潜在流先验的训练自由框架，用于解决线性逆问题。它通过流匹配在潜在空间中进行ODE采样，并引入理论推导的后验协方差来实现有效的流引导。


<details>
  <summary>Details</summary>
Motivation: 当前基于流的逆问题求解器存在两个主要限制：(i)直接在像素空间操作，计算资源需求大且难以扩展到高分辨率图像；(ii)使用先验无关的后验协方差进行引导，可能削弱与生成轨迹的对齐并降低后验覆盖范围。

Method: LFlow利用流匹配的效率在潜在空间中进行ODE采样，沿着最优路径。这种潜在空间表述允许引入从最优向量场推导的理论基础后验协方差，实现有效的流引导。

Result: 实验结果表明，该方法在大多数任务的重建质量上优于最先进的潜在扩散求解器。

Conclusion: LFlow通过潜在空间中的流匹配和理论推导的后验协方差，提供了一种高效且高质量的线性逆问题解决方案。

Abstract: Recent advances in inverse problem solving have increasingly adopted flow
priors over diffusion models due to their ability to construct straight
probability paths from noise to data, thereby enhancing efficiency in both
training and inference. However, current flow-based inverse solvers face two
primary limitations: (i) they operate directly in pixel space, which demands
heavy computational resources for training and restricts scalability to
high-resolution images, and (ii) they employ guidance strategies with
prior-agnostic posterior covariances, which can weaken alignment with the
generative trajectory and degrade posterior coverage. In this paper, we propose
LFlow (Latent Refinement via Flows), a training-free framework for solving
linear inverse problems via pretrained latent flow priors. LFlow leverages the
efficiency of flow matching to perform ODE sampling in latent space along an
optimal path. This latent formulation further allows us to introduce a
theoretically grounded posterior covariance, derived from the optimal vector
field, enabling effective flow guidance. Experimental results demonstrate that
our proposed method outperforms state-of-the-art latent diffusion solvers in
reconstruction quality across most tasks. The code will be publicly available
at https://github.com/hosseinaskari-cs/LFlow .

</details>


### [62] [Scene-Aware Urban Design: A Human-AI Recommendation Framework Using Co-Occurrence Embeddings and Vision-Language Models](https://arxiv.org/abs/2511.06201)
*Rodrigo Gallardo,Oz Fishman,Alexander Htet Kyaw*

Main category: cs.CV

TL;DR: 本文提出了一种人机协同的计算机视觉框架，利用生成式AI为公共空间提供微观尺度设计干预建议，支持更持续、本地的参与。


<details>
  <summary>Details</summary>
Motivation: 旨在超越自上而下的总体规划方法，通过基于日常模式和生活经验的选择，让人们在选择和细化过程中保持控制权。

Method: 使用Grounding DINO和ADE20K数据集子集检测城市物体并构建共现嵌入，揭示常见空间配置。系统提供五个统计上可能的补充对象，然后通过视觉语言模型分析场景图像和选定对，建议完成更复杂城市策略的第三个对象。

Result: 开发了一个工作流程，能够基于城市环境分析生成微观设计干预建议，支持本地参与式规划。

Conclusion: 该人机协同框架通过结合生成式AI和空间配置分析，为公共空间设计提供了基于数据驱动和用户控制的新方法。

Abstract: This paper introduces a human-in-the-loop computer vision framework that uses
generative AI to propose micro-scale design interventions in public space and
support more continuous, local participation. Using Grounding DINO and a
curated subset of the ADE20K dataset as a proxy for the urban built
environment, the system detects urban objects and builds co-occurrence
embeddings that reveal common spatial configurations. From this analysis, the
user receives five statistically likely complements to a chosen anchor object.
A vision language model then reasons over the scene image and the selected pair
to suggest a third object that completes a more complex urban tactic. The
workflow keeps people in control of selection and refinement and aims to move
beyond top-down master planning by grounding choices in everyday patterns and
lived experience.

</details>


### [63] [MoRA: Missing Modality Low-Rank Adaptation for Visual Recognition](https://arxiv.org/abs/2511.06225)
*Shu Zhao,Nilesh Ahuja,Tan Yu,Tianyi Shen,Vijaykrishnan Narayanan*

Main category: cs.CV

TL;DR: MoRA是一种参数高效微调方法，专门处理多模态视觉识别中模态缺失的问题，通过建模跨模态交互和保持模态特定适应性，在模态缺失场景下显著提升性能并减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现实场景中多模态输入可能因隐私、收集困难或资源限制而缺失，现有方法无法有效捕捉跨模态关系且存在计算开销问题。

Method: MoRA引入文本和视觉编码器之间的模态共享参数，实现双向知识转移，同时结合模态特定参数保持跨模态交互和模态内灵活性。

Result: 在标准基准测试中，MoRA在模态缺失场景下平均性能提升5.24%，推理时间仅为SOTA方法的25.90%，可训练参数仅为全微调的0.11%。

Conclusion: MoRA有效解决了多模态视觉识别中的模态缺失问题，在保持高性能的同时显著降低了计算和参数需求。

Abstract: Pre-trained vision language models have shown remarkable performance on
visual recognition tasks, but they typically assume the availability of
complete multimodal inputs during both training and inference. In real-world
scenarios, however, modalities may be missing due to privacy constraints,
collection difficulties, or resource limitations. While previous approaches
have addressed this challenge using prompt learning techniques, they fail to
capture the cross-modal relationships necessary for effective multimodal visual
recognition and suffer from inevitable computational overhead. In this paper,
we introduce MoRA, a parameter-efficient fine-tuning method that explicitly
models cross-modal interactions while maintaining modality-specific
adaptations. MoRA introduces modality-common parameters between text and vision
encoders, enabling bidirectional knowledge transfer. Additionally, combined
with the modality-specific parameters, MoRA allows the backbone model to
maintain inter-modality interaction and enable intra-modality flexibility.
Extensive experiments on standard benchmarks demonstrate that MoRA achieves an
average performance improvement in missing-modality scenarios by 5.24% and uses
only 25.90% of the inference time compared to the SOTA method while requiring
only 0.11% of trainable parameters compared to full fine-tuning.

</details>


### [64] [Temporal-Guided Visual Foundation Models for Event-Based Vision](https://arxiv.org/abs/2511.06238)
*Ruihao Xia,Junhong Cai,Luziwei Leng,Liuyi Wang,Chengju Liu,Ran Cheng,Yang Tang,Pan Zhou*

Main category: cs.CV

TL;DR: 提出了TGVFM框架，将视觉基础模型与时间上下文融合块集成，用于事件相机视觉任务，在语义分割、深度估计和目标检测上实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 事件相机在挑战性环境中具有独特优势，但处理异步事件流仍具挑战性。现有方法依赖专门架构或资源密集型训练，而利用图像数据预训练的视觉基础模型在事件视觉中的潜力尚未充分探索

Method: 提出TGVFM框架，包含时间上下文融合块，具有三个关键组件：长距离时间注意力建模全局时间依赖、双时空注意力进行多尺度帧关联、深度特征引导机制融合语义-时间特征

Result: 在语义分割、深度估计和目标检测任务上分别比现有方法提升16%、21%和16%，实现了SOTA性能

Conclusion: 这项工作通过时间推理解锁了基于图像的视觉基础模型在事件视觉中的跨模态潜力

Abstract: Event cameras offer unique advantages for vision tasks in challenging
environments, yet processing asynchronous event streams remains an open
challenge. While existing methods rely on specialized architectures or
resource-intensive training, the potential of leveraging modern Visual
Foundation Models (VFMs) pretrained on image data remains under-explored for
event-based vision. To address this, we propose Temporal-Guided VFM (TGVFM), a
novel framework that integrates VFMs with our temporal context fusion block
seamlessly to bridge this gap. Our temporal block introduces three key
components: (1) Long-Range Temporal Attention to model global temporal
dependencies, (2) Dual Spatiotemporal Attention for multi-scale frame
correlation, and (3) Deep Feature Guidance Mechanism to fuse semantic-temporal
features. By retraining event-to-video models on real-world data and leveraging
transformer-based VFMs, TGVFM preserves spatiotemporal dynamics while
harnessing pretrained representations. Experiments demonstrate SoTA performance
across semantic segmentation, depth estimation, and object detection, with
improvements of 16%, 21%, and 16% over existing methods, respectively. Overall,
this work unlocks the cross-modality potential of image-based VFMs for
event-based vision with temporal reasoning. Code is available at
https://github.com/XiaRho/TGVFM.

</details>


### [65] [Gait Recognition via Collaborating Discriminative and Generative Diffusion Models](https://arxiv.org/abs/2511.06245)
*Haijun Xiong,Bin Feng,Bang Wang,Xinggang Wang,Wenyu Liu*

Main category: cs.CV

TL;DR: CoD²是一个结合扩散模型和数据分布建模能力与判别模型语义表示学习优势的新型步态识别框架，通过多级条件控制策略实现身份一致的步态序列生成，并在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 尽管判别模型在步态识别领域取得了显著成功，但生成模型的潜力尚未被充分探索。本文旨在结合扩散模型的数据分布建模能力和判别模型的语义表示学习优势，提取更鲁棒的步态特征。

Method: 提出CoD²框架，采用多级条件控制策略：高层身份感知语义条件指导生成身份一致的步态序列，低层视觉细节（如外观和运动）被保留以增强一致性。生成的序列反过来促进判别提取器的学习。

Result: 在四个数据集（SUSTech1K、CCPG、GREW和Gait3D）上的广泛实验表明，CoD²实现了最先进的性能，并能与现有判别方法无缝集成，带来一致的性能提升。

Conclusion: CoD²成功地将生成模型和判别模型相结合，通过多级条件控制策略有效提升了步态识别的性能，证明了生成模型在步态识别领域的巨大潜力。

Abstract: Gait recognition offers a non-intrusive biometric solution by identifying
individuals through their walking patterns. Although discriminative models have
achieved notable success in this domain, the full potential of generative
models remains largely underexplored. In this paper, we introduce
\textbf{CoD$^2$}, a novel framework that combines the data distribution
modeling capabilities of diffusion models with the semantic representation
learning strengths of discriminative models to extract robust gait features. We
propose a Multi-level Conditional Control strategy that incorporates both
high-level identity-aware semantic conditions and low-level visual details.
Specifically, the high-level condition, extracted by the discriminative
extractor, guides the generation of identity-consistent gait sequences, whereas
low-level visual details, such as appearance and motion, are preserved to
enhance consistency. Furthermore, the generated sequences facilitate the
discriminative extractor's learning, enabling it to capture more comprehensive
high-level semantic features. Extensive experiments on four datasets
(SUSTech1K, CCPG, GREW, and Gait3D) demonstrate that CoD$^2$ achieves
state-of-the-art performance and can be seamlessly integrated with existing
discriminative methods, yielding consistent improvements.

</details>


### [66] [AdaDrive: Self-Adaptive Slow-Fast System for Language-Grounded Autonomous Driving](https://arxiv.org/abs/2511.06253)
*Ruifei Zhang,Junlin Xie,Wei Zhang,Weikai Chen,Xiao Tan,Xiang Wan,Guanbin Li*

Main category: cs.CV

TL;DR: AdaDrive是一个自适应协作的慢-快框架，通过动态确定何时以及如何让大语言模型参与决策，在自动驾驶中平衡高级推理和实时效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么频繁激活LLM导致计算开销过大，要么使用固定调度无法适应动态驾驶条件，需要一种平衡高精度决策和实时性能的解决方案。

Method: 采用自适应协作慢-快框架：1）何时激活LLM：使用自适应激活损失函数，基于比较学习机制动态决定LLM调用；2）如何集成LLM辅助：引入自适应融合策略，基于场景复杂度和预测置信度调节连续的LLM影响程度。

Result: 在语言驱动的自动驾驶基准测试中，AdaDrive在驾驶精度和计算效率方面都达到了最先进的性能。

Conclusion: AdaDrive提供了一个灵活、上下文感知的框架，在不影响实时性能的前提下最大化决策准确性。

Abstract: Effectively integrating Large Language Models (LLMs) into autonomous driving
requires a balance between leveraging high-level reasoning and maintaining
real-time efficiency. Existing approaches either activate LLMs too frequently,
causing excessive computational overhead, or use fixed schedules, failing to
adapt to dynamic driving conditions. To address these challenges, we propose
AdaDrive, an adaptively collaborative slow-fast framework that optimally
determines when and how LLMs contribute to decision-making. (1) When to
activate the LLM: AdaDrive employs a novel adaptive activation loss that
dynamically determines LLM invocation based on a comparative learning
mechanism, ensuring activation only in complex or critical scenarios. (2) How
to integrate LLM assistance: Instead of rigid binary activation, AdaDrive
introduces an adaptive fusion strategy that modulates a continuous, scaled LLM
influence based on scene complexity and prediction confidence, ensuring
seamless collaboration with conventional planners. Through these strategies,
AdaDrive provides a flexible, context-aware framework that maximizes decision
accuracy without compromising real-time performance. Extensive experiments on
language-grounded autonomous driving benchmarks demonstrate that AdaDrive
state-of-the-art performance in terms of both driving accuracy and
computational efficiency. Code is available at
https://github.com/ReaFly/AdaDrive.

</details>


### [67] [VLDrive: Vision-Augmented Lightweight MLLMs for Efficient Language-grounded Autonomous Driving](https://arxiv.org/abs/2511.06256)
*Ruifei Zhang,Wei Zhang,Xiao Tan,Sibei Yang,Xiang Wan,Xiaonan Luo,Guanbin Li*

Main category: cs.CV

TL;DR: VLDrive提出了一种轻量级多模态大语言模型架构，通过视觉剪枝和特征聚合技术减少参数81%，在CARLA模拟器中实现了最先进的自动驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动驾驶方法存在两个关键问题：视觉表示不足导致频繁碰撞和障碍，以及LLM参数量大难以部署。

Method: 采用轻量级MLLM架构，通过循环一致性动态视觉剪枝和记忆增强特征聚合生成紧凑视觉标记，并提出距离解耦指令注意力机制来改进视觉-语言特征学习。

Result: 在CARLA模拟器中，VLDrive在闭环评估中分别在小、中、长距离上实现了15.4%、16.8%和7.6%的驾驶分数提升，同时参数量从7B减少到1.3B。

Conclusion: VLDrive通过创新的视觉表示压缩和特征学习机制，在显著减少参数的同时实现了优越的自动驾驶性能，为语言驱动的自动驾驶提供了高效解决方案。

Abstract: Recent advancements in language-grounded autonomous driving have been
significantly promoted by the sophisticated cognition and reasoning
capabilities of large language models (LLMs). However, current LLM-based
approaches encounter critical challenges: (1) Failure analysis reveals that
frequent collisions and obstructions, stemming from limitations in visual
representations, remain primary obstacles to robust driving performance. (2)
The substantial parameters of LLMs pose considerable deployment hurdles. To
address these limitations, we introduce VLDrive, a novel approach featuring a
lightweight MLLM architecture with enhanced vision components. VLDrive achieves
compact visual tokens through innovative strategies, including cycle-consistent
dynamic visual pruning and memory-enhanced feature aggregation. Furthermore, we
propose a distance-decoupled instruction attention mechanism to improve joint
visual-linguistic feature learning, particularly for long-range visual tokens.
Extensive experiments conducted in the CARLA simulator demonstrate VLDrive`s
effectiveness. Notably, VLDrive achieves state-of-the-art driving performance
while reducing parameters by 81% (from 7B to 1.3B), yielding substantial
driving score improvements of 15.4%, 16.8%, and 7.6% at tiny, short, and long
distances, respectively, in closed-loop evaluations. Code is available at
https://github.com/ReaFly/VLDrive.

</details>


### [68] [Robust Nearest Neighbour Retrieval Using Targeted Manifold Manipulation](https://arxiv.org/abs/2511.06261)
*B. Ghosh,H. Harikumar,S. Rana*

Main category: cs.CV

TL;DR: TMM-NN是一种新的最近邻检索方法，通过评估样本对目标扰动的响应程度来定义邻域，而不是基于绝对几何距离。该方法使用轻量级的查询特定触发补丁来操纵特征流形，实现语义相关的邻居检索。


<details>
  <summary>Details</summary>
Motivation: 当前最近邻检索方法依赖手动调整特征层和距离度量，存在局限性。作者希望重新定义检索概念，通过样本对目标扰动的响应性来定义邻域，提高检索的语义相关性和鲁棒性。

Method: 提出TMM-NN方法，使用查询特定的触发补丁添加到查询图像中，通过弱"后门"方式将带有补丁的输入引导至虚拟类别。相似图像只需轻微移动即可被分类为虚拟类别，而不相似图像受影响较小。通过置信度排序实现邻居检索。

Result: 鲁棒性分析和基准实验证实，这种基于触发的排序方法在噪声环境下和跨不同任务中优于传统度量方法。

Conclusion: TMM-NN通过重新概念化检索过程，基于样本对目标扰动的响应性来定义邻域，提供了一种更语义相关且鲁棒的最近邻检索方法，在多个任务中表现出优越性能。

Abstract: Nearest-neighbour retrieval is central to classification and explainable-AI
pipelines, but current practice relies on hand-tuning feature layers and
distance metrics. We propose Targeted Manifold Manipulation-Nearest Neighbour
(TMM-NN), which reconceptualises retrieval by assessing how readily each sample
can be nudged into a designated region of the feature manifold; neighbourhoods
are defined by a sample's responsiveness to a targeted perturbation rather than
absolute geometric distance. TMM-NN implements this through a lightweight,
query-specific trigger patch. The patch is added to the query image, and the
network is weakly ``backdoored'' so that any input with the patch is steered
toward a dummy class. Images similar to the query need only a slight shift and
are classified as the dummy class with high probability, while dissimilar ones
are less affected. By ranking candidates by this confidence, TMM-NN retrieves
the most semantically related neighbours. Robustness analysis and benchmark
experiments confirm this trigger-based ranking outperforms traditional metrics
under noise and across diverse tasks.

</details>


### [69] [A Mixture-of-Experts Framework with Log-Logistic Components for Survival Analysis on Histopathology Images](https://arxiv.org/abs/2511.06266)
*Ardhendu Sekhar,Vasu Soni,Keshav Aske,Shivam Madnoorkar,Pranav Jeevan,Amit Sethi*

Main category: cs.CV

TL;DR: 提出一个模块化框架，通过全切片病理图像预测癌症特异性生存率，包含四个核心组件：分位数门控补丁选择、图引导聚类、分层上下文注意力和专家驱动的混合对数逻辑分布框架。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够从病理图像中准确预测癌症特异性生存率的系统，克服现有方法的局限性，提高生存分析的准确性。

Method: 采用四组件模块化方法：分位数门控补丁选择筛选预后信息组织区域；图引导聚类捕获表型异质性；分层上下文注意力学习集群内和集群间交互；专家驱动的混合对数逻辑分布框架估计复杂生存分布。

Result: 在TCGA LUAD数据集上获得0.644的C指数，TCGA KIRC为0.751，TCGA BRCA为0.752，优于现有最先进方法。

Conclusion: 该模块化框架在多个癌症类型中显著提高了生存预测性能，证明了其在病理图像分析中的有效性。

Abstract: We propose a modular framework for predicting cancer specific survival from
whole slide pathology images (WSIs). The method integrates four components: (i)
Quantile Gated Patch Selection via quantile based thresholding to isolate
prognostically informative tissue regions; (ii) Graph Guided Clustering using a
k nearest neighbor graph to capture phenotype level heterogeneity through
spatial and morphological coherence; (iii) Hierarchical Context Attention to
learn intra and inter cluster interactions; and (iv) an Expert Driven Mixture
of Log logistics framework to estimate complex survival distributions using Log
logistics distributions. The model attains a concordance index of 0.644 on TCGA
LUAD, 0.751 on TCGA KIRC, and 0.752 on TCGA BRCA respectively, outperforming
existing state of the art approaches.

</details>


### [70] [LLM-Driven Completeness and Consistency Evaluation for Cultural Heritage Data Augmentation in Cross-Modal Retrieval](https://arxiv.org/abs/2511.06268)
*Jian Zhang,Junyi Guo,Junyi Yuan,Huanda Lu,Yanlin Zhou,Fangyu Wu,Qiufeng Wang,Dongming Lu*

Main category: cs.CV

TL;DR: C³是一个数据增强框架，通过提高LLM生成描述的完整性和一致性来增强跨模态检索性能，在文化遗产数据集和通用基准上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决文化遗产数据中文本描述不完整或不一致的问题，这些问题源于历史数据丢失和专家标注成本高，同时避免LLM生成描述中的幻觉和视觉细节缺失。

Method: 提出C³框架，包含完整性评估模块（使用视觉线索和语言模型输出来评估语义覆盖）和马尔可夫决策过程监督的思维链推理（通过自适应查询控制指导一致性评估）。

Result: 在文化遗产数据集CulTi和TimeTravel以及通用基准MSCOCO和Flickr30K上的实验表明，C³在微调和零样本设置下都达到了最先进的性能。

Conclusion: C³框架通过提高LLM生成描述的完整性和一致性，有效提升了跨模态检索性能，为文化遗产数据解释提供了可靠解决方案。

Abstract: Cross-modal retrieval is essential for interpreting cultural heritage data,
but its effectiveness is often limited by incomplete or inconsistent textual
descriptions, caused by historical data loss and the high cost of expert
annotation. While large language models (LLMs) offer a promising solution by
enriching textual descriptions, their outputs frequently suffer from
hallucinations or miss visually grounded details. To address these challenges,
we propose $C^3$, a data augmentation framework that enhances cross-modal
retrieval performance by improving the completeness and consistency of
LLM-generated descriptions. $C^3$ introduces a completeness evaluation module
to assess semantic coverage using both visual cues and language-model outputs.
Furthermore, to mitigate factual inconsistencies, we formulate a Markov
Decision Process to supervise Chain-of-Thought reasoning, guiding consistency
evaluation through adaptive query control. Experiments on the cultural heritage
datasets CulTi and TimeTravel, as well as on general benchmarks MSCOCO and
Flickr30K, demonstrate that $C^3$ achieves state-of-the-art performance in both
fine-tuned and zero-shot settings.

</details>


### [71] [RelightMaster: Precise Video Relighting with Multi-plane Light Images](https://arxiv.org/abs/2511.06271)
*Weikang Bian,Xiaoyu Shi,Zhaoyang Huang,Jianhong Bai,Qinghe Wang,Xintao Wang,Pengfei Wan,Kun Gai,Hongsheng Li*

Main category: cs.CV

TL;DR: RelightMaster是一个用于精确可控视频重照明的框架，通过构建RelightVideo数据集、引入多平面光照图像(MPLI)作为视觉提示，以及设计光照图像适配器来注入预训练视频扩散模型中，实现物理合理的光照和阴影生成。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频模型缺乏细粒度光照控制，文本描述光照细节有限，且缺乏光照相关预训练数据。高质量重照明训练数据难以获取，真实可控光照数据稀缺。

Method: 1) 基于Unreal Engine构建RelightVideo数据集；2) 提出多平面光照图像(MPLI)建模3D光源位置、强度和颜色；3) 设计光照图像适配器将MPLI注入预训练视频扩散变换器(DiT)中。

Result: RelightMaster能够生成物理合理的光照和阴影，同时保持原始场景内容，支持多光源场景并泛化到未见过的光照设置。

Conclusion: 该框架解决了视频重照明的关键挑战，实现了精确可控的视频光照编辑，为塑造场景氛围和引导观众注意力提供了有效工具。

Abstract: Recent advances in diffusion models enable high-quality video generation and
editing, but precise relighting with consistent video contents, which is
critical for shaping scene atmosphere and viewer attention, remains unexplored.
Mainstream text-to-video (T2V) models lack fine-grained lighting control due to
text's inherent limitation in describing lighting details and insufficient
pre-training on lighting-related prompts. Additionally, constructing
high-quality relighting training data is challenging, as real-world
controllable lighting data is scarce. To address these issues, we propose
RelightMaster, a novel framework for accurate and controllable video
relighting. First, we build RelightVideo, the first dataset with identical
dynamic content under varying precise lighting conditions based on the Unreal
Engine. Then, we introduce Multi-plane Light Image (MPLI), a novel visual
prompt inspired by Multi-Plane Image (MPI). MPLI models lighting via K
depth-aligned planes, representing 3D light source positions, intensities, and
colors while supporting multi-source scenarios and generalizing to unseen light
setups. Third, we design a Light Image Adapter that seamlessly injects MPLI
into pre-trained Video Diffusion Transformers (DiT): it compresses MPLI via a
pre-trained Video VAE and injects latent light features into DiT blocks,
leveraging the base model's generative prior without catastrophic forgetting.
Experiments show that RelightMaster generates physically plausible lighting and
shadows and preserves original scene content. Demos are available at
https://wkbian.github.io/Projects/RelightMaster/.

</details>


### [72] [LaneDiffusion: Improving Centerline Graph Learning via Prior Injected BEV Feature Generation](https://arxiv.org/abs/2511.06272)
*Zijie Wang,Weiming Zhang,Wei Zhang,Xiao Tan,Hongxing Liu,Yaowei Wang,Guanbin Li*

Main category: cs.CV

TL;DR: LaneDiffusion是一种基于扩散模型的生成式中心线图学习方法，通过在BEV特征层面生成车道中心线先验，有效解决了传统确定性方法在空间推理和遮挡处理方面的不足，在多个数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统确定性方法在自动驾驶路径规划中缺乏空间推理能力，难以处理被遮挡或不可见的中心线，而生成式方法在此领域尚未充分探索。

Method: 提出LaneDiffusion方法，使用扩散模型在BEV特征层面生成车道中心线先验，包含车道先验注入模块(LPIM)和车道先验扩散模块(LPDM)来构建扩散目标和管理扩散过程，然后从先验注入的BEV特征解码向量化中心线和拓扑结构。

Result: 在nuScenes和Argoverse2数据集上的广泛评估显示，LaneDiffusion显著优于现有方法，在点级指标(GEO F1、TOPO F1、JTOPO F1、APLS、SDA)上分别提升4.2%、4.6%、4.7%、6.4%和1.8%，在段级指标(IoU、mAP_cf、DET_l、TOP_ll)上分别提升2.3%、6.4%、6.8%和2.1%。

Conclusion: LaneDiffusion在中心线图学习中实现了最先进的性能，为该任务的生成模型提供了新的见解。

Abstract: Centerline graphs, crucial for path planning in autonomous driving, are
traditionally learned using deterministic methods. However, these methods often
lack spatial reasoning and struggle with occluded or invisible centerlines.
Generative approaches, despite their potential, remain underexplored in this
domain. We introduce LaneDiffusion, a novel generative paradigm for centerline
graph learning. LaneDiffusion innovatively employs diffusion models to generate
lane centerline priors at the Bird's Eye View (BEV) feature level, instead of
directly predicting vectorized centerlines. Our method integrates a Lane Prior
Injection Module (LPIM) and a Lane Prior Diffusion Module (LPDM) to effectively
construct diffusion targets and manage the diffusion process. Furthermore,
vectorized centerlines and topologies are then decoded from these
prior-injected BEV features. Extensive evaluations on the nuScenes and
Argoverse2 datasets demonstrate that LaneDiffusion significantly outperforms
existing methods, achieving improvements of 4.2%, 4.6%, 4.7%, 6.4% and 1.8% on
fine-grained point-level metrics (GEO F1, TOPO F1, JTOPO F1, APLS and SDA) and
2.3%, 6.4%, 6.8% and 2.1% on segment-level metrics (IoU, mAP_cf, DET_l and
TOP_ll). These results establish state-of-the-art performance in centerline
graph learning, offering new insights into generative models for this task.

</details>


### [73] [VideoSSR: Video Self-Supervised Reinforcement Learning](https://arxiv.org/abs/2511.06281)
*Zefeng He,Xiaoye Qu,Yafu Li,Siyuan Huang,Daizong Liu,Yu Cheng*

Main category: cs.CV

TL;DR: 本文提出VideoSSR框架，利用视频内在信息自生成高质量可验证训练数据，通过三个自监督预训练任务（异常定位、物体计数、时间拼图）构建VideoSSR-30K数据集，在17个基准测试中平均提升模型性能超过5%。


<details>
  <summary>Details</summary>
Motivation: 现有视频数据集复杂度跟不上MLLMs发展速度，而人工标注高质量数据成本过高，因此探索能否利用视频内在信息自生成高质量可验证训练数据。

Method: 提出三个自监督预训练任务构建VideoSSR-30K数据集，并开发VideoSSR视频自监督强化学习框架用于RLVR。

Result: 在17个基准测试（涵盖通用视频QA、长视频QA、时间定位和复杂推理四个主要视频领域）中，VideoSSR持续提升模型性能，平均改进超过5%。

Conclusion: VideoSSR为开发更先进的MLLMs视频理解能力提供了强大的基础框架。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has substantially
advanced the video understanding capabilities of Multimodal Large Language
Models (MLLMs). However, the rapid progress of MLLMs is outpacing the
complexity of existing video datasets, while the manual annotation of new,
high-quality data remains prohibitively expensive. This work investigates a
pivotal question: Can the rich, intrinsic information within videos be
harnessed to self-generate high-quality, verifiable training data? To
investigate this, we introduce three self-supervised pretext tasks: Anomaly
Grounding, Object Counting, and Temporal Jigsaw. We construct the Video
Intrinsic Understanding Benchmark (VIUBench) to validate their difficulty,
revealing that current state-of-the-art MLLMs struggle significantly on these
tasks. Building upon these pretext tasks, we develop the VideoSSR-30K dataset
and propose VideoSSR, a novel video self-supervised reinforcement learning
framework for RLVR. Extensive experiments across 17 benchmarks, spanning four
major video domains (General Video QA, Long Video QA, Temporal Grounding, and
Complex Reasoning), demonstrate that VideoSSR consistently enhances model
performance, yielding an average improvement of over 5\%. These results
establish VideoSSR as a potent foundational framework for developing more
advanced video understanding in MLLMs. The code is available at
https://github.com/lcqysl/VideoSSR.

</details>


### [74] [From ACR O-RADS 2022 to Explainable Deep Learning: Comparative Performance of Expert Radiologists, Convolutional Neural Networks, Vision Transformers, and Fusion Models in Ovarian Masses](https://arxiv.org/abs/2511.06282)
*Ali Abbasian Ardakani,Afshin Mohammadi,Alisa Mohebbi,Anushya Vijayananthan,Sook Sam Leong,Lim Yi Ting,Mohd Kamil Bin Mohamad Fabell,U Rajendra Acharya,Sepideh Hatamikia*

Main category: cs.CV

TL;DR: 该研究比较了放射科医生使用O-RADS v2022标准与深度学习模型在卵巢附件病变诊断中的表现，发现ViT模型表现最佳，且人机混合框架能进一步提升诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管O-RADS v2022标准改进了卵巢附件病变的风险分层，但人工判读仍存在变异性和保守阈值问题，需要评估深度学习模型和混合人机框架的诊断价值。

Method: 回顾性研究纳入512个卵巢附件肿块图像，训练和验证了16个深度学习模型（包括CNN和ViT），并构建了结合放射科医生O-RADS评分与DL预测概率的混合模型。

Result: 放射科医生单独使用O-RADS的AUC为0.683，准确率68.0%；ViT16-384模型表现最佳（AUC 0.941，准确率87.4%）；混合人机框架显著提升了CNN模型性能。

Conclusion: 深度学习模型显著优于放射科医生单独使用O-RADS v2022评估，专家评分与AI结合可获得最高的诊断准确性和鉴别能力，混合人机模式具有标准化盆腔超声判读的巨大潜力。

Abstract: Background: The 2022 update of the Ovarian-Adnexal Reporting and Data System
(O-RADS) ultrasound classification refines risk stratification for adnexal
lesions, yet human interpretation remains subject to variability and
conservative thresholds. Concurrently, deep learning (DL) models have
demonstrated promise in image-based ovarian lesion characterization. This study
evaluates radiologist performance applying O-RADS v2022, compares it to leading
convolutional neural network (CNN) and Vision Transformer (ViT) models, and
investigates the diagnostic gains achieved by hybrid human-AI frameworks.
Methods: In this single-center, retrospective cohort study, a total of 512
adnexal mass images from 227 patients (110 with at least one malignant cyst)
were included. Sixteen DL models, including DenseNets, EfficientNets, ResNets,
VGGs, Xception, and ViTs, were trained and validated. A hybrid model
integrating radiologist O-RADS scores with DL-predicted probabilities was also
built for each scheme. Results: Radiologist-only O-RADS assessment achieved an
AUC of 0.683 and an overall accuracy of 68.0%. CNN models yielded AUCs of 0.620
to 0.908 and accuracies of 59.2% to 86.4%, while ViT16-384 reached the best
performance, with an AUC of 0.941 and an accuracy of 87.4%. Hybrid human-AI
frameworks further significantly enhanced the performance of CNN models;
however, the improvement for ViT models was not statistically significant
(P-value >0.05). Conclusions: DL models markedly outperform radiologist-only
O-RADS v2022 assessment, and the integration of expert scores with AI yields
the highest diagnostic accuracy and discrimination. Hybrid human-AI paradigms
hold substantial potential to standardize pelvic ultrasound interpretation,
reduce false positives, and improve detection of high-risk lesions.

</details>


### [75] [TinyChemVL: Advancing Chemical Vision-Language Models via Efficient Visual Token Reduction and Complex Reaction Tasks](https://arxiv.org/abs/2511.06283)
*Xuanle Zhao,Shuxin Zeng,Yinyuan Cai,Xiang Cheng,Duzhen Zhang,Xiuyi Chen,Bo Xu*

Main category: cs.CV

TL;DR: 本文提出了TinyChemVL，一种高效且强大的化学视觉语言模型，通过视觉令牌减少和反应级任务来提高模型效率和推理能力。该模型仅使用4B参数，在分子和反应任务上均表现出色，同时推理和训练速度更快。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在化学领域的应用有限，主要问题包括：处理整个化学图像的计算效率低下，以及仅关注分子级任务限制了化学推理的进展。

Method: 提出TinyChemVL模型，采用视觉令牌减少技术和反应级任务设计，同时构建了ChemRxn-V反应级基准来评估基于视觉的反应识别和预测任务。

Result: TinyChemVL仅用4B参数就在分子和反应任务上取得优异性能，推理和训练速度更快，仅使用ChemVLM 1/16的视觉令牌就超越了其性能。

Conclusion: 通过协同设计模型架构和任务复杂性，成功构建了高效且强大的化学领域视觉语言模型。

Abstract: While Vision Language Models (VLMs) have demonstrated remarkable capabilities
in general visual understanding, their application in the chemical domain has
been limited, with previous works predominantly focusing on text and thus
overlooking critical visual information, such as molecular structures. Current
approaches that directly adopt standard VLMs for chemical tasks suffer from two
primary issues: (i) computational inefficiency of processing entire chemical
images with non-informative backgrounds. (ii) a narrow scope on molecular-level
tasks that restricts progress in chemical reasoning. In this work, we propose
\textbf{TinyChemVL}, an efficient and powerful chemical VLM that leverages
visual token reduction and reaction-level tasks to improve model efficiency and
reasoning capacity. Also, we propose \textbf{ChemRxn-V}, a reaction-level
benchmark for assessing vision-based reaction recognition and prediction tasks.
Directly predicting reaction products from molecular images poses a non-trivial
challenge, as it requires models to integrate both recognition and reasoning
capacities. Our results demonstrate that with only 4B parameters, TinyChemVL
achieves superior performance on both molecular and reaction tasks while
demonstrating faster inference and training speeds compared to existing models.
Notably, TinyChemVL outperforms ChemVLM while utilizing only 1/16th of the
visual tokens. This work builds efficient yet powerful VLMs for chemical
domains by co-designing model architecture and task complexity.

</details>


### [76] [Learning-Based Vision Systems for Semi-Autonomous Forklift Operation in Industrial Warehouse Environments](https://arxiv.org/abs/2511.06295)
*Vamshika Sutar,Mahek Maheshwari,Archak Mittal*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉的托盘和托盘孔检测与映射框架，使用单个标准摄像头，通过优化的YOLOv8和YOLOv11架构实现高精度检测，为叉车提供经济有效的感知模块。


<details>
  <summary>Details</summary>
Motivation: 仓库物料搬运自动化需要可靠、低成本的叉车和AGV感知系统，现有系统成本较高且不够灵活。

Method: 使用YOLOv8和YOLOv11架构，通过Optuna驱动的超参数优化和空间后处理增强性能，开发创新的托盘孔映射模块将检测结果转换为可操作的空间表示。

Result: 在包含真实仓库图像的自定义数据集上，YOLOv8实现高精度检测，YOLOv11在优化配置下提供更优精度和稳定收敛。

Conclusion: 该研究证明了经济有效、可改装视觉感知模块的可行性，为仓库自动化提供了可扩展方法，促进更安全、经济和智能的物流操作。

Abstract: The automation of material handling in warehouses increasingly relies on
robust, low cost perception systems for forklifts and Automated Guided Vehicles
(AGVs). This work presents a vision based framework for pallet and pallet hole
detection and mapping using a single standard camera. We utilized YOLOv8 and
YOLOv11 architectures, enhanced through Optuna driven hyperparameter
optimization and spatial post processing. An innovative pallet hole mapping
module converts the detections into actionable spatial representations,
enabling accurate pallet and pallet hole association for forklift operation.
Experiments on a custom dataset augmented with real warehouse imagery show that
YOLOv8 achieves high pallet and pallet hole detection accuracy, while YOLOv11,
particularly under optimized configurations, offers superior precision and
stable convergence. The results demonstrate the feasibility of a cost
effective, retrofittable visual perception module for forklifts. This study
proposes a scalable approach to advancing warehouse automation, promoting
safer, economical, and intelligent logistics operations.

</details>


### [77] [Physics-Informed Deformable Gaussian Splatting: Towards Unified Constitutive Laws for Time-Evolving Material Field](https://arxiv.org/abs/2511.06299)
*Haoqin Hong,Ding Fan,Fubin Dou,Zhi-Li Zhou,Haoran Sun,Congcong Zhu,Jingrun Chen*

Main category: cs.CV

TL;DR: 本文提出PIDG方法，将3D高斯粒子视为拉格朗日材料点，结合物理约束和光学流监督，提升动态场景重建的物理一致性和质量。


<details>
  <summary>Details</summary>
Motivation: 纯数据驱动的3D高斯溅射在捕捉动态场景中多样化的物理驱动运动模式方面存在困难，需要引入物理知识来提升重建质量。

Method: 采用静态-动态解耦的4D分解哈希编码重建几何和运动；施加柯西动量残差作为物理约束；通过时间演化材料场独立预测粒子速度和本构应力；将拉格朗日粒子流与相机补偿光学流匹配进行数据拟合监督。

Result: 在自定义物理驱动数据集以及标准合成和真实世界数据集上的实验显示，在物理一致性和单目动态重建质量方面取得了显著提升。

Conclusion: PIDG方法通过结合物理约束和光学流监督，有效提升了动态场景重建的物理一致性和重建质量，为动态新视角合成提供了更可靠的解决方案。

Abstract: Recently, 3D Gaussian Splatting (3DGS), an explicit scene representation
technique, has shown significant promise for dynamic novel-view synthesis from
monocular video input. However, purely data-driven 3DGS often struggles to
capture the diverse physics-driven motion patterns in dynamic scenes. To fill
this gap, we propose Physics-Informed Deformable Gaussian Splatting (PIDG),
which treats each Gaussian particle as a Lagrangian material point with
time-varying constitutive parameters and is supervised by 2D optical flow via
motion projection. Specifically, we adopt static-dynamic decoupled 4D
decomposed hash encoding to reconstruct geometry and motion efficiently.
Subsequently, we impose the Cauchy momentum residual as a physics constraint,
enabling independent prediction of each particle's velocity and constitutive
stress via a time-evolving material field. Finally, we further supervise data
fitting by matching Lagrangian particle flow to camera-compensated optical
flow, which accelerates convergence and improves generalization. Experiments on
a custom physics-driven dataset as well as on standard synthetic and real-world
datasets demonstrate significant gains in physical consistency and monocular
dynamic reconstruction quality.

</details>


### [78] [Adaptive 3D Reconstruction via Diffusion Priors and Forward Curvature-Matching Likelihood Updates](https://arxiv.org/abs/2511.06310)
*Seunghyeok Shin,Dabin Kim,Hongki Lim*

Main category: cs.CV

TL;DR: 本文提出了一种新的前向曲率匹配（FCM）更新方法，结合扩散采样来解决图像到高质量点云重建的挑战。该方法通过动态确定最优步长，支持单视图和多视图输入，无需重新训练即可适应不同输入模态。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的方法存在灵活性不足的问题：需要训练时的条件信号、仅支持固定数量的输入视图，以及需要为不同测量重新训练。最近的扩散方法尝试通过结合先验模型和似然更新来解决，但依赖于启发式固定步长，导致收敛慢和重建质量不佳。

Method: 提出前向曲率匹配（FCM）更新方法，仅使用前向自动微分和有限差分曲率估计动态确定最优步长，实现似然更新的精确优化。该方法支持单视图和多视图输入，通过简单的算子替换支持各种输入模态，无需重新训练。

Result: 在ShapeNet和CO3D数据集上的实验表明，该方法在相同或更低的NFEs下实现了优越的重建质量，获得了更高的F-score和更低的CD和EMD，验证了其效率和实际应用的适应性。

Conclusion: FCM方法通过动态步长优化显著提高了点云重建的质量和效率，支持灵活的输入配置和模态，为实际应用提供了高效且适应性强的解决方案。

Abstract: Reconstructing high-quality point clouds from images remains challenging in
computer vision. Existing generative-model-based approaches, particularly
diffusion-model approaches that directly learn the posterior, may suffer from
inflexibility -- they require conditioning signals during training, support
only a fixed number of input views, and need complete retraining for different
measurements. Recent diffusion-based methods have attempted to address this by
combining prior models with likelihood updates, but they rely on heuristic
fixed step sizes for the likelihood update that lead to slow convergence and
suboptimal reconstruction quality. We advance this line of approach by
integrating our novel Forward Curvature-Matching (FCM) update method with
diffusion sampling. Our method dynamically determines optimal step sizes using
only forward automatic differentiation and finite-difference curvature
estimates, enabling precise optimization of the likelihood update. This
formulation enables high-fidelity reconstruction from both single-view and
multi-view inputs, and supports various input modalities through simple
operator substitution -- all without retraining. Experiments on ShapeNet and
CO3D datasets demonstrate that our method achieves superior reconstruction
quality at matched or lower NFEs, yielding higher F-score and lower CD and EMD,
validating its efficiency and adaptability for practical applications. Code is
available at https://github.com/Seunghyeok0715/FCM

</details>


### [79] [Seq2Seq Models Reconstruct Visual Jigsaw Puzzles without Seeing Them](https://arxiv.org/abs/2511.06315)
*Gur Elkn,Ofir Itzhak Shahar,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: 本文提出了一种使用语言模型解决方形拼图的新方法，无需视觉输入，通过专门的标记器将拼图块转换为离散标记序列，将拼图重组视为序列到序列预测任务。


<details>
  <summary>Details</summary>
Motivation: 探索从语言角度而非传统视觉角度解决拼图问题的新方法，研究语言模型在非本领域问题上的能力。

Method: 引入专门的标记器将每个拼图块转换为离散标记序列，使用编码器-解码器变换器作为"盲"求解器，仅基于标记序列进行推理来重建原始布局。

Result: 尽管刻意限制访问视觉输入，该方法在多个基准测试中达到了最先进的结果，通常优于基于视觉的方法。

Conclusion: 研究结果突显了语言模型解决超出其原生领域问题的惊人能力，表明非常规方法可以为拼图求解研究带来有前景的新方向。

Abstract: Jigsaw puzzles are primarily visual objects, whose algorithmic solutions have
traditionally been framed from a visual perspective. In this work, however, we
explore a fundamentally different approach: solving square jigsaw puzzles using
language models, without access to raw visual input. By introducing a
specialized tokenizer that converts each puzzle piece into a discrete sequence
of tokens, we reframe puzzle reassembly as a sequence-to-sequence prediction
task. Treated as "blind" solvers, encoder-decoder transformers accurately
reconstruct the original layout by reasoning over token sequences alone.
Despite being deliberately restricted from accessing visual input, our models
achieve state-of-the-art results across multiple benchmarks, often
outperforming vision-based methods. These findings highlight the surprising
capability of language models to solve problems beyond their native domain, and
suggest that unconventional approaches can inspire promising directions for
puzzle-solving research.

</details>


### [80] [CINEMAE: Leveraging Frozen Masked Autoencoders for Cross-Generator AI Image Detection](https://arxiv.org/abs/2511.06325)
*Minsuk Jang,Hyeonseo Jeong,Minseok Son,Changick Kim*

Main category: cs.CV

TL;DR: CINEMAE是一种新的AIGC图像检测方法，通过将文本检测的核心原理应用到视觉领域，利用掩码自编码器的重建过程来量化局部语义异常，实现了强大的跨生成器泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于上下文的文本检测器在AI生成文本检测方面表现出色，但基于图像的检测器仍然过度依赖生成器特定的伪影，缺乏泛化能力。

Method: 使用掩码自编码器（MAE）重建被掩码的图像块，通过计算条件负对数似然来量化局部语义异常，并将这些补丁级统计与全局MAE特征通过学习融合进行聚合。

Result: 仅在Stable Diffusion v1.4上训练，在GenImage基准测试的八个未见生成器上实现了超过95%的准确率，显著优于现有最先进的检测器。

Conclusion: 基于上下文条件重建不确定性的方法为AIGC检测提供了稳健且可迁移的信号。

Abstract: While context-based detectors have achieved strong generalization for
AI-generated text by measuring distributional inconsistencies, image-based
detectors still struggle with overfitting to generator-specific artifacts. We
introduce CINEMAE, a novel paradigm for AIGC image detection that adapts the
core principles of text detection methods to the visual domain. Our key insight
is that Masked AutoEncoder (MAE), trained to reconstruct masked patches
conditioned on visible context, naturally encodes semantic consistency
expectations. We formalize this reconstruction process probabilistically,
computing conditional Negative Log-Likelihood (NLL, p(masked | visible)) to
quantify local semantic anomalies. By aggregating these patch-level statistics
with global MAE features through learned fusion, CINEMAE achieves strong
cross-generator generalization. Trained exclusively on Stable Diffusion v1.4,
our method achieves over 95% accuracy on all eight unseen generators in the
GenImage benchmark, substantially outperforming state-of-the-art detectors.
This demonstrates that context-conditional reconstruction uncertainty provides
a robust, transferable signal for AIGC detection.

</details>


### [81] [Label-Efficient 3D Forest Mapping: Self-Supervised and Transfer Learning for Individual, Structural, and Species Analysis](https://arxiv.org/abs/2511.06331)
*Aldino Rizaldy,Fabian Ewald Fassnacht,Ahmed Jamal Afifi,Hua Jiang,Richard Gloaguen,Pedram Ghamisi*

Main category: cs.CV

TL;DR: 本文探讨了使用自监督学习和迁移学习来减少对大量标注数据的依赖，以改进从激光扫描点云中提取树木信息的三个任务：实例分割、语义分割和树木分类。


<details>
  <summary>Details</summary>
Motivation: 精确林业、生物多样性保护和碳制图需要详细的树木结构信息，但深度学习模型需要大量标注数据，这在复杂森林环境中难以获得。

Method: 采用自监督学习和迁移学习架构，结合领域自适应技术，构建统一框架从原始点云到树木划分、结构分析和物种分类。

Result: 自监督学习结合领域自适应显著提升实例分割性能（AP50 +16.98%），自监督学习足以改进语义分割（mIoU +1.79%），分层迁移学习能准确分类未见物种（Jaccard +6.07%）。

Conclusion: 该方法能有效减少对标注数据的依赖，预训练模型降低能耗约21%，开源框架有助于加速从激光扫描点云中提取树木信息的操作化应用。

Abstract: Detailed structural and species information on individual tree level is
increasingly important to support precision forestry, biodiversity
conservation, and provide reference data for biomass and carbon mapping. Point
clouds from airborne and ground-based laser scanning are currently the most
suitable data source to rapidly derive such information at scale. Recent
advancements in deep learning improved segmenting and classifying individual
trees and identifying semantic tree components. However, deep learning models
typically require large amounts of annotated training data which limits further
improvement. Producing dense, high-quality annotations for 3D point clouds,
especially in complex forests, is labor-intensive and challenging to scale. We
explore strategies to reduce dependence on large annotated datasets using
self-supervised and transfer learning architectures. Our objective is to
improve performance across three tasks: instance segmentation, semantic
segmentation, and tree classification using realistic and operational training
sets. Our findings indicate that combining self-supervised learning with domain
adaptation significantly enhances instance segmentation compared to training
from scratch (AP50 +16.98%), self-supervised learning suffices for semantic
segmentation (mIoU +1.79%), and hierarchical transfer learning enables accurate
classification of unseen species (Jaccard +6.07%). To simplify use and
encourage uptake, we integrated the tasks into a unified framework,
streamlining the process from raw point clouds to tree delineation, structural
analysis, and species classification. Pretrained models reduce energy
consumption and carbon emissions by ~21%. This open-source contribution aims to
accelerate operational extraction of individual tree information from laser
scanning point clouds to support forestry, biodiversity, and carbon mapping.

</details>


### [82] [BuildingWorld: A Structured 3D Building Dataset for Urban Foundation Models](https://arxiv.org/abs/2511.06337)
*Shangfeng Huang,Ruisheng Wang,Xin Wang*

Main category: cs.CV

TL;DR: BuildingWorld是一个全面的结构化3D建筑数据集，旨在解决现有建筑数据集在建筑风格多样性方面的不足，包含来自全球各地约500万个LOD2建筑模型，并配有真实和模拟的机载LiDAR点云数据。


<details>
  <summary>Details</summary>
Motivation: 数字孪生技术在现代城市转型中变得至关重要，但现有学习模型在建筑数据集上的建筑风格多样性有限，显著影响了它们在异构城市环境中的泛化能力。

Method: 收集来自北美、欧洲、亚洲、非洲和大洋洲等地理和建筑多样化区域的建筑数据，提供约500万个LOD2建筑模型，并引入Cyber City虚拟城市模型来生成具有定制化和结构多样化点云分布的无限训练数据。

Result: BuildingWorld提供了一个全球代表性的数据集，支持3D建筑重建、检测和分割的全面研究，并为大规模视觉模型和基础模型在结构化3D城市环境中的训练、评估和比较提供标准化评估指标。

Conclusion: BuildingWorld通过提供建筑风格多样化的全面3D建筑数据集，有效解决了现有数据集在建筑多样性方面的局限性，为城市尺度基础建模和分析提供了重要支持。

Abstract: As digital twins become central to the transformation of modern cities,
accurate and structured 3D building models emerge as a key enabler of
high-fidelity, updatable urban representations. These models underpin diverse
applications including energy modeling, urban planning, autonomous navigation,
and real-time reasoning. Despite recent advances in 3D urban modeling, most
learning-based models are trained on building datasets with limited
architectural diversity, which significantly undermines their generalizability
across heterogeneous urban environments. To address this limitation, we present
BuildingWorld, a comprehensive and structured 3D building dataset designed to
bridge the gap in stylistic diversity. It encompasses buildings from
geographically and architecturally diverse regions -- including North America,
Europe, Asia, Africa, and Oceania -- offering a globally representative dataset
for urban-scale foundation modeling and analysis. Specifically, BuildingWorld
provides about five million LOD2 building models collected from diverse
sources, accompanied by real and simulated airborne LiDAR point clouds. This
enables comprehensive research on 3D building reconstruction, detection and
segmentation. Cyber City, a virtual city model, is introduced to enable the
generation of unlimited training data with customized and structurally diverse
point cloud distributions. Furthermore, we provide standardized evaluation
metrics tailored for building reconstruction, aiming to facilitate the
training, evaluation, and comparison of large-scale vision models and
foundation models in structured 3D urban environments.

</details>


### [83] [GazeVLM: A Vision-Language Model for Multi-Task Gaze Understanding](https://arxiv.org/abs/2511.06348)
*Athul M. Mathew,Haithem Hermassi,Thariq Khalid,Arshad Ali Khan,Riad Souissi*

Main category: cs.CV

TL;DR: GazeVLM是一个新颖的视觉语言模型，用于图像中的多任务注视理解，包括人物检测、注视目标检测和注视物体识别。它通过融合RGB图像和HHA编码的深度图，结合文本提示，在GazeFollow和VideoAttentionTarget数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然建模了视觉场景中的注视线索，但仍需要一个统一系统来使用视觉和语言提示进行注视理解。

Method: 提出GazeVLM模型，集成视觉（RGB和深度）和文本模态，通过融合RGB图像与HHA编码的深度图，并引入文本提示指导，实现多任务注视理解。

Result: 消融研究表明RGB图像与HHA编码深度图的融合结合文本提示能获得最佳性能。在GazeFollow和VideoAttentionTarget数据集上取得了最先进的评估分数。

Conclusion: GazeVLM是首个将视觉语言模型应用于多任务注视理解的系统，通过视觉和语言模态的融合显著提升了注视理解性能。

Abstract: Gaze understanding unifies the detection of people, their gaze targets, and
objects of interest into a single framework, offering critical insight into
visual attention and intent estimation. Although prior research has modelled
gaze cues in visual scenes, a unified system is still needed for gaze
understanding using both visual and language prompts. This paper introduces
GazeVLM, a novel Vision-Language Model (VLM) for multi-task gaze understanding
in images, addressing person detection, gaze target detection, and gaze object
identification. While other transformer-based methods exist for gaze analysis,
GazeVLM represents, to our knowledge, the first application of a VLM to these
combined tasks, allowing for selective execution of each task. Through the
integration of visual (RGB and depth) and textual modalities, our ablation
study on visual input combinations revealed that a fusion of RGB images with
HHA-encoded depth maps, guided by text prompts, yields superior performance. We
also introduce an object-level gaze detection metric for gaze object
identification ($AP_{ob}$). Through experiments, GazeVLM demonstrates
significant improvements, notably achieving state-of-the-art evaluation scores
on GazeFollow and VideoAttentionTarget datasets.

</details>


### [84] [AesTest: Measuring Aesthetic Intelligence from Perception to Production](https://arxiv.org/abs/2511.06360)
*Guolong Wang,Heng Huang,Zhiqiang Zhang,Wentian Li,Feilong Ma,Xin Jin*

Main category: cs.CV

TL;DR: AesTest是一个用于评估多模态大语言模型美学感知和生产能力的综合基准，包含十个任务，涵盖感知、欣赏、创作和摄影等领域，基于生成学习心理学理论构建。


<details>
  <summary>Details</summary>
Motivation: 现有图像美学评估基准在感知范围上较窄或缺乏多样性，无法系统评估美学生产能力，因此需要更全面的基准来填补这一空白。

Method: 通过整合来自专业编辑工作流程、摄影构图教程和众包偏好的多样化数据，构建包含十个任务的多元选择题基准，支持基于属性的分析、情感共鸣、构图选择和风格推理等美学查询类型。

Result: 对指令调优的图像美学评估MLLM和通用MLLM在AesTest上的评估显示，构建美学智能面临显著挑战。

Conclusion: AesTest基准将公开发布，以支持该领域的未来研究，推动多模态大语言模型美学能力的发展。

Abstract: Perceiving and producing aesthetic judgments is a fundamental yet
underexplored capability for multimodal large language models (MLLMs). However,
existing benchmarks for image aesthetic assessment (IAA) are narrow in
perception scope or lack the diversity needed to evaluate systematic aesthetic
production. To address this gap, we introduce AesTest, a comprehensive
benchmark for multimodal aesthetic perception and production, distinguished by
the following features: 1) It consists of curated multiple-choice questions
spanning ten tasks, covering perception, appreciation, creation, and
photography. These tasks are grounded in psychological theories of generative
learning. 2) It integrates data from diverse sources, including professional
editing workflows, photographic composition tutorials, and crowdsourced
preferences. It ensures coverage of both expert-level principles and real-world
variation. 3) It supports various aesthetic query types, such as
attribute-based analysis, emotional resonance, compositional choice, and
stylistic reasoning. We evaluate both instruction-tuned IAA MLLMs and general
MLLMs on AesTest, revealing significant challenges in building aesthetic
intelligence. We will publicly release AesTest to support future research in
this area.

</details>


### [85] [V-Shuffle: Zero-Shot Style Transfer via Value Shuffle](https://arxiv.org/abs/2511.06365)
*Haojun Tang,Qiwei Lin,Tongda Xu,Lida Huang,Yan Wang*

Main category: cs.CV

TL;DR: V-Shuffle是一种基于注意力注入的零样本风格迁移方法，通过打乱扩散模型中自注意力层的值特征来破坏风格图像的语义内容，同时保留低层风格表示，并引入混合风格正则化来增强风格保真度。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力注入的风格迁移方法存在内容泄漏问题，即风格图像中不需要的语义内容会错误地出现在风格化输出中。

Method: V-Shuffle通过打乱扩散模型自注意力层中的值特征来隐式破坏风格图像的语义内容，同时保留低层风格表示，并引入混合风格正则化来补充高层风格纹理。

Result: 实验结果表明，V-Shuttle在使用多个风格图像时表现优异，在单张风格图像情况下也优于之前的最先进方法。

Conclusion: V-Shuffle能够有效平衡内容保持和风格保真度之间的权衡，在零样本风格迁移任务中取得了显著进展。

Abstract: Attention injection-based style transfer has achieved remarkable progress in
recent years. However, existing methods often suffer from content leakage,
where the undesired semantic content of the style image mistakenly appears in
the stylized output. In this paper, we propose V-Shuffle, a zero-shot style
transfer method that leverages multiple style images from the same style domain
to effectively navigate the trade-off between content preservation and style
fidelity. V-Shuffle implicitly disrupts the semantic content of the style
images by shuffling the value features within the self-attention layers of the
diffusion model, thereby preserving low-level style representations. We further
introduce a Hybrid Style Regularization that complements these low-level
representations with high-level style textures to enhance style fidelity.
Empirical results demonstrate that V-Shuffle achieves excellent performance
when utilizing multiple style images. Moreover, when applied to a single style
image, V-Shuffle outperforms previous state-of-the-art methods.

</details>


### [86] [InfoAffect: A Dataset for Affective Analysis of Infographics](https://arxiv.org/abs/2511.06404)
*Zihang Fu,Yunchao Wang,Chenyu Huang,Guodao Sun,Ronghua Liang*

Main category: cs.CV

TL;DR: 本文介绍了InfoAffect数据集，这是一个包含3.5k个情感标注样本的数据集，结合了文本内容和真实世界的信息图表，用于探索信息图表的情感维度。


<details>
  <summary>Details</summary>
Motivation: 信息图表被广泛用于传达复杂信息，但由于数据资源稀缺，其情感维度仍未得到充分探索。

Method: 从六个领域收集原始数据，通过预处理、伴随文本优先方法和三种策略进行对齐；构建情感表来约束标注；使用五个最先进的多模态大语言模型分析两种模态，并通过互惠排名融合算法融合输出以获得稳健的情感和置信度。

Result: 通过用户研究验证可用性，并使用复合情感一致性指数评估InfoAffect数据集，总体得分为0.986，表明准确性高。

Conclusion: InfoAffect数据集在探索信息图表情感维度方面具有高准确性和可用性。

Abstract: Infographics are widely used to convey complex information, yet their
affective dimensions remain underexplored due to the scarcity of data
resources. We introduce a 3.5k-sample affect-annotated InfoAffect dataset,
which combines textual content with real-world infographics. We first collect
the raw data from six domains and aligned them via preprocessing, the
accompanied-text-priority method, and three strategies to guarantee the quality
and compliance. After that we construct an affect table and use it to constrain
annotation. Five state-of-the-art multimodal large language models (MLLMs) then
analyze both modalities, and their outputs are fused with Reciprocal Rank
Fusion (RRF) algorithm to yield robust affects and confidences. We conducted a
user study with two experiments to validate usability and assess InfoAffect
dataset using the Composite Affect Consistency Index (CACI), achieving an
overall score of 0.986, which indicates high accuracy.

</details>


### [87] [On Modality Incomplete Infrared-Visible Object Detection: An Architecture Compatibility Perspective](https://arxiv.org/abs/2511.06406)
*Shuo Yang,Yinghui Xing,Shizhou Zhang,Zhilong Niu*

Main category: cs.CV

TL;DR: 本文提出了一种用于红外和可见光目标检测的Scarf Neck模块，通过模态无关的可变形注意力机制，使检测器能够灵活适应训练和推理过程中的单模态或双模态数据。


<details>
  <summary>Details</summary>
Motivation: 当前红外和可见光目标检测模型在面对不完整模态数据时性能显著下降，特别是在主导模态缺失的情况下。本文从架构兼容性角度深入研究了模态不完整的IVOD问题。

Method: 提出了即插即用的Scarf Neck模块，引入模态无关的可变形注意力机制；设计了伪模态丢弃策略来充分利用多模态信息；建立了全面的模态不完整IVOD基准测试。

Result: Scarf-DETR不仅在模态缺失场景下表现优异，在标准IVOD模态完整基准测试中也达到了优越性能。

Conclusion: 该方法使检测器对单模态和双模态工作模式都具有兼容性和鲁棒性，为解决模态不完整的红外和可见光目标检测问题提供了有效方案。

Abstract: Infrared and visible object detection (IVOD) is essential for numerous
around-the-clock applications. Despite notable advancements, current IVOD
models exhibit notable performance declines when confronted with incomplete
modality data, particularly if the dominant modality is missing. In this paper,
we take a thorough investigation on modality incomplete IVOD problem from an
architecture compatibility perspective. Specifically, we propose a
plug-and-play Scarf Neck module for DETR variants, which introduces a
modality-agnostic deformable attention mechanism to enable the IVOD detector to
flexibly adapt to any single or double modalities during training and
inference. When training Scarf-DETR, we design a pseudo modality dropout
strategy to fully utilize the multi-modality information, making the detector
compatible and robust to both working modes of single and double modalities.
Moreover, we introduce a comprehensive benchmark for the modality-incomplete
IVOD task aimed at thoroughly assessing situations where the absent modality is
either dominant or secondary. Our proposed Scarf-DETR not only performs
excellently in missing modality scenarios but also achieves superior
performances on the standard IVOD modality complete benchmarks. Our code will
be available at https://github.com/YinghuiXing/Scarf-DETR.

</details>


### [88] [VDNeRF: Vision-only Dynamic Neural Radiance Field for Urban Scenes](https://arxiv.org/abs/2511.06408)
*Zhengyu Zou,Jingfeng Li,Hao Li,Xiaolei Hou,Jinwen Hu,Jingkun Chen,Lechao Cheng,Dingwen Zhang*

Main category: cs.CV

TL;DR: VDNeRF是一种仅使用视觉信息的方法，能够准确恢复相机轨迹并学习动态城市场景的时空表示，无需额外的相机姿态信息或昂贵的传感器数据。


<details>
  <summary>Details</summary>
Motivation: 现有基于NeRF的方法在自动驾驶和机器人感知等应用中面临挑战，主要由于难以获取准确相机姿态以及处理大规模动态环境的限制。

Method: 使用两个独立的NeRF模型联合重建场景：静态NeRF模型优化相机姿态和静态背景，动态NeRF模型结合3D场景流确保动态对象的准确一致重建。设计了有效的训练框架解决相机运动与独立物体运动之间的模糊性。

Result: 在主流的城市驾驶数据集上的广泛评估表明，VDNeRF在相机姿态估计和动态新视角合成方面均优于最先进的基于NeRF的无姿态方法。

Conclusion: VDNeRF能够实现鲁棒的相机姿态估计和场景中静态与动态元素的自监督分解，为动态城市场景的表示学习提供了有效解决方案。

Abstract: Neural Radiance Fields (NeRFs) implicitly model continuous three-dimensional
scenes using a set of images with known camera poses, enabling the rendering of
photorealistic novel views. However, existing NeRF-based methods encounter
challenges in applications such as autonomous driving and robotic perception,
primarily due to the difficulty of capturing accurate camera poses and
limitations in handling large-scale dynamic environments. To address these
issues, we propose Vision-only Dynamic NeRF (VDNeRF), a method that accurately
recovers camera trajectories and learns spatiotemporal representations for
dynamic urban scenes without requiring additional camera pose information or
expensive sensor data. VDNeRF employs two separate NeRF models to jointly
reconstruct the scene. The static NeRF model optimizes camera poses and static
background, while the dynamic NeRF model incorporates the 3D scene flow to
ensure accurate and consistent reconstruction of dynamic objects. To address
the ambiguity between camera motion and independent object motion, we design an
effective and powerful training framework to achieve robust camera pose
estimation and self-supervised decomposition of static and dynamic elements in
a scene. Extensive evaluations on mainstream urban driving datasets demonstrate
that VDNeRF surpasses state-of-the-art NeRF-based pose-free methods in both
camera pose estimation and dynamic novel view synthesis.

</details>


### [89] [Diagnose Like A REAL Pathologist: An Uncertainty-Focused Approach for Trustworthy Multi-Resolution Multiple Instance Learning](https://arxiv.org/abs/2511.06433)
*Sungrae Hong,Sol Lee,Jisu Shin,Mun Yong Yi*

Main category: cs.CV

TL;DR: 本文提出了一种不确定性聚焦的校准多实例学习（UFC-MIL）方法，通过使用多分辨率图像来模拟病理学家的检查行为，并提供校准的诊断预测。


<details>
  <summary>Details</summary>
Motivation: 随着组织病理学标本检查和诊断报告需求的增加，多实例学习（MIL）作为AI中心诊断辅助的可行解决方案受到关注。现有基于多分辨率图像的MIL方法虽然性能有所提升，但缺乏对临床专家可信赖的校准MIL研究。

Method: UFC-MIL包含一个新颖的补丁级损失函数，学习实例的潜在模式并表达其分类不确定性；采用基于注意力的架构和邻居补丁聚合模块收集分类器特征；通过补丁级不确定性校准聚合预测，无需多次迭代推理。

Result: 在具有挑战性的公共数据集上，UFC-MIL在模型校准方面表现出优越性能，同时实现了与最先进方法相当的分类准确率。

Conclusion: UFC-MIL能够更接近地模拟病理学家的检查行为，同时提供校准的诊断预测，具有重要的临床应用价值。

Abstract: With the increasing demand for histopathological specimen examination and
diagnostic reporting, Multiple Instance Learning (MIL) has received heightened
research focus as a viable solution for AI-centric diagnostic aid. Recently, to
improve its performance and make it work more like a pathologist, several MIL
approaches based on the use of multiple-resolution images have been proposed,
delivering often higher performance than those that use single-resolution
images. Despite impressive recent developments of multiple-resolution MIL,
previous approaches only focus on improving performance, thereby lacking
research on well-calibrated MIL that clinical experts can rely on for
trustworthy diagnostic results. In this study, we propose Uncertainty-Focused
Calibrated MIL (UFC-MIL), which more closely mimics the pathologists'
examination behaviors while providing calibrated diagnostic predictions, using
multiple images with different resolutions. UFC-MIL includes a novel patch-wise
loss that learns the latent patterns of instances and expresses their
uncertainty for classification. Also, the attention-based architecture with a
neighbor patch aggregation module collects features for the classifier. In
addition, aggregated predictions are calibrated through patch-level uncertainty
without requiring multiple iterative inferences, which is a key practical
advantage. Against challenging public datasets, UFC-MIL shows superior
performance in model calibration while achieving classification accuracy
comparable to that of state-of-the-art methods.

</details>


### [90] [Countering Multi-modal Representation Collapse through Rank-targeted Fusion](https://arxiv.org/abs/2511.06450)
*Seulgi Kim,Kiran Kokilepersaud,Mohit Prabhushankar,Ghassan AlRegib*

Main category: cs.CV

TL;DR: 本文提出了一种名为Rank-enhancing Token Fuser的多模态融合框架，通过有效秩来量化并解决特征崩溃和模态崩溃问题，在动作预测任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态融合方法面临特征崩溃和模态崩溃两大问题，现有方法分别处理这两个问题，缺乏统一的解决框架。

Method: 提出基于有效秩的融合框架，选择性地将信息量较少的特征与互补特征融合，提高融合表示的有效秩；评估能够相互提升有效秩的模态组合。

Result: 在NTURGBD、UTKinect和DARai数据集上的实验表明，该方法比现有最优方法性能提升高达3.74%。

Conclusion: 有效秩是量化多模态融合中特征崩溃和模态崩溃的有效指标，提出的融合框架能显著提升性能，特别是RGB与深度模态的组合能保持表示平衡。

Abstract: Multi-modal fusion methods often suffer from two types of representation
collapse: feature collapse where individual dimensions lose their
discriminative power (as measured by eigenspectra), and modality collapse where
one dominant modality overwhelms the other. Applications like human action
anticipation that require fusing multifarious sensor data are hindered by both
feature and modality collapse. However, existing methods attempt to counter
feature collapse and modality collapse separately. This is because there is no
unifying framework that efficiently addresses feature and modality collapse in
conjunction. In this paper, we posit the utility of effective rank as an
informative measure that can be utilized to quantify and counter both the
representation collapses. We propose \textit{Rank-enhancing Token Fuser}, a
theoretically grounded fusion framework that selectively blends less
informative features from one modality with complementary features from another
modality. We show that our method increases the effective rank of the fused
representation. To address modality collapse, we evaluate modality combinations
that mutually increase each others' effective rank. We show that depth
maintains representational balance when fused with RGB, avoiding modality
collapse. We validate our method on action anticipation, where we present
\texttt{R3D}, a depth-informed fusion framework. Extensive experiments on
NTURGBD, UTKinect, and DARai demonstrate that our approach significantly
outperforms prior state-of-the-art methods by up to 3.74\%. Our code is
available at:
\href{https://github.com/olivesgatech/R3D}{https://github.com/olivesgatech/R3D}.

</details>


### [91] [EIDSeg: A Pixel-Level Semantic Segmentation Dataset for Post-Earthquake Damage Assessment from Social Media Images](https://arxiv.org/abs/2511.06456)
*Huili Huang,Chengeng Liu,Danrong Zhang,Shail Patel,Anastasiya Masalava,Sagar Sadak,Parisa Babolhavaeji,WeiHong Low,Max Mahdi Roozbahani,J. David Frost*

Main category: cs.CV

TL;DR: EIDSeg是首个专门用于震后社交媒体图像语义分割的大规模数据集，包含3,266张来自9次大地震的图像，标注了5类基础设施损坏。研究提出了跨学科标注协议，并发现Encoder-only Mask Transformer模型表现最佳，mIoU达80.8%。


<details>
  <summary>Details</summary>
Motivation: 现有遥感方法依赖昂贵的航拍图像和专家标注，只能生成二值损坏图进行早期评估。虽然社交媒体提供有价值的地面图像来源，但缺乏大规模像素级标注数据集。

Method: 构建EIDSeg数据集，包含3,266张来自9次大地震的图像，标注5类基础设施损坏。提出实用的三阶段跨学科标注协议，使非专家标注者能够进行一致的分割。

Result: 标注协议实现了超过70%的标注者间一致性。基准测试显示Encoder-only Mask Transformer模型表现最佳，mIoU达到80.8%。

Conclusion: 通过解锁社交媒体丰富的地面视角，这项工作为震后场景中更快、更细粒度的损坏评估铺平了道路。

Abstract: Rapid post-earthquake damage assessment is crucial for rescue and resource
planning. Still, existing remote sensing methods depend on costly aerial
images, expert labeling, and produce only binary damage maps for early-stage
evaluation. Although ground-level images from social networks provide a
valuable source to fill this gap, a large pixel-level annotated dataset for
this task is still unavailable. We introduce EIDSeg, the first large-scale
semantic segmentation dataset specifically for post-earthquake social media
imagery. The dataset comprises 3,266 images from nine major earthquakes
(2008-2023), annotated across five classes of infrastructure damage: Undamaged
Building, Damaged Building, Destroyed Building, Undamaged Road, and Damaged
Road. We propose a practical three-phase cross-disciplinary annotation protocol
with labeling guidelines that enables consistent segmentation by non-expert
annotators, achieving over 70% inter-annotator agreement. We benchmark several
state-of-the-art segmentation models, identifying Encoder-only Mask Transformer
(EoMT) as the top-performing method with a Mean Intersection over Union (mIoU)
of 80.8%. By unlocking social networks' rich ground-level perspective, our work
paves the way for a faster, finer-grained damage assessment in the
post-earthquake scenario.

</details>


### [92] [Inpaint360GS: Efficient Object-Aware 3D Inpainting via Gaussian Splatting for 360° Scenes](https://arxiv.org/abs/2511.06457)
*Shaoxiang Wang,Shihong Zhang,Christen Millerdurai,Rüdiger Westermann,Didier Stricker,Alain Pagani*

Main category: cs.CV

TL;DR: Inpaint360GS是一个基于3D高斯泼溅的360度场景编辑框架，解决了复杂360度场景中多目标移除和修复的三大挑战：目标识别、严重遮挡处理和跨视角一致性保持。


<details>
  <summary>Details</summary>
Motivation: 当前基于NeRF和3DGS的单目标正面修复技术已取得进展，但复杂360度场景的修复仍未被充分探索，主要面临目标识别、遮挡处理和跨视角一致性三大挑战。

Method: 通过将2D分割蒸馏到3D空间，并利用虚拟相机视角提供上下文指导，实现准确的目标级编辑和一致的场景补全。还构建了专门用于360度修复的数据集。

Result: 实验表明Inpaint360GS优于现有基线方法，达到了最先进的性能水平。

Conclusion: Inpaint360GS提供了一个灵活的360度编辑框架，有效解决了复杂场景中的多目标移除和高质量修复问题。

Abstract: Despite recent advances in single-object front-facing inpainting using NeRF
and 3D Gaussian Splatting (3DGS), inpainting in complex 360{\deg} scenes
remains largely underexplored. This is primarily due to three key challenges:
(i) identifying target objects in the 3D field of 360{\deg} environments, (ii)
dealing with severe occlusions in multi-object scenes, which makes it hard to
define regions to inpaint, and (iii) maintaining consistent and high-quality
appearance across views effectively. To tackle these challenges, we propose
Inpaint360GS, a flexible 360{\deg} editing framework based on 3DGS that
supports multi-object removal and high-fidelity inpainting in 3D space. By
distilling 2D segmentation into 3D and leveraging virtual camera views for
contextual guidance, our method enables accurate object-level editing and
consistent scene completion. We further introduce a new dataset tailored for
360{\deg} inpainting, addressing the lack of ground truth object-free scenes.
Experiments demonstrate that Inpaint360GS outperforms existing baselines and
achieves state-of-the-art performance. Project page:
https://dfki-av.github.io/inpaint360gs/

</details>


### [93] [NOAH: Benchmarking Narrative Prior driven Hallucination and Omission in Video Large Language Models](https://arxiv.org/abs/2511.06475)
*Kyuho Lee,Euntae Kim,Jinwoo Choi,Buru Chang*

Main category: cs.CV

TL;DR: 该论文提出了NOAH基准，用于评估视频大语言模型中由叙事先验引起的幻觉和遗漏错误，发现大多数模型都存在这类问题且错误模式因架构和事件相似性而异。


<details>
  <summary>Details</summary>
Motivation: 视频大语言模型在追求叙事连贯性时会引入叙事先验偏差，导致幻觉（引入不存在事件）和遗漏（抑制真实事件）错误，需要系统化评估方法。

Method: 构建NOAH基准，通过将其他来源的片段插入目标视频来创建复合视频，控制语义相似性和插入位置，设计字幕任务和三种QA任务（存在性、时间性、叙事性），生成6万多个评估样本。

Result: 大多数视频大语言模型都存在叙事先验驱动的幻觉和遗漏；错误模式因架构、事件相似性和插入位置而异；在帧数较少时对叙事先验的依赖会加剧错误。

Conclusion: NOAH是首个标准化评估视频大语言模型中叙事先验引起幻觉和遗漏的基准，为开发更可靠可信的模型奠定了基础。

Abstract: Video large language models (Video LLMs) have recently achieved strong
performance on tasks such as captioning, summarization, and question answering.
Many models and training methods explicitly encourage continuity across events
to enhance narrative coherence. While this improves fluency, it also introduces
an inductive bias that prioritizes storyline consistency over strict grounding
in visual evidence. We identify this bias, which we call narrative prior, as a
key driver of two errors: hallucinations, where non-existent events are
introduced or existing ones are misinterpreted, and omissions, where factual
events are suppressed because they are misaligned with surrounding context. To
systematically evaluate narrative prior-induced errors, we introduce NOAH, a
large-scale benchmark that constructs composite videos by inserting clips from
other sources into target videos. By varying semantic similarity and insertion
position, our benchmark enables controlled and scalable analysis of narrative
priors. We design one captioning task with tailored metrics and three QA tasks
- Existence, Temporal, and Narrative - yielding more than 60K evaluation
samples. Extensive experiments yield three key findings: (i) most Video LLMs
exhibit hallucinations and omissions driven by narrative priors, (ii) the
patterns of these errors vary across architectures and depend on event
similarity and insertion position, and (iii) reliance on narrative priors
intensifies under sampling with fewer frames, amplifying errors when event
continuity is weak. We establish NOAH as the first standardized evaluation of
narrative prior-induced hallucination and omission in Video LLMs, providing a
foundation for developing more reliable and trustworthy models. Our benchmark
and code are available at https://anonymous550520.github.io/.

</details>


### [94] [Zooming into Comics: Region-Aware RL Improves Fine-Grained Comic Understanding in Vision-Language Models](https://arxiv.org/abs/2511.06490)
*Yule Chen,Yufan Ren,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 该论文提出了AI4VA-FG基准测试，用于评估视觉语言模型在漫画理解方面的能力，发现现有模型表现不佳，并通过后训练策略（特别是区域感知强化学习）显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在自然图像上表现出色，但在处理风格化线条艺术、拟声词和密集多面板布局的漫画时存在困难，需要专门的基准测试和改进方法。

Method: 引入AI4VA-FG细粒度基准测试，评估专有和开源模型，并系统研究后训练策略，包括监督微调和强化学习，特别提出了区域感知强化学习（RARL）方法。

Result: 评估显示现有模型在漫画理解核心任务上存在显著性能缺陷，而RL和RARL方法在Qwen2.5-VL模型上显著提升了低级实体识别和高级故事情节排序能力。

Conclusion: 漫画理解仍是一个未解决的挑战，但通过后训练策略特别是区域感知强化学习，可以为漫画领域的视觉语言模型应用提供更准确和高效的解决方案。

Abstract: Complex visual narratives, such as comics, present a significant challenge to
Vision-Language Models (VLMs). Despite excelling on natural images, VLMs often
struggle with stylized line art, onomatopoeia, and densely packed multi-panel
layouts. To address this gap, we introduce AI4VA-FG, the first fine-grained and
comprehensive benchmark for VLM-based comic understanding. It spans tasks from
foundational recognition and detection to high-level character reasoning and
narrative construction, supported by dense annotations for characters, poses,
and depth. Beyond that, we evaluate state-of-the-art proprietary models,
including GPT-4o and Gemini-2.5, and open-source models such as Qwen2.5-VL,
revealing substantial performance deficits across core tasks of our benchmarks
and underscoring that comic understanding remains an unsolved challenge. To
enhance VLMs' capabilities in this domain, we systematically investigate
post-training strategies, including supervised fine-tuning on solutions
(SFT-S), supervised fine-tuning on reasoning trajectories (SFT-R), and
reinforcement learning (RL). Beyond that, inspired by the emerging "Thinking
with Images" paradigm, we propose Region-Aware Reinforcement Learning (RARL)
for VLMs, which trains models to dynamically attend to relevant regions through
zoom-in operations. We observe that when applied to the Qwen2.5-VL model, RL
and RARL yield significant gains in low-level entity recognition and high-level
storyline ordering, paving the way for more accurate and efficient VLM
applications in the comics domain.

</details>


### [95] [SportR: A Benchmark for Multimodal Large Language Model Reasoning in Sports](https://arxiv.org/abs/2511.06499)
*Haotian Xia,Haonan Ge,Junbo Zou,Hyun Woo Choi,Xuebin Zhang,Danny Suradja,Botao Rui,Ethan Tran,Wendy Jin,Zhen Ye,Xiyang Lin,Christopher Lai,Shengjie Zhang,Junwen Miao,Shichao Chen,Rhys Tracy,Vicente Ordonez,Weining Shen,Hanjie Chen*

Main category: cs.CV

TL;DR: SportR是首个多运动大规模基准测试，旨在训练和评估多模态大语言模型在体育智能推理方面的能力，包含5017张图片和2101个视频，提供7118个高质量人工标注的思维链注释。


<details>
  <summary>Details</summary>
Motivation: 当前体育基准测试要么只覆盖单一运动，要么缺乏详细的推理链和精确的视觉基础，无法在多运动背景下稳健评估模型的核心能力。

Method: 构建包含图像和视频模态的数据集，采用渐进式层次结构的问答对设计，从简单违规识别到复杂处罚预测，并提供人工边界框注释测试视觉基础。

Result: 广泛实验表明该基准测试难度很大，最先进的基线模型在最具挑战性的任务上表现不佳，即使通过监督微调和强化学习进行训练，得分仍然相对较低。

Conclusion: SportR为社区提出了新的挑战，为未来多模态体育推理研究提供了关键资源，突显了当前模型能力与体育智能需求之间的显著差距。

Abstract: Deeply understanding sports requires an intricate blend of fine-grained
visual perception and rule-based reasoning - a challenge that pushes the limits
of current multimodal models. To succeed, models must master three critical
capabilities: perceiving nuanced visual details, applying abstract sport rule
knowledge, and grounding that knowledge in specific visual evidence. Current
sports benchmarks either cover single sports or lack the detailed reasoning
chains and precise visual grounding needed to robustly evaluate these core
capabilities in a multi-sport context. To address this gap, we introduce
SportR, the first multi-sports large-scale benchmark designed to train and
evaluate MLLMs on the fundamental reasoning required for sports intelligence.
Our benchmark provides a dataset of 5,017 images and 2,101 videos. To enable
granular evaluation, we structure our benchmark around a progressive hierarchy
of question-answer (QA) pairs designed to probe reasoning at increasing depths
- from simple infraction identification to complex penalty prediction. For the
most advanced tasks requiring multi-step reasoning, such as determining
penalties or explaining tactics, we provide 7,118 high-quality, human-authored
Chain of Thought (CoT) annotations. In addition, our benchmark incorporates
both image and video modalities and provides manual bounding box annotations to
test visual grounding in the image part directly. Extensive experiments
demonstrate the profound difficulty of our benchmark. State-of-the-art baseline
models perform poorly on our most challenging tasks. While training on our data
via Supervised Fine-Tuning and Reinforcement Learning improves these scores,
they remain relatively low, highlighting a significant gap in current model
capabilities. SportR presents a new challenge for the community, providing a
critical resource to drive future research in multimodal sports reasoning.

</details>


### [96] [Video Dataset for Surgical Phase, Keypoint, and Instrument Recognition in Laparoscopic Surgery (PhaKIR)](https://arxiv.org/abs/2511.06549)
*Tobias Rueckert,Raphaela Maerkl,David Rauber,Leonard Klausmann,Max Gutbrod,Daniel Rueckert,Hubertus Feussner,Dirk Wilhelm,Christoph Palm*

Main category: cs.CV

TL;DR: PhaKIR数据集是首个多中心联合提供手术阶段标签、器械姿态信息和像素级器械分割的数据集，包含8个完整腹腔镜胆囊切除术视频，支持手术阶段识别、器械关键点估计和器械实例分割三个任务。


<details>
  <summary>Details</summary>
Motivation: 现有手术视觉数据集通常处理孤立任务、忽略时间依赖性或缺乏多中心变异性，限制了计算机视觉方法在机器人辅助微创手术中的发展。

Method: 收集了来自三个医疗中心的8个完整腹腔镜胆囊切除术视频，提供帧级标注，包括手术阶段识别（485,875帧）、器械关键点估计（19,435帧）和器械实例分割（19,435帧）。

Result: 创建了首个多机构数据集，联合提供阶段标签、器械姿态信息和像素级器械分割，同时支持利用时间上下文，因为可获得完整的手术程序序列。

Conclusion: PhaKIR数据集为手术场景理解提供了基准，通过EndoVis挑战赛进一步验证了数据集的质量和相关性，现已在Zenodo平台上公开提供。

Abstract: Robotic- and computer-assisted minimally invasive surgery (RAMIS) is
increasingly relying on computer vision methods for reliable instrument
recognition and surgical workflow understanding. Developing such systems often
requires large, well-annotated datasets, but existing resources often address
isolated tasks, neglect temporal dependencies, or lack multi-center
variability. We present the Surgical Procedure Phase, Keypoint, and Instrument
Recognition (PhaKIR) dataset, comprising eight complete laparoscopic
cholecystectomy videos recorded at three medical centers. The dataset provides
frame-level annotations for three interconnected tasks: surgical phase
recognition (485,875 frames), instrument keypoint estimation (19,435 frames),
and instrument instance segmentation (19,435 frames). PhaKIR is, to our
knowledge, the first multi-institutional dataset to jointly provide phase
labels, instrument pose information, and pixel-accurate instrument
segmentations, while also enabling the exploitation of temporal context since
full surgical procedure sequences are available. It served as the basis for the
PhaKIR Challenge as part of the Endoscopic Vision (EndoVis) Challenge at MICCAI
2024 to benchmark methods in surgical scene understanding, thereby further
validating the dataset's quality and relevance. The dataset is publicly
available upon request via the Zenodo platform.

</details>


### [97] [Spatial-Frequency Enhanced Mamba for Multi-Modal Image Fusion](https://arxiv.org/abs/2511.06593)
*Hui Sun,Long Lv,Pingping Zhang,Tongdan Tang,Feng Tian,Weibing Sun,Huchuan Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为SFMFusion的新型多模态图像融合框架，通过空间-频率增强的Mamba块和动态融合机制，解决了传统CNN感受野有限和Transformer计算成本高的问题，在六个MMIF数据集上取得了优于现有方法的结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN或Transformer的多模态图像融合方法存在感受野有限或计算成本高的问题，而Mamba虽然能有效建模长距离依赖关系但缺乏完整的空间和频率感知能力。同时，如何有效利用图像重建作为辅助任务也是一个挑战。

Method: 提出了三分支结构耦合MMIF和图像重建任务，设计空间-频率增强Mamba块(SFMB)在空间和频域增强特征提取，并引入动态融合Mamba块(DFMB)实现跨分支的动态特征融合。

Result: 在六个多模态图像融合数据集上的广泛实验表明，该方法相比大多数最先进方法取得了更好的结果。

Conclusion: SFMFusion框架通过空间-频率增强和动态融合机制，有效解决了多模态图像融合中的关键问题，展现了优异的性能。

Abstract: Multi-Modal Image Fusion (MMIF) aims to integrate complementary image
information from different modalities to produce informative images. Previous
deep learning-based MMIF methods generally adopt Convolutional Neural Networks
(CNNs) or Transformers for feature extraction. However, these methods deliver
unsatisfactory performances due to the limited receptive field of CNNs and the
high computational cost of Transformers. Recently, Mamba has demonstrated a
powerful potential for modeling long-range dependencies with linear complexity,
providing a promising solution to MMIF. Unfortunately, Mamba lacks full spatial
and frequency perceptions, which are very important for MMIF. Moreover,
employing Image Reconstruction (IR) as an auxiliary task has been proven
beneficial for MMIF. However, a primary challenge is how to leverage IR
efficiently and effectively. To address the above issues, we propose a novel
framework named Spatial-Frequency Enhanced Mamba Fusion (SFMFusion) for MMIF.
More specifically, we first propose a three-branch structure to couple MMIF and
IR, which can retain complete contents from source images. Then, we propose the
Spatial-Frequency Enhanced Mamba Block (SFMB), which can enhance Mamba in both
spatial and frequency domains for comprehensive feature extraction. Finally, we
propose the Dynamic Fusion Mamba Block (DFMB), which can be deployed across
different branches for dynamic feature fusion. Extensive experiments show that
our method achieves better results than most state-of-the-art methods on six
MMIF datasets. The source code is available at
https://github.com/SunHui1216/SFMFusion.

</details>


### [98] [Explainable Cross-Disease Reasoning for Cardiovascular Risk Assessment from LDCT](https://arxiv.org/abs/2511.06625)
*Yifei Zhang,Jiashuo Zhang,Xiaofeng Yang,Liang Zhao*

Main category: cs.CV

TL;DR: 提出了一种可解释的跨疾病推理框架，通过单次低剂量胸部CT扫描实现心肺风险评估，模拟临床诊断思维过程，将肺部异常与心血管风险联系起来。


<details>
  <summary>Details</summary>
Motivation: 现有方法将肺部和心脏评估作为独立任务处理，忽略了它们之间的生理相互作用和共享成像生物标志物。低剂量胸部CT天然同时捕捉肺部和心脏结构，为联合评估提供了独特机会。

Method: 框架包含三个协同组件：肺部感知模块总结肺部异常，知识引导推理模块推断其心血管影响，心脏表征模块编码结构生物标志物。通过模拟临床诊断思维过程（感知→推理→判断）进行决策。

Result: 在NLST队列上的实验表明，该框架在心脑血管疾病筛查和死亡率预测方面达到最先进性能，优于单疾病和纯图像基线方法，并提供与心脏病学理解一致的人类可验证推理。

Conclusion: 这项工作建立了一个统一且可解释的范式，用于从低剂量胸部CT进行心血管分析，弥合了基于图像的预测与基于机制的医学解释之间的差距。

Abstract: Low-dose chest computed tomography (LDCT) inherently captures both pulmonary
and cardiac structures, offering a unique opportunity for joint assessment of
lung and cardiovascular health. However, most existing approaches treat these
domains as independent tasks, overlooking their physiological interplay and
shared imaging biomarkers. We propose an Explainable Cross-Disease Reasoning
Framework that enables interpretable cardiopulmonary risk assessment from a
single LDCT scan. The framework introduces an agentic reasoning process that
emulates clinical diagnostic thinking-first perceiving pulmonary findings, then
reasoning through established medical knowledge, and finally deriving a
cardiovascular judgment with explanatory rationale. It integrates three
synergistic components: a pulmonary perception module that summarizes lung
abnormalities, a knowledge-guided reasoning module that infers their
cardiovascular implications, and a cardiac representation module that encodes
structural biomarkers. Their outputs are fused to produce a holistic
cardiovascular risk prediction that is both accurate and physiologically
grounded. Experiments on the NLST cohort demonstrate that the proposed
framework achieves state-of-the-art performance for CVD screening and mortality
prediction, outperforming single-disease and purely image-based baselines.
Beyond quantitative gains, the framework provides human-verifiable reasoning
that aligns with cardiological understanding, revealing coherent links between
pulmonary abnormalities and cardiac stress mechanisms. Overall, this work
establishes a unified and explainable paradigm for cardiovascular analysis from
LDCT, bridging the gap between image-based prediction and mechanism-based
medical interpretation.

</details>


### [99] [DIAL-GS: Dynamic Instance Aware Reconstruction for Label-free Street Scenes with 4D Gaussian Splatting](https://arxiv.org/abs/2511.06632)
*Chenpeng Su,Wenhua Wu,Chensheng Peng,Tianchen Deng,Zhe Liu,Hesheng Wang*

Main category: cs.CV

TL;DR: DIAL-GS是一种基于4D高斯泼溅的动态实例感知重建方法，能够在无标签的街道场景中准确识别动态实例，实现动态自适应和实例感知的3D重建。


<details>
  <summary>Details</summary>
Motivation: 现有监督方法依赖昂贵的人工标注且缺乏可扩展性，而自监督方法往往混淆静态和动态元素，无法区分单个动态对象，限制了细粒度编辑能力。

Method: 通过利用扭曲渲染与实际观察之间的外观-位置不一致性准确识别动态实例，采用实例感知的4D高斯作为统一体积表示，并引入身份和动态相互增强的互惠机制。

Result: 在城市驾驶场景实验中，DIAL-GS在重建质量和实例级编辑方面超越了现有的自监督基线方法。

Conclusion: DIAL-GS为城市场景建模提供了一个简洁而强大的解决方案，实现了高质量的重建和实例级编辑能力。

Abstract: Urban scene reconstruction is critical for autonomous driving, enabling
structured 3D representations for data synthesis and closed-loop testing.
Supervised approaches rely on costly human annotations and lack scalability,
while current self-supervised methods often confuse static and dynamic elements
and fail to distinguish individual dynamic objects, limiting fine-grained
editing. We propose DIAL-GS, a novel dynamic instance-aware reconstruction
method for label-free street scenes with 4D Gaussian Splatting. We first
accurately identify dynamic instances by exploiting appearance-position
inconsistency between warped rendering and actual observation. Guided by
instance-level dynamic perception, we employ instance-aware 4D Gaussians as the
unified volumetric representation, realizing dynamic-adaptive and
instance-aware reconstruction. Furthermore, we introduce a reciprocal mechanism
through which identity and dynamics reinforce each other, enhancing both
integrity and consistency. Experiments on urban driving scenarios show that
DIAL-GS surpasses existing self-supervised baselines in reconstruction quality
and instance-level editing, offering a concise yet powerful solution for urban
scene modeling.

</details>


### [100] [FreqGRL: Suppressing Low-Frequency Bias and Mining High-Frequency Knowledge for Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2511.06648)
*Siqi Hui,Sanping Zhou,Ye deng,Wenli Huang,Jinjun Wang*

Main category: cs.CV

TL;DR: 本文提出FreqGRL框架，从频域角度解决跨域小样本学习中的数据不平衡问题，通过低频替换、高频增强和全局频率滤波来提升模型的跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 跨域小样本学习中，源域数据丰富而目标域数据稀缺的严重不平衡问题阻碍了有效的表示学习。现有方法难以解决模型偏向源域特定知识以及目标数据稀疏导致的高频特征学习困难。

Method: 提出FreqGRL框架：1）低频替换模块用目标域低频分量替换源任务低频分量；2）高频增强模块在频域中直接学习高频特征；3）全局频率滤波器抑制噪声频率并强调信息频率。

Result: 在五个标准CD-FSL基准测试上的广泛实验表明，该频域引导框架实现了最先进的性能。

Conclusion: 从频域角度分析并解决跨域小样本学习中的数据不平衡问题是有效的，FreqGRL框架通过频域操作显著提升了模型的跨域泛化能力。

Abstract: Cross-domain few-shot learning (CD-FSL) aims to recognize novel classes with
only a few labeled examples under significant domain shifts. While recent
approaches leverage a limited amount of labeled target-domain data to improve
performance, the severe imbalance between abundant source data and scarce
target data remains a critical challenge for effective representation learning.
We present the first frequency-space perspective to analyze this issue and
identify two key challenges: (1) models are easily biased toward
source-specific knowledge encoded in the low-frequency components of source
data, and (2) the sparsity of target data hinders the learning of
high-frequency, domain-generalizable features. To address these challenges, we
propose \textbf{FreqGRL}, a novel CD-FSL framework that mitigates the impact of
data imbalance in the frequency space. Specifically, we introduce a
Low-Frequency Replacement (LFR) module that substitutes the low-frequency
components of source tasks with those from the target domain to create new
source tasks that better align with target characteristics, thus reducing
source-specific biases and promoting generalizable representation learning. We
further design a High-Frequency Enhancement (HFE) module that filters out
low-frequency components and performs learning directly on high-frequency
features in the frequency space to improve cross-domain generalization.
Additionally, a Global Frequency Filter (GFF) is incorporated to suppress noisy
or irrelevant frequencies and emphasize informative ones, mitigating
overfitting risks under limited target supervision. Extensive experiments on
five standard CD-FSL benchmarks demonstrate that our frequency-guided framework
achieves state-of-the-art performance.

</details>


### [101] [NOVO: Bridging LLaVA and SAM with Visual-only Prompts for Reasoning Segmentation](https://arxiv.org/abs/2511.06651)
*Kyung-Yoon Yoon,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: NOVO是一个通过纯视觉提示连接视觉语言模型和分割模型的新框架，使用粗掩码和点提示替代文本嵌入，与SAM兼容，并引入无训练细化模块提升边界质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法将文本派生的SEG标记嵌入输入分割模型，NOVO旨在通过纯视觉提示更好地利用预训练分割模型的能力，提升分割质量。

Method: 从VLM输出生成粗掩码和点提示作为视觉提示输入SAM，并引入无训练细化模块减少视觉伪影，实现实例级分割。

Result: 在包含918张图像和2533个实例级掩码的RISeg基准测试中，NOVO在多个指标和模型规模上达到最先进性能。

Conclusion: NOVO在推理分割任务中展现出有效性和可扩展性，通过视觉提示成功连接了视觉语言模型和分割模型。

Abstract: In this study, we propose NOVO (NO text, Visual-Only prompts), a novel
framework that bridges vision-language models (VLMs) and segmentation models
through visual-only prompts. Unlike prior approaches that feed text-derived SEG
token embeddings into segmentation models, NOVO instead generates a coarse mask
and point prompts from the VLM output. These visual prompts are compatible with
the Segment Anything Model (SAM), preserving alignment with its pretrained
capabilities. To further enhance boundary quality and enable instance-level
segmentation, we introduce a training-free refinement module that reduces
visual artifacts and improves the quality of segmentation masks. We also
present RISeg, a new benchmark comprising 918 images, 2,533 instance-level
masks, and diverse reasoning queries to evaluate this task. Experiments
demonstrate that NOVO achieves state-of-the-art performance across multiple
metrics and model sizes, demonstrating its effectiveness and scalability in
reasoning segmentation.

</details>


### [102] [HiMo-CLIP: Modeling Semantic Hierarchy and Monotonicity in Vision-Language Alignment](https://arxiv.org/abs/2511.06653)
*Ruijia Wu,Ping Chen,Fei Shen,Shaoan Zhao,Qiang Hui,Huanlin Gao,Ting Lu,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: HiMo-CLIP通过引入层次分解模块和单调性感知对比损失，增强CLIP模型处理复杂、组合性和长文本描述的能力，在图像-文本检索任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP模型将文本视为扁平序列，无法有效处理复杂、组合性和长文本描述，特别是缺乏对语义层次结构和语义单调性的建模能力。

Method: 提出HiMo-CLIP框架，包含层次分解模块通过批内PCA提取潜在语义组件，以及单调性感知对比损失联合对齐全局和组件级表示。

Result: 在多个图像-文本检索基准测试中，HiMo-CLIP持续优于强基线模型，特别是在处理长文本或组合描述时表现突出。

Conclusion: HiMo-CLIP能够生成结构化、认知对齐的跨模态表示，有效解决了CLIP模型在处理复杂语言结构时的局限性。

Abstract: Contrastive vision-language models like CLIP have achieved impressive results
in image-text retrieval by aligning image and text representations in a shared
embedding space. However, these models often treat text as flat sequences,
limiting their ability to handle complex, compositional, and long-form
descriptions. In particular, they fail to capture two essential properties of
language: semantic hierarchy, which reflects the multi-level compositional
structure of text, and semantic monotonicity, where richer descriptions should
result in stronger alignment with visual content.To address these limitations,
we propose HiMo-CLIP, a representation-level framework that enhances CLIP-style
models without modifying the encoder architecture. HiMo-CLIP introduces two key
components: a hierarchical decomposition (HiDe) module that extracts latent
semantic components from long-form text via in-batch PCA, enabling flexible,
batch-aware alignment across different semantic granularities, and a
monotonicity-aware contrastive loss (MoLo) that jointly aligns global and
component-level representations, encouraging the model to internalize semantic
ordering and alignment strength as a function of textual completeness.These
components work in concert to produce structured, cognitively-aligned
cross-modal representations. Experiments on multiple image-text retrieval
benchmarks show that HiMo-CLIP consistently outperforms strong baselines,
particularly under long or compositional descriptions. The code is available at
https://github.com/UnicomAI/HiMo-CLIP.

</details>


### [103] [Active Learning for Animal Re-Identification with Ambiguity-Aware Sampling](https://arxiv.org/abs/2511.06658)
*Depanshu Sani,Mehar Khurana,Saket Anand*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的主动学习动物重识别框架，通过互补聚类方法挖掘嵌入空间中结构模糊区域的样本对，利用must-link和cannot-link约束进行简单标注，仅需0.033%的标注量就能显著超越现有基础模型、无监督学习和主动学习方法。


<details>
  <summary>Details</summary>
Motivation: 动物重识别在生物多样性监测中具有重要价值，但面临细微区分模式、新物种处理和开放集特性等挑战。现有基础模型的零样本重识别性能存在显著差距，而无监督和主动学习方法在动物重识别中表现不佳，需要大量标注且依赖领域专家。

Method: 提出新颖的主动学习重识别框架，利用互补聚类方法发现嵌入空间中的结构模糊区域，挖掘既具信息性又具广泛代表性的样本对。通过must-link和cannot-link约束的简单标注界面，与现有无监督学习方法集成，采用约束聚类优化算法。

Result: 在13个野生动物数据集上，仅使用0.033%的标注量，平均mAP分别比基础模型、无监督学习和主动学习方法提高10.49%、11.19%和3.99%，在每个数据集上都达到最先进性能。在开放世界设置中，对未知个体的性能分别提高11.09%、8.2%和2.06%。

Conclusion: 所提出的主动学习框架通过挖掘结构模糊区域的样本对，结合简单标注界面和约束聚类优化，显著提升了动物重识别的性能，在极低标注成本下超越了现有所有方法，为动物重识别提供了有效的解决方案。

Abstract: Animal Re-ID has recently gained substantial attention in the AI research
community due to its high impact on biodiversity monitoring and unique research
challenges arising from environmental factors. The subtle distinguishing
patterns, handling new species and the inherent open-set nature make the
problem even harder. To address these complexities, foundation models trained
on labeled, large-scale and multi-species animal Re-ID datasets have recently
been introduced to enable zero-shot Re-ID. However, our benchmarking reveals
significant gaps in their zero-shot Re-ID performance for both known and
unknown species. While this highlights the need for collecting labeled data in
new domains, exhaustive annotation for Re-ID is laborious and requires domain
expertise. Our analyses show that existing unsupervised (USL) and AL Re-ID
methods underperform for animal Re-ID. To address these limitations, we
introduce a novel AL Re-ID framework that leverages complementary clustering
methods to uncover and target structurally ambiguous regions in the embedding
space for mining pairs of samples that are both informative and broadly
representative. Oracle feedback on these pairs, in the form of must-link and
cannot-link constraints, facilitates a simple annotation interface, which
naturally integrates with existing USL methods through our proposed constrained
clustering refinement algorithm. Through extensive experiments, we demonstrate
that, by utilizing only 0.033% of all annotations, our approach consistently
outperforms existing foundational, USL and AL baselines. Specifically, we
report an average improvement of 10.49%, 11.19% and 3.99% (mAP) on 13 wildlife
datasets over foundational, USL and AL methods, respectively, while attaining
state-of-the-art performance on each dataset. Furthermore, we also show an
improvement of 11.09%, 8.2% and 2.06% for unknown individuals in an open-world
setting.

</details>


### [104] [Sim4Seg: Boosting Multimodal Multi-disease Medical Diagnosis Segmentation with Region-Aware Vision-Language Similarity Masks](https://arxiv.org/abs/2511.06665)
*Lingran Song,Yucheng Zhou,Jianbing Shen*

Main category: cs.CV

TL;DR: 该论文提出医学诊断分割（MDS）任务，结合医学图像分割和诊断，并构建了M3DS数据集和Sim4Seg框架，通过RVLS2M模块提升诊断分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割模型很少联合探索医学分割和诊断任务，但为患者提供可解释的诊断结果与分割结果同样重要。

Method: 提出Sim4Seg框架，利用区域感知视觉语言相似性到掩码（RVLS2M）模块，并研究MDS任务的测试时缩放策略。

Result: 实验结果表明，该方法在分割和诊断方面均优于基线模型。

Conclusion: 通过联合医学分割和诊断任务，能够为患者提供更全面的可解释性医疗分析结果。

Abstract: Despite significant progress in pixel-level medical image analysis, existing
medical image segmentation models rarely explore medical segmentation and
diagnosis tasks jointly. However, it is crucial for patients that models can
provide explainable diagnoses along with medical segmentation results. In this
paper, we introduce a medical vision-language task named Medical Diagnosis
Segmentation (MDS), which aims to understand clinical queries for medical
images and generate the corresponding segmentation masks as well as diagnostic
results. To facilitate this task, we first present the Multimodal Multi-disease
Medical Diagnosis Segmentation (M3DS) dataset, containing diverse multimodal
multi-disease medical images paired with their corresponding segmentation masks
and diagnosis chain-of-thought, created via an automated diagnosis
chain-of-thought generation pipeline. Moreover, we propose Sim4Seg, a novel
framework that improves the performance of diagnosis segmentation by taking
advantage of the Region-Aware Vision-Language Similarity to Mask (RVLS2M)
module. To improve overall performance, we investigate a test-time scaling
strategy for MDS tasks. Experimental results demonstrate that our method
outperforms the baselines in both segmentation and diagnosis.

</details>


### [105] [AnoStyler: Text-Driven Localized Anomaly Generation via Lightweight Style Transfer](https://arxiv.org/abs/2511.06687)
*Yulim So,Seokho Kang*

Main category: cs.CV

TL;DR: AnoStyler是一个轻量级的零样本异常生成方法，通过文本引导的风格转换将正常图像转换为视觉真实的异常图像，解决了现有方法在视觉真实性、数据依赖性和模型复杂度方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有异常生成方法存在视觉真实性不足、依赖大量真实图像、使用内存密集型模型架构等限制，阻碍了实际部署。

Method: 提出AnoStyler方法，将零样本异常生成构建为文本引导的风格转换。使用单个正常图像及其类别标签和预期缺陷类型，通过类别无关程序生成异常掩码和两类文本提示，利用轻量级U-Net模型和基于CLIP的损失函数进行风格转换。

Result: 在MVTec-AD和VisA数据集上的广泛实验表明，AnoStyler在生成高质量和多样化异常图像方面优于现有方法。

Conclusion: AnoStyler能够生成视觉真实的异常图像，使用这些生成的异常有助于提升异常检测性能。

Abstract: Anomaly generation has been widely explored to address the scarcity of
anomaly images in real-world data. However, existing methods typically suffer
from at least one of the following limitations, hindering their practical
deployment: (1) lack of visual realism in generated anomalies; (2) dependence
on large amounts of real images; and (3) use of memory-intensive, heavyweight
model architectures. To overcome these limitations, we propose AnoStyler, a
lightweight yet effective method that frames zero-shot anomaly generation as
text-guided style transfer. Given a single normal image along with its category
label and expected defect type, an anomaly mask indicating the localized
anomaly regions and two-class text prompts representing the normal and anomaly
states are generated using generalizable category-agnostic procedures. A
lightweight U-Net model trained with CLIP-based loss functions is used to
stylize the normal image into a visually realistic anomaly image, where
anomalies are localized by the anomaly mask and semantically aligned with the
text prompts. Extensive experiments on the MVTec-AD and VisA datasets show that
AnoStyler outperforms existing anomaly generation methods in generating
high-quality and diverse anomaly images. Furthermore, using these generated
anomalies helps enhance anomaly detection performance.

</details>


### [106] [SPAN: Spatial-Projection Alignment for Monocular 3D Object Detection](https://arxiv.org/abs/2511.06702)
*Yifan Wang,Yian Zhao,Fanqi Pu,Xiaochen Yang,Yang Tang,Xi Chen,Wenming Yang*

Main category: cs.CV

TL;DR: 提出SPAN方法解决单目3D检测中解耦预测范式忽略几何约束的问题，通过空间点对齐和3D-2D投影对齐增强几何一致性，结合分层任务学习策略提升训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D检测器采用解耦预测范式分别估计几何中心、深度、尺寸和旋转角度，但忽略了不同属性间的几何协作约束，导致缺乏几何一致性先验，性能受限。

Method: 提出空间投影对齐(SPAN)方法，包含：(1)空间点对齐施加全局空间约束；(2)3D-2D投影对齐确保投影3D框与2D检测框对齐；结合分层任务学习策略渐进式训练。

Result: 实验表明该方法可轻松集成到现有单目3D检测器中，并带来显著的性能提升。

Conclusion: SPAN方法通过增强几何一致性约束，有效解决了单目3D检测中解耦预测的局限性，提升了检测性能。

Abstract: Existing monocular 3D detectors typically tame the pronounced nonlinear
regression of 3D bounding box through decoupled prediction paradigm, which
employs multiple branches to estimate geometric center, depth, dimensions, and
rotation angle separately. Although this decoupling strategy simplifies the
learning process, it inherently ignores the geometric collaborative constraints
between different attributes, resulting in the lack of geometric consistency
prior, thereby leading to suboptimal performance. To address this issue, we
propose novel Spatial-Projection Alignment (SPAN) with two pivotal components:
(i). Spatial Point Alignment enforces an explicit global spatial constraint
between the predicted and ground-truth 3D bounding boxes, thereby rectifying
spatial drift caused by decoupled attribute regression. (ii). 3D-2D Projection
Alignment ensures that the projected 3D box is aligned tightly within its
corresponding 2D detection bounding box on the image plane, mitigating
projection misalignment overlooked in previous works. To ensure training
stability, we further introduce a Hierarchical Task Learning strategy that
progressively incorporates spatial-projection alignment as 3D attribute
predictions refine, preventing early stage error propagation across attributes.
Extensive experiments demonstrate that the proposed method can be easily
integrated into any established monocular 3D detector and delivers significant
performance improvements.

</details>


### [107] [K-Stain: Keypoint-Driven Correspondence for H&E-to-IHC Virtual Staining](https://arxiv.org/abs/2511.06709)
*Sicheng Yang,Zhaohu Xing,Haipeng Zhou,Lei Zhu*

Main category: cs.CV

TL;DR: K-Stain是一个基于关键点的虚拟染色框架，通过利用关键点作为空间对应关系的鲁棒指示器，将H&E图像转换为IHC图像，解决了组织切片不对齐的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法由于组织切片不对齐而难以有效利用空间信息，需要一种能够更精确对齐和整合结构细节的方法来提高合成IHC图像的保真度。

Method: K-Stain包含三个主要组件：分层空间关键点检测器(HSKD)用于识别染色图像中的关键点，关键点感知增强生成器(KEG)在图像生成过程中整合关键点，以及关键点引导判别器(KGD)提高判别器对空间细节的敏感性。

Result: 广泛的实验表明，K-Stain在定量指标和视觉质量方面均优于最先进的方法。

Conclusion: 通过利用相邻切片的上下文信息，K-Stain能够生成更准确和视觉一致的IHC图像，为虚拟染色提供了一种有效的解决方案。

Abstract: Virtual staining offers a promising method for converting Hematoxylin and
Eosin (H&E) images into Immunohistochemical (IHC) images, eliminating the need
for costly chemical processes. However, existing methods often struggle to
utilize spatial information effectively due to misalignment in tissue slices.
To overcome this challenge, we leverage keypoints as robust indicators of
spatial correspondence, enabling more precise alignment and integration of
structural details in synthesized IHC images. We introduce K-Stain, a novel
framework that employs keypoint-based spatial and semantic relationships to
enhance synthesized IHC image fidelity. K-Stain comprises three main
components: (1) a Hierarchical Spatial Keypoint Detector (HSKD) for identifying
keypoints in stain images, (2) a Keypoint-aware Enhancement Generator (KEG)
that integrates these keypoints during image generation, and (3) a Keypoint
Guided Discriminator (KGD) that improves the discriminator's sensitivity to
spatial details. Our approach leverages contextual information from adjacent
slices, resulting in more accurate and visually consistent IHC images.
Extensive experiments show that K-Stain outperforms state-of-the-art methods in
quantitative metrics and visual quality.

</details>


### [108] [MirrorMamba: Towards Scalable and Robust Mirror Detection in Videos](https://arxiv.org/abs/2511.06716)
*Rui Song,Jiaying Lin,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: 本文提出了一种名为MirrorMamba的新型视频镜面检测方法，通过结合感知深度、对应关系和光流等多重线索，并利用Mamba架构的全局感受野和线性复杂度优势，显著提升了镜面检测的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视频镜面检测方法性能有限且鲁棒性不足，主要问题包括过度依赖单一不可靠的动态特征，以及基于CNN的有限感受野或Transformer的二次计算复杂度。

Method: 提出MirrorMamba方法，整合感知深度、对应关系和光流等多重线索；引入基于Mamba的多方向对应关系提取器，利用Mamba空间状态模型的全局感受野和线性复杂度；设计基于Mamba的分层边界增强解码器来解决深度图模糊导致的边界不清问题。

Result: 在基准数据集上的大量实验表明，该方法在视频镜面检测方面优于现有最先进方法；在最具挑战性和代表性的基于图像的镜面检测数据集上，达到了最先进的性能，证明了其鲁棒性和泛化能力。

Conclusion: MirrorMamba是首个在镜面检测领域成功应用Mamba架构的工作，通过多线索融合和Mamba架构的优势，实现了高效、可扩展且性能优越的镜面检测解决方案。

Abstract: Video mirror detection has received significant research attention, yet
existing methods suffer from limited performance and robustness. These
approaches often over-rely on single, unreliable dynamic features, and are
typically built on CNNs with limited receptive fields or Transformers with
quadratic computational complexity. To address these limitations, we propose a
new effective and scalable video mirror detection method, called MirrorMamba.
Our approach leverages multiple cues to adapt to diverse conditions,
incorporating perceived depth, correspondence and optical. We also introduce an
innovative Mamba-based Multidirection Correspondence Extractor, which benefits
from the global receptive field and linear complexity of the emerging Mamba
spatial state model to effectively capture correspondence properties.
Additionally, we design a Mamba-based layer-wise boundary enforcement decoder
to resolve the unclear boundary caused by the blurred depth map. Notably, this
work marks the first successful application of the Mamba-based architecture in
the field of mirror detection. Extensive experiments demonstrate that our
method outperforms existing state-of-the-art approaches for video mirror
detection on the benchmark datasets. Furthermore, on the most challenging and
representative image-based mirror detection dataset, our approach achieves
state-of-the-art performance, proving its robustness and generalizability.

</details>


### [109] [MRT: Learning Compact Representations with Mixed RWKV-Transformer for Extreme Image Compression](https://arxiv.org/abs/2511.06717)
*Han Liu,Hengyu Man,Xingtao Wang,Wenrui Li,Debin Zhao*

Main category: cs.CV

TL;DR: 提出了一种混合RWKV-Transformer架构，将图像编码为更紧凑的一维潜在表示，显著提升了极端图像压缩的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将图像压缩到二维潜在空间，仍保留大量空间冗余，限制了压缩性能。需要更紧凑的表示形式。

Method: 使用混合RWKV-Transformer架构，通过RWKV模块捕获窗口间全局依赖，Transformer块建模窗口内局部冗余，实现一维紧凑表示。还设计了专门的RWKV压缩模型处理中间特征。

Result: 在比特率低于0.02bpp时获得优越重建质量，相比最先进的二维架构GLC，在Kodak和CLIC2020数据集上分别节省43.75%和30.59%的比特率。

Conclusion: MRT框架通过一维潜在表示有效减少了空间冗余，在极端图像压缩任务中实现了显著的性能提升。

Abstract: Recent advances in extreme image compression have revealed that mapping pixel
data into highly compact latent representations can significantly improve
coding efficiency. However, most existing methods compress images into 2-D
latent spaces via convolutional neural networks (CNNs) or Swin Transformers,
which tend to retain substantial spatial redundancy, thereby limiting overall
compression performance. In this paper, we propose a novel Mixed
RWKV-Transformer (MRT) architecture that encodes images into more compact 1-D
latent representations by synergistically integrating the complementary
strengths of linear-attention-based RWKV and self-attention-based Transformer
models. Specifically, MRT partitions each image into fixed-size windows,
utilizing RWKV modules to capture global dependencies across windows and
Transformer blocks to model local redundancies within each window. The
hierarchical attention mechanism enables more efficient and compact
representation learning in the 1-D domain. To further enhance compression
efficiency, we introduce a dedicated RWKV Compression Model (RCM) tailored to
the structure characteristics of the intermediate 1-D latent features in MRT.
Extensive experiments on standard image compression benchmarks validate the
effectiveness of our approach. The proposed MRT framework consistently achieves
superior reconstruction quality at bitrates below 0.02 bits per pixel (bpp).
Quantitative results based on the DISTS metric show that MRT significantly
outperforms the state-of-the-art 2-D architecture GLC, achieving bitrate
savings of 43.75%, 30.59% on the Kodak and CLIC2020 test datasets,
respectively.

</details>


### [110] [Relative Energy Learning for LiDAR Out-of-Distribution Detection](https://arxiv.org/abs/2511.06720)
*Zizhao Li,Zhengkang Xiang,Jiayang Ao,Joseph West,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: 提出REL框架用于LiDAR点云的OOD检测，通过相对能量学习和Point Raise数据合成策略，在SemanticKITTI和STU基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中可靠的OOD检测至关重要，但现有LiDAR OOD方法难以区分罕见异常和常见类别，导致高误报率和安全关键场景中的过度自信错误。

Method: REL框架利用正负logits之间的能量差作为相对评分函数，结合Point Raise数据合成策略生成辅助异常样本而不改变内点语义。

Result: 在SemanticKITTI和STU基准测试中，REL始终以较大优势超越现有方法。

Conclusion: 相对能量建模结合简单的合成异常样本，为开放世界自动驾驶中的可靠OOD检测提供了原则性和可扩展的解决方案。

Abstract: Out-of-distribution (OOD) detection is a critical requirement for reliable
autonomous driving, where safety depends on recognizing road obstacles and
unexpected objects beyond the training distribution. Despite extensive research
on OOD detection in 2D images, direct transfer to 3D LiDAR point clouds has
been proven ineffective. Current LiDAR OOD methods struggle to distinguish rare
anomalies from common classes, leading to high false-positive rates and
overconfident errors in safety-critical settings. We propose Relative Energy
Learning (REL), a simple yet effective framework for OOD detection in LiDAR
point clouds. REL leverages the energy gap between positive (in-distribution)
and negative logits as a relative scoring function, mitigating calibration
issues in raw energy values and improving robustness across various scenes. To
address the absence of OOD samples during training, we propose a lightweight
data synthesis strategy called Point Raise, which perturbs existing point
clouds to generate auxiliary anomalies without altering the inlier semantics.
Evaluated on SemanticKITTI and the Spotting the Unexpected (STU) benchmark, REL
consistently outperforms existing methods by a large margin. Our results
highlight that modeling relative energy, combined with simple synthetic
outliers, provides a principled and scalable solution for reliable OOD
detection in open-world autonomous driving.

</details>


### [111] [AvatarTex: High-Fidelity Facial Texture Reconstruction from Single-Image Stylized Avatars](https://arxiv.org/abs/2511.06721)
*Yuda Qiu,Zitong Xiao,Yiwei Zuo,Zisheng Ye,Weikai Chen,Xiaoguang Han*

Main category: cs.CV

TL;DR: AvatarTex是一个从单张图像重建高保真面部纹理的框架，能够生成风格化和逼真的纹理。它通过三阶段扩散到GAN的流程，结合扩散模型的多样性和GAN的结构化潜在空间，解决了现有方法在风格化头像上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在风格化头像上表现不佳，主要原因是缺乏多样化的多风格数据集以及在非标准纹理中保持几何一致性的挑战。

Method: 提出三阶段扩散到GAN流程：1）基于扩散的修复完成缺失纹理区域；2）基于GAN的潜在优化细化风格和结构一致性；3）基于扩散的重新绘制增强细节。同时引入TexHub数据集，包含20,000个多风格UV纹理。

Result: AvatarTex在多风格面部纹理重建方面达到了新的最先进水平，实现了高质量、拓扑对齐的纹理合成，具有艺术和几何一致性。

Conclusion: 通过结合扩散模型和GAN的优势，AvatarTex成功解决了风格化纹理重建的挑战，TexHub数据集的发布将促进该领域的未来研究。

Abstract: We present AvatarTex, a high-fidelity facial texture reconstruction framework
capable of generating both stylized and photorealistic textures from a single
image. Existing methods struggle with stylized avatars due to the lack of
diverse multi-style datasets and challenges in maintaining geometric
consistency in non-standard textures. To address these limitations, AvatarTex
introduces a novel three-stage diffusion-to-GAN pipeline. Our key insight is
that while diffusion models excel at generating diversified textures, they lack
explicit UV constraints, whereas GANs provide a well-structured latent space
that ensures style and topology consistency. By integrating these strengths,
AvatarTex achieves high-quality topology-aligned texture synthesis with both
artistic and geometric coherence. Specifically, our three-stage pipeline first
completes missing texture regions via diffusion-based inpainting, refines style
and structure consistency using GAN-based latent optimization, and enhances
fine details through diffusion-based repainting. To address the need for a
stylized texture dataset, we introduce TexHub, a high-resolution collection of
20,000 multi-style UV textures with precise UV-aligned layouts. By leveraging
TexHub and our structured diffusion-to-GAN pipeline, AvatarTex establishes a
new state-of-the-art in multi-style facial texture reconstruction. TexHub will
be released upon publication to facilitate future research in this field.

</details>


### [112] [Argus: Quality-Aware High-Throughput Text-to-Image Inference Serving System](https://arxiv.org/abs/2511.06724)
*Shubham Agarwal,Subrata Mitra,Saud Iqbal*

Main category: cs.CV

TL;DR: Argus是一个高吞吐量的文本到图像推理系统，通过智能选择不同近似策略为每个提示选择适当的近似级别，在保持质量的同时满足固定规模集群上的吞吐量目标。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型具有高度计算密集型特性，采用迭代去噪过程生成图像，导致推理时间很长，设计高吞吐量系统面临挑战。研究发现大部分提示可以使用更快的近似模型服务，但需要为每个提示仔细校准近似设置以避免质量下降。

Method: Argus系统为每个提示选择适当的模型和兼容的近似设置，智能切换不同的近似策略来同时满足吞吐量和质量要求。

Result: 在两个真实世界工作负载跟踪上，Argus相比基线实现了10倍更少的延迟服务级别目标违规、10%更高的平均质量和40%更高的吞吐量。

Conclusion: Argus通过智能模型选择和近似策略切换，有效解决了文本到图像推理系统在吞吐量和质量之间的平衡问题，显著提升了系统性能。

Abstract: Text-to-image (T2I) models have gained significant popularity. Most of these
are diffusion models with unique computational characteristics, distinct from
both traditional small-scale ML models and large language models. They are
highly compute-bound and use an iterative denoising process to generate images,
leading to very high inference time. This creates significant challenges in
designing a high-throughput system. We discovered that a large fraction of
prompts can be served using faster, approximated models. However, the
approximation setting must be carefully calibrated for each prompt to avoid
quality degradation. Designing a high-throughput system that assigns each
prompt to the appropriate model and compatible approximation setting remains a
challenging problem. We present Argus, a high-throughput T2I inference system
that selects the right level of approximation for each prompt to maintain
quality while meeting throughput targets on a fixed-size cluster. Argus
intelligently switches between different approximation strategies to satisfy
both throughput and quality requirements. Overall, Argus achieves 10x fewer
latency service-level objective (SLO) violations, 10% higher average quality,
and 40% higher throughput compared to baselines on two real-world workload
traces.

</details>


### [113] [Rethinking Rainy 3D Scene Reconstruction via Perspective Transforming and Brightness Tuning](https://arxiv.org/abs/2511.06734)
*Qianfeng Yang,Xiang Chen,Pengpeng Li,Qiyuan Guan,Guiyue Jin,Jiyu Jin*

Main category: cs.CV

TL;DR: 本文提出了OmniRain3D数据集和REVR-GSNet框架，用于解决雨天多视角图像对3D场景重建的负面影响，通过考虑视角依赖的雨条纹变化和环境亮度降低来提升数据真实性，并实现雨天场景的高保真重建。


<details>
  <summary>Details</summary>
Motivation: 现有数据集忽略了真实雨天3D场景的两个关键特征：雨条纹在2D图像上的视角依赖变化，以及降雨期间云层覆盖导致的环境亮度降低，这导致重建结果不准确和不完整。

Method: 构建了包含视角异质性和亮度动态性的OmniRain3D数据集，提出了REVR-GSNet框架，通过递归亮度增强、高斯基元优化和GS引导的雨消除的联合交替优化，实现端到端的雨天3D场景重建。

Result: 广泛的实验证明了数据集和方法的有效性，能够从雨退化输入中实现干净3D场景的高保真重建。

Conclusion: 本文的数据集和方法为未来多视角图像去雨和雨天3D场景重建研究提供了基础。

Abstract: Rain degrades the visual quality of multi-view images, which are essential
for 3D scene reconstruction, resulting in inaccurate and incomplete
reconstruction results. Existing datasets often overlook two critical
characteristics of real rainy 3D scenes: the viewpoint-dependent variation in
the appearance of rain streaks caused by their projection onto 2D images, and
the reduction in ambient brightness resulting from cloud coverage during
rainfall. To improve data realism, we construct a new dataset named OmniRain3D
that incorporates perspective heterogeneity and brightness dynamicity, enabling
more faithful simulation of rain degradation in 3D scenes. Based on this
dataset, we propose an end-to-end reconstruction framework named REVR-GSNet
(Rain Elimination and Visibility Recovery for 3D Gaussian Splatting).
Specifically, REVR-GSNet integrates recursive brightness enhancement, Gaussian
primitive optimization, and GS-guided rain elimination into a unified
architecture through joint alternating optimization, achieving high-fidelity
reconstruction of clean 3D scenes from rain-degraded inputs. Extensive
experiments show the effectiveness of our dataset and method. Our dataset and
method provide a foundation for future research on multi-view image deraining
and rainy 3D scene reconstruction.

</details>


### [114] [SinSEMI: A One-Shot Image Generation Model and Data-Efficient Evaluation Framework for Semiconductor Inspection Equipment](https://arxiv.org/abs/2511.06740)
*ChunLiang Wu,Xiaochun Li*

Main category: cs.CV

TL;DR: SinSEMI是一种新颖的单样本学习方法，能够从单张光学图像生成多样且高度逼真的图像，解决了半导体设备开发早期数据稀缺的问题。该方法采用多尺度流模型并结合LPIPS能量引导采样，确保感知真实性和输出多样性。


<details>
  <summary>Details</summary>
Motivation: 在半导体设备开发的早期阶段，获取大量原始光学图像面临重大挑战，这种数据稀缺阻碍了AI解决方案在半导体制造中的发展。

Method: SinSEMI采用多尺度流模型，在采样过程中使用LPIPS（学习感知图像块相似度）能量引导，确保感知真实性和输出多样性。

Result: 通过与多种单样本生成技术的比较评估，SinSEMI在视觉质量、定量指标和下游任务方面表现出优越性能。生成的图像具有高保真度和有意义的多样性。

Conclusion: SinSEMI生成的图像既具有高保真度又具有有意义的多样性，适合作为半导体AI应用的训练数据。

Abstract: In the early stages of semiconductor equipment development, obtaining large
quantities of raw optical images poses a significant challenge. This data
scarcity hinder the advancement of AI-powered solutions in semiconductor
manufacturing. To address this challenge, we introduce SinSEMI, a novel
one-shot learning approach that generates diverse and highly realistic images
from single optical image. SinSEMI employs a multi-scale flow-based model
enhanced with LPIPS (Learned Perceptual Image Patch Similarity) energy guidance
during sampling, ensuring both perceptual realism and output variety. We also
introduce a comprehensive evaluation framework tailored for this application,
which enables a thorough assessment using just two reference images. Through
the evaluation against multiple one-shot generation techniques, we demonstrate
SinSEMI's superior performance in visual quality, quantitative measures, and
downstream tasks. Our experimental results demonstrate that SinSEMI-generated
images achieve both high fidelity and meaningful diversity, making them
suitable as training data for semiconductor AI applications.

</details>


### [115] [PointCubeNet: 3D Part-level Reasoning with 3x3x3 Point Cloud Blocks](https://arxiv.org/abs/2511.06744)
*Da-Yeong Kim,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: PointCubeNet是一个无需部件标注的多模态3D理解框架，通过全局和局部分支实现部件级推理，采用伪标签方法和局部损失函数进行无监督训练。


<details>
  <summary>Details</summary>
Motivation: 现有的3D理解方法通常需要部件级标注，这限制了其应用范围。本文旨在开发无需部件标注的3D部件级推理方法，以增强对整体3D对象的理解。

Method: 提出PointCubeNet框架，包含全局分支和局部分支。局部分支采用3x3x3局部块结构，结合伪标签方法和局部损失函数进行无监督训练，实现点云子区域的部件级分析。

Result: 实验结果表明，理解3D对象部件能够增强对整体3D对象的理解。这是首次实现无监督3D部件级推理，并获得了可靠且有意义的结果。

Conclusion: PointCubeNet成功实现了无需部件标注的3D部件级推理，证明了部件级分析对整体3D对象理解的重要性，为无监督3D理解开辟了新方向。

Abstract: In this paper, we propose PointCubeNet, a novel multi-modal 3D understanding
framework that achieves part-level reasoning without requiring any part
annotations. PointCubeNet comprises global and local branches. The proposed
local branch, structured into 3x3x3 local blocks, enables part-level analysis
of point cloud sub-regions with the corresponding local text labels. Leveraging
the proposed pseudo-labeling method and local loss function, PointCubeNet is
effectively trained in an unsupervised manner. The experimental results
demonstrate that understanding 3D object parts enhances the understanding of
the overall 3D object. In addition, this is the first attempt to perform
unsupervised 3D part-level reasoning and achieves reliable and meaningful
results.

</details>


### [116] [Image Restoration via Primal Dual Hybrid Gradient and Flow Generative Model](https://arxiv.org/abs/2511.06748)
*Ji Li,Chao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于原始-对偶混合梯度(PDHG)的通用高效PnP算法，将流匹配生成模型作为先验集成到图像恢复中，支持ℓ₁和ℓ₂范数损失，能够处理非高斯噪声。


<details>
  <summary>Details</summary>
Motivation: 传统PnP方法主要适用于高斯噪声的平滑平方ℓ₂数据保真度，对于更一般的数据保真项适用性不足。需要开发能够处理非高斯噪声(如泊松噪声和脉冲噪声)的通用PnP框架。

Method: 将流匹配生成模型作为先验集成到PnP框架中，基于PDHG方法设计通用高效的算法，用生成模型派生的时变去噪器替换正则化器的邻近算子。

Result: 在去噪、超分辨率、去模糊和修复等多个图像恢复任务中验证了方法的有效性，证明ℓ₁和ℓ₂保真度项在非高斯噪声下优于传统的平方ℓ₂损失。

Conclusion: 提出的PDHG启发的PnP算法计算高效、内存友好，能够处理广泛的保真度项，为处理非高斯噪声的图像恢复问题提供了有效的解决方案。

Abstract: Regularized optimization has been a classical approach to solving imaging
inverse problems, where the regularization term enforces desirable properties
of the unknown image. Recently, the integration of flow matching generative
models into image restoration has garnered significant attention, owing to
their powerful prior modeling capabilities. In this work, we incorporate such
generative priors into a Plug-and-Play (PnP) framework based on proximal
splitting, where the proximal operator associated with the regularizer is
replaced by a time-dependent denoiser derived from the generative model. While
existing PnP methods have achieved notable success in inverse problems with
smooth squared $\ell_2$ data fidelity--typically associated with Gaussian
noise--their applicability to more general data fidelity terms remains
underexplored. To address this, we propose a general and efficient PnP
algorithm inspired by the primal-dual hybrid gradient (PDHG) method. Our
approach is computationally efficient, memory-friendly, and accommodates a wide
range of fidelity terms. In particular, it supports both $\ell_1$ and $\ell_2$
norm-based losses, enabling robustness to non-Gaussian noise types such as
Poisson and impulse noise. We validate our method on several image restoration
tasks, including denoising, super-resolution, deblurring, and inpainting, and
demonstrate that $\ell_1$ and $\ell_2$ fidelity terms outperform the
conventional squared $\ell_2$ loss in the presence of non-Gaussian noise.

</details>


### [117] [Med-SORA: Symptom to Organ Reasoning in Abdomen CT Images](https://arxiv.org/abs/2511.06752)
*You-Kyoung Na,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: Med-SORA是一个用于腹部CT图像中症状到器官推理的框架，通过RAG数据构建、可学习器官锚点的软标签和2D-3D交叉注意力架构，解决了现有医学多模态模型在症状-图像关联方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有医学多模态模型依赖简单的一对一硬标签，过度简化了临床现实中症状与多个器官相关的复杂关系，且主要使用单切片2D特征而缺乏3D信息，无法捕捉完整的解剖上下文。

Method: 提出Med-SORA框架，包括：1）基于RAG的数据集构建；2）使用可学习器官锚点的软标签来捕捉一对多症状-器官关系；3）2D-3D交叉注意力架构融合局部和全局图像特征。

Result: 实验结果表明，Med-SORA优于现有医学多模态模型，能够实现准确的3D临床推理。

Conclusion: 这是医学多模态学习中首个解决症状到器官推理的工作，Med-SORA通过创新的软标签和2D-3D融合方法，显著提升了症状-图像关联分析的准确性。

Abstract: Understanding symptom-image associations is crucial for clinical reasoning.
However, existing medical multimodal models often rely on simple one-to-one
hard labeling, oversimplifying clinical reality where symptoms relate to
multiple organs. In addition, they mainly use single-slice 2D features without
incorporating 3D information, limiting their ability to capture full anatomical
context. In this study, we propose Med-SORA, a framework for symptom-to-organ
reasoning in abdominal CT images. Med-SORA introduces RAG-based dataset
construction, soft labeling with learnable organ anchors to capture one-to-many
symptom-organ relationships, and a 2D-3D cross-attention architecture to fuse
local and global image features. To our knowledge, this is the first work to
address symptom-to-organ reasoning in medical multimodal learning. Experimental
results show that Med-SORA outperforms existing medical multimodal models and
enables accurate 3D clinical reasoning.

</details>


### [118] [Robust and High-Fidelity 3D Gaussian Splatting: Fusing Pose Priors and Geometry Constraints for Texture-Deficient Outdoor Scenes](https://arxiv.org/abs/2511.06765)
*Meijun Guo,Yongliang Shi,Caiyun Liu,Yixiao Feng,Ming Ma,Tinghai Yan,Weining Lu,Bin Liang*

Main category: cs.CV

TL;DR: 本文提出了一种改进的3D高斯泼溅方法，通过结合LiDAR-IMU里程计提供先验位姿约束，并引入法向量约束和有效秩正则化来增强大尺度弱纹理场景的位姿估计和场景表示质量。


<details>
  <summary>Details</summary>
Motivation: 解决大尺度户外弱纹理或重复纹理场景中，传统3D高斯泼溅方法存在的位姿估计不稳定和场景表示失真的问题。

Method: 1. 位姿估计：利用LiDAR-IMU里程计提供先验位姿约束，结合COLMAP进行三角测量和光束法平差优化；2. 场景表示：引入法向量约束和有效秩正则化，与光度损失联合优化高斯基元的方向和形状一致性。

Result: 在公开和自采集数据集上的实验表明：位姿优化时间减少至三分之一，同时保持精度和鲁棒性；场景表示质量显著优于传统3DGS方法，特别是在弱纹理场景中表现出更好的可视化效果。

Conclusion: 所提出的方法有效解决了大尺度弱纹理场景的位姿估计和场景表示问题，在保持效率的同时显著提升了重建质量。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a key rendering pipeline for
digital asset creation due to its balance between efficiency and visual
quality. To address the issues of unstable pose estimation and scene
representation distortion caused by geometric texture inconsistency in large
outdoor scenes with weak or repetitive textures, we approach the problem from
two aspects: pose estimation and scene representation. For pose estimation, we
leverage LiDAR-IMU Odometry to provide prior poses for cameras in large-scale
environments. These prior pose constraints are incorporated into COLMAP's
triangulation process, with pose optimization performed via bundle adjustment.
Ensuring consistency between pixel data association and prior poses helps
maintain both robustness and accuracy. For scene representation, we introduce
normal vector constraints and effective rank regularization to enforce
consistency in the direction and shape of Gaussian primitives. These
constraints are jointly optimized with the existing photometric loss to enhance
the map quality. We evaluate our approach using both public and self-collected
datasets. In terms of pose optimization, our method requires only one-third of
the time while maintaining accuracy and robustness across both datasets. In
terms of scene representation, the results show that our method significantly
outperforms conventional 3DGS pipelines. Notably, on self-collected datasets
characterized by weak or repetitive textures, our approach demonstrates
enhanced visualization capabilities and achieves superior overall performance.
Codes and data will be publicly available at
https://github.com/justinyeah/normal_shape.git.

</details>


### [119] [TiS-TSL: Image-Label Supervised Surgical Video Stereo Matching via Time-Switchable Teacher-Student Learning](https://arxiv.org/abs/2511.06817)
*Rui Wang,Ying Zhou,Hao Wang,Wenwei Zhang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: TiS-TSL提出了一种时间可切换的师生学习框架，用于微创手术中的视频立体匹配，通过统一模型支持三种预测模式，采用两阶段学习策略解决稀疏标注下的时空一致性问题。


<details>
  <summary>Details</summary>
Motivation: 微创手术中的立体匹配对于导航和增强现实至关重要，但由于解剖限制，密集视差监督几乎不可能，通常只能获得少量图像级标签。现有师生学习方法仅限于图像级监督，缺乏时间一致性估计，导致视差预测不稳定和帧间闪烁伪影。

Method: 提出TiS-TSL框架，核心是统一模型支持三种模式：图像预测、前向视频预测和后向视频预测。采用两阶段学习策略：图像到视频阶段将稀疏图像级知识转移到时间建模；视频到视频阶段通过比较前后向预测计算双向时空一致性，过滤噪声伪标签并强制时间一致性。

Result: 在两个公共数据集上的实验结果表明，TiS-TSL优于其他基于图像的最先进方法，TEPE和EPE分别至少提高了2.11%和4.54%。

Conclusion: TiS-TSL通过时间可切换的师生学习框架有效解决了微创手术视频立体匹配中的时空一致性问题，在最小监督条件下实现了稳定且准确的视差预测。

Abstract: Stereo matching in minimally invasive surgery (MIS) is essential for
next-generation navigation and augmented reality. Yet, dense disparity
supervision is nearly impossible due to anatomical constraints, typically
limiting annotations to only a few image-level labels acquired before the
endoscope enters deep body cavities. Teacher-Student Learning (TSL) offers a
promising solution by leveraging a teacher trained on sparse labels to generate
pseudo labels and associated confidence maps from abundant unlabeled surgical
videos. However, existing TSL methods are confined to image-level supervision,
providing only spatial confidence and lacking temporal consistency estimation.
This absence of spatio-temporal reliability results in unstable disparity
predictions and severe flickering artifacts across video frames. To overcome
these challenges, we propose TiS-TSL, a novel time-switchable teacher-student
learning framework for video stereo matching under minimal supervision. At its
core is a unified model that operates in three distinct modes: Image-Prediction
(IP), Forward Video-Prediction (FVP), and Backward Video-Prediction (BVP),
enabling flexible temporal modeling within a single architecture. Enabled by
this unified model, TiS-TSL adopts a two-stage learning strategy. The
Image-to-Video (I2V) stage transfers sparse image-level knowledge to initialize
temporal modeling. The subsequent Video-to-Video (V2V) stage refines temporal
disparity predictions by comparing forward and backward predictions to
calculate bidirectional spatio-temporal consistency. This consistency
identifies unreliable regions across frames, filters noisy video-level pseudo
labels, and enforces temporal coherence. Experimental results on two public
datasets demonstrate that TiS-TSL exceeds other image-based state-of-the-arts
by improving TEPE and EPE by at least 2.11% and 4.54%, respectively..

</details>


### [120] [Integrating Reweighted Least Squares with Plug-and-Play Diffusion Priors for Noisy Image Restoration](https://arxiv.org/abs/2511.06823)
*Ji Li,Chao Wang*

Main category: cs.CV

TL;DR: 提出了一种基于生成扩散先验的即插即用图像恢复框架，用于有效去除包括脉冲噪声在内的各种非高斯噪声类型。


<details>
  <summary>Details</summary>
Motivation: 现有即插即用图像恢复方法主要使用高斯去噪器，而基于生成先验的去噪器在非高斯噪声（如脉冲噪声）处理方面尚未充分探索。

Method: 在MAP估计框架下，采用广义高斯尺度混合损失函数替代传统最小二乘损失，通过迭代重加权最小二乘方法求解优化问题，其中涉及生成先验的近邻步骤通过基于扩散的去噪器高效执行。

Result: 在基准数据集上的实验结果表明，该方法能有效去除非高斯脉冲噪声，并实现优越的恢复性能。

Conclusion: 所提出的基于生成扩散先验的即插即用框架为处理各种噪声分布提供了有效解决方案，特别是在非高斯噪声场景下表现出色。

Abstract: Existing plug-and-play image restoration methods typically employ
off-the-shelf Gaussian denoisers as proximal operators within classical
optimization frameworks based on variable splitting. Recently, denoisers
induced by generative priors have been successfully integrated into regularized
optimization methods for image restoration under Gaussian noise. However, their
application to non-Gaussian noise--such as impulse noise--remains largely
unexplored. In this paper, we propose a plug-and-play image restoration
framework based on generative diffusion priors for robust removal of general
noise types, including impulse noise. Within the maximum a posteriori (MAP)
estimation framework, the data fidelity term is adapted to the specific noise
model. Departing from the conventional least-squares loss used for Gaussian
noise, we introduce a generalized Gaussian scale mixture-based loss, which
approximates a wide range of noise distributions and leads to an $\ell_q$-norm
($0<q\leq2$) fidelity term. This optimization problem is addressed using an
iteratively reweighted least squares (IRLS) approach, wherein the proximal step
involving the generative prior is efficiently performed via a diffusion-based
denoiser. Experimental results on benchmark datasets demonstrate that the
proposed method effectively removes non-Gaussian impulse noise and achieves
superior restoration performance.

</details>


### [121] [MUGSQA: Novel Multi-Uncertainty-Based Gaussian Splatting Quality Assessment Method, Dataset, and Benchmarks](https://arxiv.org/abs/2511.06830)
*Tianang Chen,Jian Jin,Shilv Cai,Zhuangzi Li,Weisi Lin*

Main category: cs.CV

TL;DR: 本文提出了一个统一的多距离主观质量评估方法来评估基于高斯泼溅（GS）的3D对象重建质量，并构建了名为MUGSQA的数据集和两个基准测试。


<details>
  <summary>Details</summary>
Motivation: 随着高斯泼溅（GS）技术在3D对象重建中的快速发展，评估不同GS方法重建的3D对象感知质量成为一个重要挑战。

Method: 提出了统一的多距离主观质量评估方法，模拟人类在实际应用中的观看行为；构建了考虑输入数据多重不确定性的MUGSQA数据集，包括输入视图的数量和分辨率、观看距离以及初始点云精度。

Result: 构建了MUGSQA数据集和两个基准测试：一个用于评估各种GS重建方法在多重不确定性下的鲁棒性，另一个用于评估现有质量评估指标的性能。

Conclusion: 该研究为GS技术的质量评估提供了标准化的数据集和基准测试框架，有助于推动3D重建技术的发展。

Abstract: Gaussian Splatting (GS) has recently emerged as a promising technique for 3D
object reconstruction, delivering high-quality rendering results with
significantly improved reconstruction speed. As variants continue to appear,
assessing the perceptual quality of 3D objects reconstructed with different
GS-based methods remains an open challenge. To address this issue, we first
propose a unified multi-distance subjective quality assessment method that
closely mimics human viewing behavior for objects reconstructed with GS-based
methods in actual applications, thereby better collecting perceptual
experiences. Based on it, we also construct a novel GS quality assessment
dataset named MUGSQA, which is constructed considering multiple uncertainties
of the input data. These uncertainties include the quantity and resolution of
input views, the view distance, and the accuracy of the initial point cloud.
Moreover, we construct two benchmarks: one to evaluate the robustness of
various GS-based reconstruction methods under multiple uncertainties, and the
other to evaluate the performance of existing quality assessment metrics. Our
dataset and benchmark code will be released soon.

</details>


### [122] [ConsistTalk: Intensity Controllable Temporally Consistent Talking Head Generation with Diffusion Noise Search](https://arxiv.org/abs/2511.06833)
*Zhenjie Liu,Jianzhang Lu,Renjie Lu,Cong Liang,Shangfei Wang*

Main category: cs.CV

TL;DR: ConsistTalk是一个强度可控且时序一致的说话头生成框架，通过解耦外观-运动表示和优化推理策略，解决了现有方法中的闪烁、身份漂移和音视频同步问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频扩散模型在音频驱动的肖像动画中存在闪烁、身份漂移和音视频同步不佳的问题，主要源于纠缠的外观-运动表示和不稳定的推理策略。

Method: 提出三个关键技术：1）光流引导的时序模块解耦运动特征；2）通过多模态师生知识蒸馏获得音频到强度模型，实现音视频联合建模；3）扩散噪声初始化策略，在推理时施加背景一致性和运动连续性约束。

Result: 实验表明ConsistTalk在减少闪烁、保持身份和生成时序稳定的高质量说话头视频方面显著优于现有方法。

Conclusion: ConsistTalk通过解耦外观-运动表示和优化推理策略，有效解决了说话头生成中的关键问题，实现了更稳定和自然的动画效果。

Abstract: Recent advancements in video diffusion models have significantly enhanced
audio-driven portrait animation. However, current methods still suffer from
flickering, identity drift, and poor audio-visual synchronization. These issues
primarily stem from entangled appearance-motion representations and unstable
inference strategies. In this paper, we introduce \textbf{ConsistTalk}, a novel
intensity-controllable and temporally consistent talking head generation
framework with diffusion noise search inference. First, we propose \textbf{an
optical flow-guided temporal module (OFT)} that decouples motion features from
static appearance by leveraging facial optical flow, thereby reducing visual
flicker and improving temporal consistency. Second, we present an
\textbf{Audio-to-Intensity (A2I) model} obtained through multimodal
teacher-student knowledge distillation. By transforming audio and facial
velocity features into a frame-wise intensity sequence, the A2I model enables
joint modeling of audio and visual motion, resulting in more natural dynamics.
This further enables fine-grained, frame-wise control of motion dynamics while
maintaining tight audio-visual synchronization. Third, we introduce a
\textbf{diffusion noise initialization strategy (IC-Init)}. By enforcing
explicit constraints on background coherence and motion continuity during
inference-time noise search, we achieve better identity preservation and refine
motion dynamics compared to the current autoregressive strategy. Extensive
experiments demonstrate that ConsistTalk significantly outperforms prior
methods in reducing flicker, preserving identity, and delivering temporally
stable, high-fidelity talking head videos.

</details>


### [123] [NeuroBridge: Bio-Inspired Self-Supervised EEG-to-Image Decoding via Cognitive Priors and Bidirectional Semantic Alignment](https://arxiv.org/abs/2511.06836)
*Wenjiang Zhang,Sifeng Wang,Yuwei Su,Xinyu Li,Chen Zhang,Suyu Zhong*

Main category: cs.CV

TL;DR: NeuroBridge是一个自监督架构，通过认知先验增强和共享语义投影器实现脑电信号与视觉内容的有效跨模态对齐，在视觉神经解码任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉神经解码方法受限于高质量刺激-脑响应数据稀缺以及神经表示与视觉内容之间的语义不匹配问题，需要更有效的跨模态对齐方法。

Method: 提出NeuroBridge架构，包含认知先验增强（CPA）模拟感知变异性，以及共享语义投影器（SSP）建立双向对齐过程，将两种模态特征映射到共享语义空间。

Result: 在200路零样本检索任务中，受试者内场景下top-1准确率提升12.3%达到63.2%，top-5准确率提升10.2%达到89.9%，显著优于现有方法。

Conclusion: NeuroBridge框架在神经视觉解码中表现出有效性、鲁棒性和可扩展性，为脑机接口和人工智能应用提供了重要工具。

Abstract: Visual neural decoding seeks to reconstruct or infer perceived visual stimuli
from brain activity patterns, providing critical insights into human cognition
and enabling transformative applications in brain-computer interfaces and
artificial intelligence. Current approaches, however, remain constrained by the
scarcity of high-quality stimulus-brain response pairs and the inherent
semantic mismatch between neural representations and visual content. Inspired
by perceptual variability and co-adaptive strategy of the biological systems,
we propose a novel self-supervised architecture, named NeuroBridge, which
integrates Cognitive Prior Augmentation (CPA) with Shared Semantic Projector
(SSP) to promote effective cross-modality alignment. Specifically, CPA
simulates perceptual variability by applying asymmetric, modality-specific
transformations to both EEG signals and images, enhancing semantic diversity.
Unlike previous approaches, SSP establishes a bidirectional alignment process
through a co-adaptive strategy, which mutually aligns features from two
modalities into a shared semantic space for effective cross-modal learning.
NeuroBridge surpasses previous state-of-the-art methods under both
intra-subject and inter-subject settings. In the intra-subject scenario, it
achieves the improvements of 12.3% in top-1 accuracy and 10.2% in top-5
accuracy, reaching 63.2% and 89.9% respectively on a 200-way zero-shot
retrieval task. Extensive experiments demonstrate the effectiveness,
robustness, and scalability of the proposed framework for neural visual
decoding.

</details>


### [124] [Gaussian-Augmented Physics Simulation and System Identification with Complex Colliders](https://arxiv.org/abs/2511.06846)
*Federico Vasile,Ri-Zhao Qiu,Lorenzo Natale,Xiaolong Wang*

Main category: cs.CV

TL;DR: AS-DiffMPM是一个可微分MPM框架，能够处理任意形状碰撞体的物理属性估计，解决了现有方法只能处理平面碰撞体的限制。


<details>
  <summary>Details</summary>
Motivation: 现有基于可微分MPM的方法仅限于简化的物体-环境交互（平面碰撞体），无法处理物体与非平面表面碰撞的更具挑战性场景。

Method: 扩展现有方法，引入可微分碰撞处理机制，使目标物体能够与复杂刚体交互，同时保持端到端优化。

Result: AS-DiffMPM可以与各种新颖视图合成方法轻松对接，作为从视觉观察进行系统识别的框架。

Conclusion: 提出的框架能够处理任意形状碰撞体的物理属性估计，扩展了可微分MPM在复杂交互场景中的应用能力。

Abstract: System identification involving the geometry, appearance, and physical
properties from video observations is a challenging task with applications in
robotics and graphics. Recent approaches have relied on fully differentiable
Material Point Method (MPM) and rendering for simultaneous optimization of
these properties. However, they are limited to simplified object-environment
interactions with planar colliders and fail in more challenging scenarios where
objects collide with non-planar surfaces. We propose AS-DiffMPM, a
differentiable MPM framework that enables physical property estimation with
arbitrarily shaped colliders. Our approach extends existing methods by
incorporating a differentiable collision handling mechanism, allowing the
target object to interact with complex rigid bodies while maintaining
end-to-end optimization. We show AS-DiffMPM can be easily interfaced with
various novel view synthesis methods as a framework for system identification
from visual observations.

</details>


### [125] [Distillation Dynamics: Towards Understanding Feature-Based Distillation in Vision Transformers](https://arxiv.org/abs/2511.06848)
*Huiyuan Tian,Bonan Xu Shijian Li*

Main category: cs.CV

TL;DR: 本文分析了特征蒸馏在Vision Transformers中失效的原因，发现ViT具有独特的U形信息处理模式，师生模型之间存在表征范式不匹配问题，导致后期层特征对齐反而损害学生模型性能。


<details>
  <summary>Details</summary>
Motivation: 特征蒸馏在CNN中很有效，但在ViT中意外失效，甚至比简单的logit蒸馏效果更差，需要深入分析这一现象的根本原因。

Method: 提出了"蒸馏动力学"分析框架，结合频谱分析、信息熵度量和激活幅度跟踪，通过频域分析揭示师生模型表征范式不匹配问题。

Result: 发现教师模型在后期层采用分布式高维编码策略，而学生模型由于通道容量有限无法复制，导致后期层特征对齐产生负迁移效应。

Conclusion: ViT的成功知识传递需要超越简单的特征模仿，设计尊重表征约束的压缩策略，为有效的ViT压缩提供了重要理论指导。

Abstract: While feature-based knowledge distillation has proven highly effective for
compressing CNNs, these techniques unexpectedly fail when applied to Vision
Transformers (ViTs), often performing worse than simple logit-based
distillation. We provide the first comprehensive analysis of this phenomenon
through a novel analytical framework termed as ``distillation dynamics",
combining frequency spectrum analysis, information entropy metrics, and
activation magnitude tracking. Our investigation reveals that ViTs exhibit a
distinctive U-shaped information processing pattern: initial compression
followed by expansion. We identify the root cause of negative transfer in
feature distillation: a fundamental representational paradigm mismatch between
teacher and student models. Through frequency-domain analysis, we show that
teacher models employ distributed, high-dimensional encoding strategies in
later layers that smaller student models cannot replicate due to limited
channel capacity. This mismatch causes late-layer feature alignment to actively
harm student performance. Our findings reveal that successful knowledge
transfer in ViTs requires moving beyond naive feature mimicry to methods that
respect these fundamental representational constraints, providing essential
theoretical guidance for designing effective ViTs compression strategies. All
source code and experimental logs are provided in the supplementary material.

</details>


### [126] [Ambiguity-aware Truncated Flow Matching for Ambiguous Medical Image Segmentation](https://arxiv.org/abs/2511.06857)
*Fanding Li,Xiangyu Li,Xianghe Su,Xingyu Qiu,Suyu Dong,Wei Wang,Kuanquan Wang,Gongning Luo,Shuo Li*

Main category: cs.CV

TL;DR: 提出ATFM方法解决医学图像分割中准确性和多样性平衡问题，通过数据层次推理、高斯截断表示和分割流匹配三个创新组件，在LIDC和ISIC3数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决模糊医学图像分割中准确性和多样性预测之间的权衡问题，现有截断扩散概率模型存在预测准确性和多样性纠缠、保真度和合理性不足的挑战。

Method: 提出ATFM方法，包含三个核心组件：1) 数据层次推理 - 在数据分布和数据样本层面分别增强准确性和多样性；2) 高斯截断表示 - 在T_trunc处显式建模为高斯分布；3) 分割流匹配 - 扩展语义感知流变换。

Result: 在LIDC和ISIC3数据集上优于最先进方法，GED和HM-IoU分别提升高达12%和7.3%，同时实现更高效的推理。

Conclusion: ATFM通过新颖的推理范式和专用模型组件，成功解决了模糊医学图像分割中准确性和多样性的平衡问题，取得了显著的性能提升。

Abstract: A simultaneous enhancement of accuracy and diversity of predictions remains a
challenge in ambiguous medical image segmentation (AMIS) due to the inherent
trade-offs. While truncated diffusion probabilistic models (TDPMs) hold strong
potential with a paradigm optimization, existing TDPMs suffer from entangled
accuracy and diversity of predictions with insufficient fidelity and
plausibility. To address the aforementioned challenges, we propose
Ambiguity-aware Truncated Flow Matching (ATFM), which introduces a novel
inference paradigm and dedicated model components. Firstly, we propose
Data-Hierarchical Inference, a redefinition of AMIS-specific inference
paradigm, which enhances accuracy and diversity at data-distribution and
data-sample level, respectively, for an effective disentanglement. Secondly,
Gaussian Truncation Representation (GTR) is introduced to enhance both fidelity
of predictions and reliability of truncation distribution, by explicitly
modeling it as a Gaussian distribution at $T_{\text{trunc}}$ instead of using
sampling-based approximations.Thirdly, Segmentation Flow Matching (SFM) is
proposed to enhance the plausibility of diverse predictions by extending
semantic-aware flow transformation in Flow Matching (FM). Comprehensive
evaluations on LIDC and ISIC3 datasets demonstrate that ATFM outperforms SOTA
methods and simultaneously achieves a more efficient inference. ATFM improves
GED and HM-IoU by up to $12\%$ and $7.3\%$ compared to advanced methods.

</details>


### [127] [VAEVQ: Enhancing Discrete Visual Tokenization through Variational Modeling](https://arxiv.org/abs/2511.06863)
*Sicheng Yang,Xing Hu,Qiang Wu,Dawei Yang*

Main category: cs.CV

TL;DR: VAEVQ是一种改进的向量量化方法，通过变分潜在量化、表示一致性策略和分布一致性正则化来解决传统VQ框架中的潜在空间不平滑、量化前后表示对齐弱以及连续离散域一致性差等问题。


<details>
  <summary>Details</summary>
Motivation: 传统向量量化方法存在潜在空间不平滑、量化前后表示对齐弱、连续离散域一致性差等问题，导致码字学习不稳定和码本利用率低，影响重建和生成任务性能。

Method: 提出VAEVQ框架，包含三个关键组件：1）变分潜在量化（VLQ），用VAE替代AE进行量化；2）表示一致性策略（RCS），自适应调节量化前后特征对齐强度；3）分布一致性正则化（DCR），对齐码本分布与连续潜在分布。

Result: 在两个基准数据集上的广泛实验表明，VAEVQ优于现有最先进方法。

Conclusion: VAEVQ通过结构化潜在空间、增强表示一致性和改善码本利用率，有效提升了向量量化的性能。

Abstract: Vector quantization (VQ) transforms continuous image features into discrete
representations, providing compressed, tokenized inputs for generative models.
However, VQ-based frameworks suffer from several issues, such as non-smooth
latent spaces, weak alignment between representations before and after
quantization, and poor coherence between the continuous and discrete domains.
These issues lead to unstable codeword learning and underutilized codebooks,
ultimately degrading the performance of both reconstruction and downstream
generation tasks. To this end, we propose VAEVQ, which comprises three key
components: (1) Variational Latent Quantization (VLQ), replacing the AE with a
VAE for quantization to leverage its structured and smooth latent space,
thereby facilitating more effective codeword activation; (2) Representation
Coherence Strategy (RCS), adaptively modulating the alignment strength between
pre- and post-quantization features to enhance consistency and prevent
overfitting to noise; and (3) Distribution Consistency Regularization (DCR),
aligning the entire codebook distribution with the continuous latent
distribution to improve utilization. Extensive experiments on two benchmark
datasets demonstrate that VAEVQ outperforms state-of-the-art methods.

</details>


### [128] [Generating an Image From 1,000 Words: Enhancing Text-to-Image With Structured Captions](https://arxiv.org/abs/2511.06876)
*Eyal Gutflaish,Eliran Kachlon,Hezi Zisman,Tal Hacham,Nimrod Sarid,Alexander Visheratin,Saar Huberman,Gal Davidi,Guy Bukchin,Kfir Goldberg,Ron Mokady*

Main category: cs.CV

TL;DR: 本文提出了第一个基于长结构化标题训练的开源文本到图像模型FIBO，通过DimFusion融合机制处理长标题，并引入TaBR评估协议来直接衡量可控性和表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型主要针对短提示训练，导致在稀疏文本输入和丰富视觉输出之间存在不匹配，降低了可控性，限制了专业使用的精确度。

Method: 使用长结构化标题训练模型，每个训练样本都标注相同的细粒度属性；提出DimFusion融合机制高效处理长标题；引入TaBR评估协议通过重建循环评估可控性。

Result: 训练了大规模模型FIBO，在开源模型中实现了最先进的提示对齐效果。

Conclusion: 通过长结构化标题训练和专门的融合机制，显著提升了文本到图像模型的可控性和表达能力，为专业应用提供了更精确的控制。

Abstract: Text-to-image models have rapidly evolved from casual creative tools to
professional-grade systems, achieving unprecedented levels of image quality and
realism. Yet, most models are trained to map short prompts into detailed
images, creating a gap between sparse textual input and rich visual outputs.
This mismatch reduces controllability, as models often fill in missing details
arbitrarily, biasing toward average user preferences and limiting precision for
professional use. We address this limitation by training the first open-source
text-to-image model on long structured captions, where every training sample is
annotated with the same set of fine-grained attributes. This design maximizes
expressive coverage and enables disentangled control over visual factors. To
process long captions efficiently, we propose DimFusion, a fusion mechanism
that integrates intermediate tokens from a lightweight LLM without increasing
token length. We also introduce the Text-as-a-Bottleneck Reconstruction (TaBR)
evaluation protocol. By assessing how well real images can be reconstructed
through a captioning-generation loop, TaBR directly measures controllability
and expressiveness, even for very long captions where existing evaluation
methods fail. Finally, we demonstrate our contributions by training the
large-scale model FIBO, achieving state-of-the-art prompt alignment among
open-source models. Model weights are publicly available at
https://huggingface.co/briaai/FIBO

</details>


### [129] [A Two-Stage System for Layout-Controlled Image Generation using Large Language Models and Diffusion Models](https://arxiv.org/abs/2511.06888)
*Jan-Hendrik Koch,Jonas Krumme,Konrad Gadzicki*

Main category: cs.CV

TL;DR: 本文提出一个两阶段系统来解决文本到图像扩散模型在物体数量和空间布局控制方面的不足。第一阶段使用大语言模型生成结构化布局，第二阶段使用布局条件扩散模型合成符合布局的逼真图像。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型虽然生成能力出色，但缺乏对物体数量和空间排列的精确控制，这限制了其在复杂场景合成中的应用。

Method: 采用两阶段方法：1）使用LLM从物体列表生成结构化布局；2）使用布局条件扩散模型（比较ControlNet和GLIGEN）合成图像。通过任务分解和规则化插入提高物体召回率。

Result: 通过简化初始生成和规则化插入，复杂场景的物体召回率从57.2%提升到99.9%。ControlNet保持文本风格控制但存在物体幻觉，GLIGEN提供更好的布局保真度但降低提示可控性。

Conclusion: 解耦方法在组合控制合成中是可行的，能够成功生成具有指定物体数量和合理空间排列的图像。

Abstract: Text-to-image diffusion models exhibit remarkable generative capabilities,
but lack precise control over object counts and spatial arrangements. This work
introduces a two-stage system to address these compositional limitations. The
first stage employs a Large Language Model (LLM) to generate a structured
layout from a list of objects. The second stage uses a layout-conditioned
diffusion model to synthesize a photorealistic image adhering to this layout.
We find that task decomposition is critical for LLM-based spatial planning; by
simplifying the initial generation to core objects and completing the layout
with rule-based insertion, we improve object recall from 57.2% to 99.9% for
complex scenes. For image synthesis, we compare two leading conditioning
methods: ControlNet and GLIGEN. After domain-specific finetuning on
table-setting datasets, we identify a key trade-off: ControlNet preserves
text-based stylistic control but suffers from object hallucination, while
GLIGEN provides superior layout fidelity at the cost of reduced prompt-based
controllability. Our end-to-end system successfully generates images with
specified object counts and plausible spatial arrangements, demonstrating the
viability of a decoupled approach for compositionally controlled synthesis.

</details>


### [130] [Adaptive Morph-Patch Transformer for Arotic Vessel Segmentation](https://arxiv.org/abs/2511.06897)
*Zhenxi Zhang,Fuchen Zheng,Adnan Iltaf,Yifei Han,Zhenyu Cheng,Yue Du,Bin Li,Tianyong Liu,Shoujun Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种自适应形态块变换器（MPT），用于主动脉血管分割，通过动态生成与复杂血管结构对齐的形态感知块，解决了传统Transformer模型使用固定大小矩形块导致血管结构完整性问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的模型在主动脉血管分割中依赖固定大小的矩形块，这会损害复杂血管结构的完整性，导致分割精度不理想。

Method: 提出自适应MPT架构，包括自适应块划分策略动态生成形态感知块，以及语义聚类注意力方法动态聚合具有相似语义特征的块特征。

Result: 在三个开源数据集（AVT、AortaSeg24和TBAD）上的广泛实验表明，MPT实现了最先进的性能，在复杂血管结构分割方面有显著改进。

Conclusion: MPT通过自适应块划分和语义聚类注意力有效提升了主动脉血管分割的准确性，特别在保持复杂血管结构完整性方面表现出色。

Abstract: Accurate segmentation of aortic vascular structures is critical for
diagnosing and treating cardiovascular diseases.Traditional Transformer-based
models have shown promise in this domain by capturing long-range dependencies
between vascular features. However, their reliance on fixed-size rectangular
patches often influences the integrity of complex vascular structures, leading
to suboptimal segmentation accuracy. To address this challenge, we propose the
adaptive Morph Patch Transformer (MPT), a novel architecture specifically
designed for aortic vascular segmentation. Specifically, MPT introduces an
adaptive patch partitioning strategy that dynamically generates
morphology-aware patches aligned with complex vascular structures. This
strategy can preserve semantic integrity of complex vascular structures within
individual patches. Moreover, a Semantic Clustering Attention (SCA) method is
proposed to dynamically aggregate features from various patches with similar
semantic characteristics. This method enhances the model's capability to
segment vessels of varying sizes, preserving the integrity of vascular
structures. Extensive experiments on three open-source dataset(AVT, AortaSeg24
and TBAD) demonstrate that MPT achieves state-of-the-art performance, with
improvements in segmenting intricate vascular structures.

</details>


### [131] [Mono3DVG-EnSD: Enhanced Spatial-aware and Dimension-decoupled Text Encoding for Monocular 3D Visual Grounding](https://arxiv.org/abs/2511.06908)
*Yuzhen Li,Min Liu,Zhaoyang Li,Yuan Bian,Xueping Wang,Erbo Zhai,Yaonan Wang*

Main category: cs.CV

TL;DR: 本文提出了Mono3DVG-EnSD框架，通过CLIP-LCA动态屏蔽高确定性关键词保留空间描述，以及D2M模块解耦维度特定文本特征，解决单目3D视觉定位中过度依赖关键词和跨维度干扰的问题，在Mono3DRefer数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D视觉定位方法存在两个关键局限：过度依赖明确识别目标对象的高确定性关键词而忽视空间描述；广义文本特征包含2D和3D描述信息，导致在文本指导下精炼视觉特征时产生跨维度干扰。

Method: 提出Mono3DVG-EnSD框架，包含两个核心组件：CLIP-LCA动态屏蔽高确定性关键词保留低确定性隐含空间描述；D2M从广义文本特征中解耦维度特定（2D/3D）文本特征，指导对应维度的视觉特征，确保维度一致的跨模态交互。

Result: 在Mono3DRefer数据集上的综合比较和消融研究表明，该方法在所有指标上均达到最先进性能，特别是在具有挑战性的Far(Acc@0.5)场景中显著提升了+13.54%。

Conclusion: 通过动态关键词屏蔽和维度解耦策略，有效解决了单目3D视觉定位中的关键词依赖和跨维度干扰问题，显著提升了模型对空间关系的理解和定位精度。

Abstract: Monocular 3D Visual Grounding (Mono3DVG) is an emerging task that locates 3D
objects in RGB images using text descriptions with geometric cues. However,
existing methods face two key limitations. Firstly, they often over-rely on
high-certainty keywords that explicitly identify the target object while
neglecting critical spatial descriptions. Secondly, generalized textual
features contain both 2D and 3D descriptive information, thereby capturing an
additional dimension of details compared to singular 2D or 3D visual features.
This characteristic leads to cross-dimensional interference when refining
visual features under text guidance. To overcome these challenges, we propose
Mono3DVG-EnSD, a novel framework that integrates two key components: the
CLIP-Guided Lexical Certainty Adapter (CLIP-LCA) and the Dimension-Decoupled
Module (D2M). The CLIP-LCA dynamically masks high-certainty keywords while
retaining low-certainty implicit spatial descriptions, thereby forcing the
model to develop a deeper understanding of spatial relationships in captions
for object localization. Meanwhile, the D2M decouples dimension-specific
(2D/3D) textual features from generalized textual features to guide
corresponding visual features at same dimension, which mitigates
cross-dimensional interference by ensuring dimensionally-consistent cross-modal
interactions. Through comprehensive comparisons and ablation studies on the
Mono3DRefer dataset, our method achieves state-of-the-art (SOTA) performance
across all metrics. Notably, it improves the challenging Far(Acc@0.5) scenario
by a significant +13.54%.

</details>


### [132] [DTTNet: Improving Video Shadow Detection via Dark-Aware Guidance and Tokenized Temporal Modeling](https://arxiv.org/abs/2511.06925)
*Zhicheng Li,Kunyang Sun,Rui Yao,Hancheng Zhu,Fuyuan Hu,Jiaqi Zhao,Zhiwen Shao,Yong Zhou*

Main category: cs.CV

TL;DR: 提出DTTNet方法，通过视觉语言匹配模块和暗感知语义块解决阴影-背景模糊问题，使用标记化时序块建模动态阴影变形，实现实时高精度视频阴影检测。


<details>
  <summary>Details</summary>
Motivation: 视频阴影检测面临两个交织的困难：从复杂背景中区分阴影，以及在变化光照条件下建模动态阴影变形。

Method: 1. 视觉语言匹配模块(VMM)和暗感知语义块(DSB)提取文本引导特征区分阴影与暗色物体；2. 自适应掩码重加权在训练中降低半影区域权重；3. 在最终解码器阶段应用边缘掩码进行更好监督；4. 标记化时序块(TTB)解耦时空学习，将跨帧阴影语义总结为可学习时序标记。

Result: 在多个基准数据集上的综合实验证明了最先进的准确性和实时推理效率。

Conclusion: 该方法有效解决了视频阴影检测中的阴影-背景模糊和动态阴影建模问题，实现了高精度和实时性能。

Abstract: Video shadow detection confronts two entwined difficulties: distinguishing
shadows from complex backgrounds and modeling dynamic shadow deformations under
varying illumination. To address shadow-background ambiguity, we leverage
linguistic priors through the proposed Vision-language Match Module (VMM) and a
Dark-aware Semantic Block (DSB), extracting text-guided features to explicitly
differentiate shadows from dark objects. Furthermore, we introduce adaptive
mask reweighting to downweight penumbra regions during training and apply edge
masks at the final decoder stage for better supervision. For temporal modeling
of variable shadow shapes, we propose a Tokenized Temporal Block (TTB) that
decouples spatiotemporal learning. TTB summarizes cross-frame shadow semantics
into learnable temporal tokens, enabling efficient sequence encoding with
minimal computation overhead. Comprehensive Experiments on multiple benchmark
datasets demonstrate state-of-the-art accuracy and real-time inference
efficiency. Codes are available at https://github.com/city-cheng/DTTNet.

</details>


### [133] [PlantTraitNet: An Uncertainty-Aware Multimodal Framework for Global-Scale Plant Trait Inference from Citizen Science Data](https://arxiv.org/abs/2511.06943)
*Ayushi Sharma,Johanna Trost,Daniel Lusk,Johannes Dollinger,Julian Schrader,Christian Rossi,Javier Lopatin,Etienne Laliberté,Simon Haberstroh,Jana Eichel,Daniel Mederer,Jose Miguel Cerda-Paredes,Shyam S. Phartyal,Lisa-Maricia Schwarz,Anja Linstädter,Maria Conceição Caldeira,Teja Kattenborn*

Main category: cs.CV

TL;DR: PlantTraitNet利用公民科学植物照片，通过多模态多任务深度学习框架预测四种关键植物性状，并生成全球性状分布图，在准确性上优于现有全球性状产品。


<details>
  <summary>Details</summary>
Motivation: 现有植物性状地图受限于实地测量成本高和地理覆盖稀疏的问题，而公民科学倡议提供了超过5000万张地理标记植物照片的未开发资源。

Method: 提出PlantTraitNet框架，采用多模态多任务不确定性感知深度学习，利用弱监督从公民科学照片中预测植物高度、叶面积、比叶面积和氮含量四种性状。

Result: 通过空间聚合个体性状预测生成全球性状分布图，与独立植被调查数据验证对比，在所有评估性状上均优于现有全球性状产品。

Conclusion: 公民科学图像与计算机视觉和地理空间AI结合，不仅能够实现可扩展的全球性状制图，还能提供更准确的结果，为生态研究和地球系统建模开辟了新途径。

Abstract: Global plant maps of plant traits, such as leaf nitrogen or plant height, are
essential for understanding ecosystem processes, including the carbon and
energy cycles of the Earth system. However, existing trait maps remain limited
by the high cost and sparse geographic coverage of field-based measurements.
Citizen science initiatives offer a largely untapped resource to overcome these
limitations, with over 50 million geotagged plant photographs worldwide
capturing valuable visual information on plant morphology and physiology. In
this study, we introduce PlantTraitNet, a multi-modal, multi-task
uncertainty-aware deep learning framework that predictsfour key plant traits
(plant height, leaf area, specific leaf area, and nitrogen content) from
citizen science photos using weak supervision. By aggregating individual trait
predictions across space, we generate global maps of trait distributions. We
validate these maps against independent vegetation survey data (sPlotOpen) and
benchmark them against leading global trait products. Our results show that
PlantTraitNet consistently outperforms existing trait maps across all evaluated
traits, demonstrating that citizen science imagery, when integrated with
computer vision and geospatial AI, enables not only scalable but also more
accurate global trait mapping. This approach offers a powerful new pathway for
ecological research and Earth system modeling.

</details>


### [134] [From Attribution to Action: Jointly ALIGNing Predictions and Explanations](https://arxiv.org/abs/2511.06944)
*Dongsheng Hong,Chao Chen,Yanhui Chen,Shanshan Lin,Zhihao Chen,Xiangwen Liao*

Main category: cs.CV

TL;DR: ALIGN框架通过联合训练分类器和掩码生成器，使用高质量的任务相关软掩码作为监督信号，在提升模型可解释性的同时改善泛化性能，在领域泛化基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有解释引导学习方法依赖外部标注或启发式分割作为监督信号，这些信号通常质量较低、噪声大且难以扩展，反而可能降低模型性能。

Method: 提出ALIGN框架，以迭代方式联合训练分类器和掩码生成器。掩码生成器学习生成任务相关的软掩码，分类器同时优化预测准确性和其显著性图与学习掩码的对齐度。

Result: 在VLCS和Terra Incognita两个领域泛化基准测试中，ALIGN在分布内和分布外设置下均优于六个强基线方法，同时产生更高质量的解释。

Conclusion: ALIGN通过利用高质量掩码作为指导，有效提升了模型的可解释性和泛化能力，证明了其在生成准确且可解释模型方面的有效性。

Abstract: Explanation-guided learning (EGL) has shown promise in aligning model
predictions with interpretable reasoning, particularly in computer vision
tasks. However, most approaches rely on external annotations or heuristic-based
segmentation to supervise model explanations, which can be noisy, imprecise and
difficult to scale. In this work, we provide both empirical and theoretical
evidence that low-quality supervision signals can degrade model performance
rather than improve it. In response, we propose ALIGN, a novel framework that
jointly trains a classifier and a masker in an iterative manner. The masker
learns to produce soft, task-relevant masks that highlight informative regions,
while the classifier is optimized for both prediction accuracy and alignment
between its saliency maps and the learned masks. By leveraging high-quality
masks as guidance, ALIGN improves both interpretability and generalizability,
showing its superiority across various settings. Experiments on the two domain
generalization benchmarks, VLCS and Terra Incognita, show that ALIGN
consistently outperforms six strong baselines in both in-distribution and
out-of-distribution settings. Besides, ALIGN also yields superior explanation
quality concerning sufficiency and comprehensiveness, highlighting its
effectiveness in producing accurate and interpretable models.

</details>


### [135] [FoCLIP: A Feature-Space Misalignment Framework for CLIP-Based Image Manipulation and Detection](https://arxiv.org/abs/2511.06947)
*Yulin Chen,Zeyuan Wang,Tianyuan Yu,Yingmei Wei,Liang Bai*

Main category: cs.CV

TL;DR: FoCLIP是一个针对CLIP-based图像质量评估指标的对抗攻击框架，通过特征空间错位来欺骗CLIPscore。该框架使用随机梯度下降技术，结合特征对齐、分数分布平衡和像素保护正则化，生成在人类感知上不可识别或语义不匹配但CLIPscore得分高的欺骗图像。


<details>
  <summary>Details</summary>
Motivation: CLIP-based模型的对齐特性使其成为广泛应用的图像质量评估指标，但这种基于CLIP的指标因其精细的多模态对齐而容易受到攻击。

Method: 基于随机梯度下降技术，FoCLIP整合三个关键组件：特征对齐作为核心模块减少图像-文本模态差距，分数分布平衡模块和像素保护正则化，共同优化CLIPscore性能和图像质量之间的多模态输出平衡。

Result: 在十个艺术杰作提示和ImageNet子集上的实验表明，优化后的图像在保持高视觉保真度的同时，CLIPscore显著提高。此外，灰度转换会导致欺骗图像特征退化，CLIPscore显著降低但保持与原始图像的统计一致性。基于此现象提出的颜色通道敏感性驱动的篡改检测机制在标准基准上达到91%的准确率。

Conclusion: 这项工作为基于CLIP的多模态系统中的特征错位建立了实用路径，并提出了相应的防御方法。

Abstract: The well-aligned attribute of CLIP-based models enables its effective
application like CLIPscore as a widely adopted image quality assessment metric.
However, such a CLIP-based metric is vulnerable for its delicate multimodal
alignment. In this work, we propose \textbf{FoCLIP}, a feature-space
misalignment framework for fooling CLIP-based image quality metric. Based on
the stochastic gradient descent technique, FoCLIP integrates three key
components to construct fooling examples: feature alignment as the core module
to reduce image-text modality gaps, the score distribution balance module and
pixel-guard regularization, which collectively optimize multimodal output
equilibrium between CLIPscore performance and image quality. Such a design can
be engineered to maximize the CLIPscore predictions across diverse input
prompts, despite exhibiting either visual unrecognizability or semantic
incongruence with the corresponding adversarial prompts from human perceptual
perspectives. Experiments on ten artistic masterpiece prompts and ImageNet
subsets demonstrate that optimized images can achieve significant improvement
in CLIPscore while preserving high visual fidelity. In addition, we found that
grayscale conversion induces significant feature degradation in fooling images,
exhibiting noticeable CLIPscore reduction while preserving statistical
consistency with original images. Inspired by this phenomenon, we propose a
color channel sensitivity-driven tampering detection mechanism that achieves
91% accuracy on standard benchmarks. In conclusion, this work establishes a
practical pathway for feature misalignment in CLIP-based multimodal systems and
the corresponding defense method.

</details>


### [136] [PADM: A Physics-aware Diffusion Model for Attenuation Correction](https://arxiv.org/abs/2511.06948)
*Trung Kien Pham,Hoang Minh Vu,Anh Duc Chu,Dac Thai Nguyen,Trung Thanh Nguyen,Thao Nguyen Truong,Mai Hong Son,Thanh Trung Nguyen,Phi Le Nguyen*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的CT-free心脏SPECT衰减校正方法PADM，通过物理感知的师生蒸馏机制，仅需非衰减校正输入即可实现衰减伪影校正，并在CardiAC数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 心脏SPECT成像中的衰减伪影严重影响诊断准确性，而混合SPECT/CT系统虽然能缓解此问题，但存在成本高、可及性差和额外辐射暴露等限制，因此需要开发无需CT的衰减校正解决方案。

Method: 提出物理感知衰减校正扩散模型(PADM)，采用基于扩散的生成方法，通过师生蒸馏机制融入显式物理先验，仅使用非衰减校正输入进行训练和推理。

Result: 在CardiAC数据集上的广泛实验表明，PADM在定量指标和视觉评估方面均优于现有最先进的生成模型，提供了卓越的重建保真度。

Conclusion: PADM作为一种CT-free解决方案，成功解决了心脏SPECT中的衰减伪影问题，为临床提供了一种成本效益高且易于推广的衰减校正方法。

Abstract: Attenuation artifacts remain a significant challenge in cardiac Myocardial
Perfusion Imaging (MPI) using Single-Photon Emission Computed Tomography
(SPECT), often compromising diagnostic accuracy and reducing clinical
interpretability. While hybrid SPECT/CT systems mitigate these artifacts
through CT-derived attenuation maps, their high cost, limited accessibility,
and added radiation exposure hinder widespread clinical adoption. In this
study, we propose a novel CT-free solution to attenuation correction in cardiac
SPECT. Specifically, we introduce Physics-aware Attenuation Correction
Diffusion Model (PADM), a diffusion-based generative method that incorporates
explicit physics priors via a teacher--student distillation mechanism. This
approach enables attenuation artifact correction using only
Non-Attenuation-Corrected (NAC) input, while still benefiting from
physics-informed supervision during training. To support this work, we also
introduce CardiAC, a comprehensive dataset comprising 424 patient studies with
paired NAC and Attenuation-Corrected (AC) reconstructions, alongside
high-resolution CT-based attenuation maps. Extensive experiments demonstrate
that PADM outperforms state-of-the-art generative models, delivering superior
reconstruction fidelity across both quantitative metrics and visual assessment.

</details>


### [137] [Learning from the Right Patches: A Two-Stage Wavelet-Driven Masked Autoencoder for Histopathology Representation Learning](https://arxiv.org/abs/2511.06958)
*Raneen Younis,Louay Hamdi,Lukas Chavez,Zahra Ahmadi*

Main category: cs.CV

TL;DR: WISE-MAE提出了一种基于小波变换的补丁选择策略，通过粗到精的两步过程筛选结构丰富的组织区域，提升数字病理学中自监督学习的表示质量。


<details>
  <summary>Details</summary>
Motivation: 传统MAE预训练中的随机补丁采样往往包含无关或噪声区域，限制了模型捕捉有意义的组织模式的能力，需要引入结构性和生物学相关性的选择策略。

Method: 采用小波信息引导的补丁选择框架：在低倍镜下基于小波进行粗筛选定位结构丰富区域，然后在高分辨率下提取详细建模，模拟病理学家的诊断流程。

Result: 在多个癌症数据集（肺、肾、结直肠组织）上的评估显示，WISE-MAE在弱监督下实现了具有竞争力的表示质量和下游分类性能，同时保持效率。

Conclusion: WISE-MAE通过引入结构感知的补丁选择策略，有效提升了数字病理学中自监督表示学习的质量，为组织模式分析提供了更生物学相关的表示。

Abstract: Whole-slide images are central to digital pathology, yet their extreme size
and scarce annotations make self-supervised learning essential. Masked
Autoencoders (MAEs) with Vision Transformer backbones have recently shown
strong potential for histopathology representation learning. However,
conventional random patch sampling during MAE pretraining often includes
irrelevant or noisy regions, limiting the model's ability to capture meaningful
tissue patterns. In this paper, we present a lightweight and domain-adapted
framework that brings structure and biological relevance into MAE-based
learning through a wavelet-informed patch selection strategy. WISE-MAE applies
a two-step coarse-to-fine process: wavelet-based screening at low magnification
to locate structurally rich regions, followed by high-resolution extraction for
detailed modeling. This approach mirrors the diagnostic workflow of
pathologists and improves the quality of learned representations. Evaluations
across multiple cancer datasets, including lung, renal, and colorectal tissues,
show that WISE-MAE achieves competitive representation quality and downstream
classification performance while maintaining efficiency under weak supervision.

</details>


### [138] [Exploring the "Great Unseen" in Medieval Manuscripts: Instance-Level Labeling of Legacy Image Collections with Zero-Shot Models](https://arxiv.org/abs/2511.07004)
*Christofer Meinecke,Estelle Guéville,David Joseph Wrisley*

Main category: cs.CV

TL;DR: 使用先进技术对中世纪手稿页面进行全面分割和描述，为计算机视觉技术创建更丰富的训练数据


<details>
  <summary>Details</summary>
Motivation: 更全面地理论化中世纪手稿页面及其内容，为计算机视觉技术提供更好的训练数据

Method: 使用最先进的技术对手稿页面进行分割和描述

Result: 创建了用于实例分割和多模态模型的中世纪视觉内容训练数据

Conclusion: 该方法有助于提升计算机视觉技术在中世纪手稿分析中的应用

Abstract: We aim to theorize the medieval manuscript page and its contents more
holistically, using state-of-the-art techniques to segment and describe the
entire manuscript folio, for the purpose of creating richer training data for
computer vision techniques, namely instance segmentation, and multimodal models
for medieval-specific visual content.

</details>


### [139] [TrueCity: Real and Simulated Urban Data for Cross-Domain 3D Scene Understanding](https://arxiv.org/abs/2511.07007)
*Duc Nguyen,Yan-Ling Lai,Qilin Zhang,Prabin Gyawali,Benedikt Schwab,Olaf Wysocki,Thomas H. Kolbe*

Main category: cs.CV

TL;DR: TrueCity是首个提供厘米级精度标注的真实世界点云、语义3D城市模型和标注模拟点云的城市场景语义分割基准，用于量化合成到真实的域偏移。


<details>
  <summary>Details</summary>
Motivation: 解决3D语义场景理解中真实世界标注数据有限的问题，以及现有合成数据集无法捕捉真实世界复杂性和传感器噪声导致的域偏移问题。

Method: 创建包含真实世界点云、语义3D城市模型和模拟点云的TrueCity数据集，采用与国际3D城市建模标准对齐的分割类别。

Result: 通过广泛实验量化了域偏移，并展示了利用合成数据增强真实世界3D场景理解的策略。

Conclusion: TrueCity数据集将促进合成到真实域偏移量化方法的进一步发展，并支持通用数据驱动模型的开发。

Abstract: 3D semantic scene understanding remains a long-standing challenge in the 3D
computer vision community. One of the key issues pertains to limited real-world
annotated data to facilitate generalizable models. The common practice to
tackle this issue is to simulate new data. Although synthetic datasets offer
scalability and perfect labels, their designer-crafted scenes fail to capture
real-world complexity and sensor noise, resulting in a synthetic-to-real domain
gap. Moreover, no benchmark provides synchronized real and simulated point
clouds for segmentation-oriented domain shift analysis. We introduce TrueCity,
the first urban semantic segmentation benchmark with cm-accurate annotated
real-world point clouds, semantic 3D city models, and annotated simulated point
clouds representing the same city. TrueCity proposes segmentation classes
aligned with international 3D city modeling standards, enabling consistent
evaluation of synthetic-to-real gap. Our extensive experiments on common
baselines quantify domain shift and highlight strategies for exploiting
synthetic data to enhance real-world 3D scene understanding. We are convinced
that the TrueCity dataset will foster further development of sim-to-real gap
quantification and enable generalizable data-driven models. The data, code, and
3D models are available online: https://tum-gis.github.io/TrueCity/

</details>


### [140] [Performance Decay in Deepfake Detection: The Limitations of Training on Outdated Data](https://arxiv.org/abs/2511.07009)
*Jack Richings,Margaux Leblanc,Ian Groves,Victoria Nockles*

Main category: cs.CV

TL;DR: 本文提出了一种简单有效的两阶段深度伪造检测方法，在当代深度伪造内容上达到99.8%的AUROC，但发现检测性能会随生成技术发展而快速衰减，6个月内召回率下降超过30%。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的不断进步加剧了虚假信息、欺诈和骚扰的威胁，恶意生成的合成内容越来越难以与现实区分，需要开发有效的检测方法。

Method: 采用简单的两阶段检测方法，通过分析发现预测能力主要来自静态帧级伪影而非时间不一致性。

Result: 检测方法在当代深度伪造上表现优异（AUROC>99.8%），但模型在仅6个月后的生成技术上召回率下降超过30%，显示检测性能随威胁演变而显著衰减。

Conclusion: 稳健的深度伪造检测需要持续构建大规模多样化数据集，并开发先进的帧级特征检测器，预测能力主要来自静态帧级伪影。

Abstract: The continually advancing quality of deepfake technology exacerbates the
threats of disinformation, fraud, and harassment by making
maliciously-generated synthetic content increasingly difficult to distinguish
from reality. We introduce a simple yet effective two-stage detection method
that achieves an AUROC of over 99.8% on contemporary deepfakes. However, this
high performance is short-lived. We show that models trained on this data
suffer a recall drop of over 30% when evaluated on deepfakes created with
generation techniques from just six months later, demonstrating significant
decay as threats evolve. Our analysis reveals two key insights for robust
detection. Firstly, continued performance requires the ongoing curation of
large, diverse datasets. Second, predictive power comes primarily from static,
frame-level artifacts, not temporal inconsistencies. The future of effective
deepfake detection therefore depends on rapid data collection and the
development of advanced frame-level feature detectors.

</details>


### [141] [Certified L2-Norm Robustness of 3D Point Cloud Recognition in the Frequency Domain](https://arxiv.org/abs/2511.07029)
*Liang Zhou,Qiming Wang,Tianze Chen*

Main category: cs.CV

TL;DR: FreqCert是一个新颖的3D点云分类认证框架，通过将鲁棒性分析转移到频域来防御结构化扰动，相比传统空间域方法能提供更好的认证鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有认证防御方法只限制点级扰动，忽略了保持单个点但改变整体结构的几何失真，这在安全关键应用中存在风险。

Method: 通过图傅里叶变换将点云转换到频域，使用结构化频率感知子采样生成多个子点云，每个子云独立分类后通过多数投票获得最终预测。

Result: 在ModelNet40和ScanObjectNN数据集上的实验表明，FreqCert在强扰动下始终获得更高的认证准确率和经验准确率。

Conclusion: 频谱表示为实现3D点云识别中可认证鲁棒性提供了有效途径。

Abstract: 3D point cloud classification is a fundamental task in safety-critical
applications such as autonomous driving, robotics, and augmented reality.
However, recent studies reveal that point cloud classifiers are vulnerable to
structured adversarial perturbations and geometric corruptions, posing risks to
their deployment in safety-critical scenarios. Existing certified defenses
limit point-wise perturbations but overlook subtle geometric distortions that
preserve individual points yet alter the overall structure, potentially leading
to misclassification. In this work, we propose FreqCert, a novel certification
framework that departs from conventional spatial domain defenses by shifting
robustness analysis to the frequency domain, enabling structured certification
against global L2-bounded perturbations. FreqCert first transforms the input
point cloud via the graph Fourier transform (GFT), then applies structured
frequency-aware subsampling to generate multiple sub-point clouds. Each
sub-cloud is independently classified by a standard model, and the final
prediction is obtained through majority voting, where sub-clouds are
constructed based on spectral similarity rather than spatial proximity, making
the partitioning more stable under L2 perturbations and better aligned with the
object's intrinsic structure. We derive a closed-form lower bound on the
certified L2 robustness radius and prove its tightness under minimal and
interpretable assumptions, establishing a theoretical foundation for frequency
domain certification. Extensive experiments on the ModelNet40 and ScanObjectNN
datasets demonstrate that FreqCert consistently achieves higher certified
accuracy and empirical accuracy under strong perturbations. Our results suggest
that spectral representations provide an effective pathway toward certifiable
robustness in 3D point cloud recognition.

</details>


### [142] [3D-ANC: Adaptive Neural Collapse for Robust 3D Point Cloud Recognition](https://arxiv.org/abs/2511.07040)
*Yuanmin Huang,Wenxuan Li,Mi Zhang,Xiaohan Zhang,Xiaoyu You,Min Yang*

Main category: cs.CV

TL;DR: 3D-ANC是一种利用神经崩溃机制增强3D点云识别模型对抗鲁棒性的新方法，通过解耦特征空间来抵御多方面的对抗攻击。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云识别模型对对抗扰动存在脆弱性，传统防御机制难以应对不断演变的攻击模式，主要原因是特征空间纠缠使得攻击容易进行。

Method: 结合ETF对齐分类模块和自适应训练框架（包括表示平衡学习和动态特征方向损失），利用神经崩溃机制构建最大可分离的类原型。

Result: 在ModelNet40数据集上，DGCNN的分类准确率从27.2%提升到80.9%，绝对增益53.7%，比领先基线方法高出34.0%。

Conclusion: 3D-ANC能够有效增强3D点云识别模型的对抗鲁棒性，在各种结构模型和数据集上都表现出显著改进。

Abstract: Deep neural networks have recently achieved notable progress in 3D point
cloud recognition, yet their vulnerability to adversarial perturbations poses
critical security challenges in practical deployments. Conventional defense
mechanisms struggle to address the evolving landscape of multifaceted attack
patterns. Through systematic analysis of existing defenses, we identify that
their unsatisfactory performance primarily originates from an entangled feature
space, where adversarial attacks can be performed easily. To this end, we
present 3D-ANC, a novel approach that capitalizes on the Neural Collapse (NC)
mechanism to orchestrate discriminative feature learning. In particular, NC
depicts where last-layer features and classifier weights jointly evolve into a
simplex equiangular tight frame (ETF) arrangement, establishing maximally
separable class prototypes. However, leveraging this advantage in 3D
recognition confronts two substantial challenges: (1) prevalent class imbalance
in point cloud datasets, and (2) complex geometric similarities between object
categories. To tackle these obstacles, our solution combines an ETF-aligned
classification module with an adaptive training framework consisting of
representation-balanced learning (RBL) and dynamic feature direction loss
(FDL). 3D-ANC seamlessly empowers existing models to develop disentangled
feature spaces despite the complexity in 3D data distribution. Comprehensive
evaluations state that 3D-ANC significantly improves the robustness of models
with various structures on two datasets. For instance, DGCNN's classification
accuracy is elevated from 27.2% to 80.9% on ModelNet40 -- a 53.7% absolute gain
that surpasses leading baselines by 34.0%.

</details>


### [143] [RaLD: Generating High-Resolution 3D Radar Point Clouds with Latent Diffusion](https://arxiv.org/abs/2511.07067)
*Ruijie Zhang,Bixin Zeng,Shengpeng Wang,Fuhui Zhou,Wei Wang*

Main category: cs.CV

TL;DR: 本文提出RaLD框架，通过集成场景级视锥LiDAR自编码、顺序不变潜在表示和直接雷达频谱条件，从稀疏毫米波雷达点云生成密集准确的3D点云。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达在恶劣条件下具有鲁棒性且成本低，但其点云稀疏且分辨率低，限制了需要密集准确3D感知的任务。现有生成方法依赖密集体素表示，效率低且难以保留结构细节。

Method: 采用潜在扩散模型(LDMs)，结合场景级视锥LiDAR自编码、顺序不变潜在表示和直接雷达频谱条件，实现更紧凑和表达力强的生成过程。

Result: 实验表明，RaLD能够从原始雷达频谱生成密集准确的3D点云，为恶劣环境下的鲁棒感知提供有前景的解决方案。

Conclusion: RaLD框架有效解决了雷达点云稀疏性问题，通过创新的表示和条件策略实现了高质量的3D点云生成，提升了毫米波雷达在自主系统中的实用性。

Abstract: Millimeter-wave radar offers a promising sensing modality for autonomous
systems thanks to its robustness in adverse conditions and low cost. However,
its utility is significantly limited by the sparsity and low resolution of
radar point clouds, which poses challenges for tasks requiring dense and
accurate 3D perception. Despite that recent efforts have shown great potential
by exploring generative approaches to address this issue, they often rely on
dense voxel representations that are inefficient and struggle to preserve
structural detail. To fill this gap, we make the key observation that latent
diffusion models (LDMs), though successful in other modalities, have not been
effectively leveraged for radar-based 3D generation due to a lack of compatible
representations and conditioning strategies. We introduce RaLD, a framework
that bridges this gap by integrating scene-level frustum-based LiDAR
autoencoding, order-invariant latent representations, and direct radar spectrum
conditioning. These insights lead to a more compact and expressive generation
process. Experiments show that RaLD produces dense and accurate 3D point clouds
from raw radar spectrums, offering a promising solution for robust perception
in challenging environments.

</details>


### [144] [ClusterMine: Robust Label-Free Visual Out-Of-Distribution Detection via Concept Mining from Text Corpora](https://arxiv.org/abs/2511.07068)
*Nikolas Adaloglou,Diana Petrusheva,Mohamed Asker,Felix Michels,Markus Kollmann*

Main category: cs.CV

TL;DR: 本文提出ClusterMine方法，利用文本语料库进行正标签挖掘，实现无需预定义正标签的无监督OOD检测，在CLIP模型上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视觉OOD检测方法依赖预定义的正标签名称，但这些标签在实际中可能不可用、不可靠或由于分布漂移而失效，需要真正的无监督方法。

Method: 提出ClusterMine方法，通过视觉样本一致性（聚类）和零样本图像-文本一致性相结合，从大型文本语料库中提取正概念，无需正标签。

Result: ClusterMine在多种CLIP模型上具有良好扩展性，对协变量分布漂移具有最先进的鲁棒性，且无需正标签即可达到最先进OOD检测性能。

Conclusion: ClusterMine是首个无需正标签即可实现最先进OOD检测性能的方法，为真正无监督OOD检测提供了可行方案。

Abstract: Large-scale visual out-of-distribution (OOD) detection has witnessed
remarkable progress by leveraging vision-language models such as CLIP. However,
a significant limitation of current methods is their reliance on a pre-defined
set of in-distribution (ID) ground-truth label names (positives). These fixed
label names can be unavailable, unreliable at scale, or become less relevant
due to in-distribution shifts after deployment. Towards truly unsupervised OOD
detection, we utilize widely available text corpora for positive label mining,
bypassing the need for positives. In this paper, we utilize widely available
text corpora for positive label mining under a general concept mining paradigm.
Within this framework, we propose ClusterMine, a novel positive label mining
method. ClusterMine is the first method to achieve state-of-the-art OOD
detection performance without access to positive labels. It extracts positive
concepts from a large text corpus by combining visual-only sample consistency
(via clustering) and zero-shot image-text consistency. Our experimental study
reveals that ClusterMine is scalable across a plethora of CLIP models and
achieves state-of-the-art robustness to covariate in-distribution shifts. The
code is available at https://github.com/HHU-MMBS/clustermine_wacv_official.

</details>


### [145] [Pandar128 dataset for lane line detection](https://arxiv.org/abs/2511.07084)
*Filip Beránek,Václav Diviš,Ivan Gruber*

Main category: cs.CV

TL;DR: Pandar128是最大的128线激光雷达车道线检测公开数据集，包含52,000+相机帧和34,000+激光雷达扫描，提供完整传感器标定和同步里程计。同时提出了轻量级基准方法SimpleLidarLane和新的多段线评估指标IAM-F1。


<details>
  <summary>Details</summary>
Motivation: 解决激光雷达车道线检测领域缺乏大规模高质量数据集和标准化评估方法的问题，推动该领域的研究发展。

Method: 1) 构建Pandar128大规模数据集；2) 提出SimpleLidarLane轻量级方法，结合BEV分割、聚类和多段线拟合；3) 提出IAM-F1多段线评估指标，采用插值感知的横向匹配。

Result: SimpleLidarLane方法在各种挑战性条件下（如雨天、稀疏点云）表现优异，证明模块化管道结合高质量数据和原则性评估可与复杂方法竞争。

Conclusion: 高质量数据集、轻量级方法和标准化评估指标的结合能够有效推动激光雷达车道线检测技术的发展，所有数据和代码已公开以支持可复现性。

Abstract: We present Pandar128, the largest public dataset for lane line detection
using a 128-beam LiDAR. It contains over 52,000 camera frames and 34,000 LiDAR
scans, captured in diverse real-world conditions in Germany. The dataset
includes full sensor calibration (intrinsics, extrinsics) and synchronized
odometry, supporting tasks such as projection, fusion, and temporal modeling.
  To complement the dataset, we also introduce SimpleLidarLane, a light-weight
baseline method for lane line reconstruction that combines BEV segmentation,
clustering, and polyline fitting. Despite its simplicity, our method achieves
strong performance under challenging various conditions (e.g., rain, sparse
returns), showing that modular pipelines paired with high-quality data and
principled evaluation can compete with more complex approaches.
  Furthermore, to address the lack of standardized evaluation, we propose a
novel polyline-based metric - Interpolation-Aware Matching F1 (IAM-F1) - that
employs interpolation-aware lateral matching in BEV space.
  All data and code are publicly released to support reproducibility in
LiDAR-based lane detection.

</details>


### [146] [How Bias Binds: Measuring Hidden Associations for Bias Control in Text-to-Image Compositions](https://arxiv.org/abs/2511.07091)
*Jeng-Lin Li,Ming-Ching Chang,Wei-Chao Chen*

Main category: cs.CV

TL;DR: 本文研究了文本到图像生成模型中语义绑定导致的偏见放大问题，提出了偏见依从性评分来量化对象-属性绑定激活的偏见，并开发了无需训练的上下文偏见控制框架，在组合生成任务中实现了超过10%的去偏见改进。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型的偏见研究主要关注单一对象提示，忽略了语义绑定中对象与属性之间的上下文关联对偏见的影响，导致现有去偏见方法在复杂语义场景下失效。

Method: 引入偏见依从性评分来量化对象-属性绑定激活的偏见；开发无需训练的上下文偏见控制框架，通过令牌解耦来消除语义绑定中的偏见。

Result: 在组合生成任务中实现了超过10%的去偏见改进；分析了各种属性-对象绑定的偏见评分和令牌去相关性，揭示了在不破坏基本语义关系的情况下减少偏见的基本挑战。

Conclusion: 当前去偏见方法在语义绑定上下文中的应用存在关键局限性，需要重新评估主流的偏见缓解策略，以解决语义绑定导致的偏见放大问题。

Abstract: Text-to-image generative models often exhibit bias related to sensitive
attributes. However, current research tends to focus narrowly on single-object
prompts with limited contextual diversity. In reality, each object or attribute
within a prompt can contribute to bias. For example, the prompt "an assistant
wearing a pink hat" may reflect female-inclined biases associated with a pink
hat. The neglected joint effects of the semantic binding in the prompts cause
significant failures in current debiasing approaches. This work initiates a
preliminary investigation on how bias manifests under semantic binding, where
contextual associations between objects and attributes influence generative
outcomes. We demonstrate that the underlying bias distribution can be amplified
based on these associations. Therefore, we introduce a bias adherence score
that quantifies how specific object-attribute bindings activate bias. To delve
deeper, we develop a training-free context-bias control framework to explore
how token decoupling can facilitate the debiasing of semantic bindings. This
framework achieves over 10% debiasing improvement in compositional generation
tasks. Our analysis of bias scores across various attribute-object bindings and
token decorrelation highlights a fundamental challenge: reducing bias without
disrupting essential semantic relationships. These findings expose critical
limitations in current debiasing approaches when applied to semantically bound
contexts, underscoring the need to reassess prevailing bias mitigation
strategies.

</details>


### [147] [GEWDiff: Geometric Enhanced Wavelet-based Diffusion Model for Hyperspectral Image Super-resolution](https://arxiv.org/abs/2511.07103)
*Sirui Wang,Jiang He,Natàlia Blasco Andreo,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 提出了一种几何增强小波扩散模型(GEWDiff)，用于高光谱图像的4倍超分辨率重建，解决了传统扩散模型处理高光谱图像时面临的内存限制、几何结构理解不足和收敛行为不直观等问题。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像(HSI)的超分辨率是重要研究方向，但生成建模面临挑战：高光谱维度导致内存密集，传统扩散模型无法直接处理；通用生成模型缺乏对遥感图像中地物拓扑和几何结构的理解；大多数扩散模型在噪声级别优化损失函数，导致收敛行为不直观且生成质量不佳。

Method: 提出GEWDiff框架：1) 引入基于小波的编码器-解码器，将HSI高效压缩到潜在空间同时保留光谱-空间信息；2) 集成几何增强扩散过程以保持几何特征；3) 设计多级损失函数指导扩散过程，促进稳定收敛和改善重建保真度。

Result: 该模型在多个维度上展示了最先进的结果，包括保真度、光谱精度、视觉真实性和清晰度。

Conclusion: GEWDiff成功解决了高光谱图像超分辨率重建中的关键挑战，通过小波压缩、几何增强和多级损失函数实现了高质量的重建效果。

Abstract: Improving the quality of hyperspectral images (HSIs), such as through
super-resolution, is a crucial research area. However, generative modeling for
HSIs presents several challenges. Due to their high spectral dimensionality,
HSIs are too memory-intensive for direct input into conventional diffusion
models. Furthermore, general generative models lack an understanding of the
topological and geometric structures of ground objects in remote sensing
imagery. In addition, most diffusion models optimize loss functions at the
noise level, leading to a non-intuitive convergence behavior and suboptimal
generation quality for complex data. To address these challenges, we propose a
Geometric Enhanced Wavelet-based Diffusion Model (GEWDiff), a novel framework
for reconstructing hyperspectral images at 4-times super-resolution. A
wavelet-based encoder-decoder is introduced that efficiently compresses HSIs
into a latent space while preserving spectral-spatial information. To avoid
distortion during generation, we incorporate a geometry-enhanced diffusion
process that preserves the geometric features. Furthermore, a multi-level loss
function was designed to guide the diffusion process, promoting stable
convergence and improved reconstruction fidelity. Our model demonstrated
state-of-the-art results across multiple dimensions, including fidelity,
spectral accuracy, visual realism, and clarity.

</details>


### [148] [Sparse4DGS: 4D Gaussian Splatting for Sparse-Frame Dynamic Scene Reconstruction](https://arxiv.org/abs/2511.07122)
*Changyue Shi,Chuxiao Yang,Xinyuan Hu,Minghao Chen,Wenwen Pan,Yan Yang,Jiajun Ding,Zhou Yu,Jun Yu*

Main category: cs.CV

TL;DR: Sparse4DGS是首个针对稀疏帧动态场景重建的方法，通过纹理感知的变形正则化和规范优化，解决了在稀疏帧设置下动态重建失败的问题。


<details>
  <summary>Details</summary>
Motivation: 现有动态高斯泼溅方法依赖密集帧视频序列，但在真实场景中由于设备限制，往往只能获取稀疏帧，导致重建质量下降。

Method: 提出纹理感知变形正则化（引入基于纹理的深度对齐损失来调节高斯变形）和纹理感知规范优化（在规范高斯梯度下降过程中加入基于纹理的噪声）。

Result: 在NeRF-Synthetic、HyperNeRF、NeRF-DS和iPhone-4D数据集上的广泛实验表明，该方法在稀疏帧输入下优于现有动态或少量样本技术。

Conclusion: Sparse4DGS成功解决了稀疏帧动态场景重建的挑战，特别是在纹理丰富区域表现优异。

Abstract: Dynamic Gaussian Splatting approaches have achieved remarkable performance
for 4D scene reconstruction. However, these approaches rely on dense-frame
video sequences for photorealistic reconstruction. In real-world scenarios, due
to equipment constraints, sometimes only sparse frames are accessible. In this
paper, we propose Sparse4DGS, the first method for sparse-frame dynamic scene
reconstruction. We observe that dynamic reconstruction methods fail in both
canonical and deformed spaces under sparse-frame settings, especially in areas
with high texture richness. Sparse4DGS tackles this challenge by focusing on
texture-rich areas. For the deformation network, we propose Texture-Aware
Deformation Regularization, which introduces a texture-based depth alignment
loss to regulate Gaussian deformation. For the canonical Gaussian field, we
introduce Texture-Aware Canonical Optimization, which incorporates
texture-based noise into the gradient descent process of canonical Gaussians.
Extensive experiments show that when taking sparse frames as inputs, our method
outperforms existing dynamic or few-shot techniques on NeRF-Synthetic,
HyperNeRF, NeRF-DS, and our iPhone-4D datasets.

</details>


### [149] [MPJudge: Towards Perceptual Assessment of Music-Induced Paintings](https://arxiv.org/abs/2511.07137)
*Shiqi Jiang,Tianyi Liang,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的音乐诱导绘画评估框架，通过直接建模音乐与视觉艺术之间的感知一致性来评估绘画是否忠实反映音乐。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖情感识别模型评估音乐与绘画的相似性，但这类模型引入大量噪声且忽略了情感之外的更广泛感知线索。

Method: 构建了首个大规模音乐绘画配对数据集MPD，并收集了成对偏好标注。提出了MPJudge模型，通过基于调制的融合机制将音乐特征整合到视觉编码器中，并采用直接偏好优化进行训练。

Result: 大量实验表明该方法优于现有方法，定性结果显示模型能更准确地识别绘画中与音乐相关的区域。

Conclusion: 所提出的框架能够有效评估音乐诱导绘画的感知一致性，为音乐与视觉艺术跨模态评估提供了新的解决方案。

Abstract: Music induced painting is a unique artistic practice, where visual artworks
are created under the influence of music. Evaluating whether a painting
faithfully reflects the music that inspired it poses a challenging perceptual
assessment task. Existing methods primarily rely on emotion recognition models
to assess the similarity between music and painting, but such models introduce
considerable noise and overlook broader perceptual cues beyond emotion. To
address these limitations, we propose a novel framework for music induced
painting assessment that directly models perceptual coherence between music and
visual art. We introduce MPD, the first large scale dataset of music painting
pairs annotated by domain experts based on perceptual coherence. To better
handle ambiguous cases, we further collect pairwise preference annotations.
Building on this dataset, we present MPJudge, a model that integrates music
features into a visual encoder via a modulation based fusion mechanism. To
effectively learn from ambiguous cases, we adopt Direct Preference Optimization
for training. Extensive experiments demonstrate that our method outperforms
existing approaches. Qualitative results further show that our model more
accurately identifies music relevant regions in paintings.

</details>


### [150] [ProcGen3D: Learning Neural Procedural Graph Representations for Image-to-3D Reconstruction](https://arxiv.org/abs/2511.07142)
*Xinyi Zhang,Daoyi Gao,Naiqi Li,Angela Dai*

Main category: cs.CV

TL;DR: ProcGen3D是一种通过生成程序化图抽象来创建3D内容的新方法，该方法使用基于图的程序化表示，结合Transformer和蒙特卡洛树搜索，从RGB图像生成3D资产。


<details>
  <summary>Details</summary>
Motivation: 受生产级3D应用中程序化生成器的启发，旨在通过程序化图抽象来创建复杂3D资产，实现从图像到3D的忠实重建。

Method: 采用序列化、基于图的程序化图表示，使用基于边的标记化编码程序化图，训练Transformer先验模型预测下一个标记，并引入蒙特卡洛树搜索引导采样以改善图像对齐。

Result: 在仙人掌、树木和桥梁等对象上的广泛实验表明，该方法在生成3D方面优于最先进的生成方法和特定领域建模技术，并在真实世界图像上表现出良好的泛化能力。

Conclusion: ProcGen3D通过程序化图生成实现了高质量的3D内容创建，即使仅使用合成数据训练也能在真实图像上取得良好效果，为3D资产生成提供了新思路。

Abstract: We introduce ProcGen3D, a new approach for 3D content creation by generating
procedural graph abstractions of 3D objects, which can then be decoded into
rich, complex 3D assets. Inspired by the prevalent use of procedural generators
in production 3D applications, we propose a sequentialized, graph-based
procedural graph representation for 3D assets. We use this to learn to
approximate the landscape of a procedural generator for image-based 3D
reconstruction. We employ edge-based tokenization to encode the procedural
graphs, and train a transformer prior to predict the next token conditioned on
an input RGB image. Crucially, to enable better alignment of our generated
outputs to an input image, we incorporate Monte Carlo Tree Search (MCTS) guided
sampling into our generation process, steering output procedural graphs towards
more image-faithful reconstructions. Our approach is applicable across a
variety of objects that can be synthesized with procedural generators.
Extensive experiments on cacti, trees, and bridges show that our neural
procedural graph generation outperforms both state-of-the-art generative 3D
methods and domain-specific modeling techniques. Furthermore, this enables
improved generalization on real-world input images, despite training only on
synthetic data.

</details>


### [151] [Federated Learning for Video Violence Detection: Complementary Roles of Lightweight CNNs and Vision-Language Models for Energy-Efficient Use](https://arxiv.org/abs/2511.07171)
*Sébastien Thuau,Siba Haidar,Rachid Chelouah*

Main category: cs.CV

TL;DR: 本文比较了三种联邦学习暴力检测方法：零样本推理的视觉语言模型、LoRA微调的LLaVA-NeXT-Video-7B模型和个性化联邦学习的3D CNN，发现在保持隐私的同时，3D CNN在能效和校准性能上表现更优。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的视频监控需要隐私保护架构，同时降低计算和环境开销。联邦学习能保护隐私，但部署大型视觉语言模型会带来能源和可持续性挑战。

Method: 在RWF-2000和RLVS数据集上比较三种策略：零样本推理的预训练VLM、LoRA微调的LLaVA-NeXT-Video-7B、个性化联邦学习的65.8M参数3D CNN，并采用层次类别分组提升多类分类准确率。

Result: 所有方法在二元暴力检测中准确率均超过90%。3D CNN在ROC AUC 92.59%的校准性能下，能耗仅为联邦LoRA的一半（240 Wh vs 570 Wh）。层次类别分组将VLM多类准确率从65.31%提升至81%。

Conclusion: 研究建议采用混合部署策略：默认使用高效的CNN进行常规推理，选择性使用VLM进行复杂上下文推理，这是首个对LoRA调优VLM和个性化CNN在联邦暴力检测中的比较研究，并量化了能源和CO2e排放。

Abstract: Deep learning-based video surveillance increasingly demands
privacy-preserving architectures with low computational and environmental
overhead. Federated learning preserves privacy but deploying large
vision-language models (VLMs) introduces major energy and sustainability
challenges. We compare three strategies for federated violence detection under
realistic non-IID splits on the RWF-2000 and RLVS datasets: zero-shot inference
with pretrained VLMs, LoRA-based fine-tuning of LLaVA-NeXT-Video-7B, and
personalized federated learning of a 65.8M-parameter 3D CNN. All methods exceed
90% accuracy in binary violence detection. The 3D CNN achieves superior
calibration (ROC AUC 92.59%) at roughly half the energy cost (240 Wh vs. 570
Wh) of federated LoRA, while VLMs provide richer multimodal reasoning.
Hierarchical category grouping (based on semantic similarity and class
exclusion) boosts VLM multiclass accuracy from 65.31% to 81% on the UCF-Crime
dataset. To our knowledge, this is the first comparative simulation study of
LoRA-tuned VLMs and personalized CNNs for federated violence detection, with
explicit energy and CO2e quantification. Our results inform hybrid deployment
strategies that default to efficient CNNs for routine inference and selectively
engage VLMs for complex contextual reasoning.

</details>


### [152] [LiteUpdate: A Lightweight Framework for Updating AI-Generated Image Detectors](https://arxiv.org/abs/2511.07192)
*Jiajie Lu,Zhenkan Fu,Na Zhao,Long Xing,Kejiang Chen,Weiming Zhang,Nenghai Yu*

Main category: cs.CV

TL;DR: LiteUpdate是一个轻量级框架，用于更新AI生成图像检测器，通过边界样本选择和模型融合来解决检测器更新中的低效率和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 生成AI快速发展导致新生成模型不断涌现，现有检测方法难以跟上，检测性能显著下降，迫切需要持续更新检测器以适应新生成器。

Method: 采用代表性样本选择模块利用图像置信度和基于梯度的判别特征精确选择边界样本，并结合模型融合模块融合多个微调轨迹的权重。

Result: 在AIDE上，Midjourney的平均检测准确率从87.63%提升到93.03%，相对提高了6.16%。

Conclusion: LiteUpdate显著提升了各种检测器的检测性能，有效平衡了对新生成器的适应性和对先前知识的保留。

Abstract: The rapid progress of generative AI has led to the emergence of new
generative models, while existing detection methods struggle to keep pace,
resulting in significant degradation in the detection performance. This
highlights the urgent need for continuously updating AI-generated image
detectors to adapt to new generators. To overcome low efficiency and
catastrophic forgetting in detector updates, we propose LiteUpdate, a
lightweight framework for updating AI-generated image detectors. LiteUpdate
employs a representative sample selection module that leverages image
confidence and gradient-based discriminative features to precisely select
boundary samples. This approach improves learning and detection accuracy on new
distributions with limited generated images, significantly enhancing detector
update efficiency. Additionally, LiteUpdate incorporates a model merging module
that fuses weights from multiple fine-tuning trajectories, including
pre-trained, representative, and random updates. This balances the adaptability
to new generators and mitigates the catastrophic forgetting of prior knowledge.
Experiments demonstrate that LiteUpdate substantially boosts detection
performance in various detectors. Specifically, on AIDE, the average detection
accuracy on Midjourney improved from 87.63% to 93.03%, a 6.16% relative
increase.

</details>


### [153] [Automated Estimation of Anatomical Risk Metrics for Endoscopic Sinus Surgery Using Deep Learning](https://arxiv.org/abs/2511.07199)
*Konrad Reuter,Lennart Thaysen,Bilkay Doruk,Sarah Latus,Brigitte Holst,Benjamin Becker,Dennis Eggert,Christian Betz,Anna-Sophie Hoffmann,Alexander Schlaefer*

Main category: cs.CV

TL;DR: 提出了一种自动化的深度学习管道，通过热图回归定位关键解剖标志来估计内窥镜鼻窦手术的解剖风险评分，避免了传统方法中耗时的手动测量。


<details>
  <summary>Details</summary>
Motivation: 内窥镜鼻窦手术需要术前评估颅底解剖以降低风险，现有解剖风险评分需要耗时的手动测量，因此需要自动化解决方案。

Method: 使用深度学习管道，通过热图回归定位关键解剖标志，比较了直接方法和专门的全局到局部学习策略。

Result: 在相关解剖测量上获得平均绝对误差：Keros评分0.506mm，Gera评分4.516度，TMS分类0.802mm/0.777mm。

Conclusion: 提出的自动化深度学习管道能够准确估计解剖风险评分，为内窥镜鼻窦手术提供有效的术前评估工具。

Abstract: Endoscopic sinus surgery requires careful preoperative assessment of the
skull base anatomy to minimize risks such as cerebrospinal fluid leakage.
Anatomical risk scores like the Keros, Gera and Thailand-Malaysia-Singapore
score offer a standardized approach but require time-consuming manual
measurements on coronal CT or CBCT scans. We propose an automated deep learning
pipeline that estimates these risk scores by localizing key anatomical
landmarks via heatmap regression. We compare a direct approach to a specialized
global-to-local learning strategy and find mean absolute errors on the relevant
anatomical measurements of 0.506mm for the Keros, 4.516{\deg} for the Gera and
0.802mm / 0.777mm for the TMS classification.

</details>


### [154] [Geometric implicit neural representations for signed distance functions](https://arxiv.org/abs/2511.07206)
*Luiz Schirmer,Tiago Novello,Vinícius da Silva,Guilherme Schardong,Daniel Perazzo,Hélio Lopes,Nuno Gonçalves,Luiz Velho*

Main category: cs.CV

TL;DR: 该综述回顾了使用隐式神经表示(INRs)近似表面场景的有向距离函数(SDFs)的文献，特别关注在损失函数中融入微分几何工具(如法线和曲率)的几何INRs方法。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示已成为在低维空间中表示信号的有前景框架，本文旨在系统梳理几何INRs在表面重建中的进展，特别是如何通过微分几何工具提升重建质量。

Method: 通过在有向点云或姿态图像集上构建几何损失函数，在损失函数中加入正则化项确保INR满足全局属性(如SDF的梯度为单位)，从微分几何角度探索INR定义、几何损失构建和采样方案。

Result: 几何INRs在有向点云和姿态图像表面重建方面取得了显著进展，通过融入微分几何约束提升了重建精度和几何一致性。

Conclusion: 几何INRs通过结合微分几何工具在表面重建中展现出强大潜力，为3D重建提供了新的技术路径。

Abstract: \textit{Implicit neural representations} (INRs) have emerged as a promising
framework for representing signals in low-dimensional spaces. This survey
reviews the existing literature on the specialized INR problem of approximating
\textit{signed distance functions} (SDFs) for surface scenes, using either
oriented point clouds or a set of posed images. We refer to neural SDFs that
incorporate differential geometry tools, such as normals and curvatures, in
their loss functions as \textit{geometric} INRs. The key idea behind this 3D
reconstruction approach is to include additional \textit{regularization} terms
in the loss function, ensuring that the INR satisfies certain global properties
that the function should hold -- such as having unit gradient in the case of
SDFs. We explore key methodological components, including the definition of
INR, the construction of geometric loss functions, and sampling schemes from a
differential geometry perspective. Our review highlights the significant
advancements enabled by geometric INRs in surface reconstruction from oriented
point clouds and posed images.

</details>


### [155] [Breaking the Stealth-Potency Trade-off in Clean-Image Backdoors with Generative Trigger Optimization](https://arxiv.org/abs/2511.07210)
*Binyan Xu,Fan Yang,Di Tang,Xilin Dai,Kehuan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的干净图像后门攻击范式GCB，通过优化触发器本身来最小化攻击对清洁准确率的影响，使用条件InfoGAN识别自然图像特征作为隐蔽触发器，在极低投毒率下实现攻击且清洁准确率下降小于1%。


<details>
  <summary>Details</summary>
Motivation: 现有的干净图像后门攻击方法存在一个关键缺陷：成功攻击所需的投毒率会导致清洁准确率的显著下降，从而影响攻击的隐蔽性。

Method: 提出生成式干净图像后门(GCB)框架，使用条件InfoGAN识别自然图像特征作为强大且隐蔽的触发器，确保这些触发器与良性任务相关特征易于分离。

Result: GCB在六个数据集、五种架构和四个任务上表现出卓越的适应性，包括在回归和分割任务中首次展示干净图像后门攻击，且对大多数现有后门防御具有韧性。

Conclusion: GCB通过优化触发器设计实现了高度隐蔽的后门攻击，在极低投毒率下保持清洁准确率，展示了在多种任务和架构中的广泛适用性。

Abstract: Clean-image backdoor attacks, which use only label manipulation in training
datasets to compromise deep neural networks, pose a significant threat to
security-critical applications. A critical flaw in existing methods is that the
poison rate required for a successful attack induces a proportional, and thus
noticeable, drop in Clean Accuracy (CA), undermining their stealthiness. This
paper presents a new paradigm for clean-image attacks that minimizes this
accuracy degradation by optimizing the trigger itself. We introduce Generative
Clean-Image Backdoors (GCB), a framework that uses a conditional InfoGAN to
identify naturally occurring image features that can serve as potent and
stealthy triggers. By ensuring these triggers are easily separable from benign
task-related features, GCB enables a victim model to learn the backdoor from an
extremely small set of poisoned examples, resulting in a CA drop of less than
1%. Our experiments demonstrate GCB's remarkable versatility, successfully
adapting to six datasets, five architectures, and four tasks, including the
first demonstration of clean-image backdoors in regression and segmentation.
GCB also exhibits resilience against most of the existing backdoor defenses.

</details>


### [156] [Omni-View: Unlocking How Generation Facilitates Understanding in Unified 3D Model based on Multiview images](https://arxiv.org/abs/2511.07222)
*JiaKui Hu,Shanshan Zhao,Qing-Guo Chen,Xuerui Qiu,Jialun Liu,Zhao Xu,Weihua Luo,Kaifu Zhang,Yanye Lu*

Main category: cs.CV

TL;DR: Omni-View是一个基于多视图图像的3D场景统一多模态理解和生成框架，通过"生成促进理解"的原则，联合建模场景理解、新视角合成和几何估计任务。


<details>
  <summary>Details</summary>
Motivation: 探索3D场景中理解和生成任务的协同作用，验证"生成促进理解"的原则，通过联合建模提升对3D场景的整体理解能力。

Method: 采用三模块架构：理解模型、纹理模块和几何模块，利用纹理模块的时空建模能力和几何模块的显式几何约束，通过两阶段训练策略进行优化。

Result: 在VSI-Bench基准测试中获得55.4的SOTA分数，超越了现有的专用3D理解模型，同时在新视角合成和3D场景生成方面也表现出色。

Conclusion: Omni-View证明了理解和生成任务在3D场景中的协同效应，通过联合建模方法实现了优越的性能，为统一多模态3D场景分析提供了有效解决方案。

Abstract: This paper presents Omni-View, which extends the unified multimodal
understanding and generation to 3D scenes based on multiview images, exploring
the principle that "generation facilitates understanding". Consisting of
understanding model, texture module, and geometry module, Omni-View jointly
models scene understanding, novel view synthesis, and geometry estimation,
enabling synergistic interaction between 3D scene understanding and generation
tasks. By design, it leverages the spatiotemporal modeling capabilities of its
texture module responsible for appearance synthesis, alongside the explicit
geometric constraints provided by its dedicated geometry module, thereby
enriching the model's holistic understanding of 3D scenes. Trained with a
two-stage strategy, Omni-View achieves a state-of-the-art score of 55.4 on the
VSI-Bench benchmark, outperforming existing specialized 3D understanding
models, while simultaneously delivering strong performance in both novel view
synthesis and 3D scene generation.

</details>


### [157] [Mapping Reduced Accessibility to WASH Facilities in Rohingya Refugee Camps with Sub-Meter Imagery](https://arxiv.org/abs/2511.07231)
*Kyeongjin Ahn,YongHun Suh,Sungwon Han,Jeasurk Yang,Hannes Taubenböck,Meeyoung Cha*

Main category: cs.CV

TL;DR: 本研究开发了一个基于遥感技术的半监督分割框架，用于在罗兴亚难民营中检测难民庇护所并量化WASH设施的可及性，发现人口增长导致设施可及性下降，特别是对妇女和女童影响更大。


<details>
  <summary>Details</summary>
Motivation: 解决难民营地中WASH服务获取这一重大公共卫生问题，特别是在世界上人口最密集的流离失所环境中。

Method: 使用亚米级卫星图像开发半监督分割框架，检测个体难民庇护所，并分析多年数据以评估WASH设施可及性。

Result: 庇护所检测F1分数达76.4%；WASH设施可及性从2022年每设施25人下降到2025年29.4人；性别分析显示妇女和女童可及性更低。

Conclusion: 需要需求响应的分配策略，高分辨率遥感和机器学习在检测不平等和指导人道主义环境中公平资源规划方面具有重要价值。

Abstract: Access to Water, Sanitation, and Hygiene (WASH) services remains a major
public health concern in refugee camps. This study introduces a remote
sensing-driven framework to quantify WASH accessibility-specifically to water
pumps, latrines, and bathing cubicles-in the Rohingya camps of Cox's Bazar, one
of the world's most densely populated displacement settings. Detecting refugee
shelters in such emergent camps presents substantial challenges, primarily due
to their dense spatial configuration and irregular geometric patterns. Using
sub-meter satellite images, we develop a semi-supervised segmentation framework
that achieves an F1-score of 76.4% in detecting individual refugee shelters.
Applying the framework across multi-year data reveals declining WASH
accessibility, driven by rapid refugee population growth and reduced facility
availability, rising from 25 people per facility in 2022 to 29.4 in 2025.
Gender-disaggregated analysis further shows that women and girls experience
reduced accessibility, in scenarios with inadequate safety-related segregation
in WASH facilities. These findings suggest the importance of demand-responsive
allocation strategies that can identify areas with under-served
populations-such as women and girls-and ensure that limited infrastructure
serves the greatest number of people in settings with fixed or shrinking
budgets. We also discuss the value of high-resolution remote sensing and
machine learning to detect inequality and inform equitable resource planning in
complex humanitarian environments.

</details>


### [158] [Noise & pattern: identity-anchored Tikhonov regularization for robust structural anomaly detection](https://arxiv.org/abs/2511.07233)
*Alexander Bauer,Klaus-Robert Müller*

Main category: cs.CV

TL;DR: 本文提出了一种自监督自动编码器方法，通过注入结构化扰动来模拟结构缺陷，结合高斯噪声作为正则化器，在MVTec AD基准测试中实现了最先进的异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 工业自动检测中难以收集所有可能的异常样本，因此需要开发能够检测细微或罕见缺陷的方法，特别是在结构异常检测方面。

Method: 使用自监督自动编码器学习修复损坏的输入，引入结构化、空间连贯的扰动来模拟结构缺陷，并在遮挡上添加高斯噪声作为Tikhonov正则化器。

Result: 在MVTec AD基准测试中取得了最先进的结果（I/P-AUROC: 99.9/99.4），验证了理论框架的有效性和实际应用价值。

Conclusion: 该方法通过结构化扰动和身份锚定正则化，稳定了重建过程并提高了检测和分割精度，为自动工业检测提供了有效的解决方案。

Abstract: Anomaly detection plays a pivotal role in automated industrial inspection,
aiming to identify subtle or rare defects in otherwise uniform visual patterns.
As collecting representative examples of all possible anomalies is infeasible,
we tackle structural anomaly detection using a self-supervised autoencoder that
learns to repair corrupted inputs. To this end, we introduce a corruption model
that injects artificial disruptions into training images to mimic structural
defects. While reminiscent of denoising autoencoders, our approach differs in
two key aspects. First, instead of unstructured i.i.d.\ noise, we apply
structured, spatially coherent perturbations that make the task a hybrid of
segmentation and inpainting. Second, and counterintuitively, we add and
preserve Gaussian noise on top of the occlusions, which acts as a Tikhonov
regularizer anchoring the Jacobian of the reconstruction function toward
identity. This identity-anchored regularization stabilizes reconstruction and
further improves both detection and segmentation accuracy. On the MVTec AD
benchmark, our method achieves state-of-the-art results (I/P-AUROC: 99.9/99.4),
supporting our theoretical framework and demonstrating its practical relevance
for automatic inspection.

</details>


### [159] [Leveraging Text-Driven Semantic Variation for Robust OOD Segmentation](https://arxiv.org/abs/2511.07238)
*Seungheon Song,Jaekoo Lee*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉-语言空间的文本驱动OOD分割方法，通过结合视觉语言模型编码器和transformer解码器，利用距离基OOD提示和OOD语义增强，在自动驾驶场景中实现鲁棒的异常物体分割。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶和机器人领域，确保道路安全和可靠决策严重依赖于OOD分割。虽然已有许多方法用于检测道路上的异常物体，但利用提供丰富语言知识的视觉-语言空间仍是一个未被充分探索的领域。作者假设在复杂的真实世界自动驾驶场景中，融入这些语言线索特别有益。

Method: 提出了一种新颖方法，训练文本驱动的OOD分割模型来学习视觉-语言空间中语义多样化的对象集合。具体包括：结合视觉语言模型编码器与transformer解码器；使用位于不同语义距离的基于距离的OOD提示；利用OOD语义增强进行OOD表示。通过对齐视觉和文本信息，该方法能有效泛化到未见过的物体。

Result: 在Fishyscapes、Segment-Me-If-You-Can和Road Anomaly等公开OOD分割数据集上进行了广泛实验，证明该方法在像素级和对象级评估中都达到了最先进的性能。

Conclusion: 这些结果强调了基于视觉-语言的OOD分割在增强未来自动驾驶系统安全性和可靠性方面的潜力。

Abstract: In autonomous driving and robotics, ensuring road safety and reliable
decision-making critically depends on out-of-distribution (OOD) segmentation.
While numerous methods have been proposed to detect anomalous objects on the
road, leveraging the vision-language space-which provides rich linguistic
knowledge-remains an underexplored field. We hypothesize that incorporating
these linguistic cues can be especially beneficial in the complex contexts
found in real-world autonomous driving scenarios.
  To this end, we present a novel approach that trains a Text-Driven OOD
Segmentation model to learn a semantically diverse set of objects in the
vision-language space. Concretely, our approach combines a vision-language
model's encoder with a transformer decoder, employs Distance-Based OOD prompts
located at varying semantic distances from in-distribution (ID) classes, and
utilizes OOD Semantic Augmentation for OOD representations. By aligning visual
and textual information, our approach effectively generalizes to unseen objects
and provides robust OOD segmentation in diverse driving environments.
  We conduct extensive experiments on publicly available OOD segmentation
datasets such as Fishyscapes, Segment-Me-If-You-Can, and Road Anomaly datasets,
demonstrating that our approach achieves state-of-the-art performance across
both pixel-level and object-level evaluations. This result underscores the
potential of vision-language-based OOD segmentation to bolster the safety and
reliability of future autonomous driving systems.

</details>


### [160] [MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs](https://arxiv.org/abs/2511.07250)
*Tianhao Peng,Haochen Wang,Yuanxing Zhang,Zekun Wang,Zili Wang,Ge Zhang,Jian Yang,Shihao Li,Yanghai Wang,Xintao Wang,Houyi Li,Wei Ji,Pengfei Wan,Wenhao Huang,Zhaoxiang Zhang,Jiaheng Liu*

Main category: cs.CV

TL;DR: MVU-Eval是首个针对多模态大语言模型的多视频理解综合评估基准，包含1,824个精心设计的问答对，覆盖4,959个来自不同领域的视频，评估8项核心能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准仅限于单视频理解，无法满足现实场景（如体育分析和自动驾驶）中对多视频理解的关键需求。

Method: 构建包含1,824个问答对的综合基准，涵盖4,959个来自不同领域的视频，评估8项核心能力，包括基础感知任务和高级推理任务。

Result: 通过对最先进的开源和闭源模型进行广泛评估，揭示了当前MLLMs在多视频理解能力方面存在显著性能差距和局限性。

Conclusion: MVU-Eval基准将公开提供，以促进未来在多视频理解领域的研究发展。

Abstract: The advent of Multimodal Large Language Models (MLLMs) has expanded AI
capabilities to visual modalities, yet existing evaluation benchmarks remain
limited to single-video understanding, overlooking the critical need for
multi-video understanding in real-world scenarios (e.g., sports analytics and
autonomous driving). To address this significant gap, we introduce MVU-Eval,
the first comprehensive benchmark for evaluating Multi-Video Understanding for
MLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies
through 1,824 meticulously curated question-answer pairs spanning 4,959 videos
from diverse domains, addressing both fundamental perception tasks and
high-order reasoning tasks. These capabilities are rigorously aligned with
real-world applications such as multi-sensor synthesis in autonomous systems
and cross-angle sports analytics. Through extensive evaluation of
state-of-the-art open-source and closed-source models, we reveal significant
performance discrepancies and limitations in current MLLMs' ability to perform
understanding across multiple videos. The benchmark will be made publicly
available to foster future research.

</details>


### [161] [StreamKV: Streaming Video Question-Answering with Segment-based KV Cache Retrieval and Compression](https://arxiv.org/abs/2511.07278)
*Yilong Chen,Xiang Bai,Zhibin Wang,Chengyu Bai,Yuhan Dai,Ming Lu,Shanghang Zhang*

Main category: cs.CV

TL;DR: StreamKV是一个无需训练的视频大语言模型框架，通过动态语义分割、摘要向量计算和引导提示来实现KV缓存的检索和压缩，显著提升了长视频问答的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型在处理长真实世界视频时面临挑战，现有的KV缓存检索和压缩方法尚未充分探索，需要更有效的解决方案。

Method: StreamKV采用动态语义分割视频流，为每个片段计算摘要向量用于检索，并通过引导提示进行KV缓存压缩，在单一模块中统一实现检索和压缩功能。

Result: 在StreamingVQA基准测试中，StreamKV显著优于现有在线视频大语言模型，在准确性、内存效率和计算延迟方面都有显著提升。

Conclusion: StreamKV为视频大语言模型提供了一种高效的KV缓存管理方法，能够更好地处理长视频问答任务，代码已开源。

Abstract: Video Large Language Models (Video-LLMs) have demonstrated significant
potential in the areas of video captioning, search, and summarization. However,
current Video-LLMs still face challenges with long real-world videos. Recent
methods have introduced a retrieval mechanism that retrieves query-relevant KV
caches for question answering, enhancing the efficiency and accuracy of long
real-world videos. However, the compression and retrieval of KV caches are
still not fully explored. In this paper, we propose \textbf{StreamKV}, a
training-free framework that seamlessly equips Video-LLMs with advanced KV
cache retrieval and compression. Compared to previous methods that used uniform
partitioning, StreamKV dynamically partitions video streams into semantic
segments, which better preserves semantic information. For KV cache retrieval,
StreamKV calculates a summary vector for each segment to retain segment-level
information essential for retrieval. For KV cache compression, StreamKV
introduces a guidance prompt designed to capture the key semantic elements
within each segment, ensuring only the most informative KV caches are retained
for answering questions. Moreover, StreamKV unifies KV cache retrieval and
compression within a single module, performing both in a layer-adaptive manner,
thereby further improving the effectiveness of streaming video question
answering. Extensive experiments on public StreamingVQA benchmarks demonstrate
that StreamKV significantly outperforms existing Online Video-LLMs, achieving
superior accuracy while substantially improving both memory efficiency and
computational latency. The code has been released at
https://github.com/sou1p0wer/StreamKV.

</details>


### [162] [Segmentation of Ischemic Stroke Lesions using Transfer Learning on Multi-sequence MRI](https://arxiv.org/abs/2511.07281)
*R. P. Chowdhury,T. Rahman*

Main category: cs.CV

TL;DR: 提出了一种基于Res-Unet架构的自动缺血性卒中病灶分割框架，在ISLES 2015数据集上验证，通过迁移学习和多数投票分类器实现高效分割。


<details>
  <summary>Details</summary>
Motivation: 手动分割缺血性卒中病灶耗时且存在观察者间差异，现有自动方法的手工特征难以捕捉病灶的不规则复杂形状。

Method: 使用Res-Unet架构，分别在有预训练权重和无预训练权重情况下训练模型，探索迁移学习效果，并集成多数投票分类器融合各轴向结果。

Result: 在ISLES 2015数据集上获得80.5%的Dice分数和74.03%的准确率。

Conclusion: 该方法能有效自动分割多序列MRI中的缺血性卒中病灶，展示了良好的分割性能。

Abstract: The accurate understanding of ischemic stroke lesions is critical for
efficient therapy and prognosis of stroke patients. Magnetic resonance imaging
(MRI) is sensitive to acute ischemic stroke and is a common diagnostic method
for stroke. However, manual lesion segmentation performed by experts is
tedious, time-consuming, and prone to observer inconsistency. Automatic medical
image analysis methods have been proposed to overcome this challenge. However,
previous approaches have relied on hand-crafted features that may not capture
the irregular and physiologically complex shapes of ischemic stroke lesions. In
this study, we present a novel framework for quickly and automatically
segmenting ischemic stroke lesions on various MRI sequences, including
T1-weighted, T2-weighted, DWI, and FLAIR. The proposed methodology is validated
on the ISLES 2015 Brain Stroke sequence dataset, where we trained our model
using the Res-Unet architecture twice: first, with pre-existing weights, and
then without, to explore the benefits of transfer learning. Evaluation metrics,
including the Dice score and sensitivity, were computed across 3D volumes.
Finally, a Majority Voting Classifier was integrated to amalgamate the outcomes
from each axis, resulting in a comprehensive segmentation method. Our efforts
culminated in achieving a Dice score of 80.5\% and an accuracy of 74.03\%,
showcasing the efficacy of our segmentation approach.

</details>


### [163] [Glioma C6: A Novel Dataset for Training and Benchmarking Cell Segmentation](https://arxiv.org/abs/2511.07286)
*Roman Malashin,Svetlana Pashkevich,Daniil Ilyukhin,Arseniy Volkov,Valeria Yachnaya,Andrey Denisov,Maria Mikhalkova*

Main category: cs.CV

TL;DR: Glioma C6是一个用于胶质瘤C6细胞实例分割的新开放数据集，包含75张高分辨率相差显微镜图像和超过12,000个标注细胞，旨在作为深度学习模型的基准和训练资源。


<details>
  <summary>Details</summary>
Motivation: 为生物医学图像分析提供现实的测试平台，通过细胞形态学分类增强癌细胞研究中图像数据的利用价值。

Method: 数据集分为两部分：第一部分用于基准测试，参数受控；第二部分用于在不同条件下的泛化测试。评估了多个通用分割模型在数据集上的表现。

Result: 实验表明，在Glioma C6数据集上训练能显著提升分割性能，证明其对开发稳健和可泛化模型的价值。

Conclusion: Glioma C6数据集为研究人员提供了公开可用的资源，有助于推动胶质瘤细胞分割和癌症研究的发展。

Abstract: We present Glioma C6, a new open dataset for instance segmentation of glioma
C6 cells, designed as both a benchmark and a training resource for deep
learning models. The dataset comprises 75 high-resolution phase-contrast
microscopy images with over 12,000 annotated cells, providing a realistic
testbed for biomedical image analysis. It includes soma annotations and
morphological cell categorization provided by biologists. Additional
categorization of cells, based on morphology, aims to enhance the utilization
of image data for cancer cell research. Glioma C6 consists of two parts: the
first is curated with controlled parameters for benchmarking, while the second
supports generalization testing under varying conditions. We evaluate the
performance of several generalist segmentation models, highlighting their
limitations on our dataset. Our experiments demonstrate that training on Glioma
C6 significantly enhances segmentation performance, reinforcing its value for
developing robust and generalizable models. The dataset is publicly available
for researchers.

</details>


### [164] [LMM-IQA: Image Quality Assessment for Low-Dose CT Imaging](https://arxiv.org/abs/2511.07298)
*Kagan Celik,Mehmet Ozan Unal,Metin Ertas,Isa Yildirim*

Main category: cs.CV

TL;DR: 提出基于LLM的低剂量CT图像质量评估系统，生成数值评分和文本描述，评估噪声、模糊和对比度损失等退化问题，并系统研究多种推理策略。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT虽然提高了患者安全性，但会增加噪声、模糊和对比度损失，影响诊断质量，因此需要一致且鲁棒的图像质量评估方法。

Method: 开发基于LLM的质量评估系统，采用从零样本到元数据集成和错误反馈等多种推理策略，系统分析每种方法对整体性能的贡献。

Result: 评估结果不仅产生高度相关的评分，还提供可解释的输出，为临床工作流程增加价值。

Conclusion: LLM-based质量评估系统能够有效评估低剂量CT图像质量，提供数值评分和文本描述，具有临床实用价值。

Abstract: Low-dose computed tomography (CT) represents a significant improvement in
patient safety through lower radiation doses, but increased noise, blur, and
contrast loss can diminish diagnostic quality. Therefore, consistency and
robustness in image quality assessment become essential for clinical
applications. In this study, we propose an LLM-based quality assessment system
that generates both numerical scores and textual descriptions of degradations
such as noise, blur, and contrast loss. Furthermore, various inference
strategies - from the zero-shot approach to metadata integration and error
feedback - are systematically examined, demonstrating the progressive
contribution of each method to overall performance. The resultant assessments
yield not only highly correlated scores but also interpretable output, thereby
adding value to clinical workflows. The source codes of our study are available
at https://github.com/itu-biai/lmms_ldct_iqa.

</details>


### [165] [VADER: Towards Causal Video Anomaly Understanding with Relation-Aware Large Language Models](https://arxiv.org/abs/2511.07299)
*Ying Cheng,Yu-Ho Lin,Min-Hung Chen,Fu-En Yang,Shang-Hong Lai*

Main category: cs.CV

TL;DR: VADER是一个基于大语言模型的视频异常理解框架，通过整合关键帧对象关系特征和视觉线索来增强对视频中异常事件的理解。


<details>
  <summary>Details</summary>
Motivation: 传统视频异常检测方法仅关注异常检测和定位，忽略了更深层次的因果关系和对象间交互，这对于理解异常行为至关重要。

Method: VADER首先使用异常评分器分配每帧异常分数，然后通过上下文感知采样策略捕获异常事件的因果上下文。关系特征提取器和对比关系编码器共同建模动态对象交互，生成紧凑的关系表示，最后与LLM集成生成详细描述。

Result: 在多个真实世界VAU基准测试中，VADER在异常描述、解释和因果推理任务上取得了强劲结果。

Conclusion: VADER推进了可解释视频异常分析的前沿，能够生成基于因果关系的详细描述并支持稳健的异常相关问答。

Abstract: Video anomaly understanding (VAU) aims to provide detailed interpretation and
semantic comprehension of anomalous events within videos, addressing
limitations of traditional methods that focus solely on detecting and
localizing anomalies. However, existing approaches often neglect the deeper
causal relationships and interactions between objects, which are critical for
understanding anomalous behaviors. In this paper, we propose VADER, an
LLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe
object Relation features with visual cues to enhance anomaly comprehension from
video. Specifically, VADER first applies an Anomaly Scorer to assign per-frame
anomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture
the causal context of each anomalous event. A Relation Feature Extractor and a
COntrastive Relation Encoder (CORE) jointly model dynamic object interactions,
producing compact relational representations for downstream reasoning. These
visual and relational cues are integrated with LLMs to generate detailed,
causally grounded descriptions and support robust anomaly-related question
answering. Experiments on multiple real-world VAU benchmarks demonstrate that
VADER achieves strong results across anomaly description, explanation, and
causal reasoning tasks, advancing the frontier of explainable video anomaly
analysis.

</details>


### [166] [Beyond Boundaries: Leveraging Vision Foundation Models for Source-Free Object Detection](https://arxiv.org/abs/2511.07301)
*Huizai Yao,Sicheng Zhao,Pengteng Li,Yi Cui,Shuo Lu,Weiyu Guo,Yunfan Lu,Yijie Xu,Hui Xiong*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的无源目标检测框架，利用视觉基础模型作为外部知识源，通过三个模块联合增强特征对齐和标签质量，在六个基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有无源目标检测方法主要依赖源模型的内部知识，限制了跨域泛化能力并导致有偏的伪标签。视觉基础模型具有强大的感知能力和广泛泛化性，但在无源目标检测中尚未充分利用。

Method: 设计了三个基于视觉基础模型的模块：基于补丁相似性加权的全局特征对齐、基于动量更新原型的实例级对比学习特征对齐，以及通过熵感知策略融合检测视觉基础模型和教师模型预测的双源增强伪标签融合。

Result: 在六个基准测试上的广泛实验表明，该方法实现了最先进的无源目标检测性能。

Conclusion: 验证了整合视觉基础模型能够同时提高可迁移性和判别性的有效性。

Abstract: Source-Free Object Detection (SFOD) aims to adapt a source-pretrained object
detector to a target domain without access to source data. However, existing
SFOD methods predominantly rely on internal knowledge from the source model,
which limits their capacity to generalize across domains and often results in
biased pseudo-labels, thereby hindering both transferability and
discriminability. In contrast, Vision Foundation Models (VFMs), pretrained on
massive and diverse data, exhibit strong perception capabilities and broad
generalization, yet their potential remains largely untapped in the SFOD
setting. In this paper, we propose a novel SFOD framework that leverages VFMs
as external knowledge sources to jointly enhance feature alignment and label
quality. Specifically, we design three VFM-based modules: (1) Patch-weighted
Global Feature Alignment (PGFA) distills global features from VFMs using
patch-similarity-based weighting to enhance global feature transferability; (2)
Prototype-based Instance Feature Alignment (PIFA) performs instance-level
contrastive learning guided by momentum-updated VFM prototypes; and (3)
Dual-source Enhanced Pseudo-label Fusion (DEPF) fuses predictions from
detection VFMs and teacher models via an entropy-aware strategy to yield more
reliable supervision. Extensive experiments on six benchmarks demonstrate that
our method achieves state-of-the-art SFOD performance, validating the
effectiveness of integrating VFMs to simultaneously improve transferability and
discriminability.

</details>


### [167] [Garbage Vulnerable Point Monitoring using IoT and Computer Vision](https://arxiv.org/abs/2511.07325)
*R. Kumar,A. Lall,S. Chaudhari,M. Kale,A. Vattem*

Main category: cs.CV

TL;DR: 本文提出了一种结合物联网和计算机视觉的智能城市固体废物管理系统，用于监测城市垃圾易发点的非法倾倒行为，通过街级摄像头和物体检测算法快速检测和监控倾倒垃圾。


<details>
  <summary>Details</summary>
Motivation: 解决城市中垃圾易发点的非法倾倒问题，通过智能技术实现高效监控和管理。

Method: 使用物联网和计算机视觉技术，结合街级摄像头和多种物体检测模型（YOLOv8、YOLOv10、YOLO11m、RT-DETR）进行垃圾检测。

Result: YOLO11m模型在垃圾检测中达到最高准确率92.39%，mAP@50为0.91，能够有效捕捉垃圾倾倒的时间模式（小时、日、周趋势）。

Conclusion: 物体检测模型非常适合用于垃圾易发点的监控和追踪，系统能够实现全面的日夜监控和垃圾处理模式分析。

Abstract: This paper proposes a smart way to manage municipal solid waste by using the
Internet of Things (IoT) and computer vision (CV) to monitor illegal waste
dumping at garbage vulnerable points (GVPs) in urban areas. The system can
quickly detect and monitor dumped waste using a street-level camera and object
detection algorithm. Data was collected from the Sangareddy district in
Telangana, India. A series of comprehensive experiments was carried out using
the proposed dataset to assess the accuracy and overall performance of various
object detection models. Specifically, we performed an in-depth evaluation of
YOLOv8, YOLOv10, YOLO11m, and RT-DETR on our dataset. Among these models,
YOLO11m achieved the highest accuracy of 92.39\% in waste detection,
demonstrating its effectiveness in detecting waste. Additionally, it attains an
mAP@50 of 0.91, highlighting its high precision. These findings confirm that
the object detection model is well-suited for monitoring and tracking waste
dumping events at GVP locations. Furthermore, the system effectively captures
waste disposal patterns, including hourly, daily, and weekly dumping trends,
ensuring comprehensive daily and nightly monitoring.

</details>


### [168] [Inference-Time Scaling of Diffusion Models for Infrared Data Generation](https://arxiv.org/abs/2511.07362)
*Kai A. Horstmann,Maxim Clouser,Kia Khezeli*

Main category: cs.CV

TL;DR: 本文提出了一种在推理时使用领域自适应CLIP验证器来提升红外图像生成质量的方法，通过微调FLUX.1-dev扩散模型并利用验证器指导采样过程，在KAIST数据集上FID分数降低了10%。


<details>
  <summary>Details</summary>
Motivation: 红外图像在低能见度条件下具有优势，但高质量标注数据稀缺限制了下游视觉模型的发展。合成红外图像生成可以解决数据不足问题，但受限于有限的数据集难以训练基础级生成模型。

Method: 采用推理时缩放方法，使用领域自适应CLIP验证器增强红外图像生成质量。通过参数高效技术在小样本红外图像上微调FLUX.1-dev扩散模型，并在推理时使用训练好的验证器指导扩散采样过程。

Result: 该方法在KAIST多光谱行人检测基准数据集上实现了生成质量的持续改进，与无指导基线样本相比，FID分数降低了10%。

Conclusion: 推理时指导为在低数据红外设置中弥合领域差距提供了一个有前景的方向。

Abstract: Infrared imagery enables temperature-based scene understanding using passive
sensors, particularly under conditions of low visibility where traditional RGB
imaging fails. Yet, developing downstream vision models for infrared
applications is hindered by the scarcity of high-quality annotated data, due to
the specialized expertise required for infrared annotation. While synthetic
infrared image generation has the potential to accelerate model development by
providing large-scale, diverse training data, training foundation-level
generative diffusion models in the infrared domain has remained elusive due to
limited datasets. In light of such data constraints, we explore an
inference-time scaling approach using a domain-adapted CLIP-based verifier for
enhanced infrared image generation quality. We adapt FLUX.1-dev, a
state-of-the-art text-to-image diffusion model, to the infrared domain by
finetuning it on a small sample of infrared images using parameter-efficient
techniques. The trained verifier is then employed during inference to guide the
diffusion sampling process toward higher quality infrared generations that
better align with input text prompts. Empirically, we find that our approach
leads to consistent improvements in generation quality, reducing FID scores on
the KAIST Multispectral Pedestrian Detection Benchmark dataset by 10% compared
to unguided baseline samples. Our results suggest that inference-time guidance
offers a promising direction for bridging the domain gap in low-data infrared
settings.

</details>


### [169] [SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards](https://arxiv.org/abs/2511.07403)
*Hunar Batra,Haoqin Tu,Hardy Chen,Yuanze Lin,Cihang Xie,Ronald Clark*

Main category: cs.CV

TL;DR: SpatialThinker是一个3D感知的多模态大语言模型，通过强化学习结合结构化空间基础和多步推理来解决MLLMs在空间理解方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的空间MLLMs依赖于显式3D输入或特定架构修改，且受限于大规模数据集或稀疏监督，无法有效处理空间理解问题。

Method: 模型模拟人类空间感知，构建任务相关对象和空间关系的场景图，通过密集空间奖励进行推理。关键贡献包括：生成STVQA-7K数据集的合成管道，以及使用多目标密集空间奖励的在线强化学习。

Result: SpatialThinker-7B在空间理解和真实世界VQA基准测试中优于监督微调和稀疏RL基线，相比稀疏RL几乎使基础模型增益翻倍，并超过GPT-4o。

Conclusion: 结果表明，将空间监督与奖励对齐推理相结合，能够在有限数据下实现稳健的3D空间理解，推动MLLMs向人类级视觉推理迈进。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress in
vision-language tasks, but they continue to struggle with spatial
understanding. Existing spatial MLLMs often rely on explicit 3D inputs or
architecture-specific modifications, and remain constrained by large-scale
datasets or sparse supervision. To address these limitations, we introduce
SpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial
grounding with multi-step reasoning. The model simulates human-like spatial
perception by constructing a scene graph of task-relevant objects and spatial
relations, and reasoning towards an answer via dense spatial rewards.
SpatialThinker consists of two key contributions: (1) a data synthesis pipeline
that generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL
with a multi-objective dense spatial reward enforcing spatial grounding.
SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline
on spatial understanding and real-world VQA benchmarks, nearly doubling the
base-model gain compared to sparse RL, and surpassing GPT-4o. These results
showcase the effectiveness of combining spatial supervision with reward-aligned
reasoning in enabling robust 3D spatial understanding with limited data and
advancing MLLMs towards human-level visual reasoning.

</details>


### [170] [DIMO: Diverse 3D Motion Generation for Arbitrary Objects](https://arxiv.org/abs/2511.07409)
*Linzhan Mou,Jiahui Lei,Chen Wang,Lingjie Liu,Kostas Daniilidis*

Main category: cs.CV

TL;DR: DIMO是一个从单张图像生成任意物体多样化3D运动的生成方法，通过利用预训练视频模型的丰富先验来提取通用运动模式，并将其嵌入到共享的低维潜在空间中。


<details>
  <summary>Details</summary>
Motivation: 从单张图像生成多样化的3D运动是一个具有挑战性的任务，需要有效利用现有视频模型的运动先验知识。

Method: 首先生成具有多样化运动的多个视频，然后将每个运动嵌入到潜在向量中，训练共享运动解码器学习由神经关键点轨迹表示的运动分布，最后用3D高斯模型驱动这些关键点来建模几何和外观。

Result: 在推理时能够通过单次前向传播即时采样多样化的3D运动，并支持3D运动插值和语言引导的运动生成等应用。

Conclusion: DIMO提供了一种有效的从单张图像生成多样化3D运动的方法，通过利用视频模型的运动先验和紧凑的运动表示实现了高质量的运动生成。

Abstract: We present DIMO, a generative approach capable of generating diverse 3D
motions for arbitrary objects from a single image. The core idea of our work is
to leverage the rich priors in well-trained video models to extract the common
motion patterns and then embed them into a shared low-dimensional latent space.
Specifically, we first generate multiple videos of the same object with diverse
motions. We then embed each motion into a latent vector and train a shared
motion decoder to learn the distribution of motions represented by a structured
and compact motion representation, i.e., neural key point trajectories. The
canonical 3D Gaussians are then driven by these key points and fused to model
the geometry and appearance. During inference time with learned latent space,
we can instantly sample diverse 3D motions in a single-forward pass and support
several interesting applications including 3D motion interpolation and
language-guided motion generation. Our project page is available at
https://linzhanm.github.io/dimo.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [171] [Lite VLA: Efficient Vision-Language-Action Control on CPU-Bound Edge Robots](https://arxiv.org/abs/2511.05642)
*Justin Williams,Kishor Datta Gupta,Roy George,Mrinmoy Sarkar*

Main category: cs.RO

TL;DR: 该研究展示了在移动机器人上部署小型视觉语言模型（VLM）的可行性，实现在严格计算约束下的实时场景理解和推理，无需云连接。


<details>
  <summary>Details</summary>
Motivation: 在GPS拒绝环境中，自主机器人需要本地、资源高效的推理能力，而现有方法通常将感知与移动分离，无法在动态环境中同时进行移动和推理。

Method: 提出一个集成紧凑VLM与多模态感知的框架，直接在嵌入式硬件上执行上下文解释，实现同时移动和推理。

Result: 实验验证了计算效率、任务准确性和系统响应性之间的平衡，在移动机器人上成功部署小型VLM实现并发推理和移动。

Conclusion: 这项工作为服务机器人、灾难响应和国防行动等应用中的可扩展、可靠自主性奠定了基础。

Abstract: The deployment of artificial intelligence models at the edge is increasingly
critical for autonomous robots operating in GPS-denied environments where
local, resource-efficient reasoning is essential. This work demonstrates the
feasibility of deploying small Vision-Language Models (VLMs) on mobile robots
to achieve real-time scene understanding and reasoning under strict
computational constraints. Unlike prior approaches that separate perception
from mobility, the proposed framework enables simultaneous movement and
reasoning in dynamic environments using only on-board hardware. The system
integrates a compact VLM with multimodal perception to perform contextual
interpretation directly on embedded hardware, eliminating reliance on cloud
connectivity. Experimental validation highlights the balance between
computational efficiency, task accuracy, and system responsiveness.
Implementation on a mobile robot confirms one of the first successful
deployments of small VLMs for concurrent reasoning and mobility at the edge.
This work establishes a foundation for scalable, assured autonomy in
applications such as service robotics, disaster response, and defense
operations.

</details>


### [172] [VLM-driven Skill Selection for Robotic Assembly Tasks](https://arxiv.org/abs/2511.05680)
*Jeong-Jung Kim,Doo-Yeol Koh,Chang-Hyun Kim*

Main category: cs.RO

TL;DR: 本文提出了一种结合视觉语言模型和模仿学习的机器人装配框架，使用配备夹爪的机器人在3D空间中执行装配操作，实现了高成功率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够整合视觉感知、自然语言理解和学习技能，实现灵活自适应机器人装配操作的框架。

Method: 将视觉语言模型与模仿学习相结合，通过结构化原始技能分解来集成视觉感知和自然语言理解。

Result: 在装配场景中取得了高成功率，同时通过结构化原始技能分解保持了系统的可解释性。

Conclusion: 该框架有效结合了视觉语言模型和模仿学习，为机器人装配操作提供了一种灵活且可解释的解决方案。

Abstract: This paper presents a robotic assembly framework that combines
Vision-Language Models (VLMs) with imitation learning for assembly manipulation
tasks. Our system employs a gripper-equipped robot that moves in 3D space to
perform assembly operations. The framework integrates visual perception,
natural language understanding, and learned primitive skills to enable flexible
and adaptive robotic manipulation. Experimental results demonstrate the
effectiveness of our approach in assembly scenarios, achieving high success
rates while maintaining interpretability through the structured primitive skill
decomposition.

</details>


### [173] [TumorMap: A Laser-based Surgical Platform for 3D Tumor Mapping and Fully-Automated Tumor Resection](https://arxiv.org/abs/2511.05723)
*Guangshen Ma,Ravi Prakash,Beatrice Schleupner,Jeffrey Everitt,Arpit Mishra,Junqin Chen,Brian Mann,Boyuan Chen,Leila Bridgeman,Pei Zhong,Mark Draelos,William C. Eward,Patrick J. Codd*

Main category: cs.RO

TL;DR: TumorMap是一个手术机器人平台，通过集成三种激光机制（光学相干断层扫描、激光诱导内源性荧光和切割激光手术刀）结合深度学习模型，实现术中3D肿瘤边界重建和自主组织切除。


<details>
  <summary>Details</summary>
Motivation: 解决恶性实体瘤手术切除中面临的挑战：缺乏高保真肿瘤重建、难以开发通用组织模型处理肿瘤诊断的复杂性，以及手术中双手操作的自然物理限制、生理震颤和疲劳蠕变。

Method: 集成三种激光机制：光学相干断层扫描用于成像，激光诱导内源性荧光用于检测，切割激光手术刀用于切除；结合深度学习模型实现全自动非接触式肿瘤切除。

Result: 在鼠骨肉瘤和软组织肉瘤肿瘤模型中验证了TumorMap，建立了新的组织病理学工作流程来评估传感器性能，实现了亚毫米级激光切除精度和多模态传感器引导的自主肿瘤手术。

Conclusion: TumorMap平台能够实现无需人工干预的多模态传感器引导自主肿瘤手术，具有亚毫米级的切除精度。

Abstract: Surgical resection of malignant solid tumors is critically dependent on the
surgeon's ability to accurately identify pathological tissue and remove the
tumor while preserving surrounding healthy structures. However, building an
intraoperative 3D tumor model for subsequent removal faces major challenges due
to the lack of high-fidelity tumor reconstruction, difficulties in developing
generalized tissue models to handle the inherent complexities of tumor
diagnosis, and the natural physical limitations of bimanual operation,
physiologic tremor, and fatigue creep during surgery. To overcome these
challenges, we introduce "TumorMap", a surgical robotic platform to formulate
intraoperative 3D tumor boundaries and achieve autonomous tissue resection
using a set of multifunctional lasers. TumorMap integrates a three-laser
mechanism (optical coherence tomography, laser-induced endogenous fluorescence,
and cutting laser scalpel) combined with deep learning models to achieve
fully-automated and noncontact tumor resection. We validated TumorMap in murine
osteoscarcoma and soft-tissue sarcoma tumor models, and established a novel
histopathological workflow to estimate sensor performance. With submillimeter
laser resection accuracy, we demonstrated multimodal sensor-guided autonomous
tumor surgery without any human intervention.

</details>


### [174] [A Unified Stochastic Mechanism Underlying Collective Behavior in Ants, Physical Systems, and Robotic Swarms](https://arxiv.org/abs/2511.05785)
*Lianhao Yin,Haiping Yu,Pascal Spino,Daniela Rus*

Main category: cs.RO

TL;DR: 该论文提出了一个统一随机模型，连接生物、物理和机器人群体，通过在不同能量函数约束下的最大化机制解释生物和物理系统的随机行为。


<details>
  <summary>Details</summary>
Motivation: 生物群体（如蚁群）通过分散随机行为实现集体目标，而物理系统（气体、液体、固体）的随机粒子运动受熵最大化支配但无集体目标。目前缺乏解释这两类系统随机行为的统一框架。

Method: 通过Formica polyctena蚂蚁的实证证据，揭示共享统计机制：在不同能量函数约束下的最大化。进一步证明受此原理支配的机器人群体可展现可扩展的分散合作，模拟物理相变行为且只需最少个体计算。

Result: 建立了连接生物、物理和机器人群体的统一随机模型，为设计稳健智能的群体机器人提供了可扩展原理。

Conclusion: 该研究提供了一个统一框架解释生物和物理系统的随机行为，并展示了其在机器人群体设计中的应用潜力。

Abstract: Biological swarms, such as ant colonies, achieve collective goals through
decentralized and stochastic individual behaviors. Similarly, physical systems
composed of gases, liquids, and solids exhibit random particle motion governed
by entropy maximization, yet do not achieve collective objectives. Despite this
analogy, no unified framework exists to explain the stochastic behavior in both
biological and physical systems. Here, we present empirical evidence from
\textit{Formica polyctena} ants that reveals a shared statistical mechanism
underlying both systems: maximization under different energy function
constraints. We further demonstrate that robotic swarms governed by this
principle can exhibit scalable, decentralized cooperation, mimicking physical
phase-like behaviors with minimal individual computation. These findings
established a unified stochastic model linking biological, physical, and
robotic swarms, offering a scalable principle for designing robust and
intelligent swarm robotics.

</details>


### [175] [VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models](https://arxiv.org/abs/2511.05791)
*Manav Kulshrestha,S. Talha Bukhari,Damon Conover,Aniket Bera*

Main category: cs.RO

TL;DR: VLAD-Grasp是一种零样本的机器人抓取检测方法，利用视觉语言模型生成目标图像来表示对握抓取，通过深度预测和点云配准恢复可执行抓取姿态，无需训练或专家标注。


<details>
  <summary>Details</summary>
Motivation: 现有机器人抓取方法依赖大规模专家标注且需要重新训练来处理新物体，限制了其通用性和适应性。

Method: 从单张RGB-D图像出发：(1)使用大型视觉语言模型生成目标图像，其中直杆"刺穿"物体表示对握抓取；(2)预测深度和分割以将生成图像提升到3D；(3)通过主成分分析和无对应优化对齐生成和观察到的物体点云以恢复可执行抓取姿态。

Result: 在Cornell和Jacquard数据集上，VLAD-Grasp的性能与最先进的监督模型相当或更优，无需训练或依赖精心策划的抓取数据集。

Conclusion: 该方法展示了视觉语言基础模型作为机器人操作强大先验的潜力，实现了零样本泛化到真实世界新物体。

Abstract: Robotic grasping is a fundamental capability for autonomous manipulation;
however, most existing methods rely on large-scale expert annotations and
necessitate retraining to handle new objects. We present VLAD-Grasp, a
Vision-Language model Assisted zero-shot approach for Detecting grasps. From a
single RGB-D image, our method (1) prompts a large vision-language model to
generate a goal image where a straight rod "impales" the object, representing
an antipodal grasp, (2) predicts depth and segmentation to lift this generated
image into 3D, and (3) aligns generated and observed object point clouds via
principal component analysis and correspondence-free optimization to recover an
executable grasp pose. Unlike prior work, our approach is training-free and
does not rely on curated grasp datasets. Despite this, VLAD-Grasp achieves
performance that is competitive with or superior to that of state-of-the-art
supervised models on the Cornell and Jacquard datasets. We further demonstrate
zero-shot generalization to novel real-world objects on a Franka Research 3
robot, highlighting vision-language foundation models as powerful priors for
robotic manipulation.

</details>


### [176] [An Open-Source, Reproducible Tensegrity Robot that can Navigate Among Obstacles](https://arxiv.org/abs/2511.05798)
*William R. Johnson III,Patrick Meng,Nelson Chen,Luca Cimatti,Augustin Vercoutere,Mridul Aanjaneya,Rebecca Kramer-Bottiglio,Kostas E. Bekris*

Main category: cs.RO

TL;DR: 本文提出了一个完整的开源可重复系统，用于三杆张拉整体机器人的导航，包括硬件设计和软件堆栈，能够在已知障碍物环境中实现无碰撞路径规划和姿态跟踪。


<details>
  <summary>Details</summary>
Motivation: 张拉整体机器人具有抗冲击、低质量和适应非结构化地形的优势，但其柔顺性和复杂的耦合动力学给建模和控制带来了挑战，阻碍了路径规划和避障能力的发展。

Method: 开发了一个完整的开源系统，包括：(i) 低成本开源硬件设计，(ii) 集成的开源软件堆栈，涵盖基于物理的建模、系统辨识、状态估计、路径规划和控制。

Result: 系统能够跟踪机器人姿态并在已知障碍物位置中执行无碰撞路径到达指定目标。通过涉及垂直跌落、斜坡和颗粒介质等未建模环境挑战的实验验证了系统鲁棒性，并在两个不同实验室进行了可重复性验证。

Conclusion: 这项工作为机器人社区提供了一个完整的导航系统，适用于柔顺、抗冲击和形状变形的机器人，旨在作为推进其他非常规机器人平台导航能力的跳板。

Abstract: Tensegrity robots, composed of rigid struts and elastic tendons, provide
impact resistance, low mass, and adaptability to unstructured terrain. Their
compliance and complex, coupled dynamics, however, present modeling and control
challenges, hindering path planning and obstacle avoidance. This paper presents
a complete, open-source, and reproducible system that enables navigation for a
3-bar tensegrity robot. The system comprises: (i) an inexpensive, open-source
hardware design, and (ii) an integrated, open-source software stack for
physics-based modeling, system identification, state estimation, path planning,
and control. All hardware and software are publicly available at
https://sites.google.com/view/tensegrity-navigation/. The proposed system
tracks the robot's pose and executes collision-free paths to a specified goal
among known obstacle locations. System robustness is demonstrated through
experiments involving unmodeled environmental challenges, including a vertical
drop, an incline, and granular media, culminating in an outdoor field
demonstration. To validate reproducibility, experiments were conducted using
robot instances at two different laboratories. This work provides the robotics
community with a complete navigation system for a compliant, impact-resistant,
and shape-morphing robot. This system is intended to serve as a springboard for
advancing the navigation capabilities of other unconventional robotic
platforms.

</details>


### [177] [Adversarial Game-Theoretic Algorithm for Dexterous Grasp Synthesis](https://arxiv.org/abs/2511.05809)
*Yu Chen,Botao He,Yuemin Mao,Arthur Jakobsson,Jeffrey Ke,Yiannis Aloimonos,Guanya Shi,Howie Choset,Jiayuan Mao,Jeffrey Ichnowski*

Main category: cs.RO

TL;DR: 提出了一种基于双玩家博弈的多指机器人抓取合成方法，通过对抗性对象运动建模提高抓取稳定性，在仿真和真实实验中均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多指机器人抓取方法通常只关注抵抗单一力矩，忽略了物体可能逃脱的对抗性运动，导致抓取不稳定和失败。

Method: 将抓取合成问题建模为双玩家博弈：一个玩家控制机器人生成可行抓取配置，另一个玩家对抗性控制物体寻求逃脱运动。

Result: 仿真实验中成功率达到75.78%，比最先进基线提高19.61%；双玩家博弈机制比无博弈方法提高27.40%成功率；平均生成时间0.28-1.04秒；真实实验中ShadowHand和LeapHand分别达到85.0%和87.5%成功率。

Conclusion: 该方法通过显式建模对抗性物体运动，显著提高了多指机器人抓取的稳定性和成功率，且计算效率适合实际部署。

Abstract: For many complex tasks, multi-finger robot hands are poised to revolutionize
how we interact with the world, but reliably grasping objects remains a
significant challenge. We focus on the problem of synthesizing grasps for
multi-finger robot hands that, given a target object's geometry and pose,
computes a hand configuration. Existing approaches often struggle to produce
reliable grasps that sufficiently constrain object motion, leading to
instability under disturbances and failed grasps. A key reason is that during
grasp generation, they typically focus on resisting a single wrench, while
ignoring the object's potential for adversarial movements, such as escaping. We
propose a new grasp-synthesis approach that explicitly captures and leverages
the adversarial object motion in grasp generation by formulating the problem as
a two-player game. One player controls the robot to generate feasible grasp
configurations, while the other adversarially controls the object to seek
motions that attempt to escape from the grasp. Simulation experiments on
various robot platforms and target objects show that our approach achieves a
success rate of 75.78%, up to 19.61% higher than the state-of-the-art baseline.
The two-player game mechanism improves the grasping success rate by 27.40% over
the method without the game formulation. Our approach requires only 0.28-1.04
seconds on average to generate a grasp configuration, depending on the robot
platform, making it suitable for real-world deployment. In real-world
experiments, our approach achieves an average success rate of 85.0% on
ShadowHand and 87.5% on LeapHand, which confirms its feasibility and
effectiveness in real robot setups.

</details>


### [178] [3D Mapping Using a Lightweight and Low-Power Monocular Camera Embedded inside a Gripper of Limbed Climbing Robots](https://arxiv.org/abs/2511.05816)
*Taku Okawara,Ryo Nishibe,Mao Kasano,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本文提出了一种用于空间探索的3D地形测绘系统，该系统使用配备单目手眼相机的有肢攀爬机器人，通过融合单目视觉约束和肢体前向运动学来解决尺度模糊问题，实现实时构建度量尺度的3D地形图。


<details>
  <summary>Details</summary>
Motivation: 传统攀爬机器人使用RGB-D相机进行3D地形测绘和可抓取点检测，但RGB-D相机体积大、功耗高。单目相机更轻量、紧凑且功耗低，但存在尺度模糊问题。

Method: 提出一种SLAM方法，基于因子图优化融合单目视觉约束和肢体前向运动学，联合估计时间序列的夹爪位姿和3D地图的全局度量尺度。

Result: 通过物理仿真和真实世界实验验证，该框架能够实时构建度量尺度的3D地形图，并使用单目手眼相机实现凸地形表面的自主抓取。

Conclusion: 该方法为未来涉及有肢攀爬机器人的空间任务提供了可扩展且节能的感知解决方案，无需依赖RGB-D相机。

Abstract: Limbed climbing robots are designed to explore challenging vertical walls,
such as the skylights of the Moon and Mars. In such robots, the primary role of
a hand-eye camera is to accurately estimate 3D positions of graspable points
(i.e., convex terrain surfaces) thanks to its close-up views. While
conventional climbing robots often employ RGB-D cameras as hand-eye cameras to
facilitate straightforward 3D terrain mapping and graspable point detection,
RGB-D cameras are large and consume considerable power.
  This work presents a 3D terrain mapping system designed for space exploration
using limbed climbing robots equipped with a monocular hand-eye camera.
Compared to RGB-D cameras, monocular cameras are more lightweight, compact
structures, and have lower power consumption. Although monocular SLAM can be
used to construct 3D maps, it suffers from scale ambiguity. To address this
limitation, we propose a SLAM method that fuses monocular visual constraints
with limb forward kinematics. The proposed method jointly estimates time-series
gripper poses and the global metric scale of the 3D map based on factor graph
optimization.
  We validate the proposed framework through both physics-based simulations and
real-world experiments. The results demonstrate that our framework constructs a
metrically scaled 3D terrain map in real-time and enables autonomous grasping
of convex terrain surfaces using a monocular hand-eye camera, without relying
on RGB-D cameras. Our method contributes to scalable and energy-efficient
perception for future space missions involving limbed climbing robots. See the
video summary here: https://youtu.be/fMBrrVNKJfc

</details>


### [179] [Gentle Manipulation Policy Learning via Demonstrations from VLM Planned Atomic Skills](https://arxiv.org/abs/2511.05855)
*Jiayu Zhou,Qiwei Wu,Jian Li,Zhe Chen,Xiaogang Xiong,Renjing Xu*

Main category: cs.RO

TL;DR: 提出了一种结合层次语义分解、强化学习、视觉语言模型和知识蒸馏的新框架，用于自主执行长时程、接触丰富的操作任务，无需昂贵的人类演示。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要大量真实世界数据和专家工程，存在成本和可扩展性挑战。本文旨在克服这些限制，实现无需人类演示的长时程操作策略学习。

Method: 将复杂任务分解为原子技能，在模拟环境中使用强化学习训练每个原始技能的策略，并加入显式力约束防止物体损伤。使用视觉语言模型进行高层任务分解和技能规划，生成多样化专家演示，通过视觉-触觉扩散策略蒸馏为统一策略。

Result: 通过全面的消融研究探索了不同VLM任务规划器，系统比较了技能蒸馏的模仿学习算法。广泛的模拟实验和物理部署验证了该方法能够实现长时程操作策略学习，且VLM引导的原子技能框架能够扩展到多样化任务。

Conclusion: 该方法成功实现了无需昂贵人类演示的长时程操作策略学习，VLM引导的原子技能框架提供了可扩展的泛化能力，为接触丰富的操作任务提供了有效的解决方案。

Abstract: Autonomous execution of long-horizon, contact-rich manipulation tasks
traditionally requires extensive real-world data and expert engineering, posing
significant cost and scalability challenges. This paper proposes a novel
framework integrating hierarchical semantic decomposition, reinforcement
learning (RL), visual language models (VLMs), and knowledge distillation to
overcome these limitations. Complex tasks are decomposed into atomic skills,
with RL-trained policies for each primitive exclusively in simulation.
Crucially, our RL formulation incorporates explicit force constraints to
prevent object damage during delicate interactions. VLMs perform high-level
task decomposition and skill planning, generating diverse expert
demonstrations. These are distilled into a unified policy via Visual-Tactile
Diffusion Policy for end-to-end execution. We conduct comprehensive ablation
studies exploring different VLM-based task planners to identify optimal
demonstration generation pipelines, and systematically compare imitation
learning algorithms for skill distillation. Extensive simulation experiments
and physical deployment validate that our approach achieves policy learning for
long-horizon manipulation without costly human demonstrations, while the
VLM-guided atomic skill framework enables scalable generalization to diverse
tasks.

</details>


### [180] [ViTaMIn-B: A Reliable and Efficient Visuo-Tactile Bimanual Manipulation Interface](https://arxiv.org/abs/2511.05858)
*Chuanyu Li,Chaoyi Liu,Daotan Wang,Shuyu Zhang,Lusong Li,Zecui Zeng,Fangchen Liu,Jing Xu,Rui Chen*

Main category: cs.RO

TL;DR: ViTaMIn-B是一个用于双手机器人操作任务的手持数据采集系统，通过新型柔性视觉触觉传感器和鲁棒的6自由度双手姿态跟踪技术，解决了现有系统在复杂交互场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 手持设备为高效收集大规模高质量演示数据提供了机会，但现有系统缺乏鲁棒的触觉传感和可靠的姿态跟踪能力，特别是在双人和接触密集的任务中。

Method: 设计了DuoTact柔性视觉触觉传感器，能承受大接触力并捕获高分辨率接触几何；提出将传感器全局变形重建为3D点云作为策略输入；开发了使用Meta Quest控制器的鲁棒6自由度双手姿态获取流程。

Result: 用户研究证实了ViTaMIn-B在新手和专家操作者中的高效性和高可用性；在四个双人操作任务上的实验表明其相对于现有系统的优越任务性能。

Conclusion: ViTaMIn-B系统在双人和接触密集的机器人操作任务中表现出更高的能力和效率，为大规模高质量演示数据收集提供了有效解决方案。

Abstract: Handheld devices have opened up unprecedented opportunities to collect
large-scale, high-quality demonstrations efficiently. However, existing systems
often lack robust tactile sensing or reliable pose tracking to handle complex
interaction scenarios, especially for bimanual and contact-rich tasks. In this
work, we propose ViTaMIn-B, a more capable and efficient handheld data
collection system for such tasks. We first design DuoTact, a novel compliant
visuo-tactile sensor built with a flexible frame to withstand large contact
forces during manipulation while capturing high-resolution contact geometry. To
enhance the cross-sensor generalizability, we propose reconstructing the
sensor's global deformation as a 3D point cloud and using it as the policy
input. We further develop a robust, unified 6-DoF bimanual pose acquisition
process using Meta Quest controllers, which eliminates the trajectory drift
issue in common SLAM-based methods. Comprehensive user studies confirm the
efficiency and high usability of ViTaMIn-B among novice and expert operators.
Furthermore, experiments on four bimanual manipulation tasks demonstrate its
superior task performance relative to existing systems.

</details>


### [181] [Fair and Safe: A Real-Time Hierarchical Control Framework for Intersections](https://arxiv.org/abs/2511.05886)
*Lei Shi,Yongju Kim,Xinzhi Zhong,Wissam Kontar,Qichao Liu,Soyoung Ahn*

Main category: cs.RO

TL;DR: 本文提出了一个公平感知的分层控制框架，将不公平厌恶明确整合到交叉口管理中，通过最大化考虑等待时间、紧急程度、控制历史和速度偏差的效用来分配控制权限，同时使用LQR和HOCBF确保实时碰撞避免。


<details>
  <summary>Details</summary>
Motivation: 确保联网自动驾驶车辆在交叉口协调中的公平性对于公平访问、社会接受度和长期系统效率至关重要，但在安全关键、实时交通控制中仍未得到充分探索。

Method: 提出公平感知分层控制框架：顶层集中分配模块通过最大化考虑等待时间、紧急程度、控制历史和速度偏差的效用来分配控制权限；底层授权车辆使用LQR执行预计算轨迹，并应用基于高阶控制屏障函数的安全过滤器进行实时碰撞避免。

Result: 在不同交通需求和需求分布下的仿真结果表明，该框架实现了近乎完美的公平性，消除了碰撞，减少了平均延迟，并保持了实时可行性。

Conclusion: 公平性可以在不牺牲安全性或性能的情况下系统地整合，为未来自主交通系统实现可扩展和公平的协调。

Abstract: Ensuring fairness in the coordination of connected and automated vehicles at
intersections is essential for equitable access, social acceptance, and
long-term system efficiency, yet it remains underexplored in safety-critical,
real-time traffic control. This paper proposes a fairness-aware hierarchical
control framework that explicitly integrates inequity aversion into
intersection management. At the top layer, a centralized allocation module
assigns control authority (i.e., selects a single vehicle to execute its
trajectory) by maximizing a utility that accounts for waiting time, urgency,
control history, and velocity deviation. At the bottom layer, the authorized
vehicle executes a precomputed trajectory using a Linear Quadratic Regulator
(LQR) and applies a high-order Control Barrier Function (HOCBF)-based safety
filter for real-time collision avoidance. Simulation results across varying
traffic demands and demand distributions demonstrate that the proposed
framework achieves near-perfect fairness, eliminates collisions, reduces
average delay, and maintains real-time feasibility. These results highlight
that fairness can be systematically incorporated without sacrificing safety or
performance, enabling scalable and equitable coordination for future autonomous
traffic systems.

</details>


### [182] [From Words to Safety: Language-Conditioned Safety Filtering for Robot Navigation](https://arxiv.org/abs/2511.05889)
*Zeyuan Feng,Haimingyue Zhang,Somil Bansal*

Main category: cs.RO

TL;DR: 提出了一个用于机器人导航的语言条件安全框架，包含LLM翻译指令、感知模块和MPC安全过滤器三个核心组件，能够将自由形式指令转化为结构化安全规范并实时执行。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在开放世界和人类中心环境中的集成度提高，需要能够解释自然语言指令并遵守安全约束的能力，现有方法往往只关注奖励函数映射或处理狭窄约束类别，限制了鲁棒性和适用性。

Method: 采用模块化框架：1) LLM模块将自由形式指令翻译为结构化安全规范；2) 感知模块通过维护对象级3D环境表示来落地这些规范；3) MPC安全过滤器实时执行语义和几何约束。

Result: 通过仿真研究和硬件实验评估，证明该框架能够在各种环境和场景中稳健地解释和执行多样化的语言指定约束。

Conclusion: 所提出的模块化框架有效解决了语言条件安全在机器人导航中的挑战，实现了对多样化语言约束的稳健解释和执行。

Abstract: As robots become increasingly integrated into open-world, human-centered
environments, their ability to interpret natural language instructions and
adhere to safety constraints is critical for effective and trustworthy
interaction. Existing approaches often focus on mapping language to reward
functions instead of safety specifications or address only narrow constraint
classes (e.g., obstacle avoidance), limiting their robustness and
applicability. We propose a modular framework for language-conditioned safety
in robot navigation. Our framework is composed of three core components: (1) a
large language model (LLM)-based module that translates free-form instructions
into structured safety specifications, (2) a perception module that grounds
these specifications by maintaining object-level 3D representations of the
environment, and (3) a model predictive control (MPC)-based safety filter that
enforces both semantic and geometric constraints in real time. We evaluate the
effectiveness of the proposed framework through both simulation studies and
hardware experiments, demonstrating that it robustly interprets and enforces
diverse language-specified constraints across a wide range of environments and
scenarios.

</details>


### [183] [10 Open Challenges Steering the Future of Vision-Language-Action Models](https://arxiv.org/abs/2511.05936)
*Soujanya Poria,Navonil Majumder,Chia-Yu Hung,Amir Ali Bagherzadeh,Chuan Li,Kenneth Kwok,Ziwei Wang,Cheston Tan,Jiajun Wu,David Hsu*

Main category: cs.RO

TL;DR: 本文讨论了视觉-语言-动作模型在具身AI领域的10个关键发展里程碑，包括多模态、推理、数据、评估等方面，并探讨了空间理解、世界动态建模等新兴趋势。


<details>
  <summary>Details</summary>
Motivation: 随着视觉-语言-动作模型在具身AI领域的日益普及，需要系统梳理其发展中的关键里程碑和新兴趋势，以促进该领域的进一步发展。

Method: 通过分析VLA模型的10个主要发展里程碑，包括多模态、推理、数据、评估、跨机器人动作泛化、效率、全身协调、安全性、智能体和人机协调，并讨论相关的新兴技术趋势。

Result: 识别了VLA模型发展的关键挑战和机遇，包括空间理解、世界动态建模、后训练和数据合成等技术方向。

Conclusion: 通过系统梳理VLA模型的发展里程碑和新兴趋势，可以为加速该技术走向更广泛接受提供研究方向指导。

Abstract: Due to their ability of follow natural language instructions,
vision-language-action (VLA) models are increasingly prevalent in the embodied
AI arena, following the widespread success of their precursors -- LLMs and
VLMs. In this paper, we discuss 10 principal milestones in the ongoing
development of VLA models -- multimodality, reasoning, data, evaluation,
cross-robot action generalization, efficiency, whole-body coordination, safety,
agents, and coordination with humans. Furthermore, we discuss the emerging
trends of using spatial understanding, modeling world dynamics, post training,
and data synthesis -- all aiming to reach these milestones. Through these
discussions, we hope to bring attention to the research avenues that may
accelerate the development of VLA models into wider acceptability.

</details>


### [184] [Robustness study of the bio-inspired musculoskeletal arm robot based on the data-driven iterative learning algorithm](https://arxiv.org/abs/2511.05995)
*Jianbo Yuan,Jing Dai,Yerui Fan,Yaxiong Wu,Yunpeng Liang,Weixin Yan*

Main category: cs.RO

TL;DR: 本文设计了一种新型轻量化肌腱驱动肌肉骨骼手臂（LTDM-Arm），采用7自由度骨骼关节系统和模块化人工肌肉系统，通过Hilly型肌肉模型和数据驱动迭代学习控制实现重复任务的精确控制，验证了系统在负载干扰下的抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 开发能够模拟人类手臂操作特性的机器人系统，包括灵巧性、顺应性和在非结构化环境中的鲁棒性，一直是研究重点。人类手臂展现的爆发力和精确性为机器人系统设计提供了重要参考。

Method: 设计了7自由度骨骼关节系统和模块化人工肌肉系统（15个执行器），采用Hilly型肌肉模型和数据驱动迭代学习控制（DDILC）来学习和优化有限时间内的重复任务激活信号。

Result: 通过仿真和实验验证了肌肉骨骼系统的抗干扰能力，LTDM-Arm系统能够有效实现期望轨迹跟踪任务，在仿真中承受20%负载干扰，实验中承受15%负载干扰。

Conclusion: 这项研究为开发具有类人操作性能的先进机器人系统奠定了基础，证明了肌腱驱动肌肉骨骼系统在复杂环境中的有效性和鲁棒性。

Abstract: The human arm exhibits remarkable capabilities, including both explosive
power and precision, which demonstrate dexterity, compliance, and robustness in
unstructured environments. Developing robotic systems that emulate human-like
operational characteristics through musculoskeletal structures has long been a
research focus. In this study, we designed a novel lightweight tendon-driven
musculoskeletal arm (LTDM-Arm), featuring a seven degree-of-freedom (DOF)
skeletal joint system and a modularized artificial muscular system (MAMS) with
15 actuators. Additionally, we employed a Hilly-type muscle model and
data-driven iterative learning control (DDILC) to learn and refine activation
signals for repetitive tasks within a finite time frame. We validated the
anti-interference capabilities of the musculoskeletal system through both
simulations and experiments. The results show that the LTDM-Arm system can
effectively achieve desired trajectory tracking tasks, even under load
disturbances of 20 % in simulation and 15 % in experiments. This research lays
the foundation for developing advanced robotic systems with human-like
operational performance.

</details>


### [185] [Development and testing of novel soft sleeve actuators](https://arxiv.org/abs/2511.06102)
*Mohammed Abboodi*

Main category: cs.RO

TL;DR: 本研究开发了一种软套筒驱动架构，通过热塑性弹性体3D打印制造线性、弯曲、扭转三种运动模式的软驱动器，解决了传统刚性辅助设备力传递效率低和舒适性差的问题。


<details>
  <summary>Details</summary>
Motivation: 人口老龄化和神经肌肉疾病增加对可穿戴移动辅助设备的需求，但现有刚性机制和笨重接口阻碍力传递并降低可穿戴性。

Method: 采用定制熔丝制造工艺制造气密柔性结构，开发三种软套筒驱动器（线性、弯曲、扭转）及组合设计，通过实验平台量化运动学和动力学性能。

Result: 实现了可重复的多轴运动，改善了向肢体的力传递，减少了对复杂附着硬件的需求。

Conclusion: 建立了一个统一且可制造的软套筒驱动框架，实现了具有增强运动学和动力学性能的紧凑型用户中心辅助技术。

Abstract: Aging populations and the rising prevalence of neurological and
musculoskeletal disorders increase the demand for wearable mobility assistive
devices that are effective, comfortable, and anatomically compatible. Many
existing systems use rigid mechanisms and bulky interfaces that impede force
transmission and reduce wearability. This study introduces a soft sleeve
actuation architecture that conforms to the limb while transmitting forces and
moments efficiently. We develop three soft sleeve actuators that produce
linear, bending, and twisting motion, and an omnidirectional design that
combines these motions in one device. Actuators are fabricated from
thermoplastic elastomers using a customized fused filament fabrication process
that produces airtight and compliant structures and resolves leakage observed
with conventional methods. A dedicated experimental platform quantifies
kinematic outputs such as displacement, angle, and twist, and kinetic outputs
such as force and torque under low pneumatic pressures. A parametric study
varies geometric features and material properties to determine their influence
on performance. Results show reproducible multi axis motion with improved
transfer of force to the limb and reduced need for complex attachment hardware.
The work establishes a unified and manufacturable framework for soft sleeve
actuation that enables compact and user centered assistive technologies with
enhanced kinematic and kinetic performance.

</details>


### [186] [PlaCo: a QP-based robot planning and control framework](https://arxiv.org/abs/2511.06141)
*Marc Duclusaud,Grégoire Passault,Vincent Padois,Olivier Ly*

Main category: cs.RO

TL;DR: PlaCo是一个用于简化机器人系统QP规划和控制问题建模与求解的软件框架，提供高层接口抽象底层数学公式，支持Python快速原型和C++实时性能。


<details>
  <summary>Details</summary>
Motivation: 简化机器人系统中基于二次规划的规划和控制问题的公式化和求解过程，降低用户处理底层数学公式的复杂性。

Method: 提供高层接口抽象QP问题的底层数学公式，允许用户以模块化和直观的方式指定任务和约束，支持Python绑定用于快速原型和C++实现用于实时性能。

Result: 开发了PlaCo框架，能够简化QP问题的建模和求解过程。

Conclusion: PlaCo框架为机器人系统的QP规划和控制问题提供了高效、易用的解决方案，兼顾了快速原型开发和实时性能需求。

Abstract: This article introduces PlaCo, a software framework designed to simplify the
formulation and solution of Quadratic Programming (QP)-based planning and
control problems for robotic systems. PlaCo provides a high-level interface
that abstracts away the low-level mathematical formulation of QP problems,
allowing users to specify tasks and constraints in a modular and intuitive
manner. The framework supports both Python bindings for rapid prototyping and a
C++ implementation for real-time performance.

</details>


### [187] [OpenVLN: Open-world aerial Vision-Language Navigation](https://arxiv.org/abs/2511.06182)
*Peican Lin,Gan Sun,Chenxi Liu,Fazeng Li,Weihong Ren,Yang Cong*

Main category: cs.RO

TL;DR: 本文提出了OpenVLN框架，用于解决无人机在复杂户外空中环境中的视觉语言导航问题，通过强化学习优化视觉语言模型，并引入长时程规划器，在有限数据条件下实现语言引导的无人机飞行。


<details>
  <summary>Details</summary>
Motivation: 户外空中环境的复杂性给数据采集带来挑战，同时无人机需要长时程轨迹规划，这为空中视觉语言导航引入了新的复杂性。现有方法在数据有限和长时程规划方面存在不足。

Method: 重新配置强化学习框架来优化视觉语言模型用于无人机导航任务，在有限训练数据下使用基于规则的策略高效微调VLM；同时引入长时程规划器进行轨迹合成，通过基于价值的奖励动态生成精确的无人机动作。

Result: 在TravelUAV基准测试上进行充分导航实验，结果显示与基线方法相比，成功率提升4.34%，Oracle成功率提升6.19%，路径长度加权成功率提升4.07%。

Conclusion: 该方法在复杂空中环境中为长时程无人机导航提供了有效的部署方案，证明了其在有限数据约束下增强长时程轨迹规划能力的效果。

Abstract: Vision-language models (VLMs) have been widely-applied in ground-based
vision-language navigation (VLN). However, the vast complexity of outdoor
aerial environments compounds data acquisition challenges and imposes
long-horizon trajectory planning requirements on Unmanned Aerial Vehicles
(UAVs), introducing novel complexities for aerial VLN. To address these
challenges, we propose a data-efficient Open-world aerial Vision-Language
Navigation (i.e., OpenVLN) framework, which could execute language-guided
flight with limited data constraints and enhance long-horizon trajectory
planning capabilities in complex aerial environments. Specifically, we
reconfigure a reinforcement learning framework to optimize the VLM for UAV
navigation tasks, which can efficiently fine-tune VLM by using rule-based
policies under limited training data. Concurrently, we introduce a long-horizon
planner for trajectory synthesis that dynamically generates precise UAV actions
via value-based rewards. To the end, we conduct sufficient navigation
experiments on the TravelUAV benchmark with dataset scaling across diverse
reward settings. Our method demonstrates consistent performance gains of up to
4.34% in Success Rate, 6.19% in Oracle Success Rate, and 4.07% in Success
weighted by Path Length over baseline methods, validating its deployment
efficacy for long-horizon UAV navigation in complex aerial environments.

</details>


### [188] [Affordance-Guided Coarse-to-Fine Exploration for Base Placement in Open-Vocabulary Mobile Manipulation](https://arxiv.org/abs/2511.06240)
*Tzu-Jung Lin,Jia-Fong Yeh,Hung-Ting Su,Chung-Yi Lin,Yi-Ting Chen,Winston H. Hsu*

Main category: cs.RO

TL;DR: 提出了一种零样本的机器人基座放置框架，通过结合视觉语言模型的语义理解和几何可行性，在开放词汇移动操作任务中实现85%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常基于接近度导航而不考虑功能可用性，导致频繁的操作失败。需要一种能够同时考虑语义理解和几何约束的基座放置方法。

Method: 提出了功能可用性引导的粗到细探索框架，构建跨模态表示（功能RGB和障碍物地图+），利用视觉语言模型的粗语义先验指导搜索，并通过迭代优化过程结合几何约束。

Result: 在五个不同的开放词汇移动操作任务上评估，系统达到85%的成功率，显著优于经典几何规划器和基于视觉语言模型的方法。

Conclusion: 展示了功能可用性感知和多模态推理在开放词汇移动操作中通用化、指令条件规划方面的潜力。

Abstract: In open-vocabulary mobile manipulation (OVMM), task success often hinges on
the selection of an appropriate base placement for the robot. Existing
approaches typically navigate to proximity-based regions without considering
affordances, resulting in frequent manipulation failures. We propose
Affordance-Guided Coarse-to-Fine Exploration, a zero-shot framework for base
placement that integrates semantic understanding from vision-language models
(VLMs) with geometric feasibility through an iterative optimization process.
Our method constructs cross-modal representations, namely Affordance RGB and
Obstacle Map+, to align semantics with spatial context. This enables reasoning
that extends beyond the egocentric limitations of RGB perception. To ensure
interaction is guided by task-relevant affordances, we leverage coarse semantic
priors from VLMs to guide the search toward task-relevant regions and refine
placements with geometric constraints, thereby reducing the risk of convergence
to local optima. Evaluated on five diverse open-vocabulary mobile manipulation
tasks, our system achieves an 85% success rate, significantly outperforming
classical geometric planners and VLM-based methods. This demonstrates the
promise of affordance-aware and multimodal reasoning for generalizable,
instruction-conditioned planning in OVMM.

</details>


### [189] [Robust Differentiable Collision Detection for General Objects](https://arxiv.org/abs/2511.06267)
*Jiayi Chen,Wei Zhao,Liangwang Ruan,Baoquan Chen,He Wang*

Main category: cs.RO

TL;DR: 提出了一种鲁棒高效的微分碰撞检测框架，支持凸面和凹面物体，通过距离导向的随机平滑、自适应采样和等效梯度传输实现稳健的梯度计算。


<details>
  <summary>Details</summary>
Motivation: 传统碰撞检测算法如GJK+EPA不可微分，限制了在接触丰富任务中的梯度优化。现有微分方法仅适用于凸面物体且缺乏鲁棒性。

Method: 采用距离导向的一阶随机平滑、自适应采样和等效梯度传输技术，构建支持凸面和凹面物体的微分碰撞检测框架。

Result: 在DexGraspNet和Objaverse的复杂网格上实验显示，相比现有基线方法有显著改进。

Conclusion: 该方法可直接应用于灵巧抓取合成等任务，提升抓取质量，为接触丰富的机器人应用提供了有效的微分碰撞检测解决方案。

Abstract: Collision detection is a core component of robotics applications such as
simulation, control, and planning. Traditional algorithms like GJK+EPA compute
witness points (i.e., the closest or deepest-penetration pairs between two
objects) but are inherently non-differentiable, preventing gradient flow and
limiting gradient-based optimization in contact-rich tasks such as grasping and
manipulation. Recent work introduced efficient first-order randomized smoothing
to make witness points differentiable; however, their direction-based
formulation is restricted to convex objects and lacks robustness for complex
geometries. In this work, we propose a robust and efficient differentiable
collision detection framework that supports both convex and concave objects
across diverse scales and configurations. Our method introduces distance-based
first-order randomized smoothing, adaptive sampling, and equivalent gradient
transport for robust and informative gradient computation. Experiments on
complex meshes from DexGraspNet and Objaverse show significant improvements
over existing baselines. Finally, we demonstrate a direct application of our
method for dexterous grasp synthesis to refine the grasp quality. The code is
available at https://github.com/JYChen18/DiffCollision.

</details>


### [190] [External Photoreflective Tactile Sensing Based on Surface Deformation Measurement](https://arxiv.org/abs/2511.06311)
*Seiichi Yamamoto,Hiroki Ishizuka,Takumi Kawasetsu,Koh Hosoda,Takayuki Kameoka,Kango Yanagida,Takato Horii,Sei Ikeda,Osamu Oshiro*

Main category: cs.RO

TL;DR: 提出了一种基于软体机器人机械柔顺性的触觉传感方法，使用外部可附加的光反射模块读取硅胶皮肤表面变形来估计接触力，无需嵌入触觉传感器。


<details>
  <summary>Details</summary>
Motivation: 将传感器定位在接触界面之外可降低损坏风险、保持柔软性，并简化制造和维护。与液体填充或导线嵌入的触觉皮肤相比，模块化附加架构增强了耐用性、减少了布线复杂性。

Method: 首先表征光学传感元件和柔顺皮肤，然后设计原型触觉传感器。通过压缩实验验证方法，使用外部光学模块读取皮肤应变模式。

Result: 压缩实验验证了该方法，表现出与理论一致的单调力输出关系、低滞后性、高重复性和对小响应压痕速度的稳定性。在软体机器人抓手上集成时，模块能可靠检测抓取事件。

Conclusion: 利用表面柔顺性与外部光学模块为软体机器人提供力感知提供了一条实用且稳健的途径，同时保持了结构灵活性和可制造性，为机器人应用和安全人机协作铺平了道路。

Abstract: We present a tactile sensing method enabled by the mechanical compliance of
soft robots; an externally attachable photoreflective module reads surface
deformation of silicone skin to estimate contact force without embedding
tactile transducers. Locating the sensor off the contact interface reduces
damage risk, preserves softness, and simplifies fabrication and maintenance. We
first characterize the optical sensing element and the compliant skin,
thendetermine the design of a prototype tactile sensor. Compression experiments
validate the approach, exhibiting a monotonic force output relationship
consistent with theory, low hysteresis, high repeatability over repeated
cycles, and small response indentation speeds. We further demonstrate
integration on a soft robotic gripper, where the module reliably detects grasp
events. Compared with liquid filled or wireembedded tactile skins, the proposed
modular add on architecture enhances durability, reduces wiring complexity, and
supports straightforward deployment across diverse robot geometries. Because
the sensing principle reads skin strain patterns, it also suggests extensions
to other somatosensory cues such as joint angle or actuator state estimation
from surface deformation. Overall, leveraging surface compliance with an
external optical module provides a practical and robust route to equip soft
robots with force perception while preserving structural flexibility and
manufacturability, paving the way for robotic applications and safe human robot
collaboration.

</details>


### [191] [Towards Adaptive Humanoid Control via Multi-Behavior Distillation and Reinforced Fine-Tuning](https://arxiv.org/abs/2511.06371)
*Yingnan Zhao,Xinmiao Wang,Dewei Wang,Xinzhe Liu,Dan Lu,Qilong Han,Peng Liu,Chenjia Bai*

Main category: cs.RO

TL;DR: 本文提出自适应人形控制(AHC)方法，通过两阶段框架学习跨技能和地形的自适应人形运动控制器，解决了现有方法需要为每个技能训练独立策略、泛化能力有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要为每个运动技能训练独立策略，导致控制器泛化能力有限，在不规则地形和多样化环境中表现脆弱。

Method: 采用两阶段框架：首先训练多个主要运动策略并进行多行为蒸馏获得基础多行为控制器，然后通过在线反馈在多样化地形上进行强化微调。

Result: 在仿真和Unitree G1机器人真实实验中，该方法在各种情境和地形上展现出强大的适应性。

Conclusion: AHC方法能够有效学习自适应人形运动控制器，在不同技能和地形上表现出良好的适应性。

Abstract: Humanoid robots are promising to learn a diverse set of human-like locomotion
behaviors, including standing up, walking, running, and jumping. However,
existing methods predominantly require training independent policies for each
skill, yielding behavior-specific controllers that exhibit limited
generalization and brittle performance when deployed on irregular terrains and
in diverse situations. To address this challenge, we propose Adaptive Humanoid
Control (AHC) that adopts a two-stage framework to learn an adaptive humanoid
locomotion controller across different skills and terrains. Specifically, we
first train several primary locomotion policies and perform a multi-behavior
distillation process to obtain a basic multi-behavior controller, facilitating
adaptive behavior switching based on the environment. Then, we perform
reinforced fine-tuning by collecting online feedback in performing adaptive
behaviors on more diverse terrains, enhancing terrain adaptability for the
controller. We conduct experiments in both simulation and real-world
experiments in Unitree G1 robots. The results show that our method exhibits
strong adaptability across various situations and terrains. Project website:
https://ahc-humanoid.github.io.

</details>


### [192] [ArtReg: Visuo-Tactile based Pose Tracking and Manipulation of Unseen Articulated Objects](https://arxiv.org/abs/2511.06378)
*Prajval Kumar Murali,Mohsen Kaboli*

Main category: cs.RO

TL;DR: 提出了一种名为ArtReg的新方法，用于在机器人交互过程中无需先验知识即可跟踪未知物体（单个、多个或铰接式）的位姿，结合视觉-触觉点云在SE(3)李群中进行点云配准。


<details>
  <summary>Details</summary>
Motivation: 机器人在真实环境中经常遇到具有复杂结构和铰接组件的未知物体，如门、抽屉、柜子和工具。无需先验几何或运动学知识就能感知、跟踪和操作这些物体仍然是机器人学的基本挑战。

Method: ArtReg方法在SE(3)李群中使用无迹卡尔曼滤波器整合视觉-触觉点云进行点云配准，通过推、拉等有目的的操作动作检测可能的铰接关节，并开发了闭环控制器用于目标驱动的铰接物体操作。

Result: 在真实机器人实验中广泛评估了各种未知物体，在低光照条件、挑战性视觉背景和不同质心物体上表现出鲁棒性，在标准铰接物体数据集上相比最先进方法提高了位姿精度。

Conclusion: 利用视觉-触觉信息的鲁棒准确位姿跟踪使机器人能够感知和交互未见过的复杂铰接物体（具有旋转或棱柱关节）。

Abstract: Robots operating in real-world environments frequently encounter unknown
objects with complex structures and articulated components, such as doors,
drawers, cabinets, and tools. The ability to perceive, track, and manipulate
these objects without prior knowledge of their geometry or kinematic properties
remains a fundamental challenge in robotics. In this work, we present a novel
method for visuo-tactile-based tracking of unseen objects (single, multiple, or
articulated) during robotic interaction without assuming any prior knowledge
regarding object shape or dynamics. Our novel pose tracking approach termed
ArtReg (stands for Articulated Registration) integrates visuo-tactile point
clouds in an unscented Kalman Filter formulation in the SE(3) Lie Group for
point cloud registration. ArtReg is used to detect possible articulated joints
in objects using purposeful manipulation maneuvers such as pushing or
hold-pulling with a two-robot team. Furthermore, we leverage ArtReg to develop
a closed-loop controller for goal-driven manipulation of articulated objects to
move the object into the desired pose configuration. We have extensively
evaluated our approach on various types of unknown objects through real robot
experiments. We also demonstrate the robustness of our method by evaluating
objects with varying center of mass, low-light conditions, and with challenging
visual backgrounds. Furthermore, we benchmarked our approach on a standard
dataset of articulated objects and demonstrated improved performance in terms
of pose accuracy compared to state-of-the-art methods. Our experiments indicate
that robust and accurate pose tracking leveraging visuo-tactile information
enables robots to perceive and interact with unseen complex articulated objects
(with revolute or prismatic joints).

</details>


### [193] [From Demonstrations to Safe Deployment: Path-Consistent Safety Filtering for Diffusion Policies](https://arxiv.org/abs/2511.06385)
*Ralf Römer,Julian Balletshofer,Jakob Thumm,Marco Pavone,Angela P. Schoellig,Matthias Althoff*

Main category: cs.RO

TL;DR: 本文提出了路径一致安全过滤（PACS）方法，用于解决扩散策略在安全保证方面的问题，通过路径一致性制动和基于集合的可达性分析，在保持任务成功率的同时提供形式化安全保证。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在复杂操作任务中表现出色，但无法保证安全行为，需要外部安全机制。然而这些机制会以训练中未见的方式改变动作，导致不可预测的行为和性能下降。

Method: 提出路径一致安全过滤（PACS）方法，对生成动作序列计算出的轨迹执行路径一致性制动，使用基于集合的可达性分析进行实时安全验证。

Result: 在仿真和三个真实世界人机交互任务中的实验表明，PACS能够提供动态环境中的形式化安全保证，保持任务成功率，相比反应式安全方法（如控制屏障函数）任务成功率提升高达68%。

Conclusion: PACS方法成功解决了扩散策略的安全性问题，在保持学习到的任务完成行为的同时提供形式化安全保证，优于现有的反应式安全方法。

Abstract: Diffusion policies (DPs) achieve state-of-the-art performance on complex
manipulation tasks by learning from large-scale demonstration datasets, often
spanning multiple embodiments and environments. However, they cannot guarantee
safe behavior, so external safety mechanisms are needed. These, however, alter
actions in ways unseen during training, causing unpredictable behavior and
performance degradation. To address these problems, we propose path-consistent
safety filtering (PACS) for DPs. Our approach performs path-consistent braking
on a trajectory computed from the sequence of generated actions. In this way,
we keep execution consistent with the policy's training distribution,
maintaining the learned, task-completing behavior. To enable a real-time
deployment and handle uncertainties, we verify safety using set-based
reachability analysis. Our experimental evaluation in simulation and on three
challenging real-world human-robot interaction tasks shows that PACS (a)
provides formal safety guarantees in dynamic environments, (b) preserves task
success rates, and (c) outperforms reactive safety approaches, such as control
barrier functions, by up to 68% in terms of task success. Videos are available
at our project website: https://tum-lsy.github.io/pacs/.

</details>


### [194] [Whole-Body Control With Terrain Estimation of A 6-DoF Wheeled Bipedal Robot](https://arxiv.org/abs/2511.06397)
*Cong Wen,Yunfei Li,Kexin Liu,Yixin Qiu,Xuanhong Liao,Tianyu Wang,Dingchuan Liu,Tao Zhang,Ximin Lyu*

Main category: cs.RO

TL;DR: 本文开发了完整的动力学模型和全身控制框架，用于6自由度轮式双足机器人，通过地形估计和分层优化解决在不平地形上的运动问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常忽略腿部动力学，限制了机器人的完整运动潜力，且机器人在不平地形上运动面临挑战。

Method: 建立包含闭环动力学和地面接触模型的完整动力学模型，使用LiDAR惯性里程计和改进的主成分分析进行地形估计，采用PD控制律和LQR进行姿态控制和基于质心动力的平衡控制，使用分层优化方法解决全身控制问题。

Result: 通过仿真和真实世界实验验证了地形估计算法的性能，证明了算法在不平地形上的鲁棒性和穿越能力。

Conclusion: 所提出的完整动力学模型和全身控制框架有效提升了轮式双足机器人在复杂地形上的运动性能。

Abstract: Wheeled bipedal robots have garnered increasing attention in exploration and
inspection. However, most research simplifies calculations by ignoring leg
dynamics, thereby restricting the robot's full motion potential. Additionally,
robots face challenges when traversing uneven terrain. To address the
aforementioned issue, we develop a complete dynamics model and design a
whole-body control framework with terrain estimation for a novel 6 degrees of
freedom wheeled bipedal robot. This model incorporates the closed-loop dynamics
of the robot and a ground contact model based on the estimated ground normal
vector. We use a LiDAR inertial odometry framework and improved Principal
Component Analysis for terrain estimation. Task controllers, including PD
control law and LQR, are employed for pose control and centroidal
dynamics-based balance control, respectively. Furthermore, a hierarchical
optimization approach is used to solve the whole-body control problem. We
validate the performance of the terrain estimation algorithm and demonstrate
the algorithm's robustness and ability to traverse uneven terrain through both
simulation and real-world experiments.

</details>


### [195] [Real Garment Benchmark (RGBench): A Comprehensive Benchmark for Robotic Garment Manipulation featuring a High-Fidelity Scalable Simulator](https://arxiv.org/abs/2511.06434)
*Wenkang Hu,Xincheng Tang,Yanzhi E,Yitong Li,Zhengjie Shu,Wei Li,Huamin Wang,Ruigang Yang*

Main category: cs.RO

TL;DR: 提出了Real Garment Benchmark (RGBench)，这是一个用于机器人衣物操作的全面基准测试，包含6000多个衣物网格模型、高性能模拟器以及评估模拟质量的协议。实验表明该模拟器在精度和速度上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然模拟数据在刚性物体机器人操作方面取得了显著进展，但由于缺乏可变形物体模型和逼真的非刚体模拟器，其在可变形物体上的应用受到阻碍。

Method: 开发了RGBench基准测试，包含多样化的衣物网格模型库、新的高性能模拟器，以及通过真实测量的衣物动力学来评估模拟质量的全面协议。

Result: 实验证明该模拟器大幅优于现有布料模拟器，模拟误差减少20%，同时保持3倍更快的运行速度。

Conclusion: RGBench将公开发布以加速未来机器人衣物操作的研究。

Abstract: While there has been significant progress to use simulated data to learn
robotic manipulation of rigid objects, applying its success to deformable
objects has been hindered by the lack of both deformable object models and
realistic non-rigid body simulators. In this paper, we present Real Garment
Benchmark (RGBench), a comprehensive benchmark for robotic manipulation of
garments. It features a diverse set of over 6000 garment mesh models, a new
high-performance simulator, and a comprehensive protocol to evaluate garment
simulation quality with carefully measured real garment dynamics. Our
experiments demonstrate that our simulator outperforms currently available
cloth simulators by a large margin, reducing simulation error by 20% while
maintaining a speed of 3 times faster. We will publicly release RGBench to
accelerate future research in robotic garment manipulation. Website:
https://rgbench.github.io/

</details>


### [196] [Sim-to-Real Transfer in Deep Reinforcement Learning for Bipedal Locomotion](https://arxiv.org/abs/2511.06465)
*Lingfan Bao,Tianhu Peng,Chengxu Zhou*

Main category: cs.RO

TL;DR: 本章探讨了双足行走深度强化学习的仿真到现实转移挑战，分析了仿真与现实差距的来源，并提出了缩小差距和强化策略的两种互补解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习在双足行走中的仿真到现实转移问题，这是实现实际应用的关键挑战。

Method: 通过分析机器人动力学、接触建模、状态估计和数值求解器等仿真差距来源，采用模型中心策略提高仿真器物理保真度，以及通过鲁棒性训练和部署后适应来强化策略。

Result: 构建了一个战略框架，为开发和评估鲁棒的仿真到现实解决方案提供了清晰路线图。

Conclusion: 通过结合提高仿真保真度和强化策略鲁棒性的互补方法，可以有效解决双足行走深度强化学习的仿真到现实转移问题。

Abstract: This chapter addresses the critical challenge of simulation-to-reality
(sim-to-real) transfer for deep reinforcement learning (DRL) in bipedal
locomotion. After contextualizing the problem within various control
architectures, we dissect the ``curse of simulation'' by analyzing the primary
sources of sim-to-real gap: robot dynamics, contact modeling, state estimation,
and numerical solvers. Building on this diagnosis, we structure the solutions
around two complementary philosophies. The first is to shrink the gap through
model-centric strategies that systematically improve the simulator's physical
fidelity. The second is to harden the policy, a complementary approach that
uses in-simulation robustness training and post-deployment adaptation to make
the policy inherently resilient to model inaccuracies. The chapter concludes by
synthesizing these philosophies into a strategic framework, providing a clear
roadmap for developing and evaluating robust sim-to-real solutions.

</details>


### [197] [A Low-Rank Method for Vision Language Model Hallucination Mitigation in Autonomous Driving](https://arxiv.org/abs/2511.06496)
*Keke Long,Jiacheng Guo,Tianyun Zhang,Hongkai Yu,Xiaopeng Li*

Main category: cs.RO

TL;DR: 提出一种自包含的低秩方法，仅使用多个VLM生成的候选描述本身，无需外部参考或模型访问，自动根据幻觉水平对描述进行排序，选择残差最小的描述作为最无幻觉的描述。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，视觉语言模型(VLM)有时会产生幻觉，即基于视觉输入的错误细节。当缺乏真实参考且无法访问模型内部时，检测和缓解幻觉具有挑战性。

Method: 通过构建句子嵌入矩阵并将其分解为低秩共识分量和稀疏残差，使用残差大小对描述进行排序。

Result: 在NuScenes数据集上的实验表明，该方法在识别无幻觉描述方面达到87%的选择准确率，比未过滤基线提高19%，比多智能体辩论方法提高6-10%。

Conclusion: 该方法产生的排序与人类对幻觉的判断有强相关性，验证了评分机制的有效性，且推理时间比辩论方法减少51-67%，适用于实时自动驾驶应用。

Abstract: Vision Language Models (VLMs) are increasingly used in autonomous driving to
help understand traffic scenes, but they sometimes produce hallucinations,
which are false details not grounded in the visual input. Detecting and
mitigating hallucinations is challenging when ground-truth references are
unavailable and model internals are inaccessible. This paper proposes a novel
self-contained low-rank approach to automatically rank multiple candidate
captions generated by multiple VLMs based on their hallucination levels, using
only the captions themselves without requiring external references or model
access. By constructing a sentence-embedding matrix and decomposing it into a
low-rank consensus component and a sparse residual, we use the residual
magnitude to rank captions: selecting the one with the smallest residual as the
most hallucination-free. Experiments on the NuScenes dataset demonstrate that
our approach achieves 87% selection accuracy in identifying hallucination-free
captions, representing a 19% improvement over the unfiltered baseline and a
6-10% improvement over multi-agent debate method. The sorting produced by
sparse error magnitudes shows strong correlation with human judgments of
hallucinations, validating our scoring mechanism. Additionally, our method,
which can be easily parallelized, reduces inference time by 51-67% compared to
debate approaches, making it practical for real-time autonomous driving
applications.

</details>


### [198] [Adaptive PID Control for Robotic Systems via Hierarchical Meta-Learning and Reinforcement Learning with Physics-Based Data Augmentation](https://arxiv.org/abs/2511.06500)
*JiaHao Wu,ShengWen Yu*

Main category: cs.RO

TL;DR: 提出了一种结合元学习和强化学习的分层控制框架，用于自动调整工业机器人的PID控制器参数，解决了手动调参耗时且需要专业知识的问题。


<details>
  <summary>Details</summary>
Motivation: PID控制器在工业机器人中广泛应用，但手动调参过程耗时且需要专业知识，需要开发自动化的参数调整方法。

Method: 采用分层控制框架，结合元学习进行PID初始化，使用强化学习进行在线适应，并引入基于物理的数据增强策略来提高样本效率。

Result: 在Franka Panda机械臂上实现16.6%的平均改进（6.26° MAE），高负载关节改进达80.4%；发现优化天花板效应：当元学习基线性能均匀强时，强化学习无额外收益。

Conclusion: 强化学习的有效性高度依赖于元学习基线质量和误差分布，为分层控制系统设计提供了重要指导，方法在10分钟训练时间内表现出鲁棒性能。

Abstract: Proportional-Integral-Derivative (PID) controllers remain the predominant
choice in industrial robotics due to their simplicity and reliability. However,
manual tuning of PID parameters for diverse robotic platforms is time-consuming
and requires extensive domain expertise. This paper presents a novel
hierarchical control framework that combines meta-learning for PID
initialization and reinforcement learning (RL) for online adaptation. To
address the sample efficiency challenge, a \textit{physics-based data
augmentation} strategy is introduced that generates virtual robot
configurations by systematically perturbing physical parameters, enabling
effective meta-learning with limited real robot data. The proposed approach is
evaluated on two heterogeneous platforms: a 9-DOF Franka Panda manipulator and
a 12-DOF Laikago quadruped robot. Experimental results demonstrate that the
proposed method achieves 16.6\% average improvement on Franka Panda (6.26{\deg}
MAE), with exceptional gains in high-load joints (J2: 80.4\% improvement from
12.36{\deg} to 2.42{\deg}). Critically, this work discovers the
\textit{optimization ceiling effect}: RL achieves dramatic improvements when
meta-learning exhibits localized high-error joints, but provides no benefit
(0.0\%) when baseline performance is uniformly strong, as observed in Laikago.
The method demonstrates robust performance under disturbances (parameter
uncertainty: +19.2\%, no disturbance: +16.6\%, average: +10.0\%) with only 10
minutes of training time. Multi-seed analysis across 100 random initializations
confirms stable performance (4.81+/-1.64\% average). These results establish
that RL effectiveness is highly dependent on meta-learning baseline quality and
error distribution, providing important design guidance for hierarchical
control systems.

</details>


### [199] [Koopman global linearization of contact dynamics for robot locomotion and manipulation enables elaborate control](https://arxiv.org/abs/2511.06515)
*Cormac O'Neill,Jasmine Terrones,H. Harry Asada*

Main category: cs.RO

TL;DR: 本文提出了一种使用Koopman算子将接触动力学统一为全局线性模型的方法，解决了机器人控制中接触边界切换带来的非凸优化问题。


<details>
  <summary>Details</summary>
Motivation: 机器人与环境动态接触的控制是一个紧迫挑战，接触边界的动力学切换使得控制变得困难，预测控制器面临非凸优化问题。

Method: 应用Koopman算子将由于接触变化产生的分段动力学统一为嵌入空间中的全局线性模型，利用粘弹性接触特性实现无近似的控制输入。

Result: 该方法实现了腿式机器人的凸模型预测控制，以及机械臂动态推动的实时控制，使机器人能够在实时时间范围内发现包含多次接触变化的复杂控制策略。

Conclusion: 该方法不仅适用于机器人领域，还可广泛应用于其他领域，为接触动力学控制提供了有效的解决方案。

Abstract: Controlling robots that dynamically engage in contact with their environment
is a pressing challenge. Whether a legged robot making-and-breaking contact
with a floor, or a manipulator grasping objects, contact is everywhere.
Unfortunately, the switching of dynamics at contact boundaries makes control
difficult. Predictive controllers face non-convex optimization problems when
contact is involved. Here, we overcome this difficulty by applying Koopman
operators to subsume the segmented dynamics due to contact changes into a
unified, globally-linear model in an embedding space. We show that viscoelastic
contact at robot-environment interactions underpins the use of Koopman
operators without approximation to control inputs. This methodology enables the
convex Model Predictive Control of a legged robot, and the real-time control of
a manipulator engaged in dynamic pushing. In this work, we show that our method
allows robots to discover elaborate control strategies in real-time over time
horizons with multiple contact changes, and the method is applicable to broad
fields beyond robotics.

</details>


### [200] [CoFineLLM: Conformal Finetuning of LLMs for Language-Instructed Robot Planning](https://arxiv.org/abs/2511.06575)
*Jun Wang,Yevgeniy Vorobeychik,Yiannis Kantaros*

Main category: cs.RO

TL;DR: 本文提出了CoFineLLM框架，通过CP感知的微调方法减少LLM规划器的预测集大小，从而降低用户干预频率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM规划器在长视野任务中可靠性不足，产生过度自信的错误输出，且传统CP方法在较高置信度下会产生过大的预测集，导致频繁的人工干预。

Method: 引入CoFineLLM框架，这是首个CP感知的LLM微调框架，通过显式减少预测集大小来降低用户干预需求。

Result: 在多个语言指令机器人规划问题上评估，相比不确定性感知和不确定性不可知的微调基线，在预测集大小和求助率方面均取得一致改进。

Conclusion: 该方法在硬件实验中展示了对外分布场景的鲁棒性，能够有效提升LLM规划器的自主部署能力。

Abstract: Large Language Models (LLMs) have recently emerged as planners for
language-instructed agents, generating sequences of actions to accomplish
natural language tasks. However, their reliability remains a challenge,
especially in long-horizon tasks, since they often produce overconfident yet
wrong outputs. Conformal Prediction (CP) has been leveraged to address this
issue by wrapping LLM outputs into prediction sets that contain the correct
action with a user-defined confidence. When the prediction set is a singleton,
the planner executes that action; otherwise, it requests help from a user. This
has led to LLM-based planners that can ensure plan correctness with a
user-defined probability. However, as LLMs are trained in an
uncertainty-agnostic manner, without awareness of prediction sets, they tend to
produce unnecessarily large sets, particularly at higher confidence levels,
resulting in frequent human interventions limiting autonomous deployment. To
address this, we introduce CoFineLLM (Conformal Finetuning for LLMs), the first
CP-aware finetuning framework for LLM-based planners that explicitly reduces
prediction-set size and, in turn, the need for user interventions. We evaluate
our approach on multiple language-instructed robot planning problems and show
consistent improvements over uncertainty-aware and uncertainty-agnostic
finetuning baselines in terms of prediction-set size, and help rates. Finally,
we demonstrate robustness of our method to out-of-distribution scenarios in
hardware experiments.

</details>


### [201] [Underactuated Biomimetic Autonomous Underwater Vehicle for Ecosystem Monitoring](https://arxiv.org/abs/2511.06578)
*Kaustubh Singh,Shivam Kumar,Shashikant Pawar,Sandeep Manjanna*

Main category: cs.RO

TL;DR: 本文介绍了一种欠驱动仿生水下机器人，适用于海洋和淡水环境的生态系统监测，提出了改进的机械设计和基于强化学习的最小驱动行为。


<details>
  <summary>Details</summary>
Motivation: 开发适合海洋和淡水环境生态系统监测的仿生水下机器人，通过最小化驱动实现高效游泳行为。

Method: 采用改进的鱼形机器人机械设计，特别是尾部摆动机制，并在FishGym模拟器中使用强化学习技术学习最小驱动行为。

Result: 初步完成了尾部摆动机制的机械设计，并在模拟器中展示了游泳行为，为强化学习测试奠定了基础。

Conclusion: 该研究为开发高效、节能的仿生水下机器人提供了可行方案，强化学习方法有望优化机器人的游泳性能。

Abstract: In this paper, we present an underactuated biomimetic underwater robot that
is suitable for ecosystem monitoring in both marine and freshwater
environments. We present an updated mechanical design for a fish-like robot and
propose minimal actuation behaviors learned using reinforcement learning
techniques. We present our preliminary mechanical design of the tail
oscillation mechanism and illustrate the swimming behaviors on FishGym
simulator, where the reinforcement learning techniques will be tested on

</details>


### [202] [Rapidly Learning Soft Robot Control via Implicit Time-Stepping](https://arxiv.org/abs/2511.06667)
*Andrew Choi,Dezhong Tong*

Main category: cs.RO

TL;DR: 本文提出使用隐式时间步进方法实现软体机器人的快速策略学习，通过Dismech仿真器和delta自然曲率控制方法，在多种软体机械臂任务中实现了比传统方法快6-40倍的速度提升，且不牺牲准确性。


<details>
  <summary>Details</summary>
Motivation: 软体机器人仿真框架稀缺且计算成本高，导致策略学习难以实现，而刚性体仿真已广泛采用。本文旨在填补这一差距，证明通过隐式时间步进可以实现快速的软体机器人策略学习。

Method: 采用Dismech作为通用全隐式软体仿真器，处理软体动力学和摩擦接触；引入delta自然曲率控制方法，类似于刚性机械臂中的delta关节位置控制；在四个不同的软体机械臂任务中与Elastica框架进行对比。

Result: 使用隐式时间步进，500个环境的并行步进在非接触情况下速度提升达6倍，在接触丰富场景下速度提升达40倍；sim-to-sim评估显示隐式时间步进在获得显著加速的同时不牺牲准确性。

Conclusion: 隐式时间步进为软体机器人策略学习提供了一个罕见的"免费午餐"：在实现显著加速的同时保持了仿真精度，解决了软体机器人仿真中的计算瓶颈问题。

Abstract: With the explosive growth of rigid-body simulators, policy learning in
simulation has become the de facto standard for most rigid morphologies. In
contrast, soft robotic simulation frameworks remain scarce and are seldom
adopted by the soft robotics community. This gap stems partly from the lack of
easy-to-use, general-purpose frameworks and partly from the high computational
cost of accurately simulating continuum mechanics, which often renders policy
learning infeasible. In this work, we demonstrate that rapid soft robot policy
learning is indeed achievable via implicit time-stepping. Our simulator of
choice, DisMech, is a general-purpose, fully implicit soft-body simulator
capable of handling both soft dynamics and frictional contact. We further
introduce delta natural curvature control, a method analogous to delta joint
position control in rigid manipulators, providing an intuitive and effective
means of enacting control for soft robot learning. To highlight the benefits of
implicit time-stepping and delta curvature control, we conduct extensive
comparisons across four diverse soft manipulator tasks against one of the most
widely used soft-body frameworks, Elastica. With implicit time-stepping,
parallel stepping of 500 environments achieves up to 6x faster speeds for
non-contact cases and up to 40x faster for contact-rich scenarios. Finally, a
comprehensive sim-to-sim gap evaluation--training policies in one simulator and
evaluating them in another--demonstrates that implicit time-stepping provides a
rare free lunch: dramatic speedups achieved without sacrificing accuracy.

</details>


### [203] [Programmable Telescopic Soft Pneumatic Actuators for Deployable and Shape Morphing Soft Robots](https://arxiv.org/abs/2511.06673)
*Joel Kemp,Andre Farinha,David Howard,Krishna Manaswi Digumarti,Josh Pinskier*

Main category: cs.RO

TL;DR: 本文提出了一种可编程伸缩式软气动执行器（PTSPA），通过参数化设计解决软机器人设计维度灾难问题，实现可部署结构和受限空间操作。


<details>
  <summary>Details</summary>
Motivation: 软机器人具有丰富的自由形态和连续体特性，但由于维度灾难，目前缺乏直接利用设计自由度的可行方法。参数化设计集为创建具有丰富体现行为的模块化软机器人提供了途径。

Method: 开发了参数化几何生成器，从高级输入自定义执行器模型；通过半自动化实验和系统参数探索研究新的设计空间；表征执行器的伸缩/弯曲、膨胀和刚度特性。

Result: 揭示了关键设计参数与性能之间的明确关系；展示了执行器在可部署软四足机器人中的应用，其腿部可展开行走，实现自动适应受限空间。

Conclusion: PTSPA为可部署和形状变形结构以及需要大长度变化的应用提供了新的设计范式。

Abstract: Soft Robotics presents a rich canvas for free-form and continuum devices
capable of exerting forces in any direction and transforming between arbitrary
configurations. However, there is no current way to tractably and directly
exploit the design freedom due to the curse of dimensionality. Parameterisable
sets of designs offer a pathway towards tractable, modular soft robotics that
appropriately harness the behavioural freeform of soft structures to create
rich embodied behaviours. In this work, we present a parametrised class of soft
actuators, Programmable Telescopic Soft Pneumatic Actuators (PTSPAs). PTSPAs
expand axially on inflation for deployable structures and manipulation in
challenging confined spaces. We introduce a parametric geometry generator to
customise actuator models from high-level inputs, and explore the new design
space through semi-automated experimentation and systematic exploration of key
parameters. Using it we characterise the actuators' extension/bending,
expansion, and stiffness and reveal clear relationships between key design
parameters and performance. Finally we demonstrate the application of the
actuators in a deployable soft quadruped whose legs deploy to walk, enabling
automatic adaptation to confined spaces. PTSPAs present new design paradigm for
deployable and shape morphing structures and wherever large length changes are
required.

</details>


### [204] [Physically-Grounded Goal Imagination: Physics-Informed Variational Autoencoder for Self-Supervised Reinforcement Learning](https://arxiv.org/abs/2511.06745)
*Lan Thi Ha Nguyen,Kien Ton Manh,Anh Do Duc,Nam Pham Hai*

Main category: cs.RO

TL;DR: 提出了PI-RIG方法，通过增强型物理信息变分自编码器将物理约束直接集成到VAE训练中，生成物理一致且可实现的目标，解决了传统方法生成物理不可行目标的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督目标条件强化学习方法如RIG使用VAE在潜在空间中生成目标，但会产生物理上不可行的目标，阻碍学习效率。需要生成物理一致且可实现的目标来提升探索效果。

Method: 提出PI-RIG方法，使用增强型物理信息变分自编码器，将潜在空间显式分离为控制物体动力学的物理变量和捕捉视觉外观的环境因素，通过微分方程约束和守恒定律强制物理一致性。

Result: 实验表明，这种物理信息目标生成显著提高了提议目标的质量，在视觉机器人操作任务（包括到达、推动和抓取放置场景）中实现了更有效的探索和更好的技能获取。

Conclusion: 将物理约束直接集成到目标生成过程中能够产生物理一致且可实现的目标，显著提升自监督目标条件强化学习在机器人技能获取中的性能。

Abstract: Self-supervised goal-conditioned reinforcement learning enables robots to
autonomously acquire diverse skills without human supervision. However, a
central challenge is the goal setting problem: robots must propose feasible and
diverse goals that are achievable in their current environment. Existing
methods like RIG (Visual Reinforcement Learning with Imagined Goals) use
variational autoencoder (VAE) to generate goals in a learned latent space but
have the limitation of producing physically implausible goals that hinder
learning efficiency. We propose Physics-Informed RIG (PI-RIG), which integrates
physical constraints directly into the VAE training process through a novel
Enhanced Physics-Informed Variational Autoencoder (Enhanced p3-VAE), enabling
the generation of physically consistent and achievable goals. Our key
innovation is the explicit separation of the latent space into physics
variables governing object dynamics and environmental factors capturing visual
appearance, while enforcing physical consistency through differential equation
constraints and conservation laws. This enables the generation of physically
consistent and achievable goals that respect fundamental physical principles
such as object permanence, collision constraints, and dynamic feasibility.
Through extensive experiments, we demonstrate that this physics-informed goal
generation significantly improves the quality of proposed goals, leading to
more effective exploration and better skill acquisition in visual robotic
manipulation tasks including reaching, pushing, and pick-and-place scenarios.

</details>


### [205] [Semi-distributed Cross-modal Air-Ground Relative Localization](https://arxiv.org/abs/2511.06749)
*Weining Lu,Deer Bin,Lian Ma,Ming Ma,Zhihao Ma,Xiangyang Chen,Longfei Wang,Yixiao Feng,Zhouxian Jiang,Yongliang Shi,Bin Liang*

Main category: cs.RO

TL;DR: 提出了一种半分布式跨模态空地相对定位框架，通过解耦相对定位与状态估计，使用深度学习关键点和全局描述符，在通信带宽受限（<0.3 Mbps）下实现高效准确的地面-空中机器人相对定位。


<details>
  <summary>Details</summary>
Motivation: 当前机器人相对定位方法主要采用相同传感器配置的分布式多机器人SLAM系统，与所有机器人的状态估计紧密耦合，限制了灵活性和准确性。

Method: UGV和UAV独立执行SLAM并提取深度学习关键点和全局描述符，UGV使用LiDAR、相机和IMU进行局部Bundle Adjustment，采用稀疏关键点优化和两阶段BA过程，并实现基于深度学习描述符的增量闭环检测算法。

Result: 实验结果表明该方法在准确性和效率方面表现优异，通信带宽有效控制在0.3 Mbps以下。

Conclusion: 该方法通过解耦相对定位与状态估计，在有限通信带宽下实现了高效准确的地面-空中机器人相对定位，优于传统的多机器人SLAM方法。

Abstract: Efficient, accurate, and flexible relative localization is crucial in
air-ground collaborative tasks. However, current approaches for robot relative
localization are primarily realized in the form of distributed multi-robot SLAM
systems with the same sensor configuration, which are tightly coupled with the
state estimation of all robots, limiting both flexibility and accuracy. To this
end, we fully leverage the high capacity of Unmanned Ground Vehicle (UGV) to
integrate multiple sensors, enabling a semi-distributed cross-modal air-ground
relative localization framework. In this work, both the UGV and the Unmanned
Aerial Vehicle (UAV) independently perform SLAM while extracting deep
learning-based keypoints and global descriptors, which decouples the relative
localization from the state estimation of all agents. The UGV employs a local
Bundle Adjustment (BA) with LiDAR, camera, and an IMU to rapidly obtain
accurate relative pose estimates. The BA process adopts sparse keypoint
optimization and is divided into two stages: First, optimizing camera poses
interpolated from LiDAR-Inertial Odometry (LIO), followed by estimating the
relative camera poses between the UGV and UAV. Additionally, we implement an
incremental loop closure detection algorithm using deep learning-based
descriptors to maintain and retrieve keyframes efficiently. Experimental
results demonstrate that our method achieves outstanding performance in both
accuracy and efficiency. Unlike traditional multi-robot SLAM approaches that
transmit images or point clouds, our method only transmits keypoint pixels and
their descriptors, effectively constraining the communication bandwidth under
0.3 Mbps. Codes and data will be publicly available on
https://github.com/Ascbpiac/cross-model-relative-localization.git.

</details>


### [206] [SlotVLA: Towards Modeling of Object-Relation Representations in Robotic Manipulation](https://arxiv.org/abs/2511.06754)
*Taisei Hanyu,Nhat Chung,Huy Le,Toan Nguyen,Yuki Ikebe,Anthony Gunderman,Duy Nguyen Ho Minh,Khoa Vo,Tung Kieu,Kashu Yamazaki,Chase Rainwater,Anh Nguyen,Ngan Le*

Main category: cs.RO

TL;DR: 本文提出了一种基于物体关系和物体中心表示的多任务机器人操作方法，通过引入LIBERO+基准数据集和SlotVLA框架，实现了更结构化、高效且可解释的视觉运动控制。


<details>
  <summary>Details</summary>
Motivation: 受人类对离散物体及其关系推理的启发，探索紧凑的物体中心和物体关系表示是否能成为多任务机器人操作的基础。现有方法依赖密集嵌入，混淆了物体和背景线索，存在效率和可解释性问题。

Method: 提出SlotVLA框架：使用基于槽注意力的视觉标记器保持时间一致的物体表示，关系中心解码器生成任务相关嵌入，LLM驱动模块将嵌入转换为可执行动作。

Result: 在LIBERO+上的实验表明，物体中心槽和物体关系槽表示显著减少了所需视觉标记数量，同时保持了有竞争力的泛化性能。

Conclusion: LIBERO+和SlotVLA共同为推进物体关系中心的机器人操作提供了紧凑、可解释且有效的基础。

Abstract: Inspired by how humans reason over discrete objects and their relationships,
we explore whether compact object-centric and object-relation representations
can form a foundation for multitask robotic manipulation. Most existing robotic
multitask models rely on dense embeddings that entangle both object and
background cues, raising concerns about both efficiency and interpretability.
In contrast, we study object-relation-centric representations as a pathway to
more structured, efficient, and explainable visuomotor control. Our
contributions are two-fold. First, we introduce LIBERO+, a fine-grained
benchmark dataset designed to enable and evaluate object-relation reasoning in
robotic manipulation. Unlike prior datasets, LIBERO+ provides object-centric
annotations that enrich demonstrations with box- and mask-level labels as well
as instance-level temporal tracking, supporting compact and interpretable
visuomotor representations. Second, we propose SlotVLA, a slot-attention-based
framework that captures both objects and their relations for action decoding.
It uses a slot-based visual tokenizer to maintain consistent temporal object
representations, a relation-centric decoder to produce task-relevant
embeddings, and an LLM-driven module that translates these embeddings into
executable actions. Experiments on LIBERO+ demonstrate that object-centric slot
and object-relation slot representations drastically reduce the number of
required visual tokens, while providing competitive generalization. Together,
LIBERO+ and SlotVLA provide a compact, interpretable, and effective foundation
for advancing object-relation-centric robotic manipulation.

</details>


### [207] [Human-Level Actuation for Humanoids](https://arxiv.org/abs/2511.06796)
*MD-Nazmus Sunbeam*

Main category: cs.RO

TL;DR: 本文提出了一个量化评估仿人机器人驱动性能的框架，通过标准化关节坐标系、定义人类等效包络和人类级别驱动评分，使"人类级别"驱动性能可测量和可比较。


<details>
  <summary>Details</summary>
Motivation: 当前仿人机器人领域普遍声称达到"人类级别"驱动性能，但缺乏量化标准。峰值扭矩或速度规格无法反映在任务相关姿态和速率下扭矩、功率和耐力的综合表现。

Method: 方法包括三个组成部分：1) 基于ISB标准的自由度图谱标准化关节坐标系和运动范围；2) 人类等效包络(HEE)定义每个关节在相同关节角度和速率下同时满足人类扭矩和功率的要求；3) 人类级别驱动评分(HLAS)综合六个物理因素：工作空间覆盖、HEE覆盖、扭矩模式带宽、效率和热可持续性。

Result: 提供了详细的测量协议，包括测力计、电功率监测和热测试，可从可重复实验中获取所有HLAS输入。通过多关节仿人机器人的实例演示了HLAS计算，揭示了峰值扭矩规格所掩盖的驱动器权衡。

Conclusion: 该框架既可作为仿人机器人开发的设计规范，也可作为比较驱动系统的基准标准，所有组件都基于已发表的人类生物力学数据。

Abstract: Claims that humanoid robots achieve ``human-level'' actuation are common but
rarely quantified. Peak torque or speed specifications tell us little about
whether a joint can deliver the right combination of torque, power, and
endurance at task-relevant postures and rates. We introduce a comprehensive
framework that makes ``human-level'' measurable and comparable across systems.
Our approach has three components. First, a kinematic \emph{DoF atlas}
standardizes joint coordinate systems and ranges of motion using ISB-based
conventions, ensuring that human and robot joints are compared in the same
reference frames. Second, \emph{Human-Equivalence Envelopes (HEE)} define
per-joint requirements by measuring whether a robot meets human torque
\emph{and} power simultaneously at the same joint angle and rate $(q,\omega)$,
weighted by positive mechanical work in task-specific bands (walking, stairs,
lifting, reaching, and hand actions). Third, the \emph{Human-Level Actuation
Score (HLAS)} aggregates six physically grounded factors: workspace coverage
(ROM and DoF), HEE coverage, torque-mode bandwidth, efficiency, and thermal
sustainability. We provide detailed measurement protocols using dynamometry,
electrical power monitoring, and thermal testing that yield every HLAS input
from reproducible experiments. A worked example demonstrates HLAS computation
for a multi-joint humanoid, showing how the score exposes actuator trade-offs
(gearing ratio versus bandwidth and efficiency) that peak-torque specifications
obscure. The framework serves as both a design specification for humanoid
development and a benchmarking standard for comparing actuation systems, with
all components grounded in published human biomechanics data.

</details>


### [208] [Vision-Aided Online A* Path Planning for Efficient and Safe Navigation of Service Robots](https://arxiv.org/abs/2511.06801)
*Praveen Kumar,Tushar Sandhan*

Main category: cs.RO

TL;DR: 提出了一种将轻量级语义感知模块与在线A*路径规划器紧密集成的方法，使低成本机器人能够在复杂环境中基于视觉上下文进行实时导航，而不仅仅是物理障碍物。


<details>
  <summary>Details</summary>
Motivation: 传统导航系统依赖昂贵的LiDAR，虽然几何精度高但缺乏语义感知能力，无法区分重要文档与普通垃圾等视觉约束，限制了服务机器人在人类环境中的部署。

Method: 采用轻量级语义分割模型识别用户定义的视觉约束，将语义感知与几何数据融合，将视觉约束投影为非几何障碍物到全局地图中，结合在线A*规划器实现实时路径规划。

Result: 通过高保真仿真和真实机器人平台的广泛实验验证，框架展现出鲁棒的实时性能，证明低成本机器人能够安全导航复杂环境并尊重传统规划器无法感知的关键视觉线索。

Conclusion: 该框架成功弥合了语义感知与实时路径规划之间的差距，使经济实惠的机器人平台能够实现上下文感知导航，为服务机器人在人类中心环境中的部署提供了可行解决方案。

Abstract: The deployment of autonomous service robots in human-centric environments is
hindered by a critical gap in perception and planning. Traditional navigation
systems rely on expensive LiDARs that, while geometrically precise, are seman-
tically unaware, they cannot distinguish a important document on an office
floor from a harmless piece of litter, treating both as physically traversable.
While advanced semantic segmentation exists, no prior work has successfully
integrated this visual intelligence into a real-time path planner that is
efficient enough for low-cost, embedded hardware. This paper presents a frame-
work to bridge this gap, delivering context-aware navigation on an affordable
robotic platform. Our approach centers on a novel, tight integration of a
lightweight perception module with an online A* planner. The perception system
employs a semantic segmentation model to identify user-defined visual
constraints, enabling the robot to navigate based on contextual importance
rather than physical size alone. This adaptability allows an operator to define
what is critical for a given task, be it sensitive papers in an office or
safety lines in a factory, thus resolving the ambiguity of what to avoid. This
semantic perception is seamlessly fused with geometric data. The identified
visual constraints are projected as non-geometric obstacles onto a global map
that is continuously updated from sensor data, enabling robust navigation
through both partially known and unknown environments. We validate our
framework through extensive experiments in high-fidelity simulations and on a
real-world robotic platform. The results demonstrate robust, real-time
performance, proving that a cost- effective robot can safely navigate complex
environments while respecting critical visual cues invisible to traditional
planners.

</details>


### [209] [Vision-Based System Identification of a Quadrotor](https://arxiv.org/abs/2511.06839)
*Selim Ahmet Iz,Mustafa Unel*

Main category: cs.RO

TL;DR: 本文探讨了基于视觉的系统辨识技术在四旋翼无人机建模与控制中的应用，通过实验验证了视觉系统在解决推力与阻力系数不确定性方面的有效性，并设计了基于视觉系统数据的LQR控制器。


<details>
  <summary>Details</summary>
Motivation: 解决四旋翼无人机建模中的复杂性和局限性，特别是推力与阻力系数的不确定性问题，探索视觉系统在系统辨识和控制中的潜力。

Method: 采用灰箱建模方法缓解不确定性，评估机载视觉系统的有效性，基于视觉系统数据构建系统辨识模型并设计LQR控制器。

Result: 模型间表现一致，验证了基于视觉的系统辨识方法的有效性，证明了视觉技术在增强四旋翼建模和控制方面的潜力。

Conclusion: 基于视觉的技术能够显著提升四旋翼无人机的建模精度和控制性能，为未来在性能增强、故障检测和决策过程方面的研究奠定了基础。

Abstract: This paper explores the application of vision-based system identification
techniques in quadrotor modeling and control. Through experiments and analysis,
we address the complexities and limitations of quadrotor modeling, particularly
in relation to thrust and drag coefficients. Grey-box modeling is employed to
mitigate uncertainties, and the effectiveness of an onboard vision system is
evaluated. An LQR controller is designed based on a system identification model
using data from the onboard vision system. The results demonstrate consistent
performance between the models, validating the efficacy of vision based system
identification. This study highlights the potential of vision-based techniques
in enhancing quadrotor modeling and control, contributing to improved
performance and operational capabilities. Our findings provide insights into
the usability and consistency of these techniques, paving the way for future
research in quadrotor performance enhancement, fault detection, and
decision-making processes.

</details>


### [210] [Multi-Agent AI Framework for Road Situation Detection and C-ITS Message Generation](https://arxiv.org/abs/2511.06892)
*Kailin Tong,Selim Solmaz,Kenan Mujkic,Gottfried Allmer,Bo Leng*

Main category: cs.RO

TL;DR: 该论文提出了一个结合多模态大语言模型和视觉感知的多智能体AI框架，用于道路状况监测。该框架处理摄像头数据，协调专门的智能体进行状况检测、距离估计、决策制定和C-ITS消息生成。


<details>
  <summary>Details</summary>
Motivation: 传统道路状况检测方法在预定义场景下表现良好，但在未见过的案例中表现不佳且缺乏语义解释能力，这对于可靠的交通推荐至关重要。

Method: 使用多智能体AI框架，结合多模态大语言模型和基于视觉的感知，处理摄像头数据并协调专门智能体执行状况检测、距离估计、决策制定和C-ITS消息生成。

Result: 在103张图像上的评估显示，状况检测召回率达到100%，消息模式正确性完美；但存在误报检测，在车道数量、行驶车道状态和原因代码方面性能下降。Gemini-2.5-Flash在检测准确性和语义理解方面表现不如Gemini-2.0-Flash，且延迟更高。

Conclusion: 这些发现促使进一步研究针对智能交通应用定制的专门LLMs或MLLMs的微调工作。

Abstract: Conventional road-situation detection methods achieve strong performance in
predefined scenarios but fail in unseen cases and lack semantic interpretation,
which is crucial for reliable traffic recommendations. This work introduces a
multi-agent AI framework that combines multimodal large language models (MLLMs)
with vision-based perception for road-situation monitoring. The framework
processes camera feeds and coordinates dedicated agents for situation
detection, distance estimation, decision-making, and Cooperative Intelligent
Transport System (C-ITS) message generation. Evaluation is conducted on a
custom dataset of 103 images extracted from 20 videos of the TAD dataset. Both
Gemini-2.0-Flash and Gemini-2.5-Flash were evaluated. The results show 100\%
recall in situation detection and perfect message schema correctness; however,
both models suffer from false-positive detections and have reduced performance
in terms of number of lanes, driving lane status and cause code. Surprisingly,
Gemini-2.5-Flash, though more capable in general tasks, underperforms
Gemini-2.0-Flash in detection accuracy and semantic understanding and incurs
higher latency (Table II). These findings motivate further work on fine-tuning
specialized LLMs or MLLMs tailored for intelligent transportation applications.

</details>


### [211] [Integration of Visual SLAM into Consumer-Grade Automotive Localization](https://arxiv.org/abs/2511.06919)
*Luis Diener,Jens Kalkkuhl,Markus Enzweiler*

Main category: cs.RO

TL;DR: 该论文提出了一种融合视觉SLAM与车辆横向动力学模型的框架，用于在线校准陀螺仪，从而提升消费级车辆的定位精度。


<details>
  <summary>Details</summary>
Motivation: 当前消费级车辆的精确定位依赖于轮式里程计和IMU等本体传感器，但其性能受限于系统误差和校准问题。虽然视觉惯性SLAM在机器人领域已成为标准，但在汽车定位中的应用仍较少探索。

Method: 提出一个融合视觉SLAM与车辆横向动力学模型的框架，通过视觉信息在线校准陀螺仪，适用于实际驾驶条件。

Result: 实验结果表明，基于视觉的集成方法显著提高了陀螺仪校准精度，从而增强了整体定位性能。在专有和公共数据集上都显示出改进的性能，在公共基准测试中优于最先进方法。

Conclusion: 该研究为提升汽车定位精度提供了一条有前景的技术路径，证明了视觉SLAM在车辆定位系统中的有效集成价值。

Abstract: Accurate ego-motion estimation in consumer-grade vehicles currently relies on
proprioceptive sensors, i.e. wheel odometry and IMUs, whose performance is
limited by systematic errors and calibration. While visual-inertial SLAM has
become a standard in robotics, its integration into automotive ego-motion
estimation remains largely unexplored. This paper investigates how visual SLAM
can be integrated into consumer-grade vehicle localization systems to improve
performance. We propose a framework that fuses visual SLAM with a lateral
vehicle dynamics model to achieve online gyroscope calibration under realistic
driving conditions. Experimental results demonstrate that vision-based
integration significantly improves gyroscope calibration accuracy and thus
enhances overall localization performance, highlighting a promising path toward
higher automotive localization accuracy. We provide results on both proprietary
and public datasets, showing improved performance and superior localization
accuracy on a public benchmark compared to state-of-the-art methods.

</details>


### [212] [Raspi$^2$USBL: An open-source Raspberry Pi-Based Passive Inverted Ultra-Short Baseline Positioning System for Underwater Robotics](https://arxiv.org/abs/2511.06998)
*Jin Huang,Yingqiang Wang,Ying Chen*

Main category: cs.RO

TL;DR: Raspi^2USBL是一个基于树莓派的开源被动倒置超短基线定位系统，为水下机器人研究提供低成本解决方案，在实验中实现了优于0.1%的斜距精度和0.1°的方位精度。


<details>
  <summary>Details</summary>
Motivation: 解决水下精确定位难题，因为GNSS信号无法穿透海面，需要提供低成本、易获取的水下定位解决方案。

Method: 采用模块化硬件架构，包括水听器阵列、多通道前置放大器、OCXO、树莓派5和DAQ板，结合C++软件框架实现高精度时钟同步、实时信号处理和单程传播时间测量。

Result: 在消声池、淡水湖和公海试验中验证，斜距精度优于0.1%，方位精度在0.1°以内，在1.3公里距离内保持稳定性能。

Conclusion: 低成本可复现硬件能够提供研究级的水下定位精度，开源平台降低了水下机器人实验室的入门门槛，促进了可重复性和协作创新。

Abstract: Precise underwater positioning remains a fundamental challenge for underwater
robotics since global navigation satellite system (GNSS) signals cannot
penetrate the sea surface. This paper presents Raspi$^2$USBL, an open-source,
Raspberry Pi-based passive inverted ultra-short baseline (piUSBL) positioning
system designed to provide a low-cost and accessible solution for underwater
robotic research. The system comprises a passive acoustic receiver and an
active beacon. The receiver adopts a modular hardware architecture that
integrates a hydrophone array, a multichannel preamplifier, an oven-controlled
crystal oscillator (OCXO), a Raspberry Pi 5, and an MCC-series data acquisition
(DAQ) board. Apart from the Pi 5, OCXO, and MCC board, the beacon comprises an
impedance-matching network, a power amplifier, and a transmitting transducer.
An open-source C++ software framework provides high-precision clock
synchronization and triggering for one-way travel-time (OWTT) messaging, while
performing real-time signal processing, including matched filtering, array
beamforming, and adaptive gain control, to estimate the time of flight (TOF)
and direction of arrival (DOA) of received signals. The Raspi$^2$USBL system
was experimentally validated in an anechoic tank, freshwater lake, and open-sea
trials. Results demonstrate a slant-range accuracy better than 0.1%, a bearing
accuracy within 0.1$^\circ$, and stable performance over operational distances
up to 1.3 km. These findings confirm that low-cost, reproducible hardware can
deliver research-grade underwater positioning accuracy. By releasing both the
hardware and software as open-source, Raspi$^2$USBL provides a unified
reference platform that lowers the entry barrier for underwater robotics
laboratories, fosters reproducibility, and promotes collaborative innovation in
underwater acoustic navigation and swarm robotics.

</details>


### [213] [Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2511.07155)
*Thomas Steinecker,Alexander Bienemann,Denis Trescher,Thorsten Luettel,Mirko Maehlisch*

Main category: cs.RO

TL;DR: 本文提出了一种将运动规划与车辆控制解耦的框架，通过虚拟车辆与真实系统的时空对齐策略，实现强化学习从仿真到现实的零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 强化学习在机器人领域有潜力，但在真实车辆上部署面临挑战，主要由于车辆动力学复杂性和仿真与现实之间的不匹配，如轮胎特性、路面条件等因素使得准确建模真实世界动力学不可行。

Method: 首先在仿真中使用运动学自行车模型训练RL智能体输出连续控制动作，然后将其行为提炼为轨迹预测智能体生成有限时域的自车轨迹，实现虚拟与真实车辆的同步。部署时使用Stanley控制器控制横向动力学，纵向对齐通过自适应更新机制补偿虚拟与真实轨迹的偏差。

Result: 在真实车辆上验证了该方法，证明所提出的对齐策略能够实现基于强化学习的运动规划从仿真到现实的鲁棒零样本迁移。

Conclusion: 该框架成功地将高层轨迹生成与低层车辆控制解耦，为强化学习在真实车辆上的应用提供了可行方案。

Abstract: Reinforcement learning (RL) has shown promise in robotics, but deploying RL
on real vehicles remains challenging due to the complexity of vehicle dynamics
and the mismatch between simulation and reality. Factors such as tire
characteristics, road surface conditions, aerodynamic disturbances, and vehicle
load make it infeasible to model real-world dynamics accurately, which hinders
direct transfer of RL agents trained in simulation. In this paper, we present a
framework that decouples motion planning from vehicle control through a spatial
and temporal alignment strategy between a virtual vehicle and the real system.
An RL agent is first trained in simulation using a kinematic bicycle model to
output continuous control actions. Its behavior is then distilled into a
trajectory-predicting agent that generates finite-horizon ego-vehicle
trajectories, enabling synchronization between virtual and real vehicles. At
deployment, a Stanley controller governs lateral dynamics, while longitudinal
alignment is maintained through adaptive update mechanisms that compensate for
deviations between virtual and real trajectories. We validate our approach on a
real vehicle and demonstrate that the proposed alignment strategy enables
robust zero-shot transfer of RL-based motion planning from simulation to
reality, successfully decoupling high-level trajectory generation from
low-level vehicle control.

</details>


### [214] [Automated Generation of Continuous-Space Roadmaps for Routing Mobile Robot Fleets](https://arxiv.org/abs/2511.07175)
*Marvin Rüdt,Constantin Enke,Kai Furmans*

Main category: cs.RO

TL;DR: 本文提出了一种自动化路线图生成方法，在连续空间中操作，结合站点间运输需求并强制执行节点和边的最小距离约束，为移动机器人车队提供高效鲁棒的路由方案。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么是基于网格的（牺牲几何精度），要么是连续空间方法（忽略实际约束），在内部物流中移动机器人车队路由效率至关重要，路线图设计直接影响车队协调和计算性能。

Method: 结合自由空间离散化、运输需求驱动的K最短路径优化和路径平滑，生成针对内部物流应用的定制路线图。

Result: 在多个内部物流用例中的评估表明，该方法始终优于现有基线（4连接网格、8连接网格和随机采样），实现了更低的结枃复杂性、更高的冗余度和接近最优的路径长度。

Conclusion: 该方法能够实现移动机器人车队的高效鲁棒路由，为内部物流应用提供了优化的路线图解决方案。

Abstract: Efficient routing of mobile robot fleets is crucial in intralogistics, where
delays and deadlocks can substantially reduce system throughput. Roadmap
design, specifying feasible transport routes, directly affects fleet
coordination and computational performance. Existing approaches are either
grid-based, compromising geometric precision, or continuous-space approaches
that disregard practical constraints. This paper presents an automated roadmap
generation approach that bridges this gap by operating in continuous-space,
integrating station-to-station transport demand and enforcing minimum distance
constraints for nodes and edges. By combining free space discretization,
transport demand-driven $K$-shortest-path optimization, and path smoothing, the
approach produces roadmaps tailored to intralogistics applications. Evaluation
across multiple intralogistics use cases demonstrates that the proposed
approach consistently outperforms established baselines (4-connected grid,
8-connected grid, and random sampling), achieving lower structural complexity,
higher redundancy, and near-optimal path lengths, enabling efficient and robust
routing of mobile robot fleets.

</details>


### [215] [Robotic versus Human Teleoperation for Remote Ultrasound](https://arxiv.org/abs/2511.07275)
*David Black,Septimiu Salcudean*

Main category: cs.RO

TL;DR: 本文比较了人类远程操作与机器人远程操作在远程超声检查中的性能差异，发现人类远程操作在完成时间和位置精度上没有显著差异，且能提供更一致的力应用，同时更具实用性和可及性。


<details>
  <summary>Details</summary>
Motivation: 解决农村地区超声检查专家资源不足的问题，比较人类远程操作和机器人远程操作这两种远程超声技术的相对优势和性能差异。

Method: 通过实验比较人类与机器人远程操作的性能指标，包括设置时间、灵活性、完成时间、位置跟踪和力一致性等。

Result: 人类远程操作在完成时间（平均差异1.8%）和位置精度（平均差异0.5%）上没有统计学显著差异，且能提供更一致的力应用。

Conclusion: 人类远程操作在性能上与机器人远程操作相当，同时具有更低的成本和复杂性，更适合小型社区的实际应用。

Abstract: Diagnostic medical ultrasound is widely used, safe, and relatively low cost
but requires a high degree of expertise to acquire and interpret the images.
Personnel with this expertise are often not available outside of larger cities,
leading to difficult, costly travel and long wait times for rural populations.
To address this issue, tele-ultrasound techniques are being developed,
including robotic teleoperation and recently human teleoperation, in which a
novice user is remotely guided in a hand-over-hand manner through mixed reality
to perform an ultrasound exam. These methods have not been compared, and their
relative strengths are unknown. Human teleoperation may be more practical than
robotics for small communities due to its lower cost and complexity, but this
is only relevant if the performance is comparable. This paper therefore
evaluates the differences between human and robotic teleoperation, examining
practical aspects such as setup time and flexibility and experimentally
comparing performance metrics such as completion time, position tracking, and
force consistency. It is found that human teleoperation does not lead to
statistically significant differences in completion time or position accuracy,
with mean differences of 1.8% and 0.5%, respectively, and provides more
consistent force application despite being substantially more practical and
accessible.

</details>


### [216] [PlanT 2.0: Exposing Biases and Structural Flaws in Closed-Loop Driving](https://arxiv.org/abs/2511.07292)
*Simon Gerstenecker,Andreas Geiger,Katrin Renz*

Main category: cs.RO

TL;DR: 本文系统分析了自动驾驶模型失败的原因，提出了PlanT 2.0规划变换器，在CARLA基准测试中达到最先进性能，并揭示了模型缺乏场景理解、专家行为僵化等关键问题，主张转向以数据为中心的开发方法。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶研究过于关注基准性能和方法创新，缺乏对模型失败、偏见和捷径学习的深入分析，导致改进有限且无法理解失败的根本原因。

Method: 引入PlanT 2.0，一个轻量级、以对象为中心的规划变换器，通过扰动输入（如改变对象位置或增减对象）进行受控分析，并对原始PlanT进行多项升级以应对CARLA Leaderboard 2.0的挑战场景。

Result: 在Longest6 v2、Bench2Drive和CARLA验证路线上达到最先进性能，分析揭示了模型缺乏场景理解、专家行为僵化导致可被利用的捷径、对固定专家轨迹过度拟合等深刻失败模式。

Conclusion: 基于分析结果，主张转向以数据为中心的开发方法，重点关注更丰富、更鲁棒、偏见更少的数据集，并开源了代码和模型。

Abstract: Most recent work in autonomous driving has prioritized benchmark performance
and methodological innovation over in-depth analysis of model failures, biases,
and shortcut learning. This has led to incremental improvements without a deep
understanding of the current failures. While it is straightforward to look at
situations where the model fails, it is hard to understand the underlying
reason. This motivates us to conduct a systematic study, where inputs to the
model are perturbed and the predictions observed. We introduce PlanT 2.0, a
lightweight, object-centric planning transformer designed for autonomous
driving research in CARLA. The object-level representation enables controlled
analysis, as the input can be easily perturbed (e.g., by changing the location
or adding or removing certain objects), in contrast to sensor-based models. To
tackle the scenarios newly introduced by the challenging CARLA Leaderboard 2.0,
we introduce multiple upgrades to PlanT, achieving state-of-the-art performance
on Longest6 v2, Bench2Drive, and the CARLA validation routes. Our analysis
exposes insightful failures, such as a lack of scene understanding caused by
low obstacle diversity, rigid expert behaviors leading to exploitable
shortcuts, and overfitting to a fixed set of expert trajectories. Based on
these findings, we argue for a shift toward data-centric development, with a
focus on richer, more robust, and less biased datasets. We open-source our code
and model at https://github.com/autonomousvision/plant2.

</details>


### [217] [Exact Smooth Reformulations for Trajectory Optimization Under Signal Temporal Logic Specifications](https://arxiv.org/abs/2511.07375)
*Shaohang Han,Joris Verhagen,Jana Tumova*

Main category: cs.RO

TL;DR: 提出了一种基于信号时序逻辑（STL）的运动规划方法，通过精确重构max和min算子实现可微优化问题，确保方法的精确性、平滑性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 研究在信号时序逻辑（STL）规范下的运动规划问题，STL是一种用于指定时空需求的有用形式化方法。

Method: 将STL合成问题构建为轨迹优化问题，利用STL鲁棒性语义，引入max和min算子的精确重构以获得无近似误差的可微问题。

Result: 该方法在数值模拟中得到验证，展示了其实际性能。

Conclusion: 所提出的方法是精确的、平滑的且可靠的，适用于STL规范下的运动规划任务。

Abstract: We study motion planning under Signal Temporal Logic (STL), a useful
formalism for specifying spatial-temporal requirements. We pose STL synthesis
as a trajectory optimization problem leveraging the STL robustness semantics.
To obtain a differentiable problem without approximation error, we introduce an
exact reformulation of the max and min operators. The resulting method is
exact, smooth, and sound. We validate it in numerical simulations,
demonstrating its practical performance.

</details>


### [218] [Unified Humanoid Fall-Safety Policy from a Few Demonstrations](https://arxiv.org/abs/2511.07407)
*Zhengjie Xu,Ye Li,Kwan-yee Lin,Stella X. Yu*

Main category: cs.RO

TL;DR: 本文提出了一种统一策略，将人形机器人的防摔倒、冲击缓解和快速恢复整合到一个策略中，通过结合稀疏人类演示、强化学习和自适应扩散记忆来实现安全自主的摔倒恢复全过程。


<details>
  <summary>Details</summary>
Motivation: 人形机器人摔倒是一个固有风险，现有方法只关注摔倒的孤立方面（防摔、控制下降或站起），缺乏完整的摔倒恢复策略。本文旨在实现从防摔到恢复的完整安全自主过程。

Method: 融合稀疏人类演示与强化学习，结合自适应扩散记忆的安全反应，学习自适应全身行为，统一防摔、冲击缓解和快速恢复。

Result: 在仿真和Unitree G1机器人上的实验展示了鲁棒的仿真到现实迁移、更低的冲击力和在各种干扰下一致的快速恢复能力。

Conclusion: 该方法为实现更安全、更具韧性的人形机器人在真实环境中应用指明了方向。

Abstract: Falling is an inherent risk of humanoid mobility. Maintaining stability is
thus a primary safety focus in robot control and learning, yet no existing
approach fully averts loss of balance. When instability does occur, prior work
addresses only isolated aspects of falling: avoiding falls, choreographing a
controlled descent, or standing up afterward. Consequently, humanoid robots
lack integrated strategies for impact mitigation and prompt recovery when real
falls defy these scripts. We aim to go beyond keeping balance to make the
entire fall-and-recovery process safe and autonomous: prevent falls when
possible, reduce impact when unavoidable, and stand up when fallen. By fusing
sparse human demonstrations with reinforcement learning and an adaptive
diffusion-based memory of safe reactions, we learn adaptive whole-body
behaviors that unify fall prevention, impact mitigation, and rapid recovery in
one policy. Experiments in simulation and on a Unitree G1 demonstrate robust
sim-to-real transfer, lower impact forces, and consistently fast recovery
across diverse disturbances, pointing towards safer, more resilient humanoids
in real environments. Videos are available at https://firm2025.github.io/.

</details>


### [219] [Using Vision Language Models as Closed-Loop Symbolic Planners for Robotic Applications: A Control-Theoretic Perspective](https://arxiv.org/abs/2511.07410)
*Hao Wang,Sathwik Karnik,Bea Lim,Somil Bansal*

Main category: cs.RO

TL;DR: 本文从控制理论视角研究如何将视觉语言模型(VLMs)用作机器人应用中的闭环符号规划器，重点探讨控制时域和预热启动对VLM符号规划器性能的影响。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型(LLMs)和视觉语言模型(VLMs)在具身符号规划中广泛应用，但如何有效用于闭环符号规划仍待探索。由于这些模型是黑箱，可能产生不可预测或代价高昂的错误，使其在高级机器人规划中面临挑战。

Method: 从控制理论视角研究VLM作为闭环符号规划器，设计并进行了控制实验，重点分析控制时域和预热启动对性能的影响。

Result: 通过实验获得了对VLM作为闭环符号规划器广泛适用的见解，并提出了改进VLM符号规划器性能的建议。

Conclusion: 研究为有效利用VLM作为闭环符号规划器提供了控制理论视角的指导，相关发现和建议有助于提升VLM符号规划器的性能。

Abstract: Large Language Models (LLMs) and Vision Language Models (VLMs) have been
widely used for embodied symbolic planning. Yet, how to effectively use these
models for closed-loop symbolic planning remains largely unexplored. Because
they operate as black boxes, LLMs and VLMs can produce unpredictable or costly
errors, making their use in high-level robotic planning especially challenging.
In this work, we investigate how to use VLMs as closed-loop symbolic planners
for robotic applications from a control-theoretic perspective. Concretely, we
study how the control horizon and warm-starting impact the performance of VLM
symbolic planners. We design and conduct controlled experiments to gain
insights that are broadly applicable to utilizing VLMs as closed-loop symbolic
planners, and we discuss recommendations that can help improve the performance
of VLM symbolic planners.

</details>


### [220] [Robot Learning from a Physical World Model](https://arxiv.org/abs/2511.07416)
*Jiageng Mao,Sicheng He,Hao-Ning Wu,Yang You,Shuyang Sun,Zhicheng Wang,Yanan Bao,Huizhong Chen,Leonidas Guibas,Vitor Guizilini,Howard Zhou,Yue Wang*

Main category: cs.RO

TL;DR: PhysWorld是一个通过物理世界建模实现机器人从视频生成中学习的框架，将视频生成与物理世界重建相结合，将视觉指导转化为物理可执行的机器人轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型可以从语言命令和图像合成逼真的视觉演示，但直接将生成的像素运动重定向到机器人会忽略物理约束，导致操作不准确。

Method: 结合视频生成与物理世界重建，通过对象中心残差强化学习和物理世界模型，将生成的视频运动转化为物理准确的动作。

Result: 在多样化真实世界任务上的实验表明，PhysWorld相比先前方法显著提高了操作准确性。

Conclusion: PhysWorld能够将隐式视觉指导转化为物理可执行的机器人轨迹，无需真实机器人数据收集，实现零样本可泛化的机器人操作。

Abstract: We introduce PhysWorld, a framework that enables robot learning from video
generation through physical world modeling. Recent video generation models can
synthesize photorealistic visual demonstrations from language commands and
images, offering a powerful yet underexplored source of training signals for
robotics. However, directly retargeting pixel motions from generated videos to
robots neglects physics, often resulting in inaccurate manipulations. PhysWorld
addresses this limitation by coupling video generation with physical world
reconstruction. Given a single image and a task command, our method generates
task-conditioned videos and reconstructs the underlying physical world from the
videos, and the generated video motions are grounded into physically accurate
actions through object-centric residual reinforcement learning with the
physical world model. This synergy transforms implicit visual guidance into
physically executable robotic trajectories, eliminating the need for real robot
data collection and enabling zero-shot generalizable robotic manipulation.
Experiments on diverse real-world tasks demonstrate that PhysWorld
substantially improves manipulation accuracy compared to previous approaches.
Visit \href{https://pointscoder.github.io/PhysWorld_Web/}{the project webpage}
for details.

</details>


### [221] [Lightning Grasp: High Performance Procedural Grasp Synthesis with Contact Fields](https://arxiv.org/abs/2511.07418)
*Zhao-Heng Yin,Pieter Abbeel*

Main category: cs.RO

TL;DR: Lightning Grasp是一种新颖的高性能程序化抓取合成算法，通过接触场数据结构实现数量级的速度提升，支持不规则工具类物体的无监督抓取生成。


<details>
  <summary>Details</summary>
Motivation: 解决实时多样化抓取合成这一机器人学和计算机图形学中的核心挑战，克服现有方法需要精心调整能量函数和敏感初始化的局限性。

Method: 通过接触场数据结构将复杂几何计算与搜索过程解耦，实现程序化搜索，避免传统方法的限制。

Result: 实现了相比最先进方法数量级的速度提升，能够无监督生成不规则工具类物体的抓取。

Conclusion: 该方法通过接触场抽象显著降低了问题复杂度，为机器人操作领域的创新提供了开源系统支持。

Abstract: Despite years of research, real-time diverse grasp synthesis for dexterous
hands remains an unsolved core challenge in robotics and computer graphics. We
present Lightning Grasp, a novel high-performance procedural grasp synthesis
algorithm that achieves orders-of-magnitude speedups over state-of-the-art
approaches, while enabling unsupervised grasp generation for irregular,
tool-like objects. The method avoids many limitations of prior approaches, such
as the need for carefully tuned energy functions and sensitive initialization.
This breakthrough is driven by a key insight: decoupling complex geometric
computation from the search process via a simple, efficient data structure -
the Contact Field. This abstraction collapses the problem complexity, enabling
a procedural search at unprecedented speeds. We open-source our system to
propel further innovation in robotic manipulation.

</details>
