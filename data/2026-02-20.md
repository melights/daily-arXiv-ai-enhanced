<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 42]
- [cs.RO](#cs.RO) [Total: 26]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Three-dimensional Damage Visualization of Civil Structures via Gaussian Splatting-enabled Digital Twins](https://arxiv.org/abs/2602.16713)
*Shuo Wang,Shuo Wang,Xin Nie,Yasutaka Narazaki,Thomas Matiki,Billie F. Spencer*

Main category: cs.CV

TL;DR: 该研究提出了一种基于高斯溅射的数字孪生方法，用于土木基础设施的三维损伤可视化，相比传统方法在效率和细节保留方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 土木基础设施检测需要超越传统2D图像损伤识别的精确三维损伤可视化。现有方法如神经辐射场和传统摄影测量技术在场景表示、渲染质量和处理特征缺失区域方面存在局限性。

Method: 采用高斯溅射技术进行三维重建，开发多尺度重建策略平衡效率与损伤细节，并支持随时间演变的数字孪生更新。在开源合成地震后检测数据集上进行验证。

Result: 该方法能够有效可视化2D损伤分割结果并减少分割误差，在效率和损伤细节保留方面取得良好平衡，为土木基础设施数字孪生提供了全面的三维损伤可视化解决方案。

Conclusion: 基于高斯溅射的数字孪生方法为土木基础设施三维损伤可视化提供了有前景的解决方案，在场景表示、渲染质量和处理特征缺失区域方面优于传统方法。

Abstract: Recent advancements in civil infrastructure inspections underscore the need for precise three-dimensional (3D) damage visualization on digital twins, transcending traditional 2D image-based damage identifications. Compared to conventional photogrammetric 3D reconstruction techniques, modern approaches such as Neural Radiance Field (NeRF) and Gaussian Splatting (GS) excel in scene representation, rendering quality, and handling featureless regions. Among them, GS stands out for its efficiency, leveraging discrete anisotropic 3D Gaussians to represent radiance fields, unlike NeRF's continuous implicit model. This study introduces a GS-enabled digital twin method tailored for effective 3D damage visualization. The method's key contributions include: 1) utilizing GS-based 3D reconstruction to visualize 2D damage segmentation results while reducing segmentation errors; 2) developing a multi-scale reconstruction strategy to balance efficiency and damage detail; 3) enabling digital twin updates as damage evolves over time. Demonstrated on an open-source synthetic dataset for post-earthquake inspections, the proposed approach offers a promising solution for comprehensive 3D damage visualization in civil infrastructure digital twins.

</details>


### [2] [Analytic Score Optimization for Multi Dimension Video Quality Assessment](https://arxiv.org/abs/2602.16856)
*Boda Lin,Yongjie Zhu,Wenyu Qin,Meng Wang,Pengfei Wan*

Main category: cs.CV

TL;DR: UltraVQA数据集包含五个质量维度的多维度视频质量评估，通过Analytic Score Optimization方法优化离散质量评分，在实验中超越多数基线模型。


<details>
  <summary>Details</summary>
Motivation: 视频质量评估正在从单一的平均意见分数向更丰富的多维度评估发展，需要包含多个质量维度的数据集和更好的评估方法。

Method: 1) 构建UltraVQA大规模多维度VQA数据集，包含用户生成内容，标注五个关键质量维度；2) 提出Analytic Score Optimization (ASO)方法，将质量评估重新定义为正则化决策过程，获得闭式解来捕捉人类评分的序数性质。

Result: 方法在实验中超越了大多数基线模型（包括闭源API和开源模型），同时减少了质量预测的平均绝对误差(MAE)。

Conclusion: 多维度、可解释的标注和基于强化学习的对齐在推进视频质量评估中具有重要意义。

Abstract: Video Quality Assessment (VQA) is evolving beyond single-number mean opinion score toward richer, multi-faceted evaluations of video content. In this paper, we present a large-scale multi-dimensional VQA dataset UltraVQA that encompasses diverse User-Generated Content~(UGC) annotated across five key quality dimensions: Motion Quality, Motion Amplitude, Aesthetic Quality, Content Quality, and Clarity Quality. Each video in our dataset is scored by over 3 human raters on these dimensions, with fine-grained sub-attribute labels, and accompanied by an explanatory rationale generated by GPT based on the collective human judgments. To better leverage these rich annotations and improve discrete quality score assessment, we introduce Analytic Score Optimization (ASO), a theoretically grounded post-training objective derived for multi-dimensional VQA. By reframing quality assessment as a regularized decision-making process, we obtain a closed-form solution that naturally captures the ordinal nature of human ratings, ensuring alignment with human ranking preferences. In experiments, our method outperforms most baselines including closed-source APIs and open-source models, while also reducing mean absolute error (MAE) in quality prediction. Our work highlights the importance of multi-dimensional, interpretable annotations and reinforcement-based alignment in advancing video quality assessment.

</details>


### [3] [DODO: Discrete OCR Diffusion Models](https://arxiv.org/abs/2602.16872)
*Sean Man,Roy Ganz,Roi Ronen,Shahar Tsiper,Shai Mazor,Niv Nayman*

Main category: cs.CV

TL;DR: DODO是一个基于块离散扩散的视觉语言模型，用于光学字符识别任务，相比自回归方法实现了3倍加速，同时保持接近SOTA的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前基于自回归解码的视觉语言模型在OCR任务中计算成本高、速度慢，特别是处理长文档时。OCR是高度确定性的任务，理论上可以通过扩散模型实现高效并行解码，但现有掩码扩散模型存在结构不稳定问题，无法满足OCR的精确匹配要求。

Method: 提出了DODO模型，采用块离散扩散方法。通过将生成过程分解为块，缓解了全局扩散中的同步错误问题，从而在保持OCR任务精确性的同时实现并行解码。

Result: 实验表明，DODO在保持接近最先进准确率的同时，相比自回归基线实现了高达3倍的推理加速。

Conclusion: DODO首次成功将块离散扩散应用于OCR任务，克服了现有扩散模型的结构不稳定性问题，为确定性视觉语言任务提供了高效的并行解码解决方案。

Abstract: Optical Character Recognition (OCR) is a fundamental task for digitizing information, serving as a critical bridge between visual data and textual understanding. While modern Vision-Language Models (VLM) have achieved high accuracy in this domain, they predominantly rely on autoregressive decoding, which becomes computationally expensive and slow for long documents as it requires a sequential forward pass for every generated token. We identify a key opportunity to overcome this bottleneck: unlike open-ended generation, OCR is a highly deterministic task where the visual input strictly dictates a unique output sequence, theoretically enabling efficient, parallel decoding via diffusion models. However, we show that existing masked diffusion models fail to harness this potential; those introduce structural instabilities that are benign in flexible tasks, like captioning, but catastrophic for the rigid, exact-match requirements of OCR. To bridge this gap, we introduce DODO, the first VLM to utilize block discrete diffusion and unlock its speedup potential for OCR. By decomposing generation into blocks, DODO mitigates the synchronization errors of global diffusion. Empirically, our method achieves near state-of-the-art accuracy while enabling up to 3x faster inference compared to autoregressive baselines.

</details>


### [4] [StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation](https://arxiv.org/abs/2602.16915)
*Zeyu Ren,Xiang Li,Yiran Wang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: StereoAdapter-2提出基于选择性状态空间模型的ConvSS2D算子替代传统ConvGRU，结合大规模合成水下立体数据集UW-StereoDepth-80K，显著提升水下立体深度估计性能


<details>
  <summary>Details</summary>
Motivation: 水下立体深度估计面临波长相关光衰减、散射和折射导致的严重域偏移问题。现有基于GRU的迭代细化方法需要多次迭代进行长距离视差传播，在大视差和无纹理水下区域性能受限

Method: 1) 提出ConvSS2D算子，基于选择性状态空间模型，采用四向扫描策略对齐极线几何并捕获垂直结构一致性；2) 构建UW-StereoDepth-80K大规模合成水下立体数据集，通过语义感知风格迁移和几何一致新视角合成的两阶段生成流程；3) 继承StereoAdapter的动态LoRA适应

Result: 在TartanAir-UW基准上提升17%，在SQUID基准上提升7.2%，达到最先进的零样本性能。在BlueROV2平台上的真实世界验证证明了方法的鲁棒性

Conclusion: StereoAdapter-2通过ConvSS2D算子和大规模合成数据集，实现了高效的长距离空间传播和显著的水下立体深度估计性能提升，为水下机器人感知提供了有效解决方案

Abstract: Stereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models. The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2.

</details>


### [5] [Xray-Visual Models: Scaling Vision models on Industry Scale Data](https://arxiv.org/abs/2602.16918)
*Shlok Mishra,Tsung-Yu Lin,Linda Wang,Hongli Xu,Yimin Liu,Michael Hsu,Chaitanya Ahuja,Hao Yuan,Jianpeng Cheng,Hong-You Chen,Haoyuan Xu,Chao Li,Abhijeet Awasthi,Jihye Moon,Don Husa,Michael Ge,Sumedha Singla,Arkabandhu Chowdhury,Phong Dingh,Satya Narayan Shukla,Yonghuan Yang,David Jacobs,Qi Guo,Jun Xiao,Xiangjun Fan,Aashu Singh*

Main category: cs.CV

TL;DR: Xray-Visual是一个统一的大规模图像和视频理解视觉模型，基于行业级社交媒体数据训练，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够统一处理大规模图像和视频理解任务的视觉模型，利用社交媒体海量数据提升模型性能，同时保持计算效率。

Method: 采用三阶段训练流程：自监督MAE、半监督标签分类和CLIP风格对比学习；基于Vision Transformer架构，增强EViT高效令牌重组技术；使用超过150亿图像-文本对和100亿视频-标签对社交媒体数据。

Result: 在ImageNet图像分类、Kinetics和HMDB51视频理解、MSCOCO跨模态检索等基准测试中达到最先进性能；对领域偏移和对抗扰动表现出强鲁棒性；集成大语言模型作为文本编码器进一步提升检索性能和泛化能力。

Conclusion: Xray-Visual为可扩展的多模态视觉模型设立了新基准，在保持计算效率的同时实现了卓越的准确性和鲁棒性，特别适用于现实世界环境。

Abstract: We present Xray-Visual, a unified vision model architecture for large-scale image and video understanding trained on industry-scale social media data. Our model leverages over 15 billion curated image-text pairs and 10 billion video-hashtag pairs from Facebook and Instagram, employing robust data curation pipelines that incorporate balancing and noise suppression strategies to maximize semantic diversity while minimizing label noise. We introduce a three-stage training pipeline that combines self-supervised MAE, semi-supervised hashtag classification, and CLIP-style contrastive learning to jointly optimize image and video modalities. Our architecture builds on a Vision Transformer backbone enhanced with efficient token reorganization (EViT) for improved computational efficiency. Extensive experiments demonstrate that Xray-Visual achieves state-of-the-art performance across diverse benchmarks, including ImageNet for image classification, Kinetics and HMDB51 for video understanding, and MSCOCO for cross-modal retrieval. The model exhibits strong robustness to domain shift and adversarial perturbations. We further demonstrate that integrating large language models as text encoders (LLM2CLIP) significantly enhances retrieval performance and generalization capabilities, particularly in real-world environments. Xray-Visual establishes new benchmarks for scalable, multimodal vision models, while maintaining superior accuracy and computational efficiency.

</details>


### [6] [HS-3D-NeRF: 3D Surface and Hyperspectral Reconstruction From Stationary Hyperspectral Images Using Multi-Channel NeRFs](https://arxiv.org/abs/2602.16950)
*Kibon Ku,Talukder Z. Jubery,Adarsh Krishnamurthy,Baskar Ganapathysubramanian*

Main category: cs.CV

TL;DR: HSI-SC-NeRF：一种用于农业产品高通量高光谱3D重建的静态相机多通道NeRF框架，适用于自动化农业工作流程


<details>
  <summary>Details</summary>
Motivation: 高光谱成像（HSI）和3D重建技术对农业产品质量和植物表型分析至关重要，但现有方法难以在自动化表型系统中大规模集成这两种模态。传统方法硬件复杂，而基于移动相机的NeRF方法在标准室内农业环境中吞吐量和可重复性受限。

Method: 提出HSI-SC-NeRF框架：使用静态相机采集多视角高光谱数据，物体在特氟龙成像室内旋转；通过ArUco标定标记估计物体姿态并转换到相机坐标系；采用多通道NeRF公式通过复合光谱损失联合优化所有高光谱波段重建；使用两阶段训练协议分离几何初始化和辐射度优化。

Result: 在三个农业产品样本上的实验表明，HSI-SC-NeRF在可见光和近红外光谱范围内实现了高空间重建精度和强光谱保真度。

Conclusion: HSI-SC-NeRF框架适合集成到自动化农业工作流程中，为农业产品质量检测和表型分析提供了高效的高光谱3D重建解决方案。

Abstract: Advances in hyperspectral imaging (HSI) and 3D reconstruction have enabled accurate, high-throughput characterization of agricultural produce quality and plant phenotypes, both essential for advancing agricultural sustainability and breeding programs. HSI captures detailed biochemical features of produce, while 3D geometric data substantially improves morphological analysis. However, integrating these two modalities at scale remains challenging, as conventional approaches involve complex hardware setups incompatible with automated phenotyping systems. Recent advances in neural radiance fields (NeRF) offer computationally efficient 3D reconstruction but typically require moving-camera setups, limiting throughput and reproducibility in standard indoor agricultural environments. To address these challenges, we introduce HSI-SC-NeRF, a stationary-camera multi-channel NeRF framework for high-throughput hyperspectral 3D reconstruction targeting postharvest inspection of agricultural produce. Multi-view hyperspectral data is captured using a stationary camera while the object rotates within a custom-built Teflon imaging chamber providing diffuse, uniform illumination. Object poses are estimated via ArUco calibration markers and transformed to the camera frame of reference through simulated pose transformations, enabling standard NeRF training on stationary-camera data. A multi-channel NeRF formulation optimizes reconstruction across all hyperspectral bands jointly using a composite spectral loss, supported by a two-stage training protocol that decouples geometric initialization from radiometric refinement. Experiments on three agricultural produce samples demonstrate high spatial reconstruction accuracy and strong spectral fidelity across the visible and near-infrared spectrum, confirming the suitability of HSI-SC-NeRF for integration into automated agricultural workflows.

</details>


### [7] [DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.16968)
*Dahye Kim,Deepti Ghadiyaram,Raghudeep Gadde*

Main category: cs.CV

TL;DR: 提出动态标记化方法，根据内容复杂度和去噪时间步动态调整补丁大小，显著降低DiTs的计算成本而不影响生成质量


<details>
  <summary>Details</summary>
Motivation: 扩散变换器(DiTs)在图像和视频生成方面取得了最先进的性能，但其成功伴随着沉重的计算成本。这种低效率主要源于固定的标记化过程，在整个去噪阶段使用恒定大小的补丁，而不考虑内容的复杂性。

Method: 提出动态标记化策略，根据内容复杂度和去噪时间步动态调整补丁大小。关键洞察是：早期时间步只需要较粗糙的补丁来建模全局结构，而后期迭代需要更精细（更小尺寸）的补丁来细化局部细节。在推理过程中，该方法动态重新分配去噪步骤中的补丁大小。

Result: 在FLUX-1.Dev上实现高达3.52倍的加速，在Wan 2.1上实现3.2倍的加速，同时不损害生成质量和提示遵循性。

Conclusion: 动态标记化是一种高效的测试时策略，能够显著降低扩散变换器的计算成本，同时保持感知生成质量，为高效图像和视频生成提供了有前景的解决方案。

Abstract: Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to $3.52\times$ and $3.2\times$ speedup on FLUX-1.Dev and Wan $2.1$, respectively, without compromising the generation quality and prompt adherence.

</details>


### [8] [Characterizing the Predictive Impact of Modalities with Supervised Latent-Variable Modeling](https://arxiv.org/abs/2602.16979)
*Divyam Madaan,Sumit Chopra,Kyunghyun Cho*

Main category: cs.CV

TL;DR: PRIMO是一种监督式潜在变量填补模型，用于处理多模态学习中模态缺失的问题，通过建模缺失模态与观测模态的关系，实现预测并量化缺失模态对预测的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型通常假设训练和推理时所有模态都可用，但实际应用中多模态数据往往不完整（模态缺失、异步收集或仅部分可用）。需要一种方法能够利用所有可用的训练样本（无论模态是否完整），并量化缺失模态对预测的影响。

Method: PRIMO通过潜在变量建模缺失模态与观测模态在预测上下文中的关系。训练时使用所有可用样本（完整或部分模态），推理时从学习的缺失模态分布中采样多次，获得边际预测分布并分析缺失模态对每个实例预测的影响。

Result: 在合成XOR数据集、Audio-Vision MNIST和MIMIC-III（死亡率和ICD-9预测）上评估。PRIMO在模态完全缺失时性能与单模态基线相当，在所有模态可用时与多模态基线相当。通过基于方差的指标量化模态在实例级别的预测影响。

Conclusion: PRIMO能够有效处理多模态数据中的模态缺失问题，利用所有可用训练数据，提供可靠的预测，并能量化缺失模态对每个预测实例的影响，展示了不同缺失模态补全如何产生一组合理的标签。

Abstract: Despite the recent success of Multimodal Large Language Models (MLLMs), existing approaches predominantly assume the availability of multiple modalities during training and inference. In practice, multimodal data is often incomplete because modalities may be missing, collected asynchronously, or available only for a subset of examples. In this work, we propose PRIMO, a supervised latent-variable imputation model that quantifies the predictive impact of any missing modality within the multimodal learning setting. PRIMO enables the use of all available training examples, whether modalities are complete or partial. Specifically, it models the missing modality through a latent variable that captures its relationship with the observed modality in the context of prediction. During inference, we draw many samples from the learned distribution over the missing modality to both obtain the marginal predictive distribution (for the purpose of prediction) and analyze the impact of the missing modalities on the prediction for each instance. We evaluate PRIMO on a synthetic XOR dataset, Audio-Vision MNIST, and MIMIC-III for mortality and ICD-9 prediction. Across all datasets, PRIMO obtains performance comparable to unimodal baselines when a modality is fully missing and to multimodal baselines when all modalities are available. PRIMO quantifies the predictive impact of a modality at the instance level using a variance-based metric computed from predictions across latent completions. We visually demonstrate how varying completions of the missing modality result in a set of plausible labels.

</details>


### [9] [Patch-Based Spatial Authorship Attribution in Human-Robot Collaborative Paintings](https://arxiv.org/abs/2602.17030)
*Eric Chen,Patricia Alves-Oliveira*

Main category: cs.CV

TL;DR: 提出基于图像块的框架用于人机协作绘画中的空间作者归属，通过15幅抽象画验证，在块级别达到88.8%准确率，能有效识别混合创作区域。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI越来越多地参与创意生产，记录作者身份对艺术家、收藏家和法律环境变得至关重要。需要一种方法来确定人机协作绘画中不同区域的具体作者归属。

Method: 采用基于图像块的框架，使用商用平板扫描仪获取绘画图像，采用留一法交叉验证。对于协作作品，使用条件香农熵量化风格重叠，通过手动标注的混合区域验证模型检测混合作者身份的能力。

Result: 块级别准确率达到88.8%，绘画级别通过多数投票达到86.7%，优于基于纹理和预训练特征的基线方法（68.0%-84.7%）。混合创作区域的模型不确定性比纯创作区域高64%（p=0.003），表明模型能检测混合作者身份而非分类失败。

Conclusion: 该模型针对特定的人机组合，但为数据稀缺的人机创意工作流提供了样本高效的归属方法学基础，未来有潜力扩展到任何人机协作绘画的作者归属任务。

Abstract: As agentic AI becomes increasingly involved in creative production, documenting authorship has become critical for artists, collectors, and legal contexts. We present a patch-based framework for spatial authorship attribution within human-robot collaborative painting practice, demonstrated through a forensic case study of one human artist and one robotic system across 15 abstract paintings. Using commodity flatbed scanners and leave-one-painting-out cross-validation, the approach achieves 88.8% patch-level accuracy (86.7% painting-level via majority vote), outperforming texture-based and pretrained-feature baselines (68.0%-84.7%). For collaborative artworks, where ground truth is inherently ambiguous, we use conditional Shannon entropy to quantify stylistic overlap; manually annotated hybrid regions exhibit 64% higher uncertainty than pure paintings (p=0.003), suggesting the model detects mixed authorship rather than classification failure. The trained model is specific to this human-robot pair but provides a methodological grounding for sample-efficient attribution in data-scarce human-AI creative workflows that, in the future, has the potential to extend authorship attribution to any human-robot collaborative painting.

</details>


### [10] [PartRAG: Retrieval-Augmented Part-Level 3D Generation and Editing](https://arxiv.org/abs/2602.17033)
*Peize Li,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: PartRAG：基于检索增强的单图像3D生成框架，通过外部部件数据库和扩散变换器结合，实现部件级结构生成和精确编辑


<details>
  <summary>Details</summary>
Motivation: 单图像3D生成在部件级结构方面面临挑战：学习先验难以覆盖部件几何的长尾分布并保持多视图一致性，现有系统对精确局部编辑支持有限

Method: 1. 分层对比检索模块：将密集图像块与3D部件潜在空间对齐，从1236个部件标注资产库中检索多样、物理合理的示例注入去噪过程
2. 掩码部件级编辑器：在共享规范空间中操作，支持部件替换、属性细化和组合更新，无需重新生成整个对象

Result: 在Objaverse、ShapeNet和ABO数据集上取得竞争性结果：将Chamfer距离从0.1726降低到0.1528，F-Score从0.7472提升到0.844；推理时间38秒，交互编辑5-8秒；产生更清晰的部件边界、更好的薄结构保真度和对铰接对象的鲁棒性

Conclusion: PartRAG通过检索增强框架有效解决了单图像3D生成的部件级结构挑战，实现了高质量的生成和精确的局部编辑能力

Abstract: Single-image 3D generation with part-level structure remains challenging: learned priors struggle to cover the long tail of part geometries and maintain multi-view consistency, and existing systems provide limited support for precise, localized edits. We present PartRAG, a retrieval-augmented framework that integrates an external part database with a diffusion transformer to couple generation with an editable representation. To overcome the first challenge, we introduce a Hierarchical Contrastive Retrieval module that aligns dense image patches with 3D part latents at both part and object granularity, retrieving from a curated bank of 1,236 part-annotated assets to inject diverse, physically plausible exemplars into denoising. To overcome the second challenge, we add a masked, part-level editor that operates in a shared canonical space, enabling swaps, attribute refinements, and compositional updates without regenerating the whole object while preserving non-target parts and multi-view consistency. PartRAG achieves competitive results on Objaverse, ShapeNet, and ABO-reducing Chamfer Distance from 0.1726 to 0.1528 and raising F-Score from 0.7472 to 0.844 on Objaverse-with inference of 38s and interactive edits in 5-8s. Qualitatively, PartRAG produces sharper part boundaries, better thin-structure fidelity, and robust behavior on articulated objects. Code: https://github.com/AIGeeksGroup/PartRAG. Website: https://aigeeksgroup.github.io/PartRAG.

</details>


### [11] [Amber-Image: Efficient Compression of Large-Scale Diffusion Transformers](https://arxiv.org/abs/2602.17047)
*Chaojie Yang,Tian Li,Yue Zhang,Jun Gao*

Main category: cs.CV

TL;DR: 提出Amber-Image压缩框架，将60层双流MMDiT的Qwen-Image模型压缩为轻量级版本，参数减少70%，无需大规模数据工程，训练成本极低


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformer架构在文本到图像生成方面取得显著进展，但面临计算成本过高和部署障碍的问题，需要高效的压缩方案

Method: 采用时间步敏感的深度剪枝策略，通过局部权重平均重新初始化保留层，结合层间蒸馏和全参数微调；进一步引入混合流架构，将深层双流转换为单流，采用渐进蒸馏和轻量级微调

Result: 成功开发Amber-Image-10B和Amber-Image-6B模型，参数减少70%，整个压缩训练流程仅需不到2000GPU小时，在DPG-Bench和LongText-Bench等基准测试中实现高保真合成和优越文本渲染

Conclusion: 提出的压缩框架能够高效地将大型DiT模型转换为轻量级版本，在保持高质量生成能力的同时显著降低计算成本和部署门槛

Abstract: Diffusion Transformer (DiT) architectures have significantly advanced Text-to-Image (T2I) generation but suffer from prohibitive computational costs and deployment barriers. To address these challenges, we propose an efficient compression framework that transforms the 60-layer dual-stream MMDiT-based Qwen-Image into lightweight models without training from scratch. Leveraging this framework, we introduce Amber-Image, a series of streamlined T2I models. We first derive Amber-Image-10B using a timestep-sensitive depth pruning strategy, where retained layers are reinitialized via local weight averaging and optimized through layer-wise distillation and full-parameter fine-tuning. Building on this, we develop Amber-Image-6B by introducing a hybrid-stream architecture that converts deep-layer dual streams into a single stream initialized from the image branch, further refined via progressive distillation and lightweight fine-tuning. Our approach reduces parameters by 70% and eliminates the need for large-scale data engineering. Notably, the entire compression and training pipeline-from the 10B to the 6B variant-requires fewer than 2,000 GPU hours, demonstrating exceptional cost-efficiency compared to training from scratch. Extensive evaluations on benchmarks like DPG-Bench and LongText-Bench show that Amber-Image achieves high-fidelity synthesis and superior text rendering, matching much larger models.

</details>


### [12] [StructCore: Structure-Aware Image-Level Scoring for Training-Free Unsupervised Anomaly Detection](https://arxiv.org/abs/2602.17048)
*Joongwon Chae,Lihui Luo,Yang Liu,Runming Wang,Dongmei Yu,Zeming Liang,Xi Yuan,Dayan Zhang,Zhenglin Chen,Peiwu Qin,Ilmoon Chae*

Main category: cs.CV

TL;DR: StructCore是一种无需训练、结构感知的图像级异常评分方法，通过捕捉异常分数图的结构特征来改进传统最大池化的缺陷


<details>
  <summary>Details</summary>
Motivation: 传统基于内存库的无监督异常检测中，最大池化作为将异常分数图转换为图像级决策的标准方法存在缺陷。它仅依赖单个极端响应，丢弃了异常证据在图像中分布和结构的大部分信息，导致正常和异常分数经常重叠

Method: StructCore是一种无需训练的结构感知图像级评分方法。给定异常分数图，它计算低维结构描述符phi(S)来捕捉分布和空间特征，然后通过从训练良好样本估计的对角马哈拉诺比斯校准来精炼图像级评分，而不修改像素级定位

Result: StructCore在MVTec AD上实现了99.6%的图像级AUROC分数，在VisA上实现了98.4%的图像级AUROC分数，通过利用最大池化遗漏的结构特征，展示了鲁棒的图像级异常检测能力

Conclusion: StructCore超越了传统最大池化方法，通过捕捉异常分数图的结构特征，显著提高了图像级异常检测的准确性，同时保持了无需训练的优势

Abstract: Max pooling is the de facto standard for converting anomaly score maps into image-level decisions in memory-bank-based unsupervised anomaly detection (UAD). However, because it relies on a single extreme response, it discards most information about how anomaly evidence is distributed and structured across the image, often causing normal and anomalous scores to overlap.
  We propose StructCore, a training-free, structure-aware image-level scoring method that goes beyond max pooling. Given an anomaly score map, StructCore computes a low-dimensional structural descriptor phi(S) that captures distributional and spatial characteristics, and refines image-level scoring via a diagonal Mahalanobis calibration estimated from train-good samples, without modifying pixel-level localization.
  StructCore achieves image-level AUROC scores of 99.6% on MVTec AD and 98.4% on VisA, demonstrating robust image-level anomaly detection by exploiting structural signatures missed by max pooling.

</details>


### [13] [Cholec80-port: A Geometrically Consistent Trocar Port Segmentation Dataset for Robust Surgical Scene Understanding](https://arxiv.org/abs/2602.17060)
*Shunsuke Kikuchi,Atsushi Kouno,Hiroki Matsuzaki*

Main category: cs.CV

TL;DR: 该论文提出了Cholec80-port数据集，为腹腔镜手术中的套管端口提供几何一致的标注标准，以解决现有标注不一致影响下游几何任务的问题。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜手术中的套管端口会持续遮挡视野并吸引过多特征点，对图像拼接、3D重建和视觉SLAM等几何任务造成负面影响。然而公开手术数据集中缺乏高质量的端口标注，现有标注往往违反几何一致性，即使解剖区域可见也会遮挡中心开口。

Method: 提出了Cholec80-port数据集，基于Cholec80数据集提供高质量的套管端口分割标注，并制定了严格的标准操作程序（SOP），定义排除中心开口的端口套管掩码。同时按照相同SOP清理和统一了现有的公开数据集。

Result: 实验表明，几何一致的标注显著提高了跨数据集的鲁棒性，这种改进效果超越了单纯增加数据集规模所能带来的提升。

Conclusion: 几何一致的套管端口标注对于提升手术视觉任务的性能至关重要，提出的Cholec80-port数据集和SOP标准为相关研究提供了有价值的基准和规范。

Abstract: Trocar ports are camera-fixed, pseudo-static structures that can persistently occlude laparoscopic views and attract disproportionate feature points due to specular, textured surfaces. This makes ports particularly detrimental to geometry-based downstream pipelines such as image stitching, 3D reconstruction, and visual SLAM, where dynamic or non-anatomical outliers degrade alignment and tracking stability. Despite this practical importance, explicit port labels are rare in public surgical datasets, and existing annotations often violate geometric consistency by masking the central lumen (opening), even when anatomical regions are visible through it. We present Cholec80-port, a high-fidelity trocar port segmentation dataset derived from Cholec80, together with a rigorous standard operating procedure (SOP) that defines a port-sleeve mask excluding the central opening. We additionally cleanse and unify existing public datasets under the same SOP. Experiments demonstrate that geometrically consistent annotations substantially improve cross-dataset robustness beyond what dataset size alone provides.

</details>


### [14] [Cross Pseudo Labeling For Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2602.17077)
*Lee Dayeon,Kim Dongheyong,Park Chaewon,Woo Sungmin,Lee Sangyoun*

Main category: cs.CV

TL;DR: CPL-VAD是一个用于弱监督视频异常检测的双分支框架，通过交叉伪标签交换实现异常定位和类别识别的互补增强。


<details>
  <summary>Details</summary>
Motivation: 弱监督视频异常检测需要在仅有视频级标签的情况下检测异常并识别异常类别。现有方法往往难以同时实现精确的时序异常定位和准确的语义类别识别。

Method: 提出CPL-VAD双分支框架：1）二进制异常检测分支专注于片段级异常定位；2）类别分类分支利用视觉-语言对齐识别异常事件类别。两个分支通过交换伪标签实现互补优势的转移，将时序精度与语义判别能力相结合。

Result: 在XD-Violence和UCF-Crime数据集上的实验表明，CPL-VAD在异常检测和异常类别分类方面均达到了最先进的性能。

Conclusion: CPL-VAD通过双分支框架和交叉伪标签机制，有效解决了弱监督视频异常检测中的异常定位和类别识别问题，实现了时序精度和语义判别能力的协同提升。

Abstract: Weakly supervised video anomaly detection aims to detect anomalies and identify abnormal categories with only video-level labels. We propose CPL-VAD, a dual-branch framework with cross pseudo labeling. The binary anomaly detection branch focuses on snippet-level anomaly localization, while the category classification branch leverages vision-language alignment to recognize abnormal event categories. By exchanging pseudo labels, the two branches transfer complementary strengths, combining temporal precision with semantic discrimination. Experiments on XD-Violence and UCF-Crime demonstrate that CPL-VAD achieves state-of-the-art performance in both anomaly detection and abnormal category classification.

</details>


### [15] [ComptonUNet: A Deep Learning Model for GRB Localization with Compton Cameras under Noisy and Low-Statistic Conditions](https://arxiv.org/abs/2602.17085)
*Shogo Sato,Kazuo Tanaka,Shojun Ogasawara,Kazuki Yamamoto,Kazuhiko Murasaki,Ryuichi Tanida,Jun Kataoka*

Main category: cs.CV

TL;DR: ComptonUNet：一种混合深度学习框架，用于在低光子统计和高背景噪声条件下实现伽马射线暴的鲁棒定位


<details>
  <summary>Details</summary>
Motivation: 微弱伽马射线暴（GRBs）能提供早期恒星形成的独特见解，但由于低光子统计和强背景噪声，检测和定位这些弱源仍然具有挑战性。现有机器学习模型难以平衡统计鲁棒性和噪声抑制之间的权衡。

Method: 提出ComptonUNet混合深度学习框架，联合处理原始数据并重建图像，结合直接重建模型的统计效率和基于图像架构的去噪能力，在有限光子统计和强背景污染条件下有效工作。

Result: 通过模拟低地球轨道任务中GRB样事件的真实环境进行评估，ComptonUNet显著优于现有方法，在广泛的低统计和高背景场景中实现了改进的定位精度。

Conclusion: ComptonUNet为在具有挑战性的观测条件下检测和定位微弱伽马射线暴提供了一种有效的解决方案，平衡了统计鲁棒性和噪声抑制的需求。

Abstract: Gamma-ray bursts (GRBs) are among the most energetic transient phenomena in the universe and serve as powerful probes for high-energy astrophysical processes. In particular, faint GRBs originating from a distant universe may provide unique insights into the early stages of star formation. However, detecting and localizing such weak sources remains challenging owing to low photon statistics and substantial background noise. Although recent machine learning models address individual aspects of these challenges, they often struggle to balance the trade-off between statistical robustness and noise suppression. Consequently, we propose ComptonUNet, a hybrid deep learning framework that jointly processes raw data and reconstructs images for robust GRB localization. ComptonUNet was designed to operate effectively under conditions of limited photon statistics and strong background contamination by combining the statistical efficiency of direct reconstruction models with the denoising capabilities of image-based architectures. We perform realistic simulations of GRB-like events embedded in background environments representative of low-Earth orbit missions to evaluate the performance of ComptonUNet. Our results demonstrate that ComptonUNet significantly outperforms existing approaches, achieving improved localization accuracy across a wide range of low-statistic and high-background scenarios.

</details>


### [16] [3D Scene Rendering with Multimodal Gaussian Splatting](https://arxiv.org/abs/2602.17124)
*Chi-Shiang Gau,Konstantinos D. Polyzos,Athanasios Bacharis,Saketh Madhuvarasu,Tara Javidi*

Main category: cs.CV

TL;DR: 该论文提出了一种结合射频传感与3D高斯泼溅的多模态框架，用于在视觉线索不可靠的条件下实现高效、鲁棒的3D场景重建与渲染。


<details>
  <summary>Details</summary>
Motivation: 传统基于视觉的3D高斯泼溅方法需要大量相机视角来初始化高斯基元，在恶劣天气、低光照或部分遮挡等视觉线索不可靠的条件下表现不佳。射频信号对天气、光照和遮挡具有鲁棒性，因此需要一种更高效、更鲁棒的替代方案。

Method: 提出了一种多模态框架，将射频传感（如汽车雷达）与基于高斯泼溅的渲染相结合。该方法能够从稀疏的射频深度测量中高效预测深度，生成高质量的3D点云用于初始化不同高斯泼溅架构中的高斯函数。

Result: 数值测试表明，将射频传感明智地整合到高斯泼溅流程中具有显著优势，能够实现由射频信息驱动的结构精确性带来的高保真3D场景渲染。

Conclusion: 该研究展示了射频传感与高斯泼溅技术结合的有效性，为在视觉不可靠条件下实现高效、鲁棒的3D场景重建与渲染提供了新的解决方案。

Abstract: 3D scene reconstruction and rendering are core tasks in computer vision, with applications spanning industrial monitoring, robotics, and autonomous driving. Recent advances in 3D Gaussian Splatting (GS) and its variants have achieved impressive rendering fidelity while maintaining high computational and memory efficiency. However, conventional vision-based GS pipelines typically rely on a sufficient number of camera views to initialize the Gaussian primitives and train their parameters, typically incurring additional processing cost during initialization while falling short in conditions where visual cues are unreliable, such as adverse weather, low illumination, or partial occlusions. To cope with these challenges, and motivated by the robustness of radio-frequency (RF) signals to weather, lighting, and occlusions, we introduce a multimodal framework that integrates RF sensing, such as automotive radar, with GS-based rendering as a more efficient and robust alternative to vision-only GS rendering. The proposed approach enables efficient depth prediction from only sparse RF-based depth measurements, yielding a high-quality 3D point cloud for initializing Gaussian functions across diverse GS architectures. Numerical tests demonstrate the merits of judiciously incorporating RF sensing into GS pipelines, achieving high-fidelity 3D scene rendering driven by RF-informed structural accuracy.

</details>


### [17] [B$^3$-Seg: Camera-Free, Training-Free 3DGS Segmentation via Analytic EIG and Beta-Bernoulli Bayesian Updates](https://arxiv.org/abs/2602.17134)
*Hiromichi Kamata,Samuel Arthur Munro,Fuminori Homma*

Main category: cs.CV

TL;DR: B³-Seg提出了一种快速、无需训练、无需相机视角的开放词汇3D高斯溅射分割方法，基于Beta-Bernoulli贝叶斯更新和期望信息增益主动视图选择，实现秒级交互式分割。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯溅射分割方法依赖预定义相机视角、真实标签或昂贵重训练，无法满足影视游戏制作中实时编辑的低延迟需求。

Method: 将分割问题重新定义为顺序Beta-Bernoulli贝叶斯更新，通过解析期望信息增益主动选择下一个最优视图，贝叶斯框架保证了自适应单调性和子模性。

Result: 在多个数据集上实验表明，B³-Seg在几秒内完成端到端分割，性能与高成本监督方法相当，同时具有可证明的信息效率。

Conclusion: B³-Seg实现了实用、交互式的3D高斯溅射分割，具有理论保证和实际应用价值，特别适合影视游戏制作中的实时编辑场景。

Abstract: Interactive 3D Gaussian Splatting (3DGS) segmentation is essential for real-time editing of pre-reconstructed assets in film and game production. However, existing methods rely on predefined camera viewpoints, ground-truth labels, or costly retraining, making them impractical for low-latency use. We propose B$^3$-Seg (Beta-Bernoulli Bayesian Segmentation for 3DGS), a fast and theoretically grounded method for open-vocabulary 3DGS segmentation under camera-free and training-free conditions. Our approach reformulates segmentation as sequential Beta-Bernoulli Bayesian updates and actively selects the next view via analytic Expected Information Gain (EIG). This Bayesian formulation guarantees the adaptive monotonicity and submodularity of EIG, which produces a greedy $(1{-}1/e)$ approximation to the optimal view sampling policy. Experiments on multiple datasets show that B$^3$-Seg achieves competitive results to high-cost supervised methods while operating end-to-end segmentation within a few seconds. The results demonstrate that B$^3$-Seg enables practical, interactive 3DGS segmentation with provable information efficiency.

</details>


### [18] [BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning](https://arxiv.org/abs/2602.17168)
*Siyuan Liang,Yongcheng Jing,Yingjie Wang,Jiaxing Huang,Ee-chien Chang,Dacheng Tao*

Main category: cs.CV

TL;DR: BadCLIP++：针对多模态对比学习模型的新型后门攻击框架，通过语义融合QR微触发器和目标对齐子集选择实现隐蔽性，通过半径收缩、质心对齐和曲率控制实现持久性，在低毒化率下保持高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 当前针对多模态对比学习模型的后门攻击面临两大挑战：隐蔽性和持久性。现有方法在强检测或持续微调下容易失效，主要原因是跨模态不一致性暴露触发模式，以及低毒化率下的梯度稀释加速后门遗忘。这些耦合原因尚未得到充分建模和解决。

Method: 1. 隐蔽性方面：提出语义融合QR微触发器，在任务相关区域附近嵌入不可察觉的模式，保持干净数据统计特性同时产生紧凑的触发分布；采用目标对齐子集选择在低注入率下增强信号。
2. 持久性方面：通过半径收缩和质心对齐稳定触发嵌入；通过曲率控制和弹性权重巩固稳定模型参数，将解保持在低曲率宽盆地中抵抗微调。
3. 提供首个理论分析，证明在信任区域内，干净微梯度和后门目标梯度同向，攻击成功率下降具有非递增上界。

Result: 1. 仅0.3%毒化率下，在数字环境中达到99.99%攻击成功率，比基线高11.4个百分点。
2. 在19种防御方法下，攻击成功率保持在99.90%以上，干净准确率下降小于0.8%。
3. 物理攻击中达到65.03%成功率，对水印移除防御具有鲁棒性。

Conclusion: BadCLIP++是一个统一框架，有效解决了多模态对比学习模型后门攻击的隐蔽性和持久性挑战。通过创新的触发设计和稳定性机制，在极低毒化率下实现了高攻击成功率，并抵抗多种防御方法，为后门攻击研究提供了新的理论分析和实践方案。

Abstract: Research on backdoor attacks against multimodal contrastive learning models faces two key challenges: stealthiness and persistence. Existing methods often fail under strong detection or continuous fine-tuning, largely due to (1) cross-modal inconsistency that exposes trigger patterns and (2) gradient dilution at low poisoning rates that accelerates backdoor forgetting. These coupled causes remain insufficiently modeled and addressed. We propose BadCLIP++, a unified framework that tackles both challenges. For stealthiness, we introduce a semantic-fusion QR micro-trigger that embeds imperceptible patterns near task-relevant regions, preserving clean-data statistics while producing compact trigger distributions. We further apply target-aligned subset selection to strengthen signals at low injection rates. For persistence, we stabilize trigger embeddings via radius shrinkage and centroid alignment, and stabilize model parameters through curvature control and elastic weight consolidation, maintaining solutions within a low-curvature wide basin resistant to fine-tuning. We also provide the first theoretical analysis showing that, within a trust region, gradients from clean fine-tuning and backdoor objectives are co-directional, yielding a non-increasing upper bound on attack success degradation. Experiments demonstrate that with only 0.3% poisoning, BadCLIP++ achieves 99.99% attack success rate (ASR) in digital settings, surpassing baselines by 11.4 points. Across nineteen defenses, ASR remains above 99.90% with less than 0.8% drop in clean accuracy. The method further attains 65.03% success in physical attacks and shows robustness against watermark removal defenses.

</details>


### [19] [NRGS-SLAM: Monocular Non-Rigid SLAM for Endoscopy via Deformation-Aware 3D Gaussian Splatting](https://arxiv.org/abs/2602.17182)
*Jiwei Shan,Zeyu Cai,Yirui Li,Yongbo Chen,Lijun Han,Yun-hui Liu,Hesheng Wang,Shing Shin Cheng*

Main category: cs.CV

TL;DR: NRGS-SLAM：基于3D高斯泼溅的内窥镜单目非刚性SLAM系统，通过变形感知3D高斯地图和贝叶斯自监督解决相机运动与组织变形的耦合问题，实现更准确的姿态估计和高质量重建。


<details>
  <summary>Details</summary>
Motivation: 内窥镜场景中的软组织持续变形违反了SLAM的刚性假设，导致相机自身运动与内在变形之间存在强耦合模糊性。现有单目非刚性SLAM方法缺乏有效的解耦机制，依赖稀疏或低保真场景表示，导致跟踪漂移和重建质量受限。

Method: 1. 变形感知3D高斯地图：为每个高斯基元添加可学习的变形概率，通过贝叶斯自监督策略优化，无需外部非刚性标签；2. 可变形跟踪模块：采用从粗到精的姿态估计，优先处理低变形区域，然后进行高效的逐帧变形更新；3. 可变形建图模块：平衡表示能力和计算效率，逐步扩展和优化地图；4. 统一鲁棒几何损失：结合外部几何先验缓解单目非刚性SLAM的固有不适定性。

Result: 在多个公共内窥镜数据集上的广泛实验表明，NRGS-SLAM相比最先进方法实现了更准确的相机姿态估计（RMSE降低高达50%）和更高质量的逼真重建。全面的消融研究进一步验证了关键设计选择的有效性。

Conclusion: NRGS-SLAM通过创新的变形感知3D高斯表示和贝叶斯自监督策略，有效解决了内窥镜场景中相机运动与组织变形的耦合问题，为单目非刚性SLAM提供了更准确、高质量的解决方案，代码将在论文接受后公开。

Abstract: Visual simultaneous localization and mapping (V-SLAM) is a fundamental capability for autonomous perception and navigation. However, endoscopic scenes violate the rigidity assumption due to persistent soft-tissue deformations, creating a strong coupling ambiguity between camera ego-motion and intrinsic deformation. Although recent monocular non-rigid SLAM methods have made notable progress, they often lack effective decoupling mechanisms and rely on sparse or low-fidelity scene representations, which leads to tracking drift and limited reconstruction quality. To address these limitations, we propose NRGS-SLAM, a monocular non-rigid SLAM system for endoscopy based on 3D Gaussian Splatting. To resolve the coupling ambiguity, we introduce a deformation-aware 3D Gaussian map that augments each Gaussian primitive with a learnable deformation probability, optimized via a Bayesian self-supervision strategy without requiring external non-rigidity labels. Building on this representation, we design a deformable tracking module that performs robust coarse-to-fine pose estimation by prioritizing low-deformation regions, followed by efficient per-frame deformation updates. A carefully designed deformable mapping module progressively expands and refines the map, balancing representational capacity and computational efficiency. In addition, a unified robust geometric loss incorporates external geometric priors to mitigate the inherent ill-posedness of monocular non-rigid SLAM. Extensive experiments on multiple public endoscopic datasets demonstrate that NRGS-SLAM achieves more accurate camera pose estimation (up to 50\% reduction in RMSE) and higher-quality photo-realistic reconstructions than state-of-the-art methods. Comprehensive ablation studies further validate the effectiveness of our key design choices. Source code will be publicly available upon paper acceptance.

</details>


### [20] [Selective Training for Large Vision Language Models via Visual Information Gain](https://arxiv.org/abs/2602.17186)
*Seulbi Lee,Sangheum Hwang*

Main category: cs.CV

TL;DR: 提出视觉信息增益(VIG)指标来量化视觉输入对LVLM预测不确定性的减少程度，并基于此设计选择性训练方案来缓解语言偏见


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型存在语言偏见问题，经常不依赖视觉证据就生成答案。现有方法缺乏对训练样本或token从图像中获益程度的量化衡量

Method: 提出基于困惑度的视觉信息增益(VIG)指标，在样本和token层面测量视觉输入提供的预测不确定性减少；基于VIG设计选择性训练方案，优先训练高VIG的样本和token

Result: VIG能够有效识别视觉基础元素（颜色、空间关系、属性等）；VIG引导的选择性训练改善了视觉基础性，缓解了语言偏见，在显著减少监督的情况下实现了更好的性能

Conclusion: VIG为量化视觉信息贡献提供了有效工具，基于VIG的选择性训练策略能够更高效地提升LVLM的视觉基础能力，同时减少训练成本

Abstract: Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens.

</details>


### [21] [EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models](https://arxiv.org/abs/2602.17196)
*Yahong Wang,Juncheng Wu,Zhangkai Ni,Chengmei Yang,Yihang Liu,Longzhen Yang,Yuyin Zhou,Ying Wen,Lianghua He*

Main category: cs.CV

TL;DR: 提出基于矩阵熵的视觉token剪枝框架EntropyPrune，通过识别"熵崩溃层"确定剪枝时机，无需注意力图即可量化token信息价值，在LLaVA-1.5-7B上实现68.2% FLOPs减少同时保持96.0%性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型处理大量视觉token导致推理成本高昂，现有token剪枝方法依赖启发式静态层选择，缺乏可解释性和跨模型可迁移性，需要更原则性的剪枝标准。

Method: 提出矩阵熵视角识别"熵崩溃层"作为剪枝时机，开发EntropyPrune框架：1) 基于矩阵熵量化单个视觉token信息价值；2) 利用对偶Gram矩阵谱等价性降低熵计算复杂度；3) 无需注意力图即可剪枝冗余token。

Result: 在多样化多模态基准测试中，EntropyPrune在准确性和效率上均优于最先进剪枝方法。LLaVA-1.5-7B上实现68.2% FLOPs减少，保持96.0%原始性能。在高分辨率和视频模型中表现出良好泛化能力。

Conclusion: 矩阵熵为视觉token剪枝提供了原则性标准，EntropyPrune框架通过识别熵崩溃层和高效熵计算，实现了高效、可解释且可迁移的MLLM加速，具有强鲁棒性和可扩展性。

Abstract: Multimodal large language models (MLLMs) incur substantial inference cost due to the processing of hundreds of visual tokens per image. Although token pruning has proven effective for accelerating inference, determining when and where to prune remains largely heuristic. Existing approaches typically rely on static, empirically selected layers, which limit interpretability and transferability across models. In this work, we introduce a matrix-entropy perspective and identify an "Entropy Collapse Layer" (ECL), where the information content of visual representations exhibits a sharp and consistent drop, which provides a principled criterion for selecting the pruning stage. Building on this observation, we propose EntropyPrune, a novel matrix-entropy-guided token pruning framework that quantifies the information value of individual visual tokens and prunes redundant ones without relying on attention maps. Moreover, to enable efficient computation, we exploit the spectral equivalence of dual Gram matrices, reducing the complexity of entropy computation and yielding up to a 64x theoretical speedup. Extensive experiments on diverse multimodal benchmarks demonstrate that EntropyPrune consistently outperforms state-of-the-art pruning methods in both accuracy and efficiency. On LLaVA-1.5-7B, our method achieves a 68.2% reduction in FLOPs while preserving 96.0% of the original performance. Furthermore, EntropyPrune generalizes effectively to high-resolution and video-based models, highlighting the strong robustness and scalability in practical MLLM acceleration. The code will be publicly available at https://github.com/YahongWang1/EntropyPrune.

</details>


### [22] [A Multi-modal Detection System for Infrastructure-based Freight Signal Priority](https://arxiv.org/abs/2602.17252)
*Ziyan Zhang,Chuheng Wei,Xuanpeng Zhao,Siyan Li,Will Snyder,Mike Stas,Peng Hao,Kanok Boriboonsomsin,Guoyuan Wu*

Main category: cs.CV

TL;DR: 本文提出了一种基于基础设施的多模态货运车辆检测系统，整合LiDAR和摄像头传感器，用于支持货运信号优先控制策略。


<details>
  <summary>Details</summary>
Motivation: 货运车辆接近信号交叉口时需要可靠的检测和运动估计来支持基于基础设施的货运信号优先。准确及时地感知车辆类型、位置和速度对于实现有效的优先控制策略至关重要。

Method: 采用混合传感架构，包括交叉口安装子系统和中间路段子系统，通过无线通信连接实现同步数据传输。感知流程结合了基于聚类和深度学习的检测方法，并使用卡尔曼滤波器跟踪以实现稳定的实时性能。LiDAR测量被配准到地理参考框架中以支持车道级定位和一致的车辆跟踪。

Result: 现场评估表明，该系统能够以高时空分辨率可靠地监测货运车辆运动。设计和部署为开发支持货运信号优先应用的基于基础设施的传感系统提供了实用见解。

Conclusion: 该多模态货运车辆检测系统能够有效支持货运信号优先应用，通过基础设施传感实现可靠的车辆检测和跟踪，为智能交通系统提供了可行的技术方案。

Abstract: Freight vehicles approaching signalized intersections require reliable detection and motion estimation to support infrastructure-based Freight Signal Priority (FSP). Accurate and timely perception of vehicle type, position, and speed is essential for enabling effective priority control strategies. This paper presents the design, deployment, and evaluation of an infrastructure-based multi-modal freight vehicle detection system integrating LiDAR and camera sensors. A hybrid sensing architecture is adopted, consisting of an intersection-mounted subsystem and a midblock subsystem, connected via wireless communication for synchronized data transmission. The perception pipeline incorporates both clustering-based and deep learning-based detection methods with Kalman filter tracking to achieve stable real-time performance. LiDAR measurements are registered into geodetic reference frames to support lane-level localization and consistent vehicle tracking. Field evaluations demonstrate that the system can reliably monitor freight vehicle movements at high spatio-temporal resolution. The design and deployment provide practical insights for developing infrastructure-based sensing systems to support FSP applications.

</details>


### [23] [EA-Swin: An Embedding-Agnostic Swin Transformer for AI-Generated Video Detection](https://arxiv.org/abs/2602.17260)
*Hung Mai,Loi Dinh,Duc Hai Nguyen,Dat Do,Luong Doan,Khanh Nguyen Quoc,Huan Vu,Phong Ho,Naeem Ul Islam,Tuan Do*

Main category: cs.CV

TL;DR: EA-Swin是一种嵌入无关的Swin Transformer模型，直接对预训练视频嵌入建模时空依赖关系，用于AI生成视频检测，在主要生成器上达到0.97-0.99准确率，比现有方法提升5-20%。


<details>
  <summary>Details</summary>
Motivation: 现有基础视频生成器（如Sora2、Veo3等）生成的合成视频高度逼真，暴露了现有检测方法的局限性：依赖浅层嵌入轨迹、基于图像的适应方法或计算量大的MLLMs，需要更有效的检测方案。

Method: 提出EA-Swin（Embedding-Agnostic Swin Transformer），通过因子化窗口注意力设计直接在预训练视频嵌入上建模时空依赖，兼容通用的ViT风格补丁编码器。同时构建EA-Video数据集，包含13万视频，涵盖商业和开源生成器，包含未见生成器分割用于跨分布评估。

Result: EA-Swin在主要生成器上达到0.97-0.99的准确率，比现有最先进方法（通常0.8-0.9）提升5-20%，同时在未见分布上保持强大的泛化能力。

Conclusion: EA-Swin为现代AI生成视频检测提供了一个可扩展且鲁棒的解决方案，通过嵌入无关的架构和全面的数据集基准，显著提升了检测性能。

Abstract: Recent advances in foundation video generators such as Sora2, Veo3, and other commercial systems have produced highly realistic synthetic videos, exposing the limitations of existing detection methods that rely on shallow embedding trajectories, image-based adaptation, or computationally heavy MLLMs. We propose EA-Swin, an Embedding-Agnostic Swin Transformer that models spatiotemporal dependencies directly on pretrained video embeddings via a factorized windowed attention design, making it compatible with generic ViT-style patch-based encoders. Alongside the model, we construct the EA-Video dataset, a benchmark dataset comprising 130K videos that integrates newly collected samples with curated existing datasets, covering diverse commercial and open-source generators and including unseen-generator splits for rigorous cross-distribution evaluation. Extensive experiments show that EA-Swin achieves 0.97-0.99 accuracy across major generators, outperforming prior SoTA methods (typically 0.8-0.9) by a margin of 5-20%, while maintaining strong generalization to unseen distributions, establishing a scalable and robust solution for modern AI-generated video detection.

</details>


### [24] [Attachment Anchors: A Novel Framework for Laparoscopic Grasping Point Prediction in Colorectal Surgery](https://arxiv.org/abs/2602.17310)
*Dennis N. Schneider,Lars Wagner,Daniel Rueckert,Dirk Wilhelm*

Main category: cs.CV

TL;DR: 本文提出"附着锚点"表示法，用于结直肠手术中组织抓取点预测，通过编码组织与解剖附着点的几何和力学关系来减少不确定性，在90例手术数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 结直肠手术因其复杂性和持续时间长，在自主组织操作研究中代表性不足。同时，由于重复的组织操作，结直肠手术为机器学习驱动的自主支持提供了有前景的学习环境。当前需要解决的关键挑战是准确预测抓取点。

Method: 引入"附着锚点"结构化表示法，编码组织与其解剖附着点之间的局部几何和力学关系。该方法将手术场景归一化为一致的局部参考框架，可从腹腔镜图像预测附着锚点，并整合到基于机器学习的抓取框架中。

Result: 在90例结直肠手术数据集上的实验表明，附着锚点相比仅使用图像的基线方法显著改善了抓取点预测。在分布外设置（包括未见过的程序和手术医生）中表现尤为突出。

Conclusion: 附着锚点是结直肠手术中基于学习的组织操作的有效中间表示，能够减少抓取点预测的不确定性，特别是在复杂和变化的结直肠手术环境中。

Abstract: Accurate grasping point prediction is a key challenge for autonomous tissue manipulation in minimally invasive surgery, particularly in complex and variable procedures such as colorectal interventions. Due to their complexity and prolonged duration, colorectal procedures have been underrepresented in current research. At the same time, they pose a particularly interesting learning environment due to repetitive tissue manipulation, making them a promising entry point for autonomous, machine learning-driven support. Therefore, in this work, we introduce attachment anchors, a structured representation that encodes the local geometric and mechanical relationships between tissue and its anatomical attachments in colorectal surgery. This representation reduces uncertainty in grasping point prediction by normalizing surgical scenes into a consistent local reference frame. We demonstrate that attachment anchors can be predicted from laparoscopic images and incorporated into a grasping framework based on machine learning. Experiments on a dataset of 90 colorectal surgeries demonstrate that attachment anchors improve grasping point prediction compared to image-only baselines. There are particularly strong gains in out-of-distribution settings, including unseen procedures and operating surgeons. These results suggest that attachment anchors are an effective intermediate representation for learning-based tissue manipulation in colorectal surgery.

</details>


### [25] [Polaffini: A feature-based approach for robust affine and polyaffine image registration](https://arxiv.org/abs/2602.17337)
*Antoine Legouhy,Cosimo Campo,Ross Callaghan,Hojjat Azadbakht,Hui Zhang*

Main category: cs.CV

TL;DR: Polaffini是一个基于解剖结构的医学图像配准框架，利用深度学习分割模型提取解剖区域质心作为特征点，通过闭式解实现高效全局和局部仿射匹配，生成从仿射到多仿射的可调平滑变换。


<details>
  <summary>Details</summary>
Motivation: 传统基于强度的医学图像配准方法依赖对齐质量的替代度量，而理论上更理想的基于特征的方法因难以可靠提取特征而较少使用。深度学习分割模型的进步使得能够即时获得可靠、细粒度的解剖分割，为创建新的基于解剖结构的图像配准算法提供了机会。

Method: Polaffini利用预训练分割模型获得解剖区域分割，提取这些区域的质心作为具有1对1对应关系的解剖特征点。通过这些特征点，使用闭式解实现高效的全局和局部仿射匹配，生成从仿射到多仿射的可调平滑变换。多仿射变换在log-Euclidean框架中嵌入，确保微分同胚特性。

Result: Polaffini在结构对齐方面优于流行的基于强度的配准方法，并为后续非线性配准提供了更好的初始化。该方法快速、鲁棒且准确，特别适合集成到医学图像处理流程中。

Conclusion: Polaffini是一个基于解剖结构的医学图像配准框架，通过利用深度学习分割模型提取解剖特征点，实现了高效、鲁棒和准确的配准。该方法既可作为独立配准工具，也可作为非线性配准的预对齐步骤，在医学图像处理中具有重要应用价值。

Abstract: In this work we present Polaffini, a robust and versatile framework for anatomically grounded registration. Medical image registration is dominated by intensity-based registration methods that rely on surrogate measures of alignment quality. In contrast, feature-based approaches that operate by identifying explicit anatomical correspondences, while more desirable in theory, have largely fallen out of favor due to the challenges of reliably extracting features. However, such challenges are now significantly overcome thanks to recent advances in deep learning, which provide pre-trained segmentation models capable of instantly delivering reliable, fine-grained anatomical delineations. We aim to demonstrate that these advances can be leveraged to create new anatomically-grounded image registration algorithms. To this end, we propose Polaffini, which obtains, from these segmented regions, anatomically grounded feature points with 1-to-1 correspondence in a particularly simple way: extracting their centroids. These enable efficient global and local affine matching via closed-form solutions. Those are used to produce an overall transformation ranging from affine to polyaffine with tunable smoothness. Polyaffine transformations can have many more degrees of freedom than affine ones allowing for finer alignment, and their embedding in the log-Euclidean framework ensures diffeomorphic properties. Polaffini has applications both for standalone registration and as pre-alignment for subsequent non-linear registration, and we evaluate it against popular intensity-based registration techniques. Results demonstrate that Polaffini outperforms competing methods in terms of structural alignment and provides improved initialisation for downstream non-linear registration. Polaffini is fast, robust, and accurate, making it particularly well-suited for integration into medical image processing pipelines.

</details>


### [26] [Tree crop mapping of South America reveals links to deforestation and conservation](https://arxiv.org/abs/2602.17372)
*Yuchang Jiang,Anton Raichuk,Xiaoye Tong,Vivien Sainte Fare Garnot,Daniel Ortiz-Gonzalo,Dan Morris,Konrad Schindler,Jan Dirk Wegner,Maxim Neumann*

Main category: cs.CV

TL;DR: 开发首个10米分辨率南美木本作物地图，用于监测零毁林政策下的作物扩张，发现现有监管地图常将小农户农林业误判为森林，导致不公平处罚风险。


<details>
  <summary>Details</summary>
Motivation: 欧盟零毁林产品法规等政策需要监测木本作物扩张，但缺乏高分辨率数据来区分不同农业系统与森林，现有监管地图存在误判风险。

Method: 使用多模态时空深度学习模型，基于Sentinel-1和Sentinel-2卫星影像时间序列，生成10米分辨率的南美木本作物地图。

Result: 识别出约1100万公顷木本作物，其中23%与2000-2020年森林覆盖损失相关；发现现有监管地图常将已建立的农业（特别是小农户农林业）误分类为"森林"。

Conclusion: 提供高分辨率基线数据可减轻误判风险，支持更有效、包容和公平的保护政策，避免对小农户的不公平处罚。

Abstract: Monitoring tree crop expansion is vital for zero-deforestation policies like the European Union's Regulation on Deforestation-free Products (EUDR). However, these efforts are hindered by a lack of highresolution data distinguishing diverse agricultural systems from forests. Here, we present the first 10m-resolution tree crop map for South America, generated using a multi-modal, spatio-temporal deep learning model trained on Sentinel-1 and Sentinel-2 satellite imagery time series. The map identifies approximately 11 million hectares of tree crops, 23% of which is linked to 2000-2020 forest cover loss. Critically, our analysis reveals that existing regulatory maps supporting the EUDR often classify established agriculture, particularly smallholder agroforestry, as "forest". This discrepancy risks false deforestation alerts and unfair penalties for small-scale farmers. Our work mitigates this risk by providing a high-resolution baseline, supporting conservation policies that are effective, inclusive, and equitable.

</details>


### [27] [DRetHTR: Linear-Time Decoder-Only Retentive Network for Handwritten Text Recognition](https://arxiv.org/abs/2602.17387)
*Changhun Kim,Martin Mayr,Thomas Gorges,Fei Wu,Mathias Seuret,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: DRetHTR是一个基于Retentive Networks的手写文本识别模型，相比传统Transformer，推理速度提升1.6-1.9倍，内存使用减少38-42%，同时保持相同准确率。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的手写文本识别系统存在KV缓存增长问题，导致解码速度慢且内存占用大。需要一种既能保持Transformer级别准确率，又能显著提升解码效率和内存效率的解决方案。

Method: 采用Retentive Networks构建仅解码器模型，用无softmax的retention机制替代softmax注意力，注入多尺度序列先验，避免KV缓存增长。通过层间gamma缩放，在深层逐步扩大有效retention范围，恢复注意力机制的局部到全局归纳偏置。

Result: 在多个数据集上取得最佳字符错误率：IAM-A（英语）2.26%，RIMES（法语）1.81%，Bentham（英语）3.46%，在READ-2016（德语）上达到4.21%。相比同等规模的仅解码器Transformer基线，推理速度快1.6-1.9倍，内存使用减少38-42%。

Conclusion: 基于RetNet的仅解码器模型能够在保持Transformer级别手写文本识别准确率的同时，显著提升解码速度和内存效率，为解决传统Transformer的KV缓存增长问题提供了有效方案。

Abstract: State-of-the-art handwritten text recognition (HTR) systems commonly use Transformers, whose growing key-value (KV) cache makes decoding slow and memory-intensive. We introduce DRetHTR, a decoder-only model built on Retentive Networks (RetNet). Compared to an equally sized decoder-only Transformer baseline, DRetHTR delivers 1.6-1.9x faster inference with 38-42% less memory usage, without loss of accuracy. By replacing softmax attention with softmax-free retention and injecting multi-scale sequential priors, DRetHTR avoids a growing KV cache: decoding is linear in output length in both time and memory. To recover the local-to-global inductive bias of attention, we propose layer-wise gamma scaling, which progressively enlarges the effective retention horizon in deeper layers. This encourages early layers to model short-range dependencies and later layers to capture broader context, mitigating the flexibility gap introduced by removing softmax. Consequently, DRetHTR achieves best reported test character error rates of 2.26% (IAM-A, en), 1.81% (RIMES, fr), and 3.46% (Bentham, en), and is competitive on READ-2016 (de) with 4.21%. This demonstrates that decoder-only RetNet enables Transformer-level HTR accuracy with substantially improved decoding speed and memory efficiency.

</details>


### [28] [SpectralGCD: Spectral Concept Selection and Cross-modal Representation Learning for Generalized Category Discovery](https://arxiv.org/abs/2602.17395)
*Lorenzo Caselli,Marco Mistretta,Simone Magistri,Andrew D. Bagdanov*

Main category: cs.CV

TL;DR: SpectralGCD是一种高效的多模态广义类别发现方法，利用CLIP跨模态相似性作为统一表示，通过谱过滤保留相关概念，在六个基准测试中达到或超越SOTA性能，计算成本显著降低。


<details>
  <summary>Details</summary>
Motivation: 广义类别发现(GCD)旨在利用少量已知类别的标注数据来识别未标注数据中的新类别。现有方法存在两个主要问题：1)仅基于图像特征的参数化分类器容易对旧类别过拟合；2)最近的多模态方法虽然通过引入文本信息提升了性能，但独立处理不同模态且计算成本高昂。

Method: 提出SpectralGCD方法：1)使用CLIP跨模态图像-概念相似性作为统一的跨模态表示；2)将每个图像表示为大型任务无关字典中语义概念的混合，将学习锚定在显式语义上；3)引入谱过滤技术，利用强教师模型测量的softmax相似性的跨模态协方差矩阵自动保留字典中的相关概念；4)通过正向和反向知识蒸馏确保学生的跨模态表示既语义充分又对齐良好。

Result: 在六个基准测试中，SpectralGCD达到了与最先进方法相当或显著更优的准确率，同时计算成本仅为其他方法的一小部分。

Conclusion: SpectralGCD通过统一的跨模态表示、显式语义锚定和谱过滤技术，实现了高效且有效的广义类别发现，在保持语义质量的同时大幅降低了计算复杂度。

Abstract: Generalized Category Discovery (GCD) aims to identify novel categories in unlabeled data while leveraging a small labeled subset of known classes. Training a parametric classifier solely on image features often leads to overfitting to old classes, and recent multimodal approaches improve performance by incorporating textual information. However, they treat modalities independently and incur high computational cost. We propose SpectralGCD, an efficient and effective multimodal approach to GCD that uses CLIP cross-modal image-concept similarities as a unified cross-modal representation. Each image is expressed as a mixture over semantic concepts from a large task-agnostic dictionary, which anchors learning to explicit semantics and reduces reliance on spurious visual cues. To maintain the semantic quality of representations learned by an efficient student, we introduce Spectral Filtering which exploits a cross-modal covariance matrix over the softmaxed similarities measured by a strong teacher model to automatically retain only relevant concepts from the dictionary. Forward and reverse knowledge distillation from the same teacher ensures that the cross-modal representations of the student remain both semantically sufficient and well-aligned. Across six benchmarks, SpectralGCD delivers accuracy comparable to or significantly superior to state-of-the-art methods at a fraction of the computational cost. The code is publicly available at: https://github.com/miccunifi/SpectralGCD.

</details>


### [29] [A High-Level Survey of Optical Remote Sensing](https://arxiv.org/abs/2602.17397)
*Panagiotis Koletsis,Vasilis Efthymiou,Maria Vakalopoulou,Nikos Komodakis,Anastasios Doulamis,Georgios Th. Papadopoulos*

Main category: cs.CV

TL;DR: 该论文是一篇关于无人机光学遥感领域的综合性综述，旨在为进入该领域的研究者提供全面的能力概览、关键数据集和见解，填补了现有文献中缺乏整体视角的空白。


<details>
  <summary>Details</summary>
Motivation: 近年来计算机视觉的显著进展推动了遥感技术的发展，同时无人机的应用日益广泛。大多数无人机默认配备RGB相机，这些相机既鲁棒又易于使用和解释。光学遥感文献庞大且多样化，每个任务或方法都可能需要专门的综述。现有文献缺乏从整体视角出发的综述，因此需要一篇能够全面概述该领域能力、提供关键信息和见解的指南性文章。

Method: 该论文采用综述研究方法，对无人机光学遥感领域进行全面梳理。它不专注于某个特定任务或方法，而是提供该领域的整体能力概览，包括关键数据集、技术见解和实际应用信息，旨在为研究者提供高层次的指导。

Result: 该论文填补了无人机光学遥感领域缺乏整体视角综述的空白，提供了一个全面的能力概览框架。它整合了该领域的多样化任务、能力和方法，为研究者提供了进入该领域的路线图和参考资源。

Conclusion: 这篇综述为无人机光学遥感领域提供了一个独特的整体视角，旨在帮助新进入该领域的研究者快速了解全局，聚焦于最相关的研究方向。它不仅是技术综述，更是一个指导性的资源，有助于推动该领域的进一步发展。

Abstract: In recent years, significant advances in computer vision have also propelled progress in remote sensing. Concurrently, the use of drones has expanded, with many organizations incorporating them into their operations. Most drones are equipped by default with RGB cameras, which are both robust and among the easiest sensors to use and interpret. The body of literature on optical remote sensing is vast, encompassing diverse tasks, capabilities, and methodologies. Each task or methodology could warrant a dedicated survey. This work provides a comprehensive overview of the capabilities of the field, while also presenting key information, such as datasets and insights. It aims to serve as a guide for researchers entering the field, offering high-level insights and helping them focus on areas most relevant to their interests. To the best of our knowledge, no existing survey addresses this holistic perspective.

</details>


### [30] [When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs](https://arxiv.org/abs/2602.17659)
*Yu Fang,Yuchun Feng,Dong Jing,Jiaqi Liu,Yue Yang,Zhenyu Wei,Daniel Szafir,Mingyu Ding*

Main category: cs.CV

TL;DR: 论文提出LIBERO-CF基准测试来评估VLA模型的语言跟随能力，发现现有模型存在反事实失败问题，并提出CAG方法通过双分支推理机制改善这一问题。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型在实际应用中经常无法忠实跟随语言指令，当遇到缺乏场景特定监督的指令时，模型会基于数据集偏见产生的视觉捷径来行动，反复执行已学习的行为并选择训练中常见的对象，而忽略语言意图。

Method: 提出Counterfactual Action Guidance方法：一个简单有效的双分支推理方案，将标准VLA策略与语言无关的视觉-动作模块结合，在动作选择时进行反事实比较。该方法无需额外演示或修改现有架构，可直接集成到各种VLA模型中。

Result: 在LIBERO-CF基准测试中，CAG将语言跟随准确率提高了9.7%，在未充分观察任务上的任务成功率提高了3.6%。与VA模型结合时，分别获得15.5%和8.5%的进一步提升。在真实世界评估中，CAG平均减少了9.4%的反事实失败，任务成功率提高了17.2%。

Conclusion: 反事实失败是VLA模型中普遍存在但未被充分研究的问题，CAG方法通过简单的双分支设计有效减少了模型对视觉捷径的依赖，提高了语言跟随的鲁棒性，且具有即插即用的优势。

Abstract: Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.

</details>


### [31] [EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models](https://arxiv.org/abs/2602.17419)
*Xiaomeng Peng,Xilang Huang,Seon Han Choi*

Main category: cs.CV

TL;DR: EAGLE框架无需微调，通过专家模型输出指导多模态大语言模型，在工业异常检测中实现准确检测和可解释的异常描述，性能媲美微调方法。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测中，现有深度学习方法通常只提供二元决策且解释性有限，而多模态大语言模型虽然能生成细粒度语言分析，但需要昂贵的微调且不一定能提升检测精度。

Method: 提出EAGLE框架，无需微调，通过集成专家模型输出来指导多模态大语言模型，使其同时实现准确异常检测和可解释的异常描述。还研究了EAGLE如何影响MLLMs内部机制，通过分析中间层对异常图像区域的注意力分布。

Result: 在MVTec-AD和VisA数据集上的实验表明，EAGLE无需参数更新就能提升多个MLLMs的异常检测性能，达到与基于微调方法相当的结果。成功异常检测与异常区域注意力集中度增加相关，EAGLE能促进这种对齐。

Conclusion: EAGLE是一种无需微调的框架，通过专家模型指导有效提升了多模态大语言模型在工业异常检测中的性能，同时提供可解释的异常描述，为MLLMs在工业检测中的应用提供了新思路。

Abstract: Industrial anomaly detection is important for smart manufacturing, but many deep learning approaches produce only binary decisions and provide limited semantic explanations. Multimodal large language models (MLLMs) can potentially generate fine-grained, language-based analyses, yet existing methods often require costly fine-tuning and do not consistently improve anomaly detection accuracy compared to lightweight specialist detectors. We propose expert-augmented attention guidance for industrial anomaly detection in MLLMs (EAGLE), a tuning-free framework that integrates outputs from expert model to guide MLLMs toward both accurate detection and interpretable anomaly descriptions. We further study how EAGLE affects MLLMs internals by examining the attention distribution of MLLMs to the anomalous image regions in the intermediate layers. We observe that successful anomaly detection is associated with increased attention concentration on anomalous regions, and EAGLE tends to encourage this alignment. Experiments on MVTec-AD and VisA show that EAGLE improves anomaly detection performance across multiple MLLMs without any parameter updates, achieving results comparable to fine-tuning based methods. Code is available at \href{https://github.com/shengtun/Eagle}{https://github.com/shengtun/Eagle}

</details>


### [32] [4D Monocular Surgical Reconstruction under Arbitrary Camera Motions](https://arxiv.org/abs/2602.17473)
*Jiwei Shan,Zeyu Cai,Cheng-Tai Hsieh,Yirui Li,Hao Liu,Lijun Han,Hesheng Wang,Shing Shin Cheng*

Main category: cs.CV

TL;DR: Local-EndoGS：针对单目内窥镜序列的4D重建框架，通过局部可变形场景模型处理大相机运动，无需立体深度或准确SfM初始化


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对固定视角的可变形场景，依赖立体深度或准确SfM进行初始化和优化，难以处理真实临床环境中具有大相机运动的单目序列

Method: 提出渐进式窗口化全局表示，为每个观察窗口分配局部可变形场景模型；采用从粗到精策略整合多视图几何、跨窗口信息和单目深度先验；引入长程2D像素轨迹约束和物理运动先验

Result: 在三个公开内窥镜数据集上的实验表明，Local-EndoGS在表观质量和几何重建方面始终优于现有方法

Conclusion: Local-EndoGS能够有效处理具有任意相机运动的单目内窥镜序列，实现高质量的4D重建，为临床环境提供了更实用的解决方案

Abstract: Reconstructing deformable surgical scenes from endoscopic videos is challenging and clinically important. Recent state-of-the-art methods based on implicit neural representations or 3D Gaussian splatting have made notable progress. However, most are designed for deformable scenes with fixed endoscope viewpoints and rely on stereo depth priors or accurate structure-from-motion for initialization and optimization, limiting their ability to handle monocular sequences with large camera motion in real clinical settings. To address this, we propose Local-EndoGS, a high-quality 4D reconstruction framework for monocular endoscopic sequences with arbitrary camera motion. Local-EndoGS introduces a progressive, window-based global representation that allocates local deformable scene models to each observed window, enabling scalability to long sequences with substantial motion. To overcome unreliable initialization without stereo depth or accurate structure-from-motion, we design a coarse-to-fine strategy integrating multi-view geometry, cross-window information, and monocular depth priors, providing a robust foundation for optimization. We further incorporate long-range 2D pixel trajectory constraints and physical motion priors to improve deformation plausibility. Experiments on three public endoscopic datasets with deformable scenes and varying camera motions show that Local-EndoGS consistently outperforms state-of-the-art methods in appearance quality and geometry. Ablation studies validate the effectiveness of our key designs. Code will be released upon acceptance at: https://github.com/IRMVLab/Local-EndoGS.

</details>


### [33] [QuPAINT: Physics-Aware Instruction Tuning Approach to Quantum Material Discovery](https://arxiv.org/abs/2602.17478)
*Xuan-Bac Nguyen,Hoang-Quan Nguyen,Sankalp Pandey,Tim Faltermeier,Nicholas Borys,Hugh Churchill,Khoa Luu*

Main category: cs.CV

TL;DR: 该研究提出了一个物理感知的多模态框架，通过合成数据生成、指令数据集构建、物理感知指令调优和标准化基准，解决二维量子材料光学显微图像分析中的挑战。


<details>
  <summary>Details</summary>
Motivation: 二维量子材料光学显微图像分析面临三大挑战：层依赖对比度微弱、标注数据有限、不同实验室和成像设备间差异显著。现有视觉模型因缺乏物理先验知识，难以泛化到新材料或新硬件条件。

Method: 1. 开发Synthia物理合成数据生成器，模拟量子材料薄片在薄膜干涉下的光学响应；2. 构建QMat-Instruct大规模指令数据集，包含物理信息的多模态问答对；3. 提出QuPAINT物理感知指令调优架构，集成物理感知注意力模块融合视觉嵌入和光学先验；4. 建立QF-Bench综合基准，涵盖多种材料、基底和成像设置。

Result: 该框架从数据和模型两个角度解决了现有局限性：Synthia生成多样高质量合成数据减少对专家标注的依赖；QMat-Instruct为MLLMs提供理解薄片外观和厚度的训练数据；QuPAINT通过物理先验增强模型鲁棒性和判别性；QF-Bench为公平可复现评估提供标准化协议。

Conclusion: 该研究提出的物理感知多模态框架通过合成数据生成、指令数据集、物理感知架构和标准化基准，显著提升了二维量子材料光学显微图像分析的准确性和泛化能力，为材料科学中的自动化表征提供了新方法。

Abstract: Characterizing two-dimensional quantum materials from optical microscopy images is challenging due to the subtle layer-dependent contrast, limited labeled data, and significant variation across laboratories and imaging setups. Existing vision models struggle in this domain since they lack physical priors and cannot generalize to new materials or hardware conditions. This work presents a new physics-aware multimodal framework that addresses these limitations from both the data and model perspectives. We first present Synthia, a physics-based synthetic data generator that simulates realistic optical responses of quantum material flakes under thin-film interference. Synthia produces diverse and high-quality samples, helping reduce the dependence on expert manual annotation. We introduce QMat-Instruct, the first large-scale instruction dataset for quantum materials, comprising multimodal, physics-informed question-answer pairs designed to teach Multimodal Large Language Models (MLLMs) to understand the appearance and thickness of flakes. Then, we propose Physics-Aware Instruction Tuning (QuPAINT), a multimodal architecture that incorporates a Physics-Informed Attention module to fuse visual embeddings with optical priors, enabling more robust and discriminative flake representations. Finally, we establish QF-Bench, a comprehensive benchmark spanning multiple materials, substrates, and imaging settings, offering standardized protocols for fair and reproducible evaluation.

</details>


### [34] [Tracing Copied Pixels and Regularizing Patch Affinity in Copy Detection](https://arxiv.org/abs/2602.17484)
*Yichen Lu,Siwei Nie,Minlong Lu,Xudong Yang,Xiaobo Zhang,Peng Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的图像复制检测方法，通过像素级追踪和几何引导对比学习来解决现有自监督学习方法在复杂编辑图像上表现不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于自监督学习的图像复制检测方法在处理复杂编辑图像时存在局限性，主要是因为视图级对比学习方法缺乏细粒度对应关系学习，无法有效处理复杂的编辑变换。

Method: 提出了两个关键创新：1) PixTrace - 像素坐标追踪模块，保持编辑变换中的显式空间映射；2) CopyNCE - 几何引导对比损失函数，利用PixTrace验证的映射计算重叠率来正则化块相似性。

Result: 在DISC21数据集上取得了最先进的性能：匹配器达到88.7% uAP / 83.9% RP90，描述符达到72.6% uAP / 68.4% RP90，同时比现有方法具有更好的可解释性。

Conclusion: 该方法成功地将像素级可追踪性与块级相似性学习相结合，有效抑制了自监督学习训练中的监督噪声，显著提升了图像复制检测的性能和可解释性。

Abstract: Image Copy Detection (ICD) aims to identify manipulated content between image pairs through robust feature representation learning. While self-supervised learning (SSL) has advanced ICD systems, existing view-level contrastive methods struggle with sophisticated edits due to insufficient fine-grained correspondence learning. We address this limitation by exploiting the inherent geometric traceability in edited content through two key innovations. First, we propose PixTrace - a pixel coordinate tracking module that maintains explicit spatial mappings across editing transformations. Second, we introduce CopyNCE, a geometrically-guided contrastive loss that regularizes patch affinity using overlap ratios derived from PixTrace's verified mappings. Our method bridges pixel-level traceability with patch-level similarity learning, suppressing supervision noise in SSL training. Extensive experiments demonstrate not only state-of-the-art performance (88.7% uAP / 83.9% RP90 for matcher, 72.6% uAP / 68.4% RP90 for descriptor on DISC21 dataset) but also better interpretability over existing methods.

</details>


### [35] [FoundationPose-Initialized 3D-2D Liver Registration for Surgical Augmented Reality](https://arxiv.org/abs/2602.17517)
*Hanyuan Zhang,Lucas He,Runlong He,Abdolrahim Kadkhodamohammadi,Danail Stoyanov,Brian R. Davidson,Evangelos B. Mazomenos,Matthew J. Clarkson*

Main category: cs.CV

TL;DR: 该研究提出了一种结合腹腔镜深度图和基础姿态估计器的轻量化肿瘤定位方法，用非刚性迭代最近点替代有限元变形模型，降低工程复杂度，在真实患者数据上达到9.91毫米平均配准误差。


<details>
  <summary>Details</summary>
Motivation: 现有腹腔镜肝脏手术中的肿瘤定位配准流程通常依赖器官轮廓，且变形配准多采用有限元模型结合降维或机器学习组件，这需要较高的工程建模复杂度和专业知识。研究旨在开发更轻量、工程友好的替代方案。

Method: 整合腹腔镜深度图与基础姿态估计器进行相机-肝脏姿态估计，用非刚性迭代最近点替代有限元变形模型，采用刚性-NICP联合配准流程，降低工程建模复杂度。

Result: 在真实患者数据上，深度增强的基础姿态方法在3个病例中达到9.91毫米平均配准误差。刚性-NICP联合配准优于纯刚性配准，证明NICP可作为有限元变形模型的有效替代。

Conclusion: 该配准流程在保持临床相关精度的同时，提供了比有限元变形模型更轻量、工程友好的替代方案，降低了专业知识和建模复杂度要求。

Abstract: Augmented reality can improve tumor localization in laparoscopic liver surgery. Existing registration pipelines typically depend on organ contours; deformable (non-rigid) alignment is often handled with finite-element (FE) models coupled to dimensionality-reduction or machine-learning components. We integrate laparoscopic depth maps with a foundation pose estimator for camera-liver pose estimation and replace FE-based deformation with non-rigid iterative closest point (NICP) to lower engineering/modeling complexity and expertise requirements. On real patient data, the depth-augmented foundation pose approach achieved 9.91 mm mean registration error in 3 cases. Combined rigid-NICP registration outperformed rigid-only registration, demonstrating NICP as an efficient substitute for finite-element deformable models. This pipeline achieves clinically relevant accuracy while offering a lightweight, engineering-friendly alternative to FE-based deformation.

</details>


### [36] [LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs](https://arxiv.org/abs/2602.17535)
*Behzad Bozorgtabar,Dwarikanath Mahapatra,Sudipta Roy,Muzammal Naseer,Imran Razzak,Zongyuan Ge*

Main category: cs.CV

TL;DR: LATA：一种无需训练和标签的Laplacian辅助转导适应方法，通过图像-图像k-NN图平滑零样本概率，结合失败感知的conformal评分，在保持SCP有效性的同时提高医学视觉语言模型在领域转移下的预测效率和类别平衡。


<details>
  <summary>Details</summary>
Motivation: 医学视觉语言模型在零样本识别方面表现强大，但在领域转移下的可靠性依赖于有保证的校准不确定性。传统分割conformal预测方法存在预测集过大（效率低）和类别间覆盖不平衡的问题，特别是在少样本、不平衡场景下，且直接适应校准标签会破坏可交换性并失去理论保证。

Method: 提出LATA方法：1）基于图像-图像k-NN图使用少量CCCP平均场更新平滑零样本概率；2）引入失败感知的conformal评分，结合视觉语言不确定性框架，提供实例级难度和标签可信度；3）保持黑盒特性（无需更新VLM）、计算轻量（窗口化转导、无反向传播）；4）包含可选先验调节，可严格无标签运行或使用校准边缘信息。

Result: 在三个医学VLMs和九个下游任务上，LATA一致性地减少了预测集大小和类别覆盖差距，同时匹配或收紧目标覆盖率，优于先前的转导基线方法，缩小了与使用标签方法的差距，且计算量远小于传统方法。

Conclusion: LATA通过Laplacian辅助的转导适应和失败感知评分，能够在保持可交换性的同时锐化零样本预测，为医学视觉语言模型在领域转移下提供更高效、更平衡的不确定性校准方法。

Abstract: Medical vision-language models (VLMs) are strong zero-shot recognizers for medical imaging, but their reliability under domain shift hinges on calibrated uncertainty with guarantees. Split conformal prediction (SCP) offers finite-sample coverage, yet prediction sets often become large (low efficiency) and class-wise coverage unbalanced-high class-conditioned coverage gap (CCV), especially in few-shot, imbalanced regimes; moreover, naively adapting to calibration labels breaks exchangeability and voids guarantees. We propose \texttt{\textbf{LATA}} (Laplacian-Assisted Transductive Adaptation), a \textit{training- and label-free} refinement that operates on the joint calibration and test pool by smoothing zero-shot probabilities over an image-image k-NN graph using a small number of CCCP mean-field updates, preserving SCP validity via a deterministic transform. We further introduce a \textit{failure-aware} conformal score that plugs into the vision-language uncertainty (ViLU) framework, providing instance-level difficulty and label plausibility to improve prediction set efficiency and class-wise balance at fixed coverage. \texttt{\textbf{LATA}} is black-box (no VLM updates), compute-light (windowed transduction, no backprop), and includes an optional prior knob that can run strictly label-free or, if desired, in a label-informed variant using calibration marginals once. Across \textbf{three} medical VLMs and \textbf{nine} downstream tasks, \texttt{\textbf{LATA}} consistently reduces set size and CCV while matching or tightening target coverage, outperforming prior transductive baselines and narrowing the gap to label-using methods, while using far less compute. Comprehensive ablations and qualitative analyses show that \texttt{\textbf{LATA}} sharpens zero-shot predictions without compromising exchangeability.

</details>


### [37] [GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking](https://arxiv.org/abs/2602.17555)
*Zixu Cheng,Da Li,Jian Hu,Ziquan Liu,Wei Li,Shaogang Gong*

Main category: cs.CV

TL;DR: GraphThinker：基于强化微调的视频推理方法，通过构建事件级场景图和增强视觉定位来减少幻觉


<details>
  <summary>Details</summary>
Motivation: 视频推理需要理解事件间的因果关系，但这些关系通常是隐式的且标注成本高。现有的多模态大语言模型通过密集字幕或视频摘要进行推理，缺乏明确的因果结构建模，导致推理过程中出现幻觉问题。

Method: 提出GraphThinker方法：1）使用MLLM构建事件级视频场景图（EVSG），显式建模事件内和事件间关系；2）将场景图作为中间思考过程整合到MLLM中；3）在强化微调中引入视觉注意力奖励，增强视频定位能力。

Result: 在RexTime和VidHalluc数据集上评估，GraphThinker在捕捉对象和事件关系方面表现优异，具有更精确的事件定位能力，相比先前方法减少了视频推理中的幻觉。

Conclusion: 通过显式建模事件级场景图结构和增强视觉定位，GraphThinker有效减少了视频推理中的幻觉问题，提升了因果理解和推理能力。

Abstract: Video reasoning requires understanding the causal relationships between events in a video. However, such relationships are often implicit and costly to annotate manually. While existing multimodal large language models (MLLMs) often infer event relations through dense captions or video summaries for video reasoning, such modeling still lacks causal understanding. Without explicit causal structure modeling within and across video events, these models suffer from hallucinations during the video reasoning. In this work, we propose GraphThinker, a reinforcement finetuning-based method that constructs structural event-level scene graphs and enhances visual grounding to jointly reduce hallucinations in video reasoning. Specifically, we first employ an MLLM to construct an event-based video scene graph (EVSG) that explicitly models both intra- and inter-event relations, and incorporate these formed scene graphs into the MLLM as an intermediate thinking process. We also introduce a visual attention reward during reinforcement finetuning, which strengthens video grounding and further mitigates hallucinations. We evaluate GraphThinker on two datasets, RexTime and VidHalluc, where it shows superior ability to capture object and event relations with more precise event localization, reducing hallucinations in video reasoning compared to prior methods.

</details>


### [38] [Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment](https://arxiv.org/abs/2602.17599)
*Ivan Rinaldi,Matteo Mendula,Nicola Fanelli,Florence Levé,Matteo Testi,Giovanna Castellano,Gennaro Vessio*

Main category: cs.CV

TL;DR: ArtToMus：首个直接从艺术作品生成音乐的框架，无需图像到文本转换，使用视觉嵌入指导潜在扩散模型生成音乐


<details>
  <summary>Details</summary>
Motivation: 现有图像条件音乐生成系统存在两个根本限制：1）通常基于自然照片训练，无法捕捉艺术作品丰富的语义、风格和文化内容；2）大多依赖图像到文本转换阶段，使用语言作为语义捷径，阻碍了直接的视觉到音频学习

Method: 提出ArtSound数据集（105,884个艺术作品-音乐配对）和ArtToMus框架，将视觉嵌入投影到潜在扩散模型的调节空间，实现无需语言监督的直接艺术作品到音乐生成

Result: ArtToMus生成音乐连贯、风格一致，能反映源艺术作品的显著视觉线索。虽然绝对对齐分数低于文本条件系统（考虑到去除语言监督的难度），但在感知质量和有意义的跨模态对应方面表现竞争性

Conclusion: 这项工作确立了直接视觉到音乐生成作为一个独特且具有挑战性的研究方向，为多媒体艺术、文化遗产和AI辅助创意实践提供了资源支持

Abstract: Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifies conditioning but prevents direct visual-to-audio learning. Motivated by these gaps, we introduce ArtSound, a large-scale multimodal dataset of 105,884 artwork-music pairs enriched with dual-modality captions, obtained by extending ArtGraph and the Free Music Archive. We further propose ArtToMus, the first framework explicitly designed for direct artwork-to-music generation, which maps digitized artworks to music without image-to-text translation or language-based semantic supervision. The framework projects visual embeddings into the conditioning space of a latent diffusion model, enabling music synthesis guided solely by visual information. Experimental results show that ArtToMus generates musically coherent and stylistically consistent outputs that reflect salient visual cues of the source artworks. While absolute alignment scores remain lower than those of text-conditioned systems-as expected given the substantially increased difficulty of removing linguistic supervision-ArtToMus achieves competitive perceptual quality and meaningful cross-modal correspondence. This work establishes direct visual-to-music generation as a distinct and challenging research direction, and provides resources that support applications in multimedia art, cultural heritage, and AI-assisted creative practice. Code and dataset will be publicly released upon acceptance.

</details>


### [39] [Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery](https://arxiv.org/abs/2602.17605)
*Jowaria Khan,Anindya Sarkar,Yevgeniy Vorobeychik,Elizabeth Bondi-Kelly*

Main category: cs.CV

TL;DR: 该论文提出了一种统一的地理空间发现框架，通过结合主动学习、在线元学习和概念引导推理，在数据稀缺且环境动态变化的场景下，有效发现隐藏目标。


<details>
  <summary>Details</summary>
Motivation: 在环境监测、灾害响应、公共卫生等现实场景中，数据收集成本高、难度大，且环境动态变化。稀疏且有偏的地理空间真实数据限制了现有基于学习的方法（如强化学习）的适用性。需要一种能在资源约束下高效发现隐藏目标的方法。

Method: 提出了一个统一的地理空间发现框架，整合主动学习、在线元学习和概念引导推理。核心创新包括：1）概念加权不确定性采样策略，基于领域特定概念（如土地覆盖、污染源接近度）学习概念相关性来调节不确定性；2）相关性感知元批次形成策略，在在线元更新中促进语义多样性，提高动态环境下的泛化能力。

Result: 在真实世界的PFAS（全氟和多氟烷基物质）致癌污染数据集上进行测试，展示了该方法在有限数据和变化环境下可靠发现目标的能力。

Conclusion: 该方法通过引入概念相关性的统一框架，有效解决了地理空间发现中数据稀缺和环境动态变化的挑战，在真实世界污染监测等应用中表现出色。

Abstract: In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of *concept relevance*, which captures how domain-specific factors influence target presence: a *concept-weighted uncertainty sampling strategy*, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e.g., land cover, source proximity); and a *relevance-aware meta-batch formation strategy* that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments. Our experiments include testing on a real-world dataset of cancer-causing PFAS (Per- and polyfluoroalkyl substances) contamination, showcasing our method's reliability at uncovering targets with limited data and a varying environment.

</details>


### [40] [CORAL: Correspondence Alignment for Improved Virtual Try-On](https://arxiv.org/abs/2602.17636)
*Jiyoung Kim,Youngjin Shin,Siyoon Jin,Dahyun Chung,Jisu Nam,Tongmin Kim,Jongjae Park,Hyeonwoo Kang,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出CORAL框架，通过显式对齐查询-键匹配来改善虚拟试衣中服装细节保留问题，解决了现有方法在未配对设置下难以保持精细服装细节的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试衣方法在未配对设置下难以保持精细服装细节，缺乏显式的人-服装对齐机制，且未能解释扩散变换器中对应关系如何产生。

Method: 首先分析DiT架构中的完整3D注意力机制，发现人-服装对应关系依赖于查询-键的精确匹配。基于此提出CORAL框架，包含两个组件：对应关系蒸馏损失（对齐可靠匹配与人-服装注意力）和熵最小化损失（锐化注意力分布）。

Result: CORAL在基线上持续改进，增强了全局形状转换和局部细节保留。大量消融实验验证了设计选择的有效性。

Conclusion: 通过显式对齐查询-键匹配与外部对应关系，CORAL框架有效解决了虚拟试衣中服装细节保留问题，并提出了基于VLM的评估协议以更好地反映人类偏好。

Abstract: Existing methods for Virtual Try-On (VTON) often struggle to preserve fine garment details, especially in unpaired settings where accurate person-garment correspondence is required. These methods do not explicitly enforce person-garment alignment and fail to explain how correspondence emerges within Diffusion Transformers (DiTs). In this paper, we first analyze full 3D attention in DiT-based architecture and reveal that the person-garment correspondence critically depends on precise person-garment query-key matching within the full 3D attention. Building on this insight, we then introduce CORrespondence ALignment (CORAL), a DiT-based framework that explicitly aligns query-key matching with robust external correspondences. CORAL integrates two complementary components: a correspondence distillation loss that aligns reliable matches with person-garment attention, and an entropy minimization loss that sharpens the attention distribution. We further propose a VLM-based evaluation protocol to better reflect human preference. CORAL consistently improves over the baseline, enhancing both global shape transfer and local detail preservation. Extensive ablations validate our design choices.

</details>


### [41] [IntRec: Intent-based Retrieval with Contrastive Refinement](https://arxiv.org/abs/2602.17639)
*Pourya Shamsolmoali,Masoumeh Zareapoor,Eric Granger,Yue Lu*

Main category: cs.CV

TL;DR: IntRec是一个交互式物体检索框架，通过用户反馈精炼预测，在复杂场景中准确检索用户指定物体，特别是在查询模糊或涉及多个相似物体时表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇检测器以一次性方式操作，缺乏基于用户反馈精炼预测的能力，在处理模糊查询或多相似物体场景时存在局限性。

Method: 提出IntRec交互框架，核心是意图状态（IS）维护正锚点（确认线索）和负约束（拒绝假设）的双重记忆集，通过对比对齐函数对候选物体排序，最大化与正线索相似性同时惩罚被拒绝假设。

Result: 在LVIS数据集上达到35.4 AP，优于OVMR、CoDet和CAKE分别+2.3、+3.7和+0.5；在LVIS-Ambiguous基准上，单次纠正反馈后性能比一次性基线提升+7.9 AP，每次交互增加延迟小于30毫秒。

Conclusion: IntRec通过交互式反馈机制显著提升物体检索精度，无需额外监督，在模糊和复杂场景中实现细粒度消歧，具有实用性和高效性。

Abstract: Retrieving user-specified objects from complex scenes remains a challenging task, especially when queries are ambiguous or involve multiple similar objects. Existing open-vocabulary detectors operate in a one-shot manner, lacking the ability to refine predictions based on user feedback. To address this, we propose IntRec, an interactive object retrieval framework that refines predictions based on user feedback. At its core is an Intent State (IS) that maintains dual memory sets for positive anchors (confirmed cues) and negative constraints (rejected hypotheses). A contrastive alignment function ranks candidate objects by maximizing similarity to positive cues while penalizing rejected ones, enabling fine-grained disambiguation in cluttered scenes. Our interactive framework provides substantial improvements in retrieval accuracy without additional supervision. On LVIS, IntRec achieves 35.4 AP, outperforming OVMR, CoDet, and CAKE by +2.3, +3.7, and +0.5, respectively. On the challenging LVIS-Ambiguous benchmark, it improves performance by +7.9 AP over its one-shot baseline after a single corrective feedback, with less than 30 ms of added latency per interaction.

</details>


### [42] [OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents](https://arxiv.org/abs/2602.17665)
*Akashah Shabbir,Muhammad Umer Sheikh,Muhammad Akhtar Munir,Hiyam Debary,Mustansar Fiaz,Muhammad Zaigham Zaheer,Paolo Fraccaro,Fahad Shahbaz Khan,Muhammad Haris Khan,Xiao Xiang Zhu,Salman Khan*

Main category: cs.CV

TL;DR: OpenEarthAgent是一个用于开发工具增强地理空间智能体的统一框架，通过卫星影像、自然语言查询和详细推理轨迹训练，实现多模态地理空间推理。


<details>
  <summary>Details</summary>
Motivation: 将多模态推理能力扩展到遥感领域面临挑战，因为模型需要在空间尺度、地理结构和多光谱指数上进行推理，同时保持连贯的多步逻辑。

Method: 提出统一框架，通过监督微调结构化推理轨迹来训练工具增强的地理空间智能体，包含14,538个训练实例和1,169个评估实例，涵盖城市、环境、灾害和基础设施等领域，整合GIS操作和NDVI、NBR、NDBI等指数分析。

Result: 训练出的智能体展现出结构化推理、稳定的空间理解和可解释的行为，在多样化条件下通过工具驱动的地理空间交互表现优异，相对于强基线有持续改进，与近期开源和闭源模型相比具有竞争力。

Conclusion: OpenEarthAgent成功地将多模态推理能力扩展到遥感领域，通过工具增强的地理空间智能体实现了结构化推理和可解释的行为，为地理空间分析提供了有效的解决方案。

Abstract: Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks. Extending such capabilities to the remote sensing domain remains challenging, as models must reason over spatial scale, geographic structures, and multispectral indices while maintaining coherent multi-step logic. To bridge this gap, OpenEarthAgent introduces a unified framework for developing tool-augmented geospatial agents trained on satellite imagery, natural-language queries, and detailed reasoning traces. The training pipeline relies on supervised fine-tuning over structured reasoning trajectories, aligning the model with verified multistep tool interactions across diverse analytical contexts. The accompanying corpus comprises 14,538 training and 1,169 evaluation instances, with more than 100K reasoning steps in the training split and over 7K reasoning steps in the evaluation split. It spans urban, environmental, disaster, and infrastructure domains, and incorporates GIS-based operations alongside index analyses such as NDVI, NBR, and NDBI. Grounded in explicit reasoning traces, the learned agent demonstrates structured reasoning, stable spatial understanding, and interpretable behaviour through tool-driven geospatial interactions across diverse conditions. We report consistent improvements over a strong baseline and competitive performance relative to recent open and closed-source models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [43] [ICP-Based Pallet Tracking for Unloading on Inclined Surfaces by Autonomous Forklifts](https://arxiv.org/abs/2602.16744)
*Takuro Kato,Mitsuharu Morisawa*

Main category: cs.RO

TL;DR: 提出一种用于自主叉车在倾斜表面上卸载托盘的防拖拽控制方法


<details>
  <summary>Details</summary>
Motivation: 解决自主叉车在倾斜表面（如卡车货箱）卸载托盘时，叉子抽出时可能拖拽托盘的问题

Method: 使用迭代最近点（ICP）算法处理托盘上方的点云数据，实时追踪托盘与叉子之间的相对位置和姿态角度差，使叉子与目标表面平行对齐

Result: 通过动态仿真和真实叉车实验验证了方法的有效性，成功在卡车倾斜货箱上完成无拖拽的托盘卸载操作

Conclusion: 提出的控制方法能够有效实现自主叉车在倾斜表面上的无拖拽托盘卸载，提高了自动化物料搬运的安全性和可靠性

Abstract: This paper proposes a control method for autonomous forklifts to unload pallets on inclined surfaces, enabling the fork to be withdrawn without dragging the pallets. The proposed method applies the Iterative Closest Point (ICP) algorithm to point clouds measured from the upper region of the pallet and thereby tracks the relative position and attitude angle difference between the pallet and the fork during the unloading operation in real-time. According to the tracking result, the fork is aligned parallel to the target surface. After the fork is aligned, it is possible to complete the unloading process by withdrawing the fork along the tilt, preventing any dragging of the pallet. The effectiveness of the proposed method is verified through dynamic simulations and experiments using a real forklift that replicate unloading operations onto the inclined bed of a truck.

</details>


### [44] [Smooth trajectory generation and hybrid B-splines-Quaternions based tool path interpolation for a 3T1R parallel kinematic milling robot](https://arxiv.org/abs/2602.16758)
*Sina Akhbari,Mehran Mahboubkhah*

Main category: cs.RO

TL;DR: 本文提出了一种用于四自由度并联铣削机器人的平滑轨迹生成方法，结合B样条和四元数插值技术处理解耦的位置和姿态数据点，通过分段贝塞尔曲线实现同步优化。


<details>
  <summary>Details</summary>
Motivation: 传统轨迹生成方法在处理并联机器人位置和姿态同步时存在精度不足、速度波动大、计算效率低等问题，需要一种更平滑、高效的轨迹生成方法。

Method: 采用B样条和四元数插值技术处理解耦的位置和姿态数据；通过分段贝塞尔曲线拟合路径长度与工具姿态的非线性关系；利用贝塞尔曲线的凸包特性确保空间和时间约束；使用单位四元数进行姿态插值避免万向节锁；采用修正多项式进行位置插值；分两阶段优化时间轨迹（任务空间→关节空间）。

Result: 实验结果表明，与传统插值方法相比，该方法具有更高的精度、更小的速度波动和更好的计算效率，能够在低成本微控制器上实现。

Conclusion: 该方法为并联铣削机器人提供了一种有效的平滑轨迹生成方案，在精度、平滑性和计算效率方面均有显著改进，适用于实际工业应用。

Abstract: This paper presents a smooth trajectory generation method for a four-degree-of-freedom parallel kinematic milling robot. The proposed approach integrates B-spline and Quaternion interpolation techniques to manage decoupled position and orientation data points. The synchronization of orientation and arc-length-parameterized position data is achieved through the fitting of smooth piece-wise Bezier curves, which describe the non-linear relationship between path length and tool orientation, solved via sequential quadratic programming. By leveraging the convex hull properties of Bezier curves, the method ensures spatial and temporal separation constraints for multi-agent trajectory generation. Unit quaternions are employed for orientation interpolation, providing a robust and efficient representation that avoids gimbal lock and facilitates smooth, continuous rotation. Modifier polynomials are used for position interpolation. Temporal trajectories are optimized using minimum jerk, time-optimal piece-wise Bezier curves in two stages: task space followed by joint space, implemented on a low-cost microcontroller. Experimental results demonstrate that the proposed method offers enhanced accuracy, reduced velocity fluctuations, and computational efficiency compared to conventional interpolation methods.

</details>


### [45] [RRT$^η$: Sampling-based Motion Planning and Control from STL Specifications using Arithmetic-Geometric Mean Robustness](https://arxiv.org/abs/2602.16825)
*Ahmad Ahmad,Shuo Liu,Roberto Tron,Calin Belta*

Main category: cs.RO

TL;DR: RRT$^η$：一种结合算术几何平均（AGM）鲁棒性度量的采样运动规划框架，用于处理STL时空约束，相比传统方法在复杂多约束场景中表现更优


<details>
  <summary>Details</summary>
Motivation: 传统基于STL的运动规划方法使用min-max鲁棒性度量，仅关注临界时间点和子公式，导致非平滑的优化景观和尖锐的决策边界，阻碍了高效的树探索

Method: 提出RRT$^η$框架，集成AGM鲁棒性度量评估所有时间点和子公式的满足程度；包括AGM鲁棒性区间语义、高效增量监控算法和基于FPL的增强满意度方向向量

Result: 在三个机器人系统（双积分点机器人、独轮移动机器人、7自由度机械臂）上验证，相比传统STL鲁棒性规划器，在有限引导信号的多约束场景中表现出优越性能

Conclusion: RRT$^η$框架能够合成满足STL规范的高鲁棒性动态可行控制序列，同时保持RRT$^\ast$的概率完备性和渐近最优性，特别适合复杂多约束场景

Abstract: Sampling-based motion planning has emerged as a powerful approach for robotics, enabling exploration of complex, high-dimensional configuration spaces. When combined with Signal Temporal Logic (STL), a temporal logic widely used for formalizing interpretable robotic tasks, these methods can address complex spatiotemporal constraints. However, traditional approaches rely on min-max robustness measures that focus only on critical time points and subformulae, creating non-smooth optimization landscapes with sharp decision boundaries that hinder efficient tree exploration.
  We propose RRT$^η$, a sampling-based planning framework that integrates the Arithmetic-Geometric Mean (AGM) robustness measure to evaluate satisfaction across all time points and subformulae. Our key contributions include: (1) AGM robustness interval semantics for reasoning about partial trajectories during tree construction, (2) an efficient incremental monitoring algorithm computing these intervals, and (3) enhanced Direction of Increasing Satisfaction vectors leveraging Fulfillment Priority Logic (FPL) for principled objective composition. Our framework synthesizes dynamically feasible control sequences satisfying STL specifications with high robustness while maintaining the probabilistic completeness and asymptotic optimality of RRT$^\ast$. We validate our approach on three robotic systems. A double integrator point robot, a unicycle mobile robot, and a 7-DOF robot arm, demonstrating superior performance over traditional STL robustness-based planners in multi-constraint scenarios with limited guidance signals.

</details>


### [46] [Sound of Touch: Active Acoustic Tactile Sensing via String Vibrations](https://arxiv.org/abs/2602.16846)
*Xili Yi,Ying Xing,Zachary Manchester,Nima Fazeli*

Main category: cs.RO

TL;DR: Sound of Touch：一种基于振动弦的主动声学触觉传感方法，使用少量拾音器通过频谱变化检测接触位置、法向力和滑动


<details>
  <summary>Details</summary>
Motivation: 分布式触觉传感在大面积应用中面临挑战：密集传感器阵列增加布线、成本和脆弱性，而许多替代方案覆盖有限或无法捕捉快速交互动态

Method: 使用电磁连续激振的张力弦作为传感元件，通过少量接触式麦克风观察接触引起的频谱变化，基于物理的弦振动模拟器预测接触位置和力如何改变振动模式

Result: 实验证明毫米级定位精度、可靠的力估计和实时滑动检测

Conclusion: 提出了一种轻量级、可扩展的弦基触觉传感硬件概念，用于机器人扩展表面；建立了物理基础的模拟分析工具；开发了实时推理管道将振动测量映射到接触状态

Abstract: Distributed tactile sensing remains difficult to scale over large areas: dense sensor arrays increase wiring, cost, and fragility, while many alternatives provide limited coverage or miss fast interaction dynamics. We present Sound of Touch, an active acoustic tactile-sensing methodology that uses vibrating tensioned strings as sensing elements. The string is continuously excited electromagnetically, and a small number of pickups (contact microphones) observe spectral changes induced by contact. From short-duration audio signals, our system estimates contact location and normal force, and detects slip. To guide design and interpret the sensing mechanism, we derive a physics-based string-vibration simulator that predicts how contact position and force shift vibration modes. Experiments demonstrate millimeter-scale localization, reliable force estimation, and real-time slip detection. Our contributions are: (i) a lightweight, scalable string-based tactile sensing hardware concept for instrumenting extended robot surfaces; (ii) a physics-grounded simulation and analysis tool for contact-induced spectral shifts; and (iii) a real-time inference pipeline that maps vibration measurements to contact state.

</details>


### [47] ["Hello, I'm Delivering. Let Me Pass By": Navigating Public Pathways with Walk-along with Robots in Crowded City Streets](https://arxiv.org/abs/2602.16861)
*EunJeong Cheon,Do Yeon Shin*

Main category: cs.RO

TL;DR: 提出"Walk-Along with Robots"方法，用于研究公共场所自主移动机器人的真实交互


<details>
  <summary>Details</summary>
Motivation: 现有HRI研究在公共场所的实地研究通常限于受控实验或结构化观察方法，而当前自主移动机器人（如配送机器人）在研究者控制之外运行，需要更贴近真实场景的研究方法

Method: 借鉴城市研究、地理学和社会学中的公共领域民族志，提出Walk-Along with Robots方法，详细阐述了该方法的关键特征、实施步骤、独特见解和评估方式

Result: 提出了一套系统化的研究方法论，能够捕捉自主机器人在动态路线和不可预测环境中的真实交互情况

Conclusion: WawR方法为研究公共场所自主机器人提供了更贴近现实的研究框架，希望促进该领域研究方法的进一步讨论

Abstract: As the presence of autonomous robots in public spaces increases-whether navigating campus walkways or neighborhood sidewalks-understanding how to carefully study these robots becomes critical. While HRI research has conducted field studies in public spaces, these are often limited to controlled experiments with prototype robots or structured observational methods, such as the Wizard of Oz technique. However, the autonomous mobile robots we encounter today, particularly delivery robots, operate beyond the control of researchers, navigating dynamic routes and unpredictable environments. To address this challenge, a more deliberate approach is required. Drawing inspiration from public realm ethnography in urban studies, geography, and sociology, this paper proposes the Walk-Along with Robots (WawR) methodology. We outline the key features of this method, the steps we applied in our study, the unique insights it offers, and the ways it can be evaluated. We hope this paper stimulates further discussion on research methodologies for studying autonomous robots in public spaces.

</details>


### [48] [SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation](https://arxiv.org/abs/2602.16863)
*Kushal Kedia,Tyler Ga Wei Lum,Jeannette Bohg,C. Karen Liu*

Main category: cs.RO

TL;DR: SimToolReal：通过程序化生成大量工具状物体基元并训练单一RL策略，实现零样本的通用工具操作，无需特定对象或任务训练


<details>
  <summary>Details</summary>
Motivation: 工具操作是机器人灵巧性的重要挑战，需要抓取薄物体、手内旋转和强力交互。由于收集遥操作数据困难，仿真到现实的强化学习是有前景的替代方案，但现有方法需要大量工程努力来建模对象和调整奖励函数

Method: 提出SimToolReal方法，程序化生成大量工具状物体基元，训练单一RL策略以通用目标操作每个物体到随机目标姿态。该方法在测试时无需任何对象或任务特定训练

Result: SimToolReal比先前重定向和固定抓取方法性能提升37%，同时与针对特定目标对象和任务训练的专家RL策略性能相当。在真实世界120次部署中，跨越24个任务、12个对象实例和6个工具类别，展示了强大的零样本性能

Conclusion: SimToolReal通过程序化生成多样化工具状物体和训练通用RL策略，实现了通用灵巧工具操作，为仿真到现实的工具操作提供了一种更通用、更高效的解决方案

Abstract: The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.

</details>


### [49] [Boreas Road Trip: A Multi-Sensor Autonomous Driving Dataset on Challenging Roads](https://arxiv.org/abs/2602.16870)
*Daniil Lisus,Katya M. Papais,Cedric Le Gentil,Elliot Preston-Krebs,Andrew Lambert,Keith Y. K. Leung,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: Boreas-RT数据集扩展了原有Boreas数据集，包含9条真实路线共643公里驾驶数据，提供多传感器融合和厘米级地面真值，用于评估自动驾驶算法在多样化道路条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶算法在简单驾驶环境中容易过拟合，在多样化、具有挑战性的真实道路条件下性能显著下降。需要一个新的数据集来评估多模态算法在不同道路条件下的鲁棒性。

Method: 收集了60个序列，覆盖9条真实路线，每条路线多次遍历以评估相同环境下不同交通和天气条件的影响。使用多传感器平台包括相机、雷达、激光雷达、IMU和轮速编码器，提供厘米级GNSS-INS地面真值。

Result: 基准测试显示许多最先进的里程计和定位算法在简单环境中过拟合，在更具挑战性的Boreas-RT路线上性能显著下降。数据集提供了统一的评估平台。

Conclusion: Boreas-RT为评估多模态自动驾驶算法在多样化道路条件下的性能提供了重要数据集，揭示了现有算法在实际复杂环境中的局限性，并提供了公开的开发工具和排行榜。

Abstract: The Boreas Road Trip (Boreas-RT) dataset extends the multi-season Boreas dataset to new and diverse locations that pose challenges for modern autonomous driving algorithms. Boreas-RT comprises 60 sequences collected over 9 real-world routes, totalling 643 km of driving. Each route is traversed multiple times, enabling evaluation in identical environments under varying traffic and, in some cases, weather conditions. The data collection platform includes a 5MP FLIR Blackfly S camera, a 360 degree Navtech RAS6 Doppler-enabled spinning radar, a 128-channel 360 degree Velodyne Alpha Prime lidar, an Aeva Aeries II FMCW Doppler-enabled lidar, a Silicon Sensing DMU41 inertial measurement unit, and a Dynapar wheel encoder. Centimetre-level ground truth is provided via post-processed Applanix POS LV GNSS-INS data. The dataset includes precise extrinsic and intrinsic calibrations, a publicly available development kit, and a live leaderboard for odometry and metric localization. Benchmark results show that many state-of-the-art odometry and localization algorithms overfit to simple driving environments and degrade significantly on the more challenging Boreas-RT routes. Boreas-RT provides a unified dataset for evaluating multi-modal algorithms across diverse road conditions. The dataset, leaderboard, and development kit are available at www.boreas.utias.utoronto.ca.

</details>


### [50] [MALLVI: a multi agent framework for integrated generalized robotics manipulation](https://arxiv.org/abs/2602.16898)
*Iman Ahmadi,Mehrshad Taji,Arad Mahdinezhad Kashani,AmirHossein Jadidi,Saina Kashani,Babak Khalaj*

Main category: cs.RO

TL;DR: MALLVi是一个多智能体大语言视觉框架，通过闭环反馈驱动的机器人操作，使用专门的智能体协调处理感知、定位、推理和规划，提高零样本操作任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的机器人任务规划方法通常采用专门模型、微调或提示调整，并以开环方式运行，缺乏鲁棒的环境反馈，在动态环境中表现脆弱。

Method: MALLVi采用多智能体协调框架，包含分解器、定位器、思考器和反射器等专门智能体，通过视觉语言模型评估环境反馈，实现闭环反馈驱动的机器人操作。

Result: 在仿真和真实世界实验中，迭代闭环多智能体协调提高了泛化能力，增加了零样本操作任务的成功率。

Conclusion: MALLVi框架通过多智能体协调和闭环反馈机制，显著提升了机器人操作的鲁棒性和成功率，特别是在动态环境中。

Abstract: Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.We present MALLVi, a Multi Agent Large Language and Vision framework that enables closed loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVi generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step.Rather than using a single model, MALLVi coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning.Experiments in simulation and real world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks.Code available at https://github.com/iman1234ahmadi/MALLVI.

</details>


### [51] [SparTa: Sparse Graphical Task Models from a Handful of Demonstrations](https://arxiv.org/abs/2602.16911)
*Adrian Röfer,Nick Heppert,Abhinav Valada*

Main category: cs.RO

TL;DR: 该论文提出了一种从演示中学习长时程操作任务的方法，通过图形化对象关系表示场景状态演化，使用演示分割和池化技术提取操作图，并利用预训练视觉特征进行对象匹配以提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 高效学习长时程操作任务是机器人学习演示中的核心挑战。现有方法主要关注在动作域直接学习任务，而本文关注推断机器人应该实现什么（目标），而不是如何实现（方法）。

Method: 1. 使用图形化对象关系表示演化场景状态；2. 提出演示分割和池化方法，提取操作图并估计任务各阶段的对象状态分布；3. 与仅捕获部分交互或短时间窗口的先前图方法不同，本方法捕获从控制开始到操作结束的完整对象交互；4. 使用预训练视觉特征进行对象匹配以提高多演示学习的鲁棒性。

Result: 1. 在大量实验中评估了演示分割的准确性；2. 评估了从多演示中学习寻找期望最小任务模型的效用；3. 在仿真和真实机器人上部署了拟合模型，证明所得任务表示支持跨环境的可靠执行。

Conclusion: 该方法通过图形化对象关系表示和演示分割池化技术，能够有效学习长时程操作任务，捕获完整的对象交互，并在多演示学习中保持鲁棒性，最终在仿真和真实环境中实现可靠的任务执行。

Abstract: Learning long-horizon manipulation tasks efficiently is a central challenge in robot learning from demonstration. Unlike recent endeavors that focus on directly learning the task in the action domain, we focus on inferring what the robot should achieve in the task, rather than how to do so. To this end, we represent evolving scene states using a series of graphical object relationships. We propose a demonstration segmentation and pooling approach that extracts a series of manipulation graphs and estimates distributions over object states across task phases. In contrast to prior graph-based methods that capture only partial interactions or short temporal windows, our approach captures complete object interactions spanning from the onset of control to the end of the manipulation. To improve robustness when learning from multiple demonstrations, we additionally perform object matching using pre-trained visual features. In extensive experiments, we evaluate our method's demonstration segmentation accuracy and the utility of learning from multiple demonstrations for finding a desired minimal task model. Finally, we deploy the fitted models both in simulation and on a real robot, demonstrating that the resulting task representations support reliable execution across environments.

</details>


### [52] [Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success](https://arxiv.org/abs/2602.17101)
*Varun Burde,Pavel Burget,Torsten Sattler*

Main category: cs.RO

TL;DR: 该论文提出了一个基于物理的大规模基准测试，用于评估6D姿态估计器和3D网格模型在机器人抓取任务中的功能有效性，揭示了重建质量对下游任务的影响。


<details>
  <summary>Details</summary>
Motivation: 当前3D重建方法虽然能产生视觉和几何上令人印象深刻的网格，但标准的几何评估无法反映重建质量如何影响机器人抓取等下游任务性能。需要填补这一空白，评估感知系统与物体操作之间的关系。

Method: 引入大规模、基于物理的基准测试，在不同重建的3D网格上生成抓取姿态，并在真实模型上执行这些抓取，模拟使用不完美模型生成的抓取姿态如何影响与真实物体的交互，从而评估姿态误差、抓取鲁棒性和3D重建几何不准确性的综合影响。

Result: 重建伪影显著减少了抓取姿态候选数量，但在姿态准确估计的情况下对抓取性能影响可忽略。抓取成功与姿态误差的关系主要由空间误差主导，即使是简单的平移误差也能为对称物体的抓取姿态成功提供洞察。

Conclusion: 该工作为理解感知系统如何影响机器人物体操作提供了重要见解，建立了连接3D重建质量与下游机器人任务性能的评估框架。

Abstract: 3D reconstruction serves as the foundational layer for numerous robotic perception tasks, including 6D object pose estimation and grasp pose generation. Modern 3D reconstruction methods for objects can produce visually and geometrically impressive meshes from multi-view images, yet standard geometric evaluations do not reflect how reconstruction quality influences downstream tasks such as robotic manipulation performance. This paper addresses this gap by introducing a large-scale, physics-based benchmark that evaluates 6D pose estimators and 3D mesh models based on their functional efficacy in grasping. We analyze the impact of model fidelity by generating grasps on various reconstructed 3D meshes and executing them on the ground-truth model, simulating how grasp poses generated with an imperfect model affect interaction with the real object. This assesses the combined impact of pose error, grasp robustness, and geometric inaccuracies from 3D reconstruction. Our results show that reconstruction artifacts significantly decrease the number of grasp pose candidates but have a negligible effect on grasping performance given an accurately estimated pose. Our results also reveal that the relationship between grasp success and pose error is dominated by spatial error, and even a simple translation error provides insight into the success of the grasping pose of symmetric objects. This work provides insight into how perception systems relate to object manipulation using robots.

</details>


### [53] [Grasp Synthesis Matching From Rigid To Soft Robot Grippers Using Conditional Flow Matching](https://arxiv.org/abs/2602.17110)
*Tanisha Parulekar,Ge Shi,Josh Pinskier,David Howard,Jen Jen Chung*

Main category: cs.RO

TL;DR: 提出使用条件流匹配（CFM）将刚性夹爪的抓取姿态映射到软体Fin-ray夹爪的新框架，显著提升软体夹爪的抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 刚性夹爪抓取合成方法（如Anygrasp）无法有效适应软体夹爪的柔性特性，导致数据需求大且准确性低，需要在刚性夹爪和软体夹爪之间建立有效的姿态映射桥梁。

Method: 使用条件流匹配（CFM）生成模型学习从刚性夹爪姿态到软体Fin-ray夹爪姿态的复杂映射。通过数据收集管道生成配对的刚性-软体抓取姿态，使用U-Net自编码器从深度图像中提取物体几何特征作为CFM的条件输入。

Result: 在7-DOF机器人上验证，CFM生成的姿态相比基线刚性姿态显著提升抓取成功率：已见物体从6%提升到34%，未见物体从25%提升到46%。对圆柱体（已见50%，未见100%）和球体（已见25%，未见31%）提升尤为显著。

Conclusion: CFM是一种数据高效且有效的方法，可用于将抓取策略从刚性夹爪迁移到软体夹爪，为其他软体机器人系统提供了可扩展的方法论。

Abstract: A representation gap exists between grasp synthesis for rigid and soft grippers. Anygrasp [1] and many other grasp synthesis methods are designed for rigid parallel grippers, and adapting them to soft grippers often fails to capture their unique compliant behaviors, resulting in data-intensive and inaccurate models. To bridge this gap, this paper proposes a novel framework to map grasp poses from a rigid gripper model to a soft Fin-ray gripper. We utilize Conditional Flow Matching (CFM), a generative model, to learn this complex transformation. Our methodology includes a data collection pipeline to generate paired rigid-soft grasp poses. A U-Net autoencoder conditions the CFM model on the object's geometry from a depth image, allowing it to learn a continuous mapping from an initial Anygrasp pose to a stable Fin-ray gripper pose. We validate our approach on a 7-DOF robot, demonstrating that our CFM-generated poses achieve a higher overall success rate for seen and unseen objects (34% and 46% respectively) compared to the baseline rigid poses (6% and 25% respectively) when executed by the soft gripper. The model shows significant improvements, particularly for cylindrical (50% and 100% success for seen and unseen objects) and spherical objects (25% and 31% success for seen and unseen objects), and successfully generalizes to unseen objects. This work presents CFM as a data-efficient and effective method for transferring grasp strategies, offering a scalable methodology for other soft robotic systems.

</details>


### [54] [Physical Human-Robot Interaction for Grasping in Augmented Reality via Rigid-Soft Robot Synergy](https://arxiv.org/abs/2602.17128)
*Huishi Huang,Jack Klusmann,Haozhe Wang,Shuchen Ji,Fengkang Ying,Yiyuan Zhang,John Nassour,Gordon Cheng,Daniela Rus,Jun Liu,Marcelo H Ang,Cecilia Laschi*

Main category: cs.RO

TL;DR: 提出基于增强现实的混合刚柔机器人遥操作框架，通过AR头显实现虚拟模型与真实系统的叠加，利用参数识别确保虚实一致性，完成简单抓取任务


<details>
  <summary>Details</summary>
Motivation: 混合刚柔机器人结合了刚性机械臂的精确性和软体手臂的柔顺性，在非结构化环境中具有广阔应用前景，但协调控制面临建模、感知和跨域运动学等挑战

Method: 开发AR物理人机交互框架，通过AR头显将机器人仿真模型叠加到真实系统上；提出从真实到仿真的参数识别流程，利用软体机器人的几何特性准确建模其静态、动态行为及控制系统响应

Result: 实现了混合刚柔机器人的直接遥操作，用户可以在虚拟环境中预先执行任务，然后部署到真实系统，确保了虚拟与物理机器人行为的一致性

Conclusion: AR框架为混合刚柔机器人的协调控制提供了有效解决方案，通过虚实结合的方法克服了建模和感知挑战，为复杂环境中的机器人操作开辟了新途径

Abstract: Hybrid rigid-soft robots combine the precision of rigid manipulators with the compliance and adaptability of soft arms, offering a promising approach for versatile grasping in unstructured environments. However, coordinating hybrid robots remains challenging, due to difficulties in modeling, perception, and cross-domain kinematics. In this work, we present a novel augmented reality (AR)-based physical human-robot interaction framework that enables direct teleoperation of a hybrid rigid-soft robot for simple reaching and grasping tasks. Using an AR headset, users can interact with a simulated model of the robotic system integrated into a general-purpose physics engine, which is superimposed on the real system, allowing simulated execution prior to real-world deployment. To ensure consistent behavior between the virtual and physical robots, we introduce a real-to-simulation parameter identification pipeline that leverages the inherent geometric properties of the soft robot, enabling accurate modeling of its static and dynamic behavior as well as the control system's response.

</details>


### [55] [Geometric Inverse Flight Dynamics on SO(3) and Application to Tethered Fixed-Wing Aircraft](https://arxiv.org/abs/2602.17166)
*Antonio Franchi,Chiara Gabellieri*

Main category: cs.RO

TL;DR: 该论文提出了一种基于SO(3)的坐标无关固定翼飞机逆飞行动力学公式，通过几何定义气动力方向，推导出轨迹到输入的闭式映射，并应用于球形平行线系留飞行分析。


<details>
  <summary>Details</summary>
Motivation: 将机器人学中的几何建模方法与航空领域的逆仿真技术相结合，为固定翼飞机提供一种坐标无关的逆动力学框架，便于轨迹设计和可行性验证。

Method: 在SO(3)上建立坐标无关公式，平移力平衡在世界坐标系中表示，旋转动力学在机体坐标系中表示；几何定义气动力方向（阻力、升力、侧力）；施加协调飞行条件（无侧滑），推导出从轨迹到姿态、角速度、推力-攻角对的闭式映射。

Result: 获得了轨迹到输入的闭式映射，能够恢复气动力矩系数；应用于球形平行线系留飞行时，得到了所需滚转角的解析表达式，并识别出零滚转轨迹；在简单升力/阻力定律下，最小推力攻角具有闭式解。

Conclusion: 该框架连接了航空逆仿真与机器人几何建模，为轨迹设计和可行性检查提供了严格的构建模块；点式准稳态逆解在轨迹和旋转动力学时不变时成为稳态飞行配平解。

Abstract: We present a robotics-oriented, coordinate-free formulation of inverse flight dynamics for fixed-wing aircraft on SO(3). Translational force balance is written in the world frame and rotational dynamics in the body frame; aerodynamic directions (drag, lift, side) are defined geometrically, avoiding local attitude coordinates. Enforcing coordinated flight (no sideslip), we derive a closed-form trajectory-to-input map yielding the attitude, angular velocity, and thrust-angle-of-attack pair, and we recover the aerodynamic moment coefficients component-wise. Applying such a map to tethered flight on spherical parallels, we obtain analytic expressions for the required bank angle and identify a specific zero-bank locus where the tether tension exactly balances centrifugal effects, highlighting the decoupling between aerodynamic coordination and the apparent gravity vector. Under a simple lift/drag law, the minimal-thrust angle of attack admits a closed form. These pointwise quasi-steady inversion solutions become steady-flight trim when the trajectory and rotational dynamics are time-invariant. The framework bridges inverse simulation in aeronautics with geometric modeling in robotics, providing a rigorous building block for trajectory design and feasibility checks.

</details>


### [56] [Nonlinear Predictive Control of the Continuum and Hybrid Dynamics of a Suspended Deformable Cable for Aerial Pick and Place](https://arxiv.org/abs/2602.17199)
*Antonio Rapuano,Yaolei Shen,Federico Califano,Chiara Gabellieri,Antonio Franchi*

Main category: cs.RO

TL;DR: 提出了一种用于空中操作可伸缩电缆的框架，结合了基于偏微分方程的高保真模型和适用于实时控制的降阶表示，实现了无人机携带悬挂柔性电缆的实时动态感知控制。


<details>
  <summary>Details</summary>
Motivation: 无人机携带悬挂柔性电缆进行空中操作时，电缆的动态特性复杂，需要高保真模型来准确描述，但同时又需要实时控制能力。现有方法难以同时满足模型精度和计算效率的要求。

Method: 1. 使用偏微分方程建立电缆的高保真模型；2. 采用有限差分法进行离散化；3. 应用本征正交分解提取降阶模型，保留主要变形模式；4. 基于降阶模型设计非线性模型预测控制方案，能够稳定电缆振荡并处理负载附着/脱离等混合过渡。

Result: 仿真结果证实了降阶模型的稳定性、效率和鲁棒性，以及控制器在各种操作条件下调节电缆动态的有效性。额外仿真展示了降阶模型在受限环境中进行轨迹规划的应用，证明了方法的通用性。

Conclusion: 该框架成功实现了无人机携带悬挂柔性电缆的实时动态感知控制，结合了高精度建模和计算效率，为空中电缆操作提供了有效的解决方案。

Abstract: This paper presents a framework for aerial manipulation of an extensible cable that combines a high-fidelity model based on partial differential equations (PDEs) with a reduced-order representation suitable for real-time control. The PDEs are discretised using a finite-difference method, and proper orthogonal decomposition is employed to extract a reduced-order model (ROM) that retains the dominant deformation modes while significantly reducing computational complexity. Based on this ROM, a nonlinear model predictive control scheme is formulated, capable of stabilizing cable oscillations and handling hybrid transitions such as payload attachment and detachment. Simulation results confirm the stability, efficiency, and robustness of the ROM, as well as the effectiveness of the controller in regulating cable dynamics under a range of operating conditions. Additional simulations illustrate the application of the ROM for trajectory planning in constrained environments, demonstrating the versatility of the proposed approach. Overall, the framework enables real-time, dynamics-aware control of unmanned aerial vehicles (UAVs) carrying suspended flexible cables.

</details>


### [57] [Multi-session Localization and Mapping Exploiting Topological Information](https://arxiv.org/abs/2602.17226)
*Lorenzo Montano-Olivan,Julio A. Placed,Luis Montano,Maria T. Lazaro*

Main category: cs.RO

TL;DR: 提出了一种基于地图定位的多会话框架，通过拓扑感知的决策机制选择性地触发建图和闭环检测，提高重复访问环境中的地图一致性。


<details>
  <summary>Details</summary>
Motivation: 自主系统在重复访问相同环境时面临建图和定位的挑战，传统方法贪婪地运行完整SLAM会话并尝试在结果地图间建立对应关系存在局限性。

Method: 提出基于地图定位的多会话框架，采用拓扑感知、不确定性感知的决策机制分析位姿图结构，检测低连通性区域，选择性地触发建图和闭环模块。

Result: 在数据集重叠序列和真实矿井环境中验证了方法的有效性，能够减少累积误差并增强全局一致性。

Conclusion: 该方法通过智能决策机制选择性地进行建图和闭环检测，为重复访问环境中的自主系统提供了更有效的多会话建图解决方案。

Abstract: Operating in previously visited environments is becoming increasingly crucial for autonomous systems, with direct applications in autonomous driving, surveying, and warehouse or household robotics. This repeated exposure to observing the same areas poses significant challenges for mapping and localization -- key components for enabling any higher-level task. In this work, we propose a novel multi-session framework that builds on map-based localization, in contrast to the common practice of greedily running full SLAM sessions and trying to find correspondences between the resulting maps. Our approach incorporates a topology-informed, uncertainty-aware decision-making mechanism that analyzes the pose-graph structure to detect low-connectivity regions, selectively triggering mapping and loop closing modules. The resulting map and pose-graph are seamlessly integrated into the existing model, reducing accumulated error and enhancing global consistency. We validate our method on overlapping sequences from datasets and demonstrate its effectiveness in a real-world mine-like environment.

</details>


### [58] [FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment](https://arxiv.org/abs/2602.17259)
*Han Zhao,Jingbo Wang,Wenxuan Song,Shuai Chen,Yang Liu,Yan Wang,Haoang Li,Donglin Wang*

Main category: cs.RO

TL;DR: FRAPPE是一种用于增强VLA模型世界建模能力的两阶段微调方法，通过预测未来观测的潜在表示并并行扩展计算负载，解决了现有方法过度关注像素重建和误差累积的问题。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型的世界建模方法存在两个主要问题：1.训练目标迫使模型过度强调像素级重建，限制了语义学习和泛化能力；2.推理过程中依赖预测的未来观测导致误差累积。需要一种更有效的方法来增强机器人的世界感知能力。

Method: FRAPPE采用两阶段微调策略：1.中期训练阶段，模型学习预测未来观测的潜在表示；2.后期训练阶段，并行扩展计算负载，同时与多个不同的视觉基础模型对齐表示。这种方法显著提高了微调效率，减少了对动作标注数据的依赖。

Result: 在RoboTwin基准测试和真实世界任务中，FRAPPE优于现有最先进方法，在长时域和未见场景中表现出强大的泛化能力。

Conclusion: FRAPPE为增强通用机器人策略的世界感知能力提供了一条可扩展且数据高效的途径，通过改进微调效率和减少对动作标注数据的依赖，有效解决了当前世界建模方法的主要局限性。

Abstract: Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.

</details>


### [59] [Contact-Anchored Proprioceptive Odometry for Quadruped Robots](https://arxiv.org/abs/2602.17393)
*Minxing Sun,Yao Mao*

Main category: cs.RO

TL;DR: 提出一种仅使用IMU和电机测量的纯本体感知状态估计器，通过接触腿作为运动学锚点抑制长期漂移，适用于双足、四足和轮腿机器人


<details>
  <summary>Details</summary>
Motivation: 解决无摄像头或LiDAR的腿式机器人里程计问题，克服IMU漂移和关节速度噪声，实现可靠的本体感知状态估计

Method: 1) 将接触腿作为运动学锚点，基于关节扭矩的足部力估计选择可靠接触点；2) 轻量级高度聚类和时间衰减校正防止高度漂移；3) 逆运动学容积卡尔曼滤波器改善足端速度观测；4) 多接触几何一致性缓解偏航漂移

Result: 在四种四足平台上测试：Astrall点足机器人A完成200米水平环路误差0.1638米，15米垂直环路误差0.219米；轮腿机器人B相应误差为0.2264米和0.199米；轮腿机器人C完成700米水平环路误差7.68米，20米垂直环路误差0.540米；Unitree Go2 EDU完成120米水平环路误差2.2138米，8米垂直环路垂直误差小于0.1米

Conclusion: 该方法仅使用本体传感器即可实现可靠的位姿和速度估计，适用于多种腿式机器人平台，在水平和垂直方向均表现出良好的精度

Abstract: Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents a purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with a unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as a kinematic anchor: joint-torque--based foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce a lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to a kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and a Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot~A, a $\sim$200\,m horizontal loop and a $\sim$15\,m vertical loop return with 0.1638\,m and 0.219\,m error, respectively; on wheel-legged robot~B, the corresponding errors are 0.2264\,m and 0.199\,m. On wheel-legged robot~C, a $\sim$700\,m horizontal loop yields 7.68\,m error and a $\sim$20\,m vertical loop yields 0.540\,m error. Unitree Go2 EDU closes a $\sim$120\,m horizontal loop with 2.2138\,m error and a $\sim$8\,m vertical loop with less than 0.1\,m vertical error. github.com/ShineMinxing/Ros2Go2Estimator.git

</details>


### [60] [Distributed Virtual Model Control for Scalable Human-Robot Collaboration in Shared Workspace](https://arxiv.org/abs/2602.17415)
*Yi Zhang,Omar Faris,Chapa Sirithunge,Kai-Fung Chu,Fumiya Iida,Fulvio Forni*

Main category: cs.RO

TL;DR: 提出基于虚拟模型控制的去中心化、智能体无关的安全人机协作框架，通过虚拟弹簧阻尼器实现运动控制，采用分布式死锁检测与协商机制，实现零死锁率和高安全性。


<details>
  <summary>Details</summary>
Motivation: 传统人机协作方法通常需要显式轨迹规划，容易导致死锁和安全性问题。需要一种更自然、安全且可扩展的协作框架，使人类和机器人能在共享工作空间中安全互动。

Method: 基于虚拟模型控制（VMC），将人类和机器人嵌入同一虚拟组件形状的工作空间，通过虚拟弹簧和阻尼器实现运动控制。采用去中心化的基于力的停滞检测器识别死锁，并通过协商机制解决。框架具有分布式实现，无需结构变化即可扩展。

Result: 在实验中，将机器人卡在块放置任务中的概率从最高61.2%降至零。框架支持最多2个机器人和2个人类的安全协作（实验），以及最多4个机器人的模拟，保持约20厘米的智能体间距离。通过调整控制参数可直观塑造机器人行为，在所有测试场景中实现无死锁操作。

Conclusion: 该去中心化、智能体无关的安全控制框架有效解决了人机协作中的死锁问题，实现了安全、直观且可扩展的协作，为复杂多智能体协作系统提供了实用解决方案。

Abstract: We present a decentralized, agent agnostic, and safety-aware control framework for human-robot collaboration based on Virtual Model Control (VMC). In our approach, both humans and robots are embedded in the same virtual-component-shaped workspace, where motion is the result of the interaction with virtual springs and dampers rather than explicit trajectory planning. A decentralized, force-based stall detector identifies deadlocks, which are resolved through negotiation. This reduces the probability of robots getting stuck in the block placement task from up to 61.2% to zero in our experiments. The framework scales without structural changes thanks to the distributed implementation: in experiments we demonstrate safe collaboration with up to two robots and two humans, and in simulation up to four robots, maintaining inter-agent separation at around 20 cm. Results show that the method shapes robot behavior intuitively by adjusting control parameters and achieves deadlock-free operation across team sizes in all tested scenarios.

</details>


### [61] [3D-printed Soft Optical sensor with a Lens (SOLen) for light guidance in mechanosensing](https://arxiv.org/abs/2602.17421)
*Diana Cafiso,Petr Trunin,Carolina Gay,Lucia Beccai*

Main category: cs.RO

TL;DR: 提出了一种名为SOLen的3D打印软光学传感方法，通过打印透镜在Y形波导中实现变形诱导的透镜旋转和焦点平移，从而产生差分输出编码运动方向和幅度。


<details>
  <summary>Details</summary>
Motivation: 增材制造使软机器人几何形状日益复杂，需要与单材料、一步制造兼容的传感解决方案。现有光学软传感器性能常受环境耦合、泄漏、散射等不受控光传播影响，而常见缓解策略通常需要多材料界面。

Method: 在Y形波导前放置打印透镜，利用变形诱导的透镜旋转和焦点平移重新分配两个分支间的光功率。使用改性丙烯酸酯聚氨酯树脂提高柔顺性和光学透射率，通过单层光学表征获得波长相关折射率和透射率，基于测量折射率设计透镜轮廓并实现亚毫米精度打印。

Result: 旋转测试展示了多个周期内可重复的分支选择性信号切换。建立了从材料到光学的可转移工作流程，为软光学传感器提供了新的功能。

Conclusion: SOLen方法为下一代软机器人提供了具有新功能的软光学传感器，实现了可转移的材料-光学工作流程，解决了单材料、一步制造兼容的光学传感需求。

Abstract: Additive manufacturing is enabling soft robots with increasingly complex geometries, creating a demand for sensing solutions that remain compatible with single-material, one-step fabrication. Optical soft sensors are attractive for monolithic printing, but their performance is often degraded by uncontrolled light propagation (ambient coupling, leakage, scattering), while common miti- gation strategies typically require multimaterial interfaces. Here, we present an approach for 3D printed soft optical sensing (SOLen), in which a printed lens is placed in front of an emitter within a Y-shaped waveguide. The sensing mechanism relies on deformation-induced lens rotation and focal-spot translation, redistributing optical power between the two branches to generate a differential output that encodes both motion direction and amplitude. An acrylate polyurethane resin was modified with lauryl acrylate to improve compliance and optical transmittance, and single-layer optical characterization was used to derive wavelength-dependent refractive index and transmittance while minimizing DLP layer-related artifacts. The measured refractive index was used in simulations to design a lens profile for a target focal distance, which was then printed with sub-millimeter fidelity. Rotational tests demonstrated reproducible branch-selective signal switching over multiple cycles. These results establish a transferable material-to-optics workflow for soft optical sensors with lens with new functionalities for next-generation soft robots

</details>


### [62] [A Cost-Effective and Climate-Resilient Air Pressure System for Rain Effect Reduction on Automated Vehicle Cameras](https://arxiv.org/abs/2602.17472)
*Mohamed Sabry,Joseba Gorospe,Cristina Olaverri-Monreal*

Main category: cs.RO

TL;DR: 本文提出了一种低成本硬件解决方案，用于提高自动驾驶车辆在雨天条件下的感知性能，通过兼容多摄像头系统提升行人检测准确率从8.3%到41.6%。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶车辆在恶劣天气条件下的感知研究主要关注算法改进，而硬件解决方案研究有限。现有方法如亲水/疏水镜头和喷雾只能部分缓解问题，工业保护系统成本高且难以规模化部署于汽车应用。

Method: 提出了一种成本效益高的硬件解决方案，专门针对雨天条件设计，能够同时兼容多个摄像头，与现有摄像头感知平台兼容，无需额外高成本传感器或硬件更换。

Result: 该系统能够显著提升深度学习模型的行人检测准确率，从8.3%提高到41.6%，同时支持可持续交通目标，减少资源消耗，支持模块化升级。

Conclusion: 该硬件解决方案为自动驾驶车辆在恶劣天气条件下的可靠运行提供了经济有效的途径，特别适用于车辆编队等感知关键应用，有助于降低系统故障导致的效率损失和排放增加。

Abstract: Recent advances in automated vehicles have focused on improving perception performance under adverse weather conditions; however, research on physical hardware solutions remains limited, despite their importance for perception critical applications such as vehicle platooning. Existing approaches, such as hydrophilic or hydrophobic lenses and sprays, provide only partial mitigation, while industrial protection systems imply high cost and they do not enable scalability for automotive deployment.
  To address these limitations, this paper presents a cost-effective hardware solution for rainy conditions, designed to be compatible with multiple cameras simultaneously.
  Beyond its technical contribution, the proposed solution supports sustainability goals in transportation systems. By enabling compatibility with existing camera-based sensing platforms, the system extends the operational reliability of automated vehicles without requiring additional high-cost sensors or hardware replacements. This approach reduces resource consumption, supports modular upgrades, and promotes more cost-efficient deployment of automated vehicle technologies, particularly in challenging weather conditions where system failures would otherwise lead to inefficiencies and increased emissions. The proposed system was able to increase pedestrian detection accuracy of a Deep Learning model from 8.3% to 41.6%.

</details>


### [63] [Optically Sensorized Electro-Ribbon Actuator (OS-ERA)](https://arxiv.org/abs/2602.17474)
*Carolina Gay,Petr Trunin,Diana Cafiso,Yuejun Xu,Majid Taghavi,Lucia Beccai*

Main category: cs.RO

TL;DR: OS-ERA：一种光学传感的Electro-Ribbon执行器，通过嵌入软光学波导传感器实现高精度弯曲状态分类，解决了传统电容传感精度不足的问题，为闭环控制奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统Electro-Ribbon执行器（ERAs）虽然具有超高的位移和快速运动能力，但其嵌入式传感依赖于精度有限的电容传感器，这阻碍了精确控制。需要一种不影响执行性能的可靠传感解决方案。

Method: 设计并集成了两个软光学波导传感器来分析ERA运动时的复杂曲率。训练分类器将传感信号映射到八个弯曲状态。在六个保留试验中验证模型，并与训练运行中学习到的信号轨迹进行比较。

Result: 传感输出信号遵循训练流形，预测序列反映真实性能并确认可重复性。即使在执行速度存在故意训练-测试不匹配的情况下，信号轨迹保持其形状，分类保持准确，展示了实用的电压和速度不变性。

Conclusion: OS-ERA能够以高保真度分类弯曲状态，快速且可重复，解决了ERA长期存在的瓶颈问题，为实现闭环控制迈出了重要一步。

Abstract: Electro-Ribbon Actuators (ERAs) are lightweight flexural actuators that exhibit ultrahigh displacement and fast movement. However, their embedded sensing relies on capacitive sensors with limited precision, which hinders accurate control. We introduce OS-ERA, an optically sensorized ERA that yields reliable proprioceptive information, and we focus on the design and integration of a sensing solution without affecting actuation. To analyse the complex curvature of an ERA in motion, we design and embed two soft optical waveguide sensors. A classifier is trained to map the sensing signals in order to distinguish eight bending states. We validate our model on six held-out trials and compare it against signals' trajectories learned from training runs. Across all tests, the sensing output signals follow the training manifold, and the predicted sequence mirrors real performance and confirms repeatability. Despite deliberate train-test mismatches in actuation speed, the signal trajectories preserve their shape, and classification remains consistently accurate, demonstrating practical voltage- and speed-invariance. As a result, OS-ERA classifies bending states with high fidelity; it is fast and repeatable, solving a longstanding bottleneck of the ERA, enabling steps toward closed-loop control.

</details>


### [64] [Proximal powered knee placement: a case study](https://arxiv.org/abs/2602.17502)
*Kyle R. Embry,Lorenzo Vianello,Jim Lipsey,Frank Ursetta,Michael Stephens,Zhi Wang,Ann M. Simon,Andrea J. Ikeda,Suzanne B. Finucane,Shawana Anarwala,Levi J. Hargrove*

Main category: cs.RO

TL;DR: 探索性研究评估了动力假肢膝关节的膝上安装方案，相比膝下安装，膝上配置在行走速度和步频方面有所改善，证明了通过优化质量分布而非简单减重来保留动力辅助优势的可行性。


<details>
  <summary>Details</summary>
Motivation: 下肢截肢影响全球数百万人，导致行动能力受损、行走速度降低和日常社交活动受限。动力假肢膝关节可以通过主动辅助膝关节扭矩部分恢复行动能力，但动力组件增加的质量可能会抵消这些益处，对步态力学产生负面影响并增加代谢成本。

Method: 在小规模队列中评估动力假肢膝关节膝上安装方案的可行性，与膝下安装进行比较。测试包括平地行走、坡道和楼梯等多种运动任务，评估行走速度、步频、步态对称性、膝关节活动范围和峰值速度等指标。

Result: 相比膝下安装，膝上配置显示出改善的行走速度（一名参与者+9.2%）和步频（+3.6%），步态对称性效果不一。运动学测量显示两种配置的膝关节活动范围和峰值速度相似。在坡道和楼梯上的额外测试证实了控制策略在多种运动任务中的鲁棒性。

Conclusion: 初步结果表明膝上安装在功能上是可行的，通过仔细的质量分布可以在保留动力辅助益处的同时减轻额外重量的负面影响。需要进一步研究来确认这些趋势并为设计和临床建议提供指导。

Abstract: Lower limb amputation affects millions worldwide, leading to impaired mobility, reduced walking speed, and limited participation in daily and social activities. Powered prosthetic knees can partially restore mobility by actively assisting knee joint torque, improving gait symmetry, sit-to-stand transitions, and walking speed. However, added mass from powered components may diminish these benefits, negatively affecting gait mechanics and increasing metabolic cost. Consequently, optimizing mass distribution, rather than simply minimizing total mass, may provide a more effective and practical solution. In this exploratory study, we evaluated the feasibility of above-knee powertrain placement for a powered prosthetic knee in a small cohort. Compared to below-knee placement, the above-knee configuration demonstrated improved walking speed (+9.2% for one participant) and cadence (+3.6%), with mixed effects on gait symmetry. Kinematic measures indicated similar knee range of motion and peak velocity across configurations. Additional testing on ramps and stairs confirmed the robustness of the control strategy across multiple locomotion tasks. These preliminary findings suggest that above-knee placement is functionally feasible and that careful mass distribution can preserve the benefits of powered assistance while mitigating adverse effects of added weight. Further studies are needed to confirm these trends and guide design and clinical recommendations.

</details>


### [65] [IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control](https://arxiv.org/abs/2602.17537)
*Qilong Cheng,Matthew Mackay,Ali Bereyhi*

Main category: cs.RO

TL;DR: IRIS是一个低成本、基于学习的6自由度机器人相机系统，通过3D打印硬件和视觉运动模仿学习框架实现自主电影运动控制


<details>
  <summary>Details</summary>
Motivation: 工业级机器人相机系统成本高、操作复杂，限制了其广泛应用。需要开发低成本、易操作且能自主执行电影级运动控制的系统

Method: 采用3D打印硬件设计，结合基于Action Chunking with Transformers (ACT)的目标条件视觉运动模仿学习框架，直接从人类演示中学习物体感知和平滑的相机轨迹

Result: 系统成本低于1000美元，负载1.5公斤，重复精度约1毫米。实验显示能准确跟踪轨迹、可靠自主执行，并能泛化到多种电影运动

Conclusion: IRIS系统通过低成本硬件和基于学习的控制方法，实现了高质量的电影运动控制，降低了机器人相机系统的应用门槛

Abstract: Robotic camera systems enable dynamic, repeatable motion beyond human capabilities, yet their adoption remains limited by the high cost and operational complexity of industrial-grade platforms. We present the Intelligent Robotic Imaging System (IRIS), a task-specific 6-DOF manipulator designed for autonomous, learning-driven cinematic motion control. IRIS integrates a lightweight, fully 3D-printed hardware design with a goal-conditioned visuomotor imitation learning framework based on Action Chunking with Transformers (ACT). The system learns object-aware and perceptually smooth camera trajectories directly from human demonstrations, eliminating the need for explicit geometric programming. The complete platform costs under $1,000 USD, supports a 1.5 kg payload, and achieves approximately 1 mm repeatability. Real-world experiments demonstrate accurate trajectory tracking, reliable autonomous execution, and generalization across diverse cinematic motions.

</details>


### [66] [FR-GESTURE: An RGBD Dataset For Gesture-based Human-Robot Interaction In First Responder Operations](https://arxiv.org/abs/2602.17573)
*Konstantinos Foteinos,Georgios Angelidis,Aggelos Psiris,Vasileios Argyriou,Panagiotis Sarigiannidis,Georgios Th. Papadopoulos*

Main category: cs.RO

TL;DR: 该研究提出了首个专门为急救人员设计的基于手势控制无人地面车辆的数据集FR-GESTURE，包含12种手势命令、3312个RGBD数据对，并提供了基线实验和评估协议。


<details>
  <summary>Details</summary>
Motivation: 灾难强度和频率不断增加，使得急救人员的工作更加困难。人工智能和机器人技术可以协助他们的操作，弥补这些困难。目前缺乏专门为急救人员手势控制无人地面车辆设计的数据集。

Method: 1. 设计了12种手势命令，灵感来源于现有急救人员手势和战术手语，并经过经验丰富的急救人员反馈改进；2. 从2个视角和7个距离收集了3312个RGBD数据对；3. 定义了FR-GESTURE数据集的评估协议；4. 进行了基线实验。

Result: 创建了首个专门为急救人员手势控制UGV设计的数据集FR-GESTURE，包含12种手势命令、3312个RGBD数据对（从2个视角和7个距离采集），数据已公开可用，并提供了基线实验结果供后续改进。

Conclusion: 该研究填补了急救人员手势控制无人地面车辆领域的数据集空白，提出的FR-GESTURE数据集和评估协议为未来研究提供了基础，有望促进人工智能和机器人技术在灾难救援中的应用。

Abstract: The ever increasing intensity and number of disasters make even more difficult the work of First Responders (FRs). Artificial intelligence and robotics solutions could facilitate their operations, compensating these difficulties. To this end, we propose a dataset for gesture-based UGV control by FRs, introducing a set of 12 commands, drawing inspiration from existing gestures used by FRs and tactical hand signals and refined after incorporating feedback from experienced FRs. Then we proceed with the data collection itself, resulting in 3312 RGBD pairs captured from 2 viewpoints and 7 distances. To the best of our knowledge, this is the first dataset especially intended for gesture-based UGV guidance by FRs. Finally we define evaluation protocols for our RGBD dataset, termed FR-GESTURE, and we perform baseline experiments, which are put forward for improvement. We have made data publicly available to promote future research on the domain: https://doi.org/10.5281/zenodo.18131333.

</details>


### [67] [Hybrid System Planning using a Mixed-Integer ADMM Heuristic and Hybrid Zonotopes](https://arxiv.org/abs/2602.17574)
*Joshua A. Robbins,Andrew F. Thompson,Jonah J. Glunt,Herschel C. Pangborn*

Main category: cs.RO

TL;DR: 提出一种基于混合zonotopes和ADMM启发式算法的混合系统运动规划框架，用于嵌入式硬件上的自动驾驶行为与运动规划。


<details>
  <summary>Details</summary>
Motivation: 嵌入式优化规划中混合整数规划计算量大且对数值公式敏感，需要更高效的方法来处理混合系统的运动规划问题。

Method: 采用混合zonotopes集合表示法，结合新的ADMM混合整数规划启发式算法，处理分段仿射系统可达性分析并构建最优规划问题。

Result: 提出的方法相比现有技术产生更低内存复杂度和更紧凸松弛的集合，ADMM启发式算法利用混合zonotope结构，收敛速度优于现有混合整数规划启发式算法。

Conclusion: 该框架成功应用于嵌入式硬件上的自动驾驶行为与运动规划场景，证明了其在混合系统规划中的有效性和实用性。

Abstract: Embedded optimization-based planning for hybrid systems is challenging due to the use of mixed-integer programming, which is computationally intensive and often sensitive to the specific numerical formulation. To address that challenge, this article proposes a framework for motion planning of hybrid systems that pairs hybrid zonotopes - an advanced set representation - with a new alternating direction method of multipliers (ADMM) mixed-integer programming heuristic. A general treatment of piecewise affine (PWA) system reachability analysis using hybrid zonotopes is presented and extended to formulate optimal planning problems. Sets produced using the proposed identities have lower memory complexity and tighter convex relaxations than equivalent sets produced from preexisting techniques. The proposed ADMM heuristic makes efficient use of the hybrid zonotope structure. For planning problems formulated as hybrid zonotopes, the proposed heuristic achieves improved convergence rates as compared to state-of-the-art mixed-integer programming heuristics. The proposed methods for hybrid system planning on embedded hardware are experimentally applied in a combined behavior and motion planning scenario for autonomous driving.

</details>


### [68] [Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space](https://arxiv.org/abs/2602.17586)
*Antonio Guillen-Perez*

Main category: cs.RO

TL;DR: Deep-Flow是一个无监督的安全关键异常检测框架，使用最优传输条件流匹配来建模专家人类驾驶行为的概率密度，通过PCA瓶颈约束生成过程到低秩谱流形，实现运动学平滑和稳定似然估计。


<details>
  <summary>Details</summary>
Motivation: L4级自动驾驶的安全验证目前受限于无法使用传统基于规则的启发式方法扩展检测罕见、高风险的长尾场景，需要更有效的安全关键异常检测方法。

Method: 采用最优传输条件流匹配(OT-CFM)建模驾驶行为概率密度；通过PCA瓶颈将生成过程约束到低秩谱流形；使用带车道感知目标条件的早期融合Transformer编码器解决多模态歧义；引入运动学复杂性加权方案优先处理高能量机动。

Result: 在Waymo开放运动数据集上达到0.766的AUC-ROC；揭示了运动学危险与语义违规之间的根本区别；识别出传统安全过滤器忽略的分布外行为，如车道边界违规和非规范交叉口机动。

Conclusion: 该工作为定义统计安全门提供了数学严谨的基础，使自动驾驶车队的安全部署能够进行客观、数据驱动的验证，解决了传统方法无法检测的预测性差距问题。

Abstract: Safety validation for Level 4 autonomous vehicles (AVs) is currently bottlenecked by the inability to scale the detection of rare, high-risk long-tail scenarios using traditional rule-based heuristics. We present Deep-Flow, an unsupervised framework for safety-critical anomaly detection that utilizes Optimal Transport Conditional Flow Matching (OT-CFM) to characterize the continuous probability density of expert human driving behavior. Unlike standard generative approaches that operate in unstable, high-dimensional coordinate spaces, Deep-Flow constrains the generative process to a low-rank spectral manifold via a Principal Component Analysis (PCA) bottleneck. This ensures kinematic smoothness by design and enables the computation of the exact Jacobian trace for numerically stable, deterministic log-likelihood estimation. To resolve multi-modal ambiguity at complex junctions, we utilize an Early Fusion Transformer encoder with lane-aware goal conditioning, featuring a direct skip-connection to the flow head to maintain intent-integrity throughout the network. We introduce a kinematic complexity weighting scheme that prioritizes high-energy maneuvers (quantified via path tortuosity and jerk) during the simulation-free training process. Evaluated on the Waymo Open Motion Dataset (WOMD), our framework achieves an AUC-ROC of 0.766 against a heuristic golden set of safety-critical events. More significantly, our analysis reveals a fundamental distinction between kinematic danger and semantic non-compliance. Deep-Flow identifies a critical predictability gap by surfacing out-of-distribution behaviors, such as lane-boundary violations and non-normative junction maneuvers, that traditional safety filters overlook. This work provides a mathematically rigorous foundation for defining statistical safety gates, enabling objective, data-driven validation for the safe deployment of autonomous fleets.

</details>
