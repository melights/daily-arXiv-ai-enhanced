<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals](https://arxiv.org/abs/2511.10615)
*Shruti Singh Baghel,Yash Pratap Singh Rathore,Sushovan Jena,Anurag Pradhan,Amit Shukla,Arnav Bhavsar,Pawan Goyal*

Main category: cs.CV

TL;DR: 本文研究了不同参数规模的SmolVLM2模型（500M和2.2B）在盲人和低视力用户视频描述任务中的性能表现，提出了两个专门针对BLV可访问性评估的新框架，并评估了不同提示策略和移动设备部署方案。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型虽然能够生成高质量视频描述，但其高内存、计算和部署需求限制了实际应用，特别是对于依赖详细、上下文感知描述的盲人和低视力用户群体。

Method: 使用500M和2.2B参数的SmolVLM2变体在两个多样化数据集（AVCaps户外和Charades室内）上进行评估，引入了多上下文BLV框架和导航辅助框架，系统评估了四种不同提示设计策略，并在智能手机上部署FP32和INT8精度变体。

Result: 评估了模型在空间定向、社交互动、动作事件和环境氛围等上下文理解方面的表现，以及移动设备上资源受限环境下的实际性能约束。

Conclusion: 通过专门设计的评估框架和移动部署测试，为开发适合BLV用户需求的轻量级视觉语言模型提供了重要参考。

Abstract: Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.

</details>


### [2] [One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models](https://arxiv.org/abs/2511.10629)
*Aleksandr Razin,Danil Kazantsev,Ilya Makarov*

Main category: cs.CV

TL;DR: 提出了Latent Upscaler Adapter (LUA)，一种轻量级模块，在VAE解码前直接在生成器的潜在代码上执行超分辨率，避免了传统图像超分辨率的延迟和伪影问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型难以扩展到超出训练分辨率，直接高分辨率采样成本高，而传统图像超分辨率方法在解码后操作会引入伪影和额外延迟。

Method: LUA作为即插即用组件集成，无需修改基础模型或添加额外扩散阶段，使用共享Swin风格主干和尺度特定像素洗牌头支持2倍和4倍放大，在潜在空间通过单次前向传递实现高分辨率合成。

Result: LUA在感知质量上与像素空间SR相当，但解码和上采样时间降低近3倍（1024px生成仅增加0.42秒，而相同SwinIR架构的像素空间SR需要1.87秒），且在不同VAE的潜在空间上表现出强泛化能力。

Conclusion: LUA在保持原生高分辨率生成保真度的同时，为现代扩散管道提供了实用且高效的可扩展高保真图像合成路径。

Abstract: Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.

</details>


### [3] [Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling](https://arxiv.org/abs/2511.10648)
*Jiahao Wang,Weiye Xu,Aijun Yang,Wengang Zhou,Lewei Lu,Houqiang Li,Xiaohua Wang,Jinguo Zhu*

Main category: cs.CV

TL;DR: 本文提出自一致性采样（SCS）方法，解决多模态大语言模型在强化学习中因猜测正确选项而获得相同奖励的问题，通过视觉扰动和轨迹重采样获得一致性分数，在多个基准测试中显著提升准确性。


<details>
  <summary>Details</summary>
Motivation: 在多模态推理基准测试的多选题设置中，强化学习面临一个被忽视的障碍：即使推理链错误但猜测正确选项的轨迹与真正正确推理的轨迹获得相同奖励，这影响了学习效果。

Method: 提出自一致性采样（SCS）方法：对每个问题引入小的视觉扰动，并对初始轨迹进行重复截断和重采样，通过结果轨迹的一致性获得可微的一致性分数，在策略更新时降低不可靠轨迹的权重。

Result: 基于Qwen2.5-VL-7B-Instruct模型，将SCS集成到RLOO、GRPO和REINFORCE++系列中，在六个多模态基准测试上准确率提升高达7.7个百分点，计算开销可忽略。在Qwen2.5-VL-3B-Instruct和InternVL3-8B模型上也获得显著提升。

Conclusion: SCS为多模态大语言模型中的结果奖励强化学习提供了一个简单通用的解决方案，有效纠正了因猜测正确而获得不当奖励的问题。

Abstract: Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [4] [Robot Crash Course: Learning Soft and Stylized Falling](https://arxiv.org/abs/2511.10635)
*Pascal Strauch,David Müller,Sammy Christen,Agon Serifi,Ruben Grandia,Espen Knoop,Moritz Bächer*

Main category: cs.RO

TL;DR: 本文提出了一种机器人无关的奖励函数，通过强化学习实现双足机器人的受控软着陆，平衡期望最终姿态与冲击最小化，并保护关键机器人部件。


<details>
  <summary>Details</summary>
Motivation: 尽管双足机器人在鲁棒运动方面取得进展，但在现实世界中仍有摔倒风险。大多数研究关注防止摔倒，而本文专注于摔倒现象本身，旨在减少物理损伤并让用户控制机器人的最终姿态。

Method: 提出机器人无关的奖励函数，在强化学习中平衡期望最终姿态达成、冲击最小化和关键部件保护；引入基于仿真的初始和最终姿态采样策略，使策略对广泛的初始摔倒条件具有鲁棒性，并能在推理时指定任意未见过的最终姿态。

Result: 通过仿真和真实世界实验证明，即使是双足机器人也能执行受控的软着陆。

Conclusion: 本文展示了双足机器人能够实现受控软着陆，通过平衡姿态控制和冲击保护来减少摔倒时的物理损伤。

Abstract: Despite recent advances in robust locomotion, bipedal robots operating in the real world remain at risk of falling. While most research focuses on preventing such events, we instead concentrate on the phenomenon of falling itself. Specifically, we aim to reduce physical damage to the robot while providing users with control over a robot's end pose. To this end, we propose a robot agnostic reward function that balances the achievement of a desired end pose with impact minimization and the protection of critical robot parts during reinforcement learning. To make the policy robust to a broad range of initial falling conditions and to enable the specification of an arbitrary and unseen end pose at inference time, we introduce a simulation-based sampling strategy of initial and end poses. Through simulated and real-world experiments, our work demonstrates that even bipedal robots can perform controlled, soft falls.

</details>
