<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 47]
- [cs.RO](#cs.RO) [Total: 13]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model](https://arxiv.org/abs/2601.00051)
*Yabo Chen,Yuanzhi Liang,Jiepeng Wang,Tingxi Chen,Junfei Cheng,Zixiao Gu,Yuyang Huang,Zicheng Jiang,Wei Li,Tian Li,Weichen Li,Zuoxin Li,Guangce Liu,Jialun Liu,Junqi Liu,Haoyuan Wang,Qizhen Weng,Xuan'er Wu,Xunzhi Xiang,Xiaoyan Yang,Xin Zhang,Shiwen Zhang,Junyu Zhou,Chengcheng Zhou,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleWorld是一个实时多模态4D世界建模框架，通过生成-重建-引导范式统一视频生成、动态场景重建和长期世界记忆，实现空间、时间和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在实时交互、长时程一致性和动态场景持久记忆方面存在局限，阻碍了其发展为实用的世界模型。需要一种能够统一视频生成、动态场景重建和长期记忆的系统。

Method: 提出生成-重建-引导范式：生成的视频流被连续重建为动态4D时空表示，该表示反过来指导后续生成以保持一致性。采用自回归扩散视频模型，增强Macro-from-Micro Planning（MMPL）层次规划方法减少误差累积，结合高效的Distribution Matching Distillation（DMD）实现实时合成。

Result: TeleWorld在静态和动态世界理解、长期一致性和实时生成效率方面表现出色，实现了动态对象建模和静态场景表示在统一4D框架中的无缝集成。

Conclusion: TeleWorld是迈向实用、交互式和计算可访问的世界模型的重要一步，为多模态生成和具身智能提供了具有记忆功能的交互式世界模型解决方案。

Abstract: World models aim to endow AI systems with the ability to represent, generate, and interact with dynamic environments in a coherent and temporally consistent manner. While recent video generation models have demonstrated impressive visual quality, they remain limited in real-time interaction, long-horizon consistency, and persistent memory of dynamic scenes, hindering their evolution into practical world models. In this report, we present TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term world memory within a closed-loop system. TeleWorld introduces a novel generation-reconstruction-guidance paradigm, where generated video streams are continuously reconstructed into a dynamic 4D spatio-temporal representation, which in turn guides subsequent generation to maintain spatial, temporal, and physical consistency. To support long-horizon generation with low latency, we employ an autoregressive diffusion-based video model enhanced with Macro-from-Micro Planning (MMPL)--a hierarchical planning method that reduces error accumulation from frame-level to segment-level-alongside efficient Distribution Matching Distillation (DMD), enabling real-time synthesis under practical computational budgets. Our approach achieves seamless integration of dynamic object modeling and static scene representation within a unified 4D framework, advancing world models toward practical, interactive, and computationally accessible systems. Extensive experiments demonstrate that TeleWorld achieves strong performance in both static and dynamic world understanding, long-term consistency, and real-time generation efficiency, positioning it as a practical step toward interactive, memory-enabled world models for multimodal generation and embodied intelligence.

</details>


### [2] [It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models](https://arxiv.org/abs/2601.00090)
*Anne Harrington,A. Sophia Koepke,Shyamgopal Karthik,Trevor Darrell,Alexei A. Efros*

Main category: cs.CV

TL;DR: 本文提出通过噪声优化解决文本到图像模型的模式崩溃问题，提升生成多样性


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型存在明显的模式崩溃问题，即相同文本提示下生成的图像缺乏多样性。先前工作主要通过引导机制或候选池筛选来解决，本文探索不同的噪声优化方向。

Method: 采用简单的噪声优化目标来缓解模式崩溃，同时保持基础模型的保真度。分析噪声的频率特性，探索不同频率特性的替代噪声初始化策略以改进优化和搜索。

Result: 实验表明，噪声优化方法在生成质量和多样性方面均取得优越结果，能有效缓解模式崩溃问题。

Conclusion: 噪声优化是解决文本到图像模型模式崩溃的有效方法，通过优化噪声初始化策略可以同时提升生成多样性和质量。

Abstract: Contemporary text-to-image models exhibit a surprising degree of mode collapse, as can be seen when sampling several images given the same text prompt. While previous work has attempted to address this issue by steering the model using guidance mechanisms, or by generating a large pool of candidates and refining them, in this work we take a different direction and aim for diversity in generations via noise optimization. Specifically, we show that a simple noise optimization objective can mitigate mode collapse while preserving the fidelity of the base model. We also analyze the frequency characteristics of the noise and show that alternative noise initializations with different frequency profiles can improve both optimization and search. Our experiments demonstrate that noise optimization yields superior results in terms of generation quality and variety.

</details>


### [3] [Spatial4D-Bench: A Versatile 4D Spatial Intelligence Benchmark](https://arxiv.org/abs/2601.00092)
*Pan Wang,Yang Liu,Guile Wu,Eduardo R. Corral-Soto,Chengjie Huang,Binbin Xu,Dongfeng Bai,Xu Yan,Yuan Ren,Xingxin Chen,Yizhe Wu,Tao Huang,Wenjun Wan,Xin Wu,Pei Zhou,Xuyang Dai,Kangbo Lv,Hongbo Zhang,Yosef Fried,Aixue Ye,Bailan Feng,Zhenyu Chen,Zhen Li,Yingcong Chen,Yiyi Liao,Bingbing Liu*

Main category: cs.CV

TL;DR: 本文提出了Spatial4D-Bench，一个包含约40,000个问答对、覆盖18个任务的4D空间智能基准测试，用于评估多模态大语言模型在4D空间推理方面的能力，发现现有模型在多种4D空间推理任务上存在显著局限性。


<details>
  <summary>Details</summary>
Motivation: 人类天生具备4D空间智能（感知物体随时间变化的能力），但多模态大语言模型是否能够达到人类水平的4D空间智能尚不明确。现有空间智能基准测试通常规模较小或多样性有限，无法全面评估模型的4D空间推理能力。

Method: 提出了Spatial4D-Bench基准测试，包含约40,000个问答对，覆盖18个明确定义的任务，这些任务被系统性地组织为六个认知类别：物体理解、场景理解、空间关系理解、时空关系理解、空间推理和时空推理。

Result: 对多种开源和专有MLLM进行了基准测试，发现它们在多种4D空间推理方面存在显著局限性，如路径规划、动作识别和物理合理性推理等任务。

Conclusion: Spatial4D-Bench为评估MLLM的空间认知能力提供了结构化、全面的基准测试，揭示了现有模型的局限性，希望该基准能够促进开发具备人类水平4D空间智能的MLLM。

Abstract: 4D spatial intelligence involves perceiving and processing how objects move or change over time. Humans naturally possess 4D spatial intelligence, supporting a broad spectrum of spatial reasoning abilities. To what extent can Multimodal Large Language Models (MLLMs) achieve human-level 4D spatial intelligence? In this work, we present Spatial4D-Bench, a versatile 4D spatial intelligence benchmark designed to comprehensively assess the 4D spatial reasoning abilities of MLLMs. Unlike existing spatial intelligence benchmarks that are often small-scale or limited in diversity, Spatial4D-Bench provides a large-scale, multi-task evaluation benchmark consisting of ~40,000 question-answer pairs covering 18 well-defined tasks. We systematically organize these tasks into six cognitive categories: object understanding, scene understanding, spatial relationship understanding, spatiotemporal relationship understanding, spatial reasoning and spatiotemporal reasoning. Spatial4D-Bench thereby offers a structured and comprehensive benchmark for evaluating the spatial cognition abilities of MLLMs, covering a broad spectrum of tasks that parallel the versatility of human spatial intelligence. We benchmark various state-of-the-art open-source and proprietary MLLMs on Spatial4D-Bench and reveal their substantial limitations in a wide variety of 4D spatial reasoning aspects, such as route plan, action recognition, and physical plausibility reasoning. We hope that the findings provided in this work offer valuable insights to the community and that our benchmark can facilitate the development of more capable MLLMs toward human-level 4D spatial intelligence. More resources can be found on our project page.

</details>


### [4] [Compressed Map Priors for 3D Perception](https://arxiv.org/abs/2601.00139)
*Brady Zhou,Philipp Krähenbühl*

Main category: cs.CV

TL;DR: CMP框架通过压缩历史遍历数据学习空间先验，显著提升3D目标检测性能，存储需求仅为32KB/km²


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶视觉系统通常将每个位置视为首次访问，忽略了历史遍历数据中蕴含的宝贵空间先验信息，导致资源利用效率低下

Method: 提出压缩地图先验(CMP)框架，使用二值化哈希图从历史遍历数据中学习空间先验，存储密度仅为32KB/km²，比密集存储减少20倍

Result: 在nuScenes数据集上，CMP框架显著且一致地提升了多种架构的3D目标检测性能，计算开销极小

Conclusion: 压缩地图先验是一种简单有效的框架，能够从历史遍历中学习空间先验，显著提升自动驾驶感知系统性能，同时保持极低的存储和计算成本

Abstract: Human drivers rarely travel where no person has gone before. After all, thousands of drivers use busy city roads every day, and only one can claim to be the first. The same holds for autonomous computer vision systems. The vast majority of the deployment area of an autonomous vision system will have been visited before. Yet, most autonomous vehicle vision systems act as if they are encountering each location for the first time. In this work, we present Compressed Map Priors (CMP), a simple but effective framework to learn spatial priors from historic traversals. The map priors use a binarized hashmap that requires only $32\text{KB}/\text{km}^2$, a $20\times$ reduction compared to the dense storage. Compressed Map Priors easily integrate into leading 3D perception systems at little to no extra computational costs, and lead to a significant and consistent improvement in 3D object detection on the nuScenes dataset across several architectures.

</details>


### [5] [FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications](https://arxiv.org/abs/2601.00150)
*Yehui Yang,Dalu Yang,Wenshuo Zhou,Fangxin Shang,Yifan Liu,Jie Ren,Haojun Fei,Qing Yang,Tao Chen*

Main category: cs.CV

TL;DR: FCMBench-V1.0：首个面向金融信贷领域的多模态基准测试，包含4043张隐私合规图像和8446个QA样本，涵盖感知、推理和鲁棒性三个维度，用于评估23个先进视觉语言模型在信贷场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 随着多模态AI在信贷风险评估和文档审核中的广泛应用，亟需一个领域特定的基准测试，能够反映金融信贷应用中的具体文档和工作流程，包含信贷特定理解能力和真实世界鲁棒性，同时满足隐私合规要求而不牺牲实用性。

Method: 通过封闭的合成-采集流水线构建所有样本：手动合成带有虚拟内容的文档模板，并在内部采集场景感知图像。评估框架包括三个维度：感知（3个基础任务）、推理（4个信贷特定任务）和鲁棒性（10种真实世界采集伪影类型）。

Result: 在评估的23个先进视觉语言模型中，Gemini 3 Pro作为商业模型获得最佳F1分数（64.61%），Qwen3-VL-235B作为开源基线获得最佳分数（57.27%），而专门针对金融信贷的模型Qfin-VL-Instruct获得最高总分（64.92%）。鲁棒性评估显示，即使表现最佳的模型在采集伪影下也会出现明显性能下降。

Conclusion: FCMBench-V1.0能够有效区分不同视觉语言模型的性能差异和鲁棒性，为金融信贷领域的多模态AI评估提供了标准化基准，同时通过封闭合成-采集流水线设计解决了隐私合规和预训练数据泄露问题。

Abstract: As multimodal AI becomes widely used for credit risk assessment and document review, a domain-specific benchmark is urgently needed that (1) reflects documents and workflows specific to financial credit applications, (2) includes credit-specific understanding and real-world robustness, and (3) preserves privacy compliance without sacrificing practical utility. Here, we introduce FCMBench-V1.0 -- a large-scale financial credit multimodal benchmark for real-world applications, covering 18 core certificate types, with 4,043 privacy-compliant images and 8,446 QA samples. The FCMBench evaluation framework consists of three dimensions: Perception, Reasoning, and Robustness, including 3 foundational perception tasks, 4 credit-specific reasoning tasks that require decision-oriented understanding of visual evidence, and 10 real-world acquisition artifact types for robustness stress testing. To reconcile compliance with realism, we construct all samples via a closed synthesis-capture pipeline: we manually synthesize document templates with virtual content and capture scenario-aware images in-house. This design also mitigates pre-training data leakage by avoiding web-sourced or publicly released images. FCMBench can effectively discriminate performance disparities and robustness across modern vision-language models. Extensive experiments were conducted on 23 state-of-the-art vision-language models (VLMs) from 14 top AI companies and research institutes. Among them, Gemini 3 Pro achieves the best F1(\%) score as a commercial model (64.61), Qwen3-VL-235B achieves the best score as an open-source baseline (57.27), and our financial credit-specific model, Qfin-VL-Instruct, achieves the top overall score (64.92). Robustness evaluations show that even top-performing models suffer noticeable performance drops under acquisition artifacts.

</details>


### [6] [Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions](https://arxiv.org/abs/2601.00156)
*Kaiwen Zheng,Junchen Fu,Songpei Xu,Yaoqing He,Joemon M. Jose,Han Hu,Xuri Ge*

Main category: cs.CV

TL;DR: 该论文提出了FaceFocalDesc问题，即针对任意选定面部区域生成和识别包含面部动作单元、情绪状态和年龄估计的多属性自然语言描述，并构建了相应数据集，提出了基于Qwen2.5-VL的Focal-RegionFace模型，通过渐进式微调实现局部面部特征的精细分析。


<details>
  <summary>Details</summary>
Motivation: 当前面部分析研究主要关注整体面部特征，缺乏对局部面部区域的细粒度多属性描述能力。作者认为系统能够聚焦于个体面部区域将带来更好的理解和控制，因此探索了这一未被充分研究的问题。

Method: 1. 构建了针对任意选定面部区域的多属性描述数据集，包含丰富的区域级标注和自然语言描述；2. 提出了基于Qwen2.5-VL的Focal-RegionFace模型，通过多个渐进式微调阶段逐步细化对局部面部特征的关注，实现可解释的年龄估计、面部动作单元和情绪检测。

Result: 实验结果表明，Focal-RegionFace在新基准测试中，无论是传统广泛使用的指标还是新提出的指标，都取得了最佳性能，充分验证了其在细粒度多属性面部区域聚焦分析场景中的有效性和多功能性。

Conclusion: 该研究成功解决了面部分析中未被充分探索的FaceFocalDesc问题，通过构建新数据集和提出Focal-RegionFace模型，实现了对任意选定面部区域的细粒度多属性自然语言描述生成和识别，为面部状态分析提供了更精细、可解释的方法。

Abstract: In this paper, we introduce an underexplored problem in facial analysis: generating and recognizing multi-attribute natural language descriptions, containing facial action units (AUs), emotional states, and age estimation, for arbitrarily selected face regions (termed FaceFocalDesc). We argue that the system's ability to focus on individual facial areas leads to better understanding and control. To achieve this capability, we construct a new multi-attribute description dataset for arbitrarily selected face regions, providing rich region-level annotations and natural language descriptions. Further, we propose a fine-tuned vision-language model based on Qwen2.5-VL, called Focal-RegionFace for facial state analysis, which incrementally refines its focus on localized facial features through multiple progressively fine-tuning stages, resulting in interpretable age estimation, FAU and emotion detection. Experimental results show that Focal-RegionFace achieves the best performance on the new benchmark in terms of traditional and widely used metrics, as well as new proposed metrics. This fully verifies its effectiveness and versatility in fine-grained multi-attribute face region-focal analysis scenarios.

</details>


### [7] [MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing](https://arxiv.org/abs/2601.00204)
*Xiaokun Sun,Zeyu Cai,Hao Tang,Ying Tai,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: MorphAny3D是一个无需训练的3D变形框架，利用结构化潜在表示(SLAT)生成高质量跨类别3D变形序列，通过创新的注意力机制融合源和目标特征。


<details>
  <summary>Details</summary>
Motivation: 3D变形面临语义一致性和时间平滑性的挑战，尤其是在跨类别变形中。现有方法难以生成高质量的变形序列，特别是在保持结构连贯性和时间一致性方面存在不足。

Method: 提出MorphAny3D框架，核心是两种注意力机制：变形交叉注意力(MCA)融合源和目标SLAT特征以保持结构连贯性；时间融合自注意力(TFSA)整合前一帧特征以增强时间一致性。还采用方向校正策略缓解变形过程中的姿态模糊问题。

Result: 实验表明该方法能生成最先进的变形序列，即使在具有挑战性的跨类别案例中也能表现出色。框架还支持解耦变形和3D风格迁移等高级应用，并能泛化到其他基于SLAT的生成模型。

Conclusion: MorphAny3D通过创新的注意力机制设计，在无需训练的情况下实现了高质量的3D变形，解决了跨类别变形中的语义一致性和时间平滑性问题，为3D生成和编辑提供了新的可能性。

Abstract: 3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/.

</details>


### [8] [CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting](https://arxiv.org/abs/2601.00207)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: 提出一种基于多视角图像和神经辐射场（NeRF）的3D实例分割框架，用于精确计数农作物，解决了户外环境中遮挡和作物聚集的挑战。


<details>
  <summary>Details</summary>
Motivation: 户外农田环境中，部分遮挡和作物聚集导致基于图像的2D分割方法难以精确计数农作物，这影响了农业管理和干预策略的有效性。

Method: 使用多视角2D图像，结合神经辐射场（NeRF）进行视图合成，引入作物可见性和掩码一致性评分，结合NeRF模型的3D信息，实现3D实例分割和精确计数。

Result: 在棉花铃、苹果和梨三个农业数据集上验证，表现出稳定的计数性能，不受作物颜色、形状和大小变化的影响，相比现有方法具有优越性能。

Conclusion: 该方法通过3D实例分割有效解决了户外农作物计数中的遮挡和聚集问题，无需作物特定参数调优，并贡献了棉花植物数据集以促进进一步研究。

Abstract: Rigorous crop counting is crucial for effective agricultural management and informed intervention strategies. However, in outdoor field environments, partial occlusions combined with inherent ambiguity in distinguishing clustered crops from individual viewpoints poses an immense challenge for image-based segmentation methods. To address these problems, we introduce a novel crop counting framework designed for exact enumeration via 3D instance segmentation. Our approach utilizes 2D images captured from multiple viewpoints and associates independent instance masks for neural radiance field (NeRF) view synthesis. We introduce crop visibility and mask consistency scores, which are incorporated alongside 3D information from a NeRF model. This results in an effective segmentation of crop instances in 3D and highly-accurate crop counts. Furthermore, our method eliminates the dependence on crop-specific parameter tuning. We validate our framework on three agricultural datasets consisting of cotton bolls, apples, and pears, and demonstrate consistent counting performance despite major variations in crop color, shape, and size. A comparative analysis against the state of the art highlights superior performance on crop counting tasks. Lastly, we contribute a cotton plant dataset to advance further research on this topic.

</details>


### [9] [IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation](https://arxiv.org/abs/2601.00212)
*Han Liu,Yubo Fan,Hao Li,Dewei Hu,Daniel Moyer,Zhoubing Xu,Benoit M. Dawant,Ipek Oguz*

Main category: cs.CV

TL;DR: IntraStyler：一种基于示例的风格合成方法，用于无监督域适应，无需先验知识即可捕获多样的域内风格，提升下游分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域适应方法主要关注源域和目标域之间的域偏移，而域内变异性研究不足。传统方法需要预先指定域内变化进行风格合成，这在实际应用中不切实际。

Method: 提出IntraStyler方法，使用示例图像指导风格合成，使输出风格与示例风格匹配。引入风格编码器，基于对比学习判别性地学习风格特征，提取纯风格特征。

Result: 在最大的跨模态域适应公共数据集CrossMoDA 2023上评估，实验证明该方法在可控风格合成方面有效，多样化的合成数据对下游分割任务有益。

Conclusion: IntraStyler能够无需先验知识捕获多样的域内风格，为无监督域适应提供了有效的风格合成方法，代码已开源。

Abstract: Image-level domain alignment is the de facto approach for unsupervised domain adaptation, where unpaired image translation is used to minimize the domain gap. Prior studies mainly focus on the domain shift between the source and target domains, whereas the intra-domain variability remains under-explored. To address the latter, an effective strategy is to diversify the styles of the synthetic target domain data during image translation. However, previous methods typically require intra-domain variations to be pre-specified for style synthesis, which may be impractical. In this paper, we propose an exemplar-based style synthesis method named IntraStyler, which can capture diverse intra-domain styles without any prior knowledge. Specifically, IntraStyler uses an exemplar image to guide the style synthesis such that the output style matches the exemplar style. To extract the style-only features, we introduce a style encoder to learn styles discriminatively based on contrastive learning. We evaluate the proposed method on the largest public dataset for cross-modality domain adaptation, CrossMoDA 2023. Our experiments show the efficacy of our method in controllable style synthesis and the benefits of diverse synthetic data for downstream segmentation. Code is available at https://github.com/han-liu/IntraStyler.

</details>


### [10] [From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning](https://arxiv.org/abs/2601.00215)
*Omar Sharif,Eftekhar Hossain,Patrick Ng*

Main category: cs.CV

TL;DR: 该论文提出使用强化学习来增强多模态大语言模型的视觉推理能力，通过设计针对不同推理方面的奖励函数，显著提升了模型在需要准确视觉感知任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型生成的推理链缺乏视觉信息的整合，限制了其在需要准确视觉感知任务（如视觉谜题）上的能力。研究表明视觉感知是这类任务的关键瓶颈，将图像转换为文本描述能显著提升性能。

Method: 采用奖励驱动的强化学习方法，设计了六个针对不同推理方面的奖励函数（包括图像理解、思考步骤和答案准确性）。使用组相对策略优化（GRPO）来显式激励更长、结构化的推理，并减少视觉信息的绕过。

Result: 在Qwen-2.5-VL-7B模型上实现了5.56%的性能提升，在领域内和领域外设置中都表现出一致的增益。实验表明将图像转换为文本描述能为Claude 3.5带来26.7%的性能提升，为Claude 3.7带来23.6%的提升。

Conclusion: 强化学习是解锁开源多模态大语言模型长视觉推理能力的有效机制，无需昂贵的监督。通过设计针对性的奖励函数和GRPO优化，可以显著提升模型在需要准确视觉感知任务上的表现。

Abstract: Reinforcement learning (RL) has emerged as a promising approach for eliciting reasoning chains before generating final answers. However, multimodal large language models (MLLMs) generate reasoning that lacks integration of visual information. This limits their ability to solve problems that demand accurate visual perception, such as visual puzzles. We show that visual perception is the key bottleneck in such tasks: converting images into textual descriptions significantly improves performance, yielding gains of 26.7% for Claude 3.5 and 23.6% for Claude 3.7.
  To address this, we investigate reward-driven RL as a mechanism to unlock long visual reasoning in open-source MLLMs without requiring costly supervision. We design and evaluate six reward functions targeting different reasoning aspects, including image understanding, thinking steps, and answer accuracy. Using group relative policy optimization (GRPO), our approach explicitly incentivizes longer, structured reasoning and mitigates bypassing of visual information. Experiments on Qwen-2.5-VL-7B achieve 5.56% improvements over the base model, with consistent gains across both in-domain and out-of-domain settings.

</details>


### [11] [LooC: Effective Low-Dimensional Codebook for Compositional Vector Quantization](https://arxiv.org/abs/2601.00222)
*Jie Li,Kwan-Yee K. Wong,Kai Han*

Main category: cs.CV

TL;DR: LooC是一种新型向量量化方法，通过低维码本组合量化实现高容量且紧凑的向量表示，在保持性能的同时显著减小码本尺寸。


<details>
  <summary>Details</summary>
Motivation: 随着数据和模型复杂度不断增加，现有向量量化方法面临容量与紧凑性的矛盾。需要一种既能保持高容量又能实现紧凑表示的向量量化方法。

Method: 1. 引入参数高效的码本设计，将码向量视为特征向量的低维组合单元而非独立匹配；2. 采用参数无关的外推-插值机制增强特征平滑；3. 实现全码本利用，避免崩溃问题；4. 可作为即插即用模块应用于现有VQ方法。

Result: 在不同任务、数据集和架构上的广泛评估表明，LooC在显著减小码本尺寸的情况下，性能优于现有VQ方法，达到最先进水平。

Conclusion: LooC成功解决了向量量化中容量与紧凑性的矛盾，通过低维码本组合量化和特征增强机制，实现了高效且性能优越的向量表示，为下游任务提供了有效的即插即用解决方案。

Abstract: Vector quantization (VQ) is a prevalent and fundamental technique that discretizes continuous feature vectors by approximating them using a codebook. As the diversity and complexity of data and models continue to increase, there is an urgent need for high-capacity, yet more compact VQ methods. This paper aims to reconcile this conflict by presenting a new approach called LooC, which utilizes an effective Low-dimensional codebook for Compositional vector quantization. Firstly, LooC introduces a parameter-efficient codebook by reframing the relationship between codevectors and feature vectors, significantly expanding its solution space. Instead of individually matching codevectors with feature vectors, LooC treats them as lower-dimensional compositional units within feature vectors and combines them, resulting in a more compact codebook with improved performance. Secondly, LooC incorporates a parameter-free extrapolation-by-interpolation mechanism to enhance and smooth features during the VQ process, which allows for better preservation of details and fidelity in feature approximation. The design of LooC leads to full codebook usage, effectively utilizing the compact codebook while avoiding the problem of collapse. Thirdly, LooC can serve as a plug-and-play module for existing methods for different downstream tasks based on VQ. Finally, extensive evaluations on different tasks, datasets, and architectures demonstrate that LooC outperforms existing VQ methods, achieving state-of-the-art performance with a significantly smaller codebook.

</details>


### [12] [Towards Syn-to-Real IQA: A Novel Perspective on Reshaping Synthetic Data Distributions](https://arxiv.org/abs/2601.00225)
*Aobo Li,Jinjian Wu,Yongxu Liu,Leida Li,Weisheng Dong*

Main category: cs.CV

TL;DR: 该论文提出SynDR-IQA框架，通过重塑合成数据分布来解决盲图像质量评估中合成数据训练模型泛化能力有限的问题。


<details>
  <summary>Details</summary>
Motivation: 盲图像质量评估面临大规模标注数据稀缺的挑战，合成数据是潜在解决方案，但现有合成数据集训练的模型泛化能力有限。研究发现合成数据学习到的表示呈现离散聚类模式，阻碍回归性能。

Method: 提出SynDR-IQA框架，基于样本多样性和冗余对泛化误差影响的理论推导，采用两种策略：1）分布感知的多样化内容上采样，增强视觉多样性同时保持内容分布；2）密度感知的冗余聚类下采样，通过减少密集聚类区域的密度来平衡样本。

Result: 在三种跨数据集设置（合成到真实、合成到算法、合成到合成）上的大量实验证明了该方法的有效性。

Conclusion: 通过重塑合成数据分布，SynDR-IQA框架显著提升了盲图像质量评估模型的泛化能力，为解决合成数据训练模型的局限性提供了有效方案。

Abstract: Blind Image Quality Assessment (BIQA) has advanced significantly through deep learning, but the scarcity of large-scale labeled datasets remains a challenge. While synthetic data offers a promising solution, models trained on existing synthetic datasets often show limited generalization ability. In this work, we make a key observation that representations learned from synthetic datasets often exhibit a discrete and clustered pattern that hinders regression performance: features of high-quality images cluster around reference images, while those of low-quality images cluster based on distortion types. Our analysis reveals that this issue stems from the distribution of synthetic data rather than model architecture. Consequently, we introduce a novel framework SynDR-IQA, which reshapes synthetic data distribution to enhance BIQA generalization. Based on theoretical derivations of sample diversity and redundancy's impact on generalization error, SynDR-IQA employs two strategies: distribution-aware diverse content upsampling, which enhances visual diversity while preserving content distribution, and density-aware redundant cluster downsampling, which balances samples by reducing the density of densely clustered areas. Extensive experiments across three cross-dataset settings (synthetic-to-authentic, synthetic-to-algorithmic, and synthetic-to-synthetic) demonstrate the effectiveness of our method. The code is available at https://github.com/Li-aobo/SynDR-IQA.

</details>


### [13] [Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection](https://arxiv.org/abs/2601.00237)
*Chao Yang,Haoyuan Zheng,Yue Ma*

Main category: cs.CV

TL;DR: 提出跨模态数据增强框架，结合CycleGAN和YOLOv8解决PCB红外缺陷检测数据稀缺问题


<details>
  <summary>Details</summary>
Motivation: 解决红外数据稀缺对PCB缺陷检测造成的瓶颈问题，传统方法依赖配对监督数据不足

Method: 使用CycleGAN进行无配对图像转换，将丰富的可见光PCB图像映射到红外域，生成高质量伪红外样本；构建异构训练策略，融合伪红外数据和有限真实红外样本训练轻量级YOLOv8检测器

Result: 该方法有效增强低数据条件下的特征学习，增强后的检测器显著优于仅使用有限真实数据的模型，接近全监督训练的性能基准

Conclusion: 伪红外合成作为工业检测的鲁棒增强策略具有高效性，为解决红外数据稀缺问题提供了有效方案

Abstract: This paper addresses the critical bottleneck of infrared (IR) data scarcity in Printed Circuit Board (PCB) defect detection by proposing a cross-modal data augmentation framework integrating CycleGAN and YOLOv8. Unlike conventional methods relying on paired supervision, we leverage CycleGAN to perform unpaired image-to-image translation, mapping abundant visible-light PCB images into the infrared domain. This generative process synthesizes high-fidelity pseudo-IR samples that preserve the structural semantics of defects while accurately simulating thermal distribution patterns. Subsequently, we construct a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a lightweight YOLOv8 detector. Experimental results demonstrate that this method effectively enhances feature learning under low-data conditions. The augmented detector significantly outperforms models trained on limited real data alone and approaches the performance benchmarks of fully supervised training, proving the efficacy of pseudo-IR synthesis as a robust augmentation strategy for industrial inspection.

</details>


### [14] [TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models](https://arxiv.org/abs/2601.00260)
*Kohei Yamamoto,Tomohiro Kikuchi*

Main category: cs.CV

TL;DR: TotalFM是一种基于器官分离概念的放射学基础模型，通过自动化创建器官体积-发现句子对，结合自监督预训练和对比学习，在3D-CT图像与语言表达对应关系学习中平衡计算效率和表示能力。


<details>
  <summary>Details</summary>
Motivation: 放射学基础模型应用于3D-CT体积数据时面临计算成本限制的挑战，需要开发既能处理3D-CT图像与语言对应关系，又保持计算效率的实用模型。

Method: 基于器官分离概念，利用14万系列大规模数据集，通过分割技术和基于LLM的放射学报告处理自动化创建器官体积-发现句子对，结合VideoMAE自监督预训练和体积-文本对的对比学习。

Result: 在零样本器官级病变分类任务中，相比CT-CLIP在83%器官上获得更高F1分数，相比Merlin在64%器官上表现更好；在零样本发现级病变分类中，相比Merlin在83%发现类别上获得更高AUROC；在放射学报告生成任务中达到与现有VLM相当的性能。

Conclusion: 器官分离学习框架可作为3D-CT基础模型实际实施的现实有效设计指南，展示了在临床评估环境中使用实际放射学报告句子时的高泛化性能。

Abstract: While foundation models in radiology are expected to be applied to various clinical tasks, computational cost constraints remain a major challenge when training on 3D-CT volumetric data. In this study, we propose TotalFM, a radiological foundation model that efficiently learns the correspondence between 3D-CT images and linguistic expressions based on the concept of organ separation, utilizing a large-scale dataset of 140,000 series. By automating the creation of organ volume and finding-sentence pairs through segmentation techniques and Large Language Model (LLM)-based radiology report processing, and by combining self-supervised pre-training via VideoMAE with contrastive learning using volume-text pairs, we aimed to balance computational efficiency and representation capability. In zero-shot organ-wise lesion classification tasks, the proposed model achieved higher F1 scores in 83% (5/6) of organs compared to CT-CLIP and 64% (9/14) of organs compared to Merlin. These results suggest that the proposed model exhibits high generalization performance in a clinical evaluation setting using actual radiology report sentences. Furthermore, in zero-shot finding-wise lesion classification tasks, our model achieved a higher AUROC in 83% (25/30) of finding categories compared to Merlin. We also confirmed performance comparable to existing Vision-Language Models (VLMs) in radiology report generation tasks. Our results demonstrate that the organ-separated learning framework can serve as a realistic and effective design guideline for the practical implementation of 3D-CT foundation models.

</details>


### [15] [S1-MMAlign: A Large-Scale, Multi-Disciplinary Dataset for Scientific Figure-Text Understanding](https://arxiv.org/abs/2601.00264)
*He Wang,Longteng Guo,Pengkang Huo,Xuanxu Lin,Yichen Yuan,Jie Jiang,Jing Liu*

Main category: cs.CV

TL;DR: S1-MMAlign是一个大规模、多学科的科学多模态数据集，包含1550万高质量图像-文本对，通过AI增强管道提升语义对齐，为科学AI研究提供基础资源。


<details>
  <summary>Details</summary>
Motivation: 多模态学习在通用领域取得了革命性进展，但在科学发现中的应用受到复杂科学图像与稀疏文本描述之间深刻语义鸿沟的阻碍。现有科学多模态数据存在对齐弱、质量差的问题。

Method: 1. 构建包含1550万图像-文本对的大规模多学科数据集，涵盖物理、生物、工程等领域，包含实验装置、热图、显微图像等多种视觉模态；2. 引入AI就绪的语义增强管道，利用Qwen-VL多模态大模型系列，通过综合论文摘要和引用上下文重新为图像生成描述，解决原始科学标题对齐弱的问题。

Result: 技术验证表明增强显著提升数据质量：基于SciBERT的伪困惑度指标显示语义模糊性降低，CLIP分数表明图像-文本对齐提高了18.21%。数据集已在HuggingFace公开可用。

Conclusion: S1-MMAlign为推进科学推理和跨模态理解提供了基础资源，有助于在AI for Science时代推动科学发现的多模态学习应用。

Abstract: Multimodal learning has revolutionized general domain tasks, yet its application in scientific discovery is hindered by the profound semantic gap between complex scientific imagery and sparse textual descriptions. We present S1-MMAlign, a large-scale, multi-disciplinary multimodal dataset comprising over 15.5 million high-quality image-text pairs derived from 2.5 million open-access scientific papers. Spanning disciplines from physics and biology to engineering, the dataset captures diverse visual modalities including experimental setups, heatmaps, and microscopic imagery. To address the pervasive issue of weak alignment in raw scientific captions, we introduce an AI-ready semantic enhancement pipeline that utilizes the Qwen-VL multimodal large model series to recaption images by synthesizing context from paper abstracts and citation contexts. Technical validation demonstrates that this enhancement significantly improves data quality: SciBERT-based pseudo-perplexity metrics show reduced semantic ambiguity, while CLIP scores indicate an 18.21% improvement in image-text alignment. S1-MMAlign provides a foundational resource for advancing scientific reasoning and cross-modal understanding in the era of AI for Science. The dataset is publicly available at https://huggingface.co/datasets/ScienceOne-AI/S1-MMAlign.

</details>


### [16] [ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching](https://arxiv.org/abs/2601.00267)
*Yi Sun,Xinhao Zhong,Hongyan Li,Yimin Zhou,Junhao Li,Bin Chen,Xuan Wang*

Main category: cs.CV

TL;DR: 提出ActErase：一种无需训练的概念擦除方法，通过激活差异区域识别和动态替换，在扩散模型中高效移除敏感概念


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型存在安全、版权和伦理问题，现有概念擦除方法依赖数据密集和计算昂贵的微调，存在关键限制

Method: 基于模型激活主要由通用概念组成、只有极小部分表示目标概念的观察，通过提示对分析识别激活差异区域，提取目标激活并在前向传播中动态替换输入激活

Result: 在三个关键擦除任务（裸露、艺术风格和对象移除）上实现最先进的擦除性能，有效保留模型整体生成能力，对对抗攻击表现出强鲁棒性

Conclusion: ActErase为扩散模型中的轻量级有效概念操作建立了新的即插即用范式，无需训练即可实现高效概念擦除

Abstract: Recent advances in text-to-image diffusion models have demonstrated remarkable generation capabilities, yet they raise significant concerns regarding safety, copyright, and ethical implications. Existing concept erasure methods address these risks by removing sensitive concepts from pre-trained models, but most of them rely on data-intensive and computationally expensive fine-tuning, which poses a critical limitation. To overcome these challenges, inspired by the observation that the model's activations are predominantly composed of generic concepts, with only a minimal component can represent the target concept, we propose a novel training-free method (ActErase) for efficient concept erasure. Specifically, the proposed method operates by identifying activation difference regions via prompt-pair analysis, extracting target activations and dynamically replacing input activations during forward passes. Comprehensive evaluations across three critical erasure tasks (nudity, artistic style, and object removal) demonstrates that our training-free method achieves state-of-the-art (SOTA) erasure performance, while effectively preserving the model's overall generative capability. Our approach also exhibits strong robustness against adversarial attacks, establishing a new plug-and-play paradigm for lightweight yet effective concept manipulation in diffusion models.

</details>


### [17] [SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting](https://arxiv.org/abs/2601.00285)
*Jun-Jee Chao,Volkan Isler*

Main category: cs.CV

TL;DR: SV-GS：一种在稀疏观测下重建动态目标的方法，通过骨架驱动变形场和运动插值，在稀疏视角和时间采样下实现高质量动态重建。


<details>
  <summary>Details</summary>
Motivation: 现实世界中动态目标重建面临挑战，因为观测通常稀疏（如安防摄像头），传统方法需要密集的视角和时间采样，这在真实场景中难以实现。

Method: 提出SV-GS框架，利用粗略骨架图和初始静态重建作为输入，优化骨架驱动变形场，包含粗粒度骨架关节姿态估计器和细粒度变形模块，仅使关节姿态估计器时间相关以实现平滑运动插值。

Result: 在合成数据集上，相比现有方法PSNR提升达34%；在真实数据集上，使用显著更少的帧数即可达到与密集单目视频方法相当的性能；还展示了可用扩散生成先验替代初始静态重建。

Conclusion: SV-GS能够在稀疏观测条件下有效重建动态目标，通过骨架驱动变形场实现高质量运动估计和几何细节保持，具有实际应用价值。

Abstract: Reconstructing a dynamic target moving over a large area is challenging. Standard approaches for dynamic object reconstruction require dense coverage in both the viewing space and the temporal dimension, typically relying on multi-view videos captured at each time step. However, such setups are only possible in constrained environments. In real-world scenarios, observations are often sparse over time and captured sparsely from diverse viewpoints (e.g., from security cameras), making dynamic reconstruction highly ill-posed. We present SV-GS, a framework that simultaneously estimates a deformation model and the object's motion over time under sparse observations. To initialize SV-GS, we leverage a rough skeleton graph and an initial static reconstruction as inputs to guide motion estimation. (Later, we show that this input requirement can be relaxed.) Our method optimizes a skeleton-driven deformation field composed of a coarse skeleton joint pose estimator and a module for fine-grained deformations. By making only the joint pose estimator time-dependent, our model enables smooth motion interpolation while preserving learned geometric details. Experiments on synthetic datasets show that our method outperforms existing approaches under sparse observations by up to 34% in PSNR, and achieves comparable performance to dense monocular video methods on real-world datasets despite using significantly fewer frames. Moreover, we demonstrate that the input initial static reconstruction can be replaced by a diffusion-based generative prior, making our method more practical for real-world scenarios.

</details>


### [18] [Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers](https://arxiv.org/abs/2601.00359)
*Söhnke Benedikt Fischedick,Daniel Seichter,Benedict Stephan,Robin Schmidt,Horst-Michael Gross*

Main category: cs.CV

TL;DR: DVEFormer：基于RGB-D Transformer的高效方法，通过知识蒸馏预测密集文本对齐视觉嵌入，替代传统语义分割，支持自然语言查询和3D建图


<details>
  <summary>Details</summary>
Motivation: 家庭环境中机器人需要全面理解周围环境，以便与未经训练的人类进行有效直观的交互。传统语义分割方法使用固定的预定义类别，缺乏灵活性。

Method: 提出DVEFormer方法，使用Alpha-CLIP的教师嵌入来指导高效的学生模型学习细粒度像素级嵌入。该方法基于RGB-D Transformer架构，通过知识蒸馏实现文本对齐的视觉嵌入预测。

Result: 在常见室内数据集上评估显示，该方法达到竞争性性能并满足实时要求：完整模型在NVIDIA Jetson AGX Orin上运行速度为26.3 FPS，较小变体为77.0 FPS。定性结果展示了在实际应用中的有效性。

Conclusion: DVEFormer可作为传统分割方法的直接替代方案，同时支持灵活的自然语言查询，并能无缝集成到移动机器人的3D建图流程中，为家庭环境中的机器人提供更全面的环境理解能力。

Abstract: In domestic environments, robots require a comprehensive understanding of their surroundings to interact effectively and intuitively with untrained humans. In this paper, we propose DVEFormer - an efficient RGB-D Transformer-based approach that predicts dense text-aligned visual embeddings (DVE) via knowledge distillation. Instead of directly performing classical semantic segmentation with fixed predefined classes, our method uses teacher embeddings from Alpha-CLIP to guide our efficient student model DVEFormer in learning fine-grained pixel-wise embeddings. While this approach still enables classical semantic segmentation, e.g., via linear probing, it further enables flexible text-based querying and other applications, such as creating comprehensive 3D maps. Evaluations on common indoor datasets demonstrate that our approach achieves competitive performance while meeting real-time requirements, operating at 26.3 FPS for the full model and 77.0 FPS for a smaller variant on an NVIDIA Jetson AGX Orin. Additionally, we show qualitative results that highlight the effectiveness and possible use cases in real-world applications. Overall, our method serves as a drop-in replacement for traditional segmentation approaches while enabling flexible natural-language querying and seamless integration into 3D mapping pipelines for mobile robotics.

</details>


### [19] [Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning and Imbalance-Aware Strategies](https://arxiv.org/abs/2601.00286)
*Ali Anaissi,Ali Braytee,Weidong Huang,Junaid Akram,Alaa Farhat,Jie Hua*

Main category: cs.CV

TL;DR: 开发基于深度学习的皮肤疾病分类诊断模型，使用Swin Transformer架构在ISIC2019数据集上达到87.71%的准确率


<details>
  <summary>Details</summary>
Motivation: 皮肤疾病日益普遍但皮肤科医生资源有限，需要智能工具支持患者和临床医生进行及时准确的皮肤疾病诊断

Method: 基于公开皮肤疾病图像数据集进行预训练，使用Swin Transformer架构，优化数据预处理流程，应用针对性数据增强技术

Result: 在ISIC2019数据集的8个皮肤病变类别上达到87.71%的预测准确率

Conclusion: 该模型有潜力作为临床医生的诊断支持工具和患者的自我评估辅助工具

Abstract: As dermatological conditions become increasingly common and the availability of dermatologists remains limited, there is a growing need for intelligent tools to support both patients and clinicians in the timely and accurate diagnosis of skin diseases. In this project, we developed a deep learning based model for the classification and diagnosis of skin conditions. By leveraging pretraining on publicly available skin disease image datasets, our model effectively extracted visual features and accurately classified various dermatological cases. Throughout the project, we refined the model architecture, optimized data preprocessing workflows, and applied targeted data augmentation techniques to improve overall performance. The final model, based on the Swin Transformer, achieved a prediction accuracy of 87.71 percent across eight skin lesion classes on the ISIC2019 dataset. These results demonstrate the model's potential as a diagnostic support tool for clinicians and a self assessment aid for patients.

</details>


### [20] [RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization](https://arxiv.org/abs/2601.00705)
*Wei-Tse Cheng,Yen-Jen Chiou,Yuan-Fu Yang*

Main category: cs.CV

TL;DR: RGS-SLAM是一种鲁棒的高斯泼溅SLAM框架，通过训练免费的对应关系到高斯初始化替代了GS-SLAM的残差驱动稠密化阶段，使用DINOv3描述符和置信度感知内点分类器进行多视角对应关系三角化，实现更稳定、更快的SLAM性能。


<details>
  <summary>Details</summary>
Motivation: 传统的GS-SLAM使用残差驱动稠密化方法，通过渐进添加高斯来揭示缺失几何结构，这种方法可能导致早期映射不稳定且收敛较慢。RGS-SLAM旨在通过更结构化的初始化方法解决这些问题。

Method: RGS-SLAM采用一次性三角化方法，从DINOv3描述符中提取密集多视角对应关系，通过置信度感知内点分类器进行精炼，生成结构感知的高斯种子作为优化前的先验。这种方法替代了传统的残差驱动稠密化阶段。

Result: 在TUM RGB-D和Replica数据集上的评估显示，RGS-SLAM相比现有高斯和基于点的SLAM系统，在定位和重建精度上达到竞争性或更优水平，收敛速度提升约20%，在纹理丰富和杂乱场景中渲染保真度更高，最高可达925 FPS的实时映射性能。

Conclusion: RGS-SLAM通过训练免费的对应关系到高斯初始化方法，提供了更稳定、更快速的SLAM解决方案，与现有GS-SLAM管道完全兼容，在保持实时性能的同时提升了渲染质量和收敛速度。

Abstract: We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS.

</details>


### [21] [TimeColor: Flexible Reference Colorization via Temporal Concatenation](https://arxiv.org/abs/2601.00296)
*Bryan Constantine Sadihin,Yihao Meng,Michael Hua Wang,Matteo Jiahao Chen,Hang Su*

Main category: cs.CV

TL;DR: TimeColor：一种基于草图的视频着色模型，支持异构、可变数量的参考图像，通过每参考区域分配和时空对应掩码注意力来提升颜色保真度、身份一致性和时间稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有着色模型通常只基于单个参考（通常是场景的第一帧），忽略了其他条件数据源，如角色设定图、背景图像或任意着色帧。这限制了颜色保真度和一致性。

Method: 1. 将参考图像编码为额外的潜在帧，在时间维度上拼接，使模型能在每个扩散步骤中并行处理参考图像，同时保持参数数量不变。2. 使用每参考区域分配机制。3. 采用时空对应掩码注意力来强制主体-参考绑定，以及模态分离的RoPE索引。这些机制缓解了捷径学习和跨身份调色板泄漏问题。

Result: 在SAKUGA-42M数据集上的实验表明，TimeColor在单参考和多参考协议下，相比先前基线方法，在颜色保真度、身份一致性和时间稳定性方面都有显著提升。

Conclusion: TimeColor通过支持异构、可变数量的参考图像，结合每参考区域分配和时空对应掩码注意力机制，有效提升了视频着色的质量，解决了传统单参考方法的局限性。

Abstract: Most colorization models condition only on a single reference, typically the first frame of the scene. However, this approach ignores other sources of conditional data, such as character sheets, background images, or arbitrary colorized frames. We propose TimeColor, a sketch-based video colorization model that supports heterogeneous, variable-count references with the use of explicit per-reference region assignment. TimeColor encodes references as additional latent frames which are concatenated temporally, permitting them to be processed concurrently in each diffusion step while keeping the model's parameter count fixed. TimeColor also uses spatiotemporal correspondence-masked attention to enforce subject-reference binding in addition to modality-disjoint RoPE indexing. These mechanisms mitigate shortcutting and cross-identity palette leakage. Experiments on SAKUGA-42M under both single- and multi-reference protocols show that TimeColor improves color fidelity, identity consistency, and temporal stability over prior baselines.

</details>


### [22] [VisNet: Efficient Person Re-Identification via Alpha-Divergence Loss, Feature Fusion and Dynamic Multi-Task Learning](https://arxiv.org/abs/2601.00307)
*Anns Ijaz,Muhammad Azeem Javed*

Main category: cs.CV

TL;DR: VisNet是一个计算高效的行人重识别模型，通过多尺度特征融合、语义聚类、动态权重平均和FIDI损失函数等技术，在保持高精度的同时显著降低计算成本，适合实际部署。


<details>
  <summary>Details</summary>
Motivation: 现有行人重识别方法虽然精度高但计算成本大，难以在计算资源有限的监控和移动应用中实时部署，需要开发既准确又高效的实际解决方案。

Method: 提出VisNet模型，包含：1) 多尺度特征融合（融合ResNet50的1-4阶段特征）；2) 语义聚类与解剖学身体分区；3) 动态权重平均平衡分类语义正则化；4) FIDI损失函数改进度量学习。

Result: 在Market-1501数据集上达到87.05% Rank-1和77.65% mAP，仅需32.41M参数和4.601 GFLOPs计算量，显著优于现有高计算成本方法。

Conclusion: VisNet提供了一个实用高效的行人重识别解决方案，在保持高精度的同时大幅降低计算需求，适合在计算资源有限的监控和移动应用中实时部署。

Abstract: Person re-identification (ReID) is an extremely important area in both surveillance and mobile applications, requiring strong accuracy with minimal computational cost. State-of-the-art methods give good accuracy but with high computational budgets. To remedy this, this paper proposes VisNet, a computationally efficient and effective re-identification model suitable for real-world scenarios. It is the culmination of conceptual contributions, including feature fusion at multiple scales with automatic attention on each, semantic clustering with anatomical body partitioning, a dynamic weight averaging technique to balance classification semantic regularization, and the use of loss function FIDI for improved metric learning tasks. The multiple scales fuse ResNet50's stages 1 through 4 without the use of parallel paths, with semantic clustering introducing spatial constraints through the use of rule-based pseudo-labeling. VisNet achieves 87.05% Rank-1 and 77.65% mAP on the Market-1501 dataset, having 32.41M parameters and 4.601 GFLOPs, hence, proposing a practical approach for real-time deployment in surveillance and mobile applications where computational resources are limited.

</details>


### [23] [OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning](https://arxiv.org/abs/2601.00352)
*Liuxiang Qiu,Hui Da,Yuzhen Niu,Tiesong Zhao,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: OmniVaT框架首次成功解决了单域泛化多模态视觉触觉学习任务，通过统一嵌入频率空间映射和离散树生成模块，有效缓解模态差异和领域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 视觉触觉学习面临视觉和触觉图像之间的模态差异问题，以及非标准化触觉传感器和不一致数据收集程序导致的领域差距。这些挑战被形式化为单域泛化多模态视觉触觉学习任务。

Method: 提出OmniVaT框架，包含两个核心组件：1) 多模态分数傅里叶适配器，将视觉和触觉嵌入映射到统一的嵌入频率空间；2) 离散树生成模块，通过分层树结构获得多样且可靠的多模态分数表示。

Result: 大量实验证明OmniVaT在SDG-VTL任务上具有优越的跨域泛化性能，无需多域训练数据或复杂的跨模态融合策略。

Conclusion: OmniVaT框架首次成功解决了单域泛化多模态视觉触觉学习任务，通过统一的嵌入频率空间映射和分层树结构表示，有效缓解了模态差异和领域偏移问题。

Abstract: Visual-tactile learning (VTL) enables embodied agents to perceive the physical world by integrating visual (VIS) and tactile (TAC) sensors. However, VTL still suffers from modality discrepancies between VIS and TAC images, as well as domain gaps caused by non-standardized tactile sensors and inconsistent data collection procedures. We formulate these challenges as a new task, termed single domain generalization for multimodal VTL (SDG-VTL). In this paper, we propose an OmniVaT framework that, for the first time, successfully addresses this task. On the one hand, OmniVaT integrates a multimodal fractional Fourier adapter (MFFA) to map VIS and TAC embeddings into a unified embedding-frequency space, thereby effectively mitigating the modality gap without multi-domain training data or careful cross-modal fusion strategies. On the other hand, it also incorporates a discrete tree generation (DTG) module that obtains diverse and reliable multimodal fractional representations through a hierarchical tree structure, thereby enhancing its adaptivity to fluctuating domain shifts in unseen domains. Extensive experiments demonstrate the superior cross-domain generalization performance of OmniVaT on the SDG-VTL task.

</details>


### [24] [BHaRNet: Reliability-Aware Body-Hand Modality Expertized Networks for Fine-grained Skeleton Action Recognition](https://arxiv.org/abs/2601.00369)
*Seungyeon Cho,Tae-kyun Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种概率双流框架，用于骨架动作识别，通过统一可靠性建模和多模态集成，在不确定条件下实现专家化学习，特别关注手部细微动作。


<details>
  <summary>Details</summary>
Motivation: 现有骨架动作识别方法大多关注身体大尺度运动，忽视了手部细微动作对于细粒度识别的重要性，且缺乏对不确定性和多模态集成的统一处理。

Method: 提出了三个关键组件：1）无需校准的预处理管道，直接从原生坐标学习；2）概率Noisy-OR融合，稳定可靠性感知的双流学习；3）从骨架模态到RGB表示的跨模态集成，耦合四种骨架模态（关节、骨骼、关节运动、骨骼运动）。

Result: 在多个基准测试（NTU RGB+D 60/120、PKU-MMD、N-UCLA）和新定义的手部中心基准上均表现出持续改进和鲁棒性，特别是在噪声和异构条件下。

Conclusion: 该概率双流框架通过统一可靠性建模和多模态集成，有效提升了骨架动作识别的性能，特别在捕捉手部细微动作方面表现出色，为细粒度动作识别提供了新思路。

Abstract: Skeleton-based human action recognition (HAR) has achieved remarkable progress with graph-based architectures. However, most existing methods remain body-centric, focusing on large-scale motions while neglecting subtle hand articulations that are crucial for fine-grained recognition. This work presents a probabilistic dual-stream framework that unifies reliability modeling and multi-modal integration, generalizing expertized learning under uncertainty across both intra-skeleton and cross-modal domains. The framework comprises three key components: (1) a calibration-free preprocessing pipeline that removes canonical-space transformations and learns directly from native coordinates; (2) a probabilistic Noisy-OR fusion that stabilizes reliability-aware dual-stream learning without requiring explicit confidence supervision; and (3) an intra- to cross-modal ensemble that couples four skeleton modalities (Joint, Bone, Joint Motion, and Bone Motion) to RGB representations, bridging structural and visual motion cues in a unified cross-modal formulation. Comprehensive evaluations across multiple benchmarks (NTU RGB+D~60/120, PKU-MMD, N-UCLA) and a newly defined hand-centric benchmark exhibit consistent improvements and robustness under noisy and heterogeneous conditions.

</details>


### [25] [NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos](https://arxiv.org/abs/2601.00393)
*Yuxue Yang,Lue Fan,Ziqi Shi,Junran Peng,Feng Wang,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: NeoVerse是一个多功能4D世界模型，能够进行4D重建、新轨迹视频生成和丰富的下游应用。它解决了现有4D建模方法在可扩展性方面的局限性，通过免姿态前馈4D重建和在线单目退化模式模拟等技术，实现了对多样化单目视频的扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前4D世界建模方法存在可扩展性限制，主要原因是需要昂贵且专业的多视角4D数据或繁琐的训练预处理。作者希望开发一个能够扩展到多样化单目视频的4D世界模型。

Method: NeoVerse采用免姿态前馈4D重建、在线单目退化模式模拟以及其他对齐良好的技术。这些设计使模型能够处理各种领域的单目视频，实现可扩展的4D世界建模。

Result: NeoVerse在标准重建和生成基准测试中实现了最先进的性能，同时展现出对各种领域的泛化能力和多功能性。

Conclusion: NeoVerse是一个可扩展的4D世界模型，能够从单目视频中进行4D重建和视频生成，在性能和泛化能力方面表现出色，为4D世界建模提供了新的解决方案。

Abstract: In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io

</details>


### [26] [RoLID-11K: A Dashcam Dataset for Small-Object Roadside Litter Detection](https://arxiv.org/abs/2601.00398)
*Tao Wu,Qing Xu,Xiangjian He,Oakleigh Weekes,James Brown,Wenting Duan*

Main category: cs.CV

TL;DR: RoLID-11K是首个用于车载摄像头路边垃圾检测的大规模数据集，包含超过1.1万张标注图像，针对极端小目标检测挑战，并评估了多种现代检测器的性能。


<details>
  <summary>Details</summary>
Motivation: 当前路边垃圾监测依赖劳动密集型调查和公众报告，空间覆盖有限。现有的视觉数据集主要针对街景静态图像、航拍场景或水生环境，无法反映车载摄像头视频中垃圾目标极小、稀疏且嵌入杂乱路侧背景的独特特征。

Method: 研究者创建了RoLID-11K数据集，包含超过1.1万张标注图像，涵盖英国多样驾驶条件，呈现显著的长尾分布和小目标分布。对多种现代检测器进行基准测试，包括基于准确性的Transformer架构和实时YOLO模型。

Result: CO-DETR及相关Transformer模型实现了最佳定位精度，但实时模型受限于粗糙的特征层次结构。该数据集为动态驾驶场景中的极端小目标检测建立了具有挑战性的基准。

Conclusion: RoLID-11K数据集旨在支持开发可扩展、低成本的路边垃圾监测系统，为动态驾驶场景中的极端小目标检测提供了重要基准。

Abstract: Roadside litter poses environmental, safety and economic challenges, yet current monitoring relies on labour-intensive surveys and public reporting, providing limited spatial coverage. Existing vision datasets for litter detection focus on street-level still images, aerial scenes or aquatic environments, and do not reflect the unique characteristics of dashcam footage, where litter appears extremely small, sparse and embedded in cluttered road-verge backgrounds. We introduce RoLID-11K, the first large-scale dataset for roadside litter detection from dashcams, comprising over 11k annotated images spanning diverse UK driving conditions and exhibiting pronounced long-tail and small-object distributions. We benchmark a broad spectrum of modern detectors, from accuracy-oriented transformer architectures to real-time YOLO models, and analyse their strengths and limitations on this challenging task. Our results show that while CO-DETR and related transformers achieve the best localisation accuracy, real-time models remain constrained by coarse feature hierarchies. RoLID-11K establishes a challenging benchmark for extreme small-object detection in dynamic driving scenes and aims to support the development of scalable, low-cost systems for roadside-litter monitoring. The dataset is available at https://github.com/xq141839/RoLID-11K.

</details>


### [27] [CPPO: Contrastive Perception for Vision Language Policy Optimization](https://arxiv.org/abs/2601.00501)
*Ahmad Rezaei,Mohsen Gholami,Saeed Ranjbar Alvar,Kevin Cannons,Mohammad Asiful Hossain,Zhou Weimin,Shunbo Zhou,Yong Zhang,Mohammad Akbari*

Main category: cs.CV

TL;DR: CPPO是一种用于微调视觉语言模型的对比感知策略优化方法，通过检测扰动输入下模型输出的熵变化来识别感知标记，并引入对比感知损失来增强感知一致性。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习在语言模型推理方面取得了进展，但将其扩展到多模态推理需要同时改进感知和推理能力。先前的工作主要使用显式感知奖励，但将感知标记与推理标记分离很困难，需要额外的LLM、真实数据、强制分离感知与推理，或对所有输出标记不加区分地应用奖励。

Method: CPPO通过检测在扰动输入图像下模型输出的熵变化来识别感知标记。然后通过对比感知损失扩展RL目标函数，该损失在信息保留扰动下强制一致性，在信息移除扰动下强制敏感性。

Result: 实验表明，CPPO超越了先前的感知奖励方法，同时避免了额外模型，使训练更加高效和可扩展。

Conclusion: CPPO提供了一种有效的方法来改进视觉语言模型的感知能力，通过对比感知损失和熵变化检测，解决了多模态强化学习中感知与推理分离的挑战。

Abstract: We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.

</details>


### [28] [MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation](https://arxiv.org/abs/2601.00504)
*Miaowei Wang,Jakub Zadrożny,Oisin Mac Aodha,Amir Vaxman*

Main category: cs.CV

TL;DR: MotionPhysics是一个端到端的可微分框架，能够从自然语言提示中推断出合理的物理参数，用于3D场景的动态模拟，无需真实轨迹或标注视频的指导。


<details>
  <summary>Details</summary>
Motivation: 传统3D物体和材料模拟需要专家知识和耗时的物理参数调整才能获得期望的动态行为，这限制了非专业用户的使用。

Method: 1. 使用多模态大语言模型估计材料参数值，并约束在合理范围内；2. 提出可学习的运动蒸馏损失，从预训练的视频扩散模型中提取鲁棒的运动先验，同时最小化外观和几何归纳偏差来指导模拟。

Result: 在30多个场景中评估，包括真实世界、人工设计和AI生成的3D物体，涵盖弹性固体、金属、泡沫、沙子以及牛顿和非牛顿流体等多种材料。MotionPhysics能够生成由自然语言引导的视觉逼真的动态模拟，超越了现有技术水平。

Conclusion: MotionPhysics框架能够自动确定物理上合理的参数，通过自然语言提示生成逼真的动态模拟，为3D物理模拟提供了更易用的解决方案。

Abstract: Accurately simulating existing 3D objects and a wide variety of materials often demands expert knowledge and time-consuming physical parameter tuning to achieve the desired dynamic behavior. We introduce MotionPhysics, an end-to-end differentiable framework that infers plausible physical parameters from a user-provided natural language prompt for a chosen 3D scene of interest, removing the need for guidance from ground-truth trajectories or annotated videos. Our approach first utilizes a multimodal large language model to estimate material parameter values, which are constrained to lie within plausible ranges. We further propose a learnable motion distillation loss that extracts robust motion priors from pretrained video diffusion models while minimizing appearance and geometry inductive biases to guide the simulation. We evaluate MotionPhysics across more than thirty scenarios, including real-world, human-designed, and AI-generated 3D objects, spanning a wide range of materials such as elastic solids, metals, foams, sand, and both Newtonian and non-Newtonian fluids. We demonstrate that MotionPhysics produces visually realistic dynamic simulations guided by natural language, surpassing the state of the art while automatically determining physically plausible parameters. The code and project page are available at: https://wangmiaowei.github.io/MotionPhysics.github.io/.

</details>


### [29] [FreeText: Training-Free Text Rendering in Diffusion Transformers via Attention Localization and Spectral Glyph Injection](https://arxiv.org/abs/2601.00535)
*Ruiqiang Zhang,Hengyi Wang,Chang Liu,Guanjie Wang,Zehua Ma,Weiming Zhang*

Main category: cs.CV

TL;DR: FreeText是一个无需训练、即插即用的框架，通过利用扩散Transformer模型的内在机制来改进文本渲染，解决了多行布局、密集排版和中文等长尾脚本的文本渲染问题。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型在开放域合成方面表现出色，但在精确文本渲染方面仍存在困难，特别是对于多行布局、密集排版和中文等长尾脚本。现有解决方案通常需要昂贵的重新训练或严格的外部布局约束，这会降低美学质量并限制灵活性。

Method: FreeText将问题分解为"在哪里写"和"写什么"两个部分。对于"在哪里写"，通过读取图像到文本注意力的token-wise空间归因来定位书写区域，使用sink-like tokens作为稳定的空间锚点，并通过拓扑感知细化产生高置信度掩码。对于"写什么"，引入频谱调制字形注入(SGMI)，注入噪声对齐的字形先验，通过频域带通调制来增强字形结构并抑制语义泄漏。

Result: 在Qwen-Image、FLUX.1-dev和SD3变体上的大量实验表明，在长文本基准测试、CVTG和作者提出的CLT-Bench上，文本可读性得到一致提升，同时很大程度上保持了语义对齐和美学质量，推理开销适中。

Conclusion: FreeText是一个无需训练、即插即用的框架，通过利用扩散Transformer模型的内在机制有效改进了文本渲染，在保持语义对齐和美学质量的同时显著提升了文本可读性。

Abstract: Large-scale text-to-image (T2I) diffusion models excel at open-domain synthesis but still struggle with precise text rendering, especially for multi-line layouts, dense typography, and long-tailed scripts such as Chinese. Prior solutions typically require costly retraining or rigid external layout constraints, which can degrade aesthetics and limit flexibility. We propose \textbf{FreeText}, a training-free, plug-and-play framework that improves text rendering by exploiting intrinsic mechanisms of \emph{Diffusion Transformer (DiT)} models. \textbf{FreeText} decomposes the problem into \emph{where to write} and \emph{what to write}. For \emph{where to write}, we localize writing regions by reading token-wise spatial attribution from endogenous image-to-text attention, using sink-like tokens as stable spatial anchors and topology-aware refinement to produce high-confidence masks. For \emph{what to write}, we introduce Spectral-Modulated Glyph Injection (SGMI), which injects a noise-aligned glyph prior with frequency-domain band-pass modulation to strengthen glyph structure and suppress semantic leakage (rendering the concept instead of the word). Extensive experiments on Qwen-Image, FLUX.1-dev, and SD3 variants across longText-Benchmark, CVTG, and our CLT-Bench show consistent gains in text readability while largely preserving semantic alignment and aesthetic quality, with modest inference overhead.

</details>


### [30] [Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios](https://arxiv.org/abs/2601.00537)
*Guangqian Guo,Pengfei Chen,Yong Guo,Huafeng Chen,Boqiang Zhang,Shan Gao*

Main category: cs.CV

TL;DR: VNS-SAM改进了SAM在视觉非显著场景下的分割性能，通过两个新设计增强对低对比度前景-背景的感知，同时保持零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: SAM在视觉非显著场景（前景与背景对比度低）中表现不佳，现有方法难以捕捉准确轮廓。需要增强SAM对此类场景的感知能力，同时保持其零样本泛化优势。

Method: 提出VNS-SAM，包含两个关键设计：1) Mask-Edge Token Interactive decoder，2) Non-Salient Feature Mining module。这些设计通过有效利用SAM的低层特征来增强对非显著特征的感知，仅需少量参数增量。还建立了VNS-SEG数据集（超过35K图像）用于训练和评估。

Result: VNS-SAM在多种VNS分割任务中表现出优越性能，特别是在零样本设置下。额外参数可在4小时内优化完成，证明了其实用性和可行性。

Conclusion: VNS-SAM成功增强了SAM在视觉非显著场景下的分割能力，同时保持了原有的零样本泛化性，具有广泛的实际应用潜力。

Abstract: Segment Anything Model (SAM), known for its remarkable zero-shot segmentation capabilities, has garnered significant attention in the community. Nevertheless, its performance is challenged when dealing with what we refer to as visually non-salient scenarios, where there is low contrast between the foreground and background. In these cases, existing methods often cannot capture accurate contours and fail to produce promising segmentation results. In this paper, we propose Visually Non-Salient SAM (VNS-SAM), aiming to enhance SAM's perception of visually non-salient scenarios while preserving its original zero-shot generalizability. We achieve this by effectively exploiting SAM's low-level features through two designs: Mask-Edge Token Interactive decoder and Non-Salient Feature Mining module. These designs help the SAM decoder gain a deeper understanding of non-salient characteristics with only marginal parameter increments and computational requirements. The additional parameters of VNS-SAM can be optimized within 4 hours, demonstrating its feasibility and practicality. In terms of data, we established VNS-SEG, a unified dataset for various VNS scenarios, with more than 35K images, in contrast to previous single-task adaptations. It is designed to make the model learn more robust VNS features and comprehensively benchmark the model's segmentation performance and generalizability on VNS scenarios. Extensive experiments across various VNS segmentation tasks demonstrate the superior performance of VNS-SAM, particularly under zero-shot settings, highlighting its potential for broad real-world applications. Codes and datasets are publicly available at https://guangqian-guo.github.io/VNS-SAM.

</details>


### [31] [DynaDrag: Dynamic Drag-Style Image Editing by Motion Prediction](https://arxiv.org/abs/2601.00542)
*Jiacheng Sui,Yujie Zhou,Li Niu*

Main category: cs.CV

TL;DR: DynaDrag是一种基于预测-移动框架的图像拖拽编辑方法，通过迭代执行运动预测和运动监督，动态调整有效处理点，实现更精确的像素级图像操作。


<details>
  <summary>Details</summary>
Motivation: 现有拖拽式图像编辑方法存在跟踪丢失、模糊跟踪、源图像与目标图像差距过大、中间点不合理导致编辑性低等问题，需要新的框架来解决这些挑战。

Method: 提出DynaDrag方法，采用预测-移动框架，迭代执行运动预测和运动监督。运动预测预测处理点应移动的位置，运动监督据此拖动处理点，并动态调整有效处理点以提高性能。

Result: 在人脸和人体数据集上的实验表明，DynaDrag方法在性能上优于先前的工作。

Conclusion: DynaDrag作为首个基于预测-移动框架的拖拽方法，通过创新的迭代预测-监督机制和动态处理点调整，有效解决了传统方法的跟踪问题和编辑性问题。

Abstract: To achieve pixel-level image manipulation, drag-style image editing which edits images using points or trajectories as conditions is attracting widespread attention. Most previous methods follow move-and-track framework, in which miss tracking and ambiguous tracking are unavoidable challenging issues. Other methods under different frameworks suffer from various problems like the huge gap between source image and target edited image as well as unreasonable intermediate point which can lead to low editability. To avoid these problems, we propose DynaDrag, the first dragging method under predict-and-move framework. In DynaDrag, Motion Prediction and Motion Supervision are performed iteratively. In each iteration, Motion Prediction first predicts where the handle points should move, and then Motion Supervision drags them accordingly. We also propose to dynamically adjust the valid handle points to further improve the performance. Experiments on face and human datasets showcase the superiority over previous works.

</details>


### [32] [SingBAG Pro: Accelerating point cloud-based iterative reconstruction for 3D photoacoustic imaging under arbitrary array](https://arxiv.org/abs/2601.00551)
*Shuang Li,Yibing Wang,Jian Gao,Chulhong Kim,Seongwook Choi,Yu Zhang,Qian Chen,Yao Yao,Changhui Li*

Main category: cs.CV

TL;DR: SlingBAG Pro是一种基于点云迭代概念的新型三维光声成像重建算法，专门针对不规则几何换能器阵列设计，相比传统方法显著提升了重建速度和质量。


<details>
  <summary>Details</summary>
Motivation: 临床应用中需要高质量三维光声成像，但传统迭代重建算法在处理不规则阵列配置时面临计算复杂度高、内存需求大、重建时间长等挑战。

Method: 基于SlingBAG方法的点云迭代概念，扩展其兼容性至任意阵列几何形状，采用分层优化策略结合零梯度滤波和逐步增加的时间采样率，快速去除冗余空间点云并加速收敛。

Result: 相比原始SlingBAG算法，SlingBAG Pro在不规则阵列几何下实现了高达2.2倍的速度提升，通过仿真和活体小鼠实验验证了方法的有效性。

Conclusion: SlingBAG Pro算法能够在不规则阵列配置下实现高质量、高效率的三维光声成像重建，为临床应用中减少换能器数量、降低成本提供了有效解决方案。

Abstract: High-quality three-dimensional (3D) photoacoustic imaging (PAI) is gaining increasing attention in clinical applications. To address the challenges of limited space and high costs, irregular geometric transducer arrays that conform to specific imaging regions are promising for achieving high-quality 3D PAI with fewer transducers. However, traditional iterative reconstruction algorithms struggle with irregular array configurations, suffering from high computational complexity, substantial memory requirements, and lengthy reconstruction times. In this work, we introduce SlingBAG Pro, an advanced reconstruction algorithm based on the point cloud iteration concept of the Sliding ball adaptive growth (SlingBAG) method, while extending its compatibility to arbitrary array geometries. SlingBAG Pro maintains high reconstruction quality, reduces the number of required transducers, and employs a hierarchical optimization strategy that combines zero-gradient filtering with progressively increased temporal sampling rates during iteration. This strategy rapidly removes redundant spatial point clouds, accelerates convergence, and significantly shortens overall reconstruction time. Compared to the original SlingBAG algorithm, SlingBAG Pro achieves up to a 2.2-fold speed improvement in point cloud-based 3D PA reconstruction under irregular array geometries. The proposed method is validated through both simulation and in vivo mouse experiments, and the source code is publicly available at https://github.com/JaegerCQ/SlingBAG_Pro.

</details>


### [33] [A Comprehensive Dataset for Human vs. AI Generated Image Detection](https://arxiv.org/abs/2601.00553)
*Rajarshi Roy,Nasrin Imanpour,Ashhar Aziz,Shashwat Bajpai,Gurpreet Singh,Shwetangshu Biswas,Kapil Wanaskar,Parth Patwa,Subhankar Ghosh,Shreyas Dixit,Nilesh Ranjan Pal,Vipula Rawte,Ritvik Garimella,Gaytri Jena,Vasu Sharma,Vinija Jain,Aman Chadha,Aishwarya Naresh Reganti,Amitava Das*

Main category: cs.CV

TL;DR: MS COCOAI数据集：包含96000个真实和AI生成图像的数据集，用于检测AI生成图像和识别生成模型


<details>
  <summary>Details</summary>
Motivation: 随着Stable Diffusion、DALL-E、MidJourney等多模态生成AI系统的普及，合成图像越来越难以与真实照片区分，导致虚假信息和操纵媒体的传播，因此迫切需要有效的检测方法。

Method: 基于MS COCO数据集构建包含96000个数据点的MS COCOAI数据集，使用五种生成器（Stable Diffusion 3、Stable Diffusion 2.1、SDXL、DALL-E 3、MidJourney v6）生成合成图像，并提出两个任务：图像真伪分类和生成模型识别。

Result: 创建了公开可用的MS COCOAI数据集，包含96000个真实和合成图像数据点，为AI生成图像检测研究提供了基准数据集。

Conclusion: MS COCOAI数据集为解决AI生成图像检测问题提供了重要资源，有助于开发更有效的检测方法来应对虚假信息和操纵媒体的挑战。

Abstract: Multimodal generative AI systems like Stable Diffusion, DALL-E, and MidJourney have fundamentally changed how synthetic images are created. These tools drive innovation but also enable the spread of misleading content, false information, and manipulated media. As generated images become harder to distinguish from photographs, detecting them has become an urgent priority. To combat this challenge, We release MS COCOAI, a novel dataset for AI generated image detection consisting of 96000 real and synthetic datapoints, built using the MS COCO dataset. To generate synthetic images, we use five generators: Stable Diffusion 3, Stable Diffusion 2.1, SDXL, DALL-E 3, and MidJourney v6. Based on the dataset, we propose two tasks: (1) classifying images as real or generated, and (2) identifying which model produced a given synthetic image. The dataset is available at https://huggingface.co/datasets/Rajarshi-Roy-research/Defactify_Image_Dataset.

</details>


### [34] [AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models](https://arxiv.org/abs/2601.00561)
*Jintao Lin,Bowen Dong,Weikang Shi,Chenyang Lei,Suiyun Zhang,Rui Liu,Xihui Liu*

Main category: cs.CV

TL;DR: AEGIS是一个评估统一多模态模型世界知识应用能力的多任务基准，包含1050个手动标注的问题，覆盖21个主题和6种推理类型，并提出确定性清单评估协议以提高评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估统一多模态模型的世界知识应用能力方面存在不足，它们通常只提供孤立的单任务评估，诊断能力有限，无法全面评估模型跨任务应用世界知识的能力。

Method: 提出AEGIS基准，包含视觉理解、生成、编辑和交错生成等多任务；包含1050个挑战性问题，覆盖STEM、人文、日常生活等21个主题和6种推理类型；提出确定性清单评估协议，用原子化的"是/否"判断替代模糊的提示评分，提高评估可靠性。

Result: 实验表明大多数统一多模态模型存在严重的世界知识缺陷，随着推理复杂度增加，性能显著下降；简单的插件式推理模块可以部分缓解这些缺陷，为未来研究指明了方向。

Conclusion: 世界知识推理是统一多模态模型发展的关键前沿，AEGIS基准和确定性清单评估协议为系统评估模型的世界知识应用能力提供了有效工具。

Abstract: The capability of Unified Multimodal Models (UMMs) to apply world knowledge across diverse tasks remains a critical, unresolved challenge. Existing benchmarks fall short, offering only siloed, single-task evaluations with limited diagnostic power. To bridge this gap, we propose AEGIS (\emph{i.e.}, \textbf{A}ssessing \textbf{E}diting, \textbf{G}eneration, \textbf{I}nterpretation-Understanding for \textbf{S}uper-intelligence), a comprehensive multi-task benchmark covering visual understanding, generation, editing, and interleaved generation. AEGIS comprises 1,050 challenging, manually-annotated questions spanning 21 topics (including STEM, humanities, daily life, etc.) and 6 reasoning types. To concretely evaluate the performance of UMMs in world knowledge scope without ambiguous metrics, we further propose Deterministic Checklist-based Evaluation (DCE), a protocol that replaces ambiguous prompt-based scoring with atomic ``Y/N'' judgments, to enhance evaluation reliability. Our extensive experiments reveal that most UMMs exhibit severe world knowledge deficits and that performance degrades significantly with complex reasoning. Additionally, simple plug-in reasoning modules can partially mitigate these vulnerabilities, highlighting a promising direction for future research. These results highlight the importance of world-knowledge-based reasoning as a critical frontier for UMMs.

</details>


### [35] [GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval](https://arxiv.org/abs/2601.00584)
*Mingyu Jeon,Sunjae Yoon,Jonghee Kim,Junyeoung Kim*

Main category: cs.CV

TL;DR: 提出GranAlign框架，通过粒度感知对齐解决零样本视频时刻检索中的语义粒度不匹配问题，无需训练即可在多个基准测试中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 零样本视频时刻检索面临的主要挑战是文本查询与视频内容之间的语义粒度不匹配。现有方法虽然利用预训练知识在联合空间中表示视频和语言，但未能平衡不同模态提供的语义粒度，导致检索不准确。

Method: 提出无需训练的GranAlign框架，包含两种互补技术：1) 基于粒度的查询重写，生成不同语义粒度的查询；2) 查询感知的标题生成，将查询意图嵌入视频内容。通过将多级查询与查询无关和查询感知的标题配对，有效解决语义不匹配问题。

Result: 在三个主要基准测试（QVHighlights、Charades-STA、ActivityNet-Captions）上均达到新的最先进水平，在具有挑战性的QVHighlights数据集上mAP@avg显著提升3.23%。

Conclusion: GranAlign框架通过粒度感知对齐有效解决了零样本视频时刻检索中的语义粒度不匹配问题，无需额外训练即可显著提升检索性能，为跨模态对齐提供了新思路。

Abstract: Zero-shot video moment retrieval (ZVMR) is the task of localizing a temporal moment within an untrimmed video using a natural language query without relying on task-specific training data. The primary challenge in this setting lies in the mismatch in semantic granularity between textual queries and visual content. Previous studies in ZVMR have attempted to achieve alignment by leveraging high-quality pre-trained knowledge that represents video and language in a joint space. However, these approaches failed to balance the semantic granularity between the pre-trained knowledge provided by each modality for a given scene. As a result, despite the high quality of each modality's representations, the mismatch in granularity led to inaccurate retrieval. In this paper, we propose a training-free framework, called Granularity-Aware Alignment (GranAlign), that bridges this gap between coarse and fine semantic representations. Our approach introduces two complementary techniques: granularity-based query rewriting to generate varied semantic granularities, and query-aware caption generation to embed query intent into video content. By pairing multi-level queries with both query-agnostic and query-aware captions, we effectively resolve semantic mismatches. As a result, our method sets a new state-of-the-art across all three major benchmarks (QVHighlights, Charades-STA, ActivityNet-Captions), with a notable 3.23% mAP@avg improvement on the challenging QVHighlights dataset.

</details>


### [36] [SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation](https://arxiv.org/abs/2601.00590)
*Yiling Wang,Zeyu Zhang,Yiran Wang,Hao Tang*

Main category: cs.CV

TL;DR: SafeMo是一个可信赖的运动生成框架，通过最小化运动遗忘技术实现安全的人体运动生成，避免了现有离散代码替换方法的问题，并在安全性和实用性之间取得了良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的运动生成方法存在安全隐患，而现有的安全方法采用离散VQ-VAE代码替换存在两个关键缺陷：1）替换被良性提示重用的代码条目会导致日常任务性能下降；2）离散标记方法引入量化和平滑度损失，导致伪影和抖动过渡。此外，现有文本到运动数据集自然包含不安全意图和相应运动，不适合安全驱动的机器学习。

Method: 提出SafeMo框架，集成最小化运动遗忘（MMU）技术，这是一种两阶段机器学习遗忘策略，能够在连续空间中实现安全的人体运动生成，保留连续运动学特性而无需代码损失。同时创建了首个安全文本到运动数据集SafeMoVAE-29K，包含重写的安全文本提示和连续精炼运动。

Result: 实验表明SafeMo在HumanML3D和Motion-X数据集上分别达到2.5倍和14.4倍更高的遗忘集FID，相比之前最先进的人类运动遗忘方法LCR表现出更强的遗忘效果，同时在安全提示上的良性性能相当或更好。

Conclusion: SafeMo通过连续空间中的最小化运动遗忘技术有效解决了现有离散代码替换方法的安全性和质量问题，实现了安全的人体运动生成，并在安全性和实用性之间取得了良好平衡。

Abstract: Text-to-motion (T2M) generation with diffusion backbones achieves strong realism and alignment. Safety concerns in T2M methods have been raised in recent years; existing methods replace discrete VQ-VAE codebook entries to steer the model away from unsafe behaviors. However, discrete codebook replacement-based methods have two critical flaws: firstly, replacing codebook entries which are reused by benign prompts leads to drifts on everyday tasks, degrading the model's benign performance; secondly, discrete token-based methods introduce quantization and smoothness loss, resulting in artifacts and jerky transitions. Moreover, existing text-to-motion datasets naturally contain unsafe intents and corresponding motions, making them unsuitable for safety-driven machine learning. To address these challenges, we propose SafeMo, a trustworthy motion generative framework integrating Minimal Motion Unlearning (MMU), a two-stage machine unlearning strategy, enabling safe human motion generation in continuous space, preserving continuous kinematics without codebook loss and delivering strong safety-utility trade-offs compared to current baselines. Additionally, we present the first safe text-to-motion dataset SafeMoVAE-29K integrating rewritten safe text prompts and continuous refined motion for trustworthy human motion unlearning. Built upon DiP, SafeMo efficiently generates safe human motions with natural transitions. Experiments demonstrate effective unlearning performance of SafeMo by showing strengthened forgetting on unsafe prompts, reaching 2.5x and 14.4x higher forget-set FID on HumanML3D and Motion-X respectively, compared to the previous SOTA human motion unlearning method LCR, with benign performance on safe prompts being better or comparable. Code: https://github.com/AIGeeksGroup/SafeMo. Website: https://aigeeksgroup.github.io/SafeMo.

</details>


### [37] [Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception](https://arxiv.org/abs/2601.00598)
*Xianhui Liu,Siqi Jiang,Yi Xie,Yuqing Lin,Siao Liu*

Main category: cs.CV

TL;DR: 本文提出了一种模态主导指数（MDI）来量化RGB-红外多模态感知中的优化偏差，并开发了模态主导感知跨模态学习（MDACL）框架，通过分层跨模态引导和对抗均衡正则化来平衡多模态融合的优化动态。


<details>
  <summary>Details</summary>
Motivation: RGB-红外多模态感知在复杂物理环境的嵌入式多媒体系统中至关重要。虽然现有的跨模态融合方法有所进展，但由于模态特征不对称导致的优化动态问题尚未充分研究。实践中，信息密度和特征质量的差异引入了持续的优化偏差，导致训练过度强调主导模态，阻碍了有效的融合。

Method: 提出了模态主导指数（MDI），通过联合建模特征熵和梯度贡献来量化模态主导程度。基于MDI开发了模态主导感知跨模态学习（MDACL）框架，包含分层跨模态引导（HCG）来增强特征对齐，以及对抗均衡正则化（AER）来平衡融合过程中的优化动态。

Result: 在三个RGB-红外基准测试上的广泛实验表明，MDACL有效缓解了优化偏差，并实现了最先进的性能。

Conclusion: MDACL框架通过量化模态主导程度并调节跨模态优化，成功解决了RGB-红外多模态感知中的优化偏差问题，为多模态融合提供了有效的解决方案。

Abstract: RGB-Infrared (RGB-IR) multimodal perception is fundamental to embodied multimedia systems operating in complex physical environments. Although recent cross-modal fusion methods have advanced RGB-IR detection, the optimization dynamics caused by asymmetric modality characteristics remain underexplored. In practice, disparities in information density and feature quality introduce persistent optimization bias, leading training to overemphasize a dominant modality and hindering effective fusion. To quantify this phenomenon, we propose the Modality Dominance Index (MDI), which measures modality dominance by jointly modeling feature entropy and gradient contribution. Based on MDI, we develop a Modality Dominance-Aware Cross-modal Learning (MDACL) framework that regulates cross-modal optimization. MDACL incorporates Hierarchical Cross-modal Guidance (HCG) to enhance feature alignment and Adversarial Equilibrium Regularization (AER) to balance optimization dynamics during fusion. Extensive experiments on three RGB-IR benchmarks demonstrate that MDACL effectively mitigates optimization bias and achieves SOTA performance.

</details>


### [38] [Noise-Robust Tiny Object Localization with Flows](https://arxiv.org/abs/2601.00617)
*Huixin Sun,Linlin Yang,Ronyu Chen,Kerui Gu,Baochang Zhang,Angela Yao,Xianbin Cao*

Main category: cs.CV

TL;DR: TOLF框架通过归一化流建模预测误差分布和不确定性引导优化，解决小目标检测中因标注噪声导致的过拟合问题，提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 小目标检测与正常尺度目标相比存在显著性能差距，研究发现小目标对标注噪声高度敏感，优化严格定位目标容易导致噪声过拟合。

Method: 提出TOLF框架：1）使用归一化流进行灵活误差建模，捕捉复杂的非高斯预测分布；2）不确定性感知梯度调制机制，抑制高不确定性噪声样本的学习。

Result: 在三个数据集上的实验验证了方法的有效性，特别是在AI-TOD数据集上将DINO基线提升了1.2% AP。

Conclusion: TOLF通过流式误差建模和不确定性引导优化，有效解决了小目标检测中的噪声过拟合问题，显著提升了小目标定位性能。

Abstract: Despite significant advances in generic object detection, a persistent performance gap remains for tiny objects compared to normal-scale objects. We demonstrate that tiny objects are highly sensitive to annotation noise, where optimizing strict localization objectives risks noise overfitting. To address this, we propose Tiny Object Localization with Flows (TOLF), a noise-robust localization framework leveraging normalizing flows for flexible error modeling and uncertainty-guided optimization. Our method captures complex, non-Gaussian prediction distributions through flow-based error modeling, enabling robust learning under noisy supervision. An uncertainty-aware gradient modulation mechanism further suppresses learning from high-uncertainty, noise-prone samples, mitigating overfitting while stabilizing training. Extensive experiments across three datasets validate our approach's effectiveness. Especially, TOLF boosts the DINO baseline by 1.2% AP on the AI-TOD dataset.

</details>


### [39] [RePose: A Real-Time 3D Human Pose Estimation and Biomechanical Analysis Framework for Rehabilitation](https://arxiv.org/abs/2601.00625)
*Junxiao Xue,Pavel Smirnov,Ziao Li,Yunyun Shi,Shi Chen,Xinyi Yin,Xiaohan Yue,Lei Wang,Yiduo Wang,Feng Lin,Yijia Chen,Xiao Ma,Xiaoran Yan,Qing Zhang,Fengjian Xue,Xuecheng Wu*

Main category: cs.CV

TL;DR: RePose：用于康复训练的实时3D人体姿态估计与运动分析方法，通过多摄像头RGB视频输入实现端到端实时监测与评估，提供即时反馈指导患者正确执行康复动作。


<details>
  <summary>Details</summary>
Motivation: 康复训练中需要实时监测和评估患者动作，提供即时反馈和指导，帮助患者正确执行康复练习，恢复肌肉力量和运动功能。现有方法在实时性、多人干扰处理和姿态平滑性方面存在不足。

Method: 1. 提出统一的端到端实时人体姿态估计与运动分析流水线；2. 针对医疗康复场景提出快速跟踪方法（单帧跟踪<1ms）；3. 改进SmoothNet用于实时姿态估计，减少误差并恢复真实运动状态；4. 使用Unity平台进行实时监测评估并显示肌肉应力状况。

Result: 实现了实时3D人体姿态估计与运动分析，能够在康复训练中实时监测患者动作，提供即时反馈，快速跟踪方法在多人干扰场景下表现优异，改进的SmoothNet有效减少了姿态估计误差并使运动更平滑。

Conclusion: RePose方法为康复训练提供了有效的实时监测与评估解决方案，通过快速跟踪、姿态平滑优化和Unity可视化平台，能够辅助患者正确执行康复动作，促进康复进程。

Abstract: We propose a real-time 3D human pose estimation and motion analysis method termed RePose for rehabilitation training. It is capable of real-time monitoring and evaluation of patients'motion during rehabilitation, providing immediate feedback and guidance to assist patients in executing rehabilitation exercises correctly. Firstly, we introduce a unified pipeline for end-to-end real-time human pose estimation and motion analysis using RGB video input from multiple cameras which can be applied to the field of rehabilitation training. The pipeline can help to monitor and correct patients'actions, thus aiding them in regaining muscle strength and motor functions. Secondly, we propose a fast tracking method for medical rehabilitation scenarios with multiple-person interference, which requires less than 1ms for tracking for a single frame. Additionally, we modify SmoothNet for real-time posture estimation, effectively reducing pose estimation errors and restoring the patient's true motion state, making it visually smoother. Finally, we use Unity platform for real-time monitoring and evaluation of patients' motion during rehabilitation, and to display the muscle stress conditions to assist patients with their rehabilitation training.

</details>


### [40] [Quality Detection of Stored Potatoes via Transfer Learning: A CNN and Vision Transformer Approach](https://arxiv.org/abs/2601.00645)
*Shrikant Kapse,Priyankkumar Dhrangdhariya,Priya Kedia,Manasi Patwardhan,Shankar Kausley,Soumyadipta Maiti,Beena Rai,Shirish Karande*

Main category: cs.CV

TL;DR: 基于图像的深度学习为马铃薯储存期间的质量监测提供非侵入式解决方案，通过预训练模型实现发芽检测、重量损失估计和保质期预测，DenseNet在发芽检测中达到98.03%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决马铃薯储存期间的质量监测挑战，包括发芽检测、重量损失估计和保质期预测，为自动化分拣和库存系统提供非侵入式、可扩展的解决方案。

Method: 在200天的温湿度控制条件下收集图像和重量数据，利用ResNet、VGG、DenseNet和Vision Transformer等预训练架构，设计两个专门模型：高精度二分类发芽检测器和先进多分类预测器（用于重量损失和保质期预测）。

Result: DenseNet在发芽检测中表现最佳，准确率达98.03%；保质期预测在粗分类（2-5类）时准确率超过89.83%，但在细分类（6-8类）时准确率下降；证明了图像模型集成到自动化系统的可行性。

Conclusion: 该方法为马铃薯质量评估提供经济高效的非破坏性方法，支持储存和分销的效率和可持续性；未来需开发适应不同品种和储存条件的通用模型，粗分类能确保稳健性能。

Abstract: Image-based deep learning provides a non-invasive, scalable solution for monitoring potato quality during storage, addressing key challenges such as sprout detection, weight loss estimation, and shelf-life prediction. In this study, images and corresponding weight data were collected over a 200-day period under controlled temperature and humidity conditions. Leveraging powerful pre-trained architectures of ResNet, VGG, DenseNet, and Vision Transformer (ViT), we designed two specialized models: (1) a high-precision binary classifier for sprout detection, and (2) an advanced multi-class predictor to estimate weight loss and forecast remaining shelf-life with remarkable accuracy. DenseNet achieved exceptional performance, with 98.03% accuracy in sprout detection. Shelf-life prediction models performed best with coarse class divisions (2-5 classes), achieving over 89.83% accuracy, while accuracy declined for finer divisions (6-8 classes) due to subtle visual differences and limited data per class. These findings demonstrate the feasibility of integrating image-based models into automated sorting and inventory systems, enabling early identification of sprouted potatoes and dynamic categorization based on storage stage. Practical implications include improved inventory management, differential pricing strategies, and reduced food waste across supply chains. While predicting exact shelf-life intervals remains challenging, focusing on broader class divisions ensures robust performance. Future research should aim to develop generalized models trained on diverse potato varieties and storage conditions to enhance adaptability and scalability. Overall, this approach offers a cost-effective, non-destructive method for quality assessment, supporting efficiency and sustainability in potato storage and distribution.

</details>


### [41] [CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models](https://arxiv.org/abs/2601.00659)
*Neeraj Anand,Samyak Jha,Udbhav Bamba,Rahul Rahaman*

Main category: cs.CV

TL;DR: CRoPS：一种无需训练的幻觉缓解框架，通过选择性移除关键文本标记构建幻觉模型，并结合广义对比解码，显著提升LVLM的可靠性


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型存在生成幻觉内容的倾向，现有无需训练方法存在两个局限：1）依赖对幻觉来源的狭隘假设；2）在生成后期（幻觉最可能发生时）效果下降。现有方法通过移除视觉标记构建幻觉模型，但视觉信息仍会传播到生成文本中。

Method: 提出CRoPS框架：1）构建新型幻觉模型，通过选择性移除关键文本标记来捕捉幻觉效应；2）引入广义对比解码，整合多个幻觉模型以表示多样化的幻觉来源。

Result: 在六个基准测试和三个LVLM家族上取得一致增益，CHAIR分数提升20%，优于最先进的无需训练方法。

Conclusion: CRoPS通过选择性移除文本标记和广义对比解码，有效缓解了LVLM的幻觉问题，提高了模型的可靠性，且无需额外训练。

Abstract: Despite the rapid success of Large Vision-Language Models (LVLMs), a persistent challenge is their tendency to generate hallucinated content, undermining reliability in real-world use. Existing training-free methods address hallucinations but face two limitations: (i) they rely on narrow assumptions about hallucination sources, and (ii) their effectiveness declines toward the end of generation, where hallucinations are most likely to occur. A common strategy is to build hallucinated models by completely or partially removing visual tokens and contrasting them with the original model. Yet, this alone proves insufficient, since visual information still propagates into generated text. Building on this insight, we propose a novel hallucinated model that captures hallucination effects by selectively removing key text tokens. We further introduce Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. Together, these ideas form CRoPS, a training-free hallucination mitigation framework that improves CHAIR scores by 20% and achieves consistent gains across six benchmarks and three LVLM families, outperforming state-of-the-art training-free methods.

</details>


### [42] [Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians](https://arxiv.org/abs/2601.00678)
*Melonie de Almeida,Daniela Ivanova,Tong Shi,John H. Williamson,Paul Henderson*

Main category: cs.CV

TL;DR: 提出一种基于单图像生成视频的新框架，通过构建3D高斯场景表示并在单次前向传递中采样合理的物体运动，实现快速、相机引导的视频生成，无需迭代去噪来注入物体运动。


<details>
  <summary>Details</summary>
Motivation: 现有单图像条件视频生成方法在时间一致性和3D一致性方面有所改进，但缺乏强大的用户可控性（如修改相机路径），且大多数相机控制的图像到视频模型在准确建模相机运动、保持时间一致性和保持几何完整性方面存在困难。虽然使用显式中间3D表示（如点云）提供了有前景的解决方案，但两步处理过程仍无法实现完全的时间一致性。

Method: 提出一种新颖框架，通过构建3D高斯场景表示并在单次前向传递中采样合理的物体运动，给定单张图像即可实现相机引导的视频生成。该方法无需迭代去噪来向渲染帧中注入物体运动，从而提高了效率。

Result: 在KITTI、Waymo、RealEstate10K和DL3DV-10K数据集上的广泛实验表明，该方法在视频质量和推理效率方面达到了最先进的水平。

Conclusion: 该方法通过构建3D高斯场景表示和单次前向传递采样物体运动，实现了快速、相机引导的视频生成，解决了现有方法在时间一致性、相机运动控制和几何完整性方面的局限性。

Abstract: Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at https://melonienimasha.github.io/Pixel-to-4D-Website.

</details>


### [43] [Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model](https://arxiv.org/abs/2601.00716)
*Hao Guan,Li Zhou*

Main category: cs.CV

TL;DR: 该研究针对病理学视觉语言模型在数据分布偏移下的性能退化检测问题，开发了DomainSAT工具箱，结合输入级数据偏移检测和输出级置信度指标，实现了更可靠的模型性能监控。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在医学图像分析中表现出强大潜力，但部署后当输入数据分布发生变化时，模型性能可能退化。检测这种性能退化对临床可靠性至关重要，但对于大型预训练VLM来说，在没有标注数据的情况下检测性能退化仍然具有挑战性。

Method: 研究开发了DomainSAT工具箱，这是一个轻量级图形界面工具，集成了代表性的偏移检测算法，便于系统分析输入数据偏移。同时研究了输出级监控，引入了基于置信度的无标签退化指标，直接捕捉模型预测置信度的变化。

Result: 分析表明，输入数据偏移检测能有效识别分布变化并提供早期诊断信号，但并不总是与实际性能退化相关。而基于置信度的退化指标与性能退化密切相关，可作为输入偏移检测的有效补充。在大规模病理数据集上的实验表明，结合两种方法能更可靠地检测和解释数据偏移下VLM的性能退化。

Conclusion: 该研究为数字病理学中基础模型的可靠性监控提供了一个实用且互补的框架，结合输入数据偏移检测和输出置信度指标，能够更有效地检测和解释视觉语言模型在数据偏移下的性能退化问题。

Abstract: Vision-Language Models have demonstrated strong potential in medical image analysis and disease diagnosis. However, after deployment, their performance may deteriorate when the input data distribution shifts from that observed during development. Detecting such performance degradation is essential for clinical reliability, yet remains challenging for large pre-trained VLMs operating without labeled data. In this study, we investigate performance degradation detection under data shift in a state-of-the-art pathology VLM. We examine both input-level data shift and output-level prediction behavior to understand their respective roles in monitoring model reliability. To facilitate systematic analysis of input data shift, we develop DomainSAT, a lightweight toolbox with a graphical interface that integrates representative shift detection algorithms and enables intuitive exploration of data shift. Our analysis shows that while input data shift detection is effective at identifying distributional changes and providing early diagnostic signals, it does not always correspond to actual performance degradation. Motivated by this observation, we further study output-based monitoring and introduce a label-free, confidence-based degradation indicator that directly captures changes in model prediction confidence. We find that this indicator exhibits a close relationship with performance degradation and serves as an effective complement to input shift detection. Experiments on a large-scale pathology dataset for tumor classification demonstrate that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift. These findings provide a practical and complementary framework for monitoring the reliability of foundation models in digital pathology.

</details>


### [44] [Grading Handwritten Engineering Exams with Multimodal Large Language Models](https://arxiv.org/abs/2601.00730)
*Janez Perš,Jon Muhovič,Andrej Košir,Boštjan Murovec*

Main category: cs.CV

TL;DR: 提出一个基于多模态大语言模型的端到端工作流，用于自动评分手写STEM考试，通过多阶段设计和参考解决方案实现可靠评分


<details>
  <summary>Details</summary>
Motivation: 手写STEM考试能够捕捉开放式的推理和图表，但人工评分速度慢且难以扩展，需要自动化的评分解决方案

Method: 开发了一个端到端工作流，使用多模态LLM处理扫描的手写工程测验。工作流包括格式/存在性检查、独立评分器集成、监督器聚合和刚性模板，参考解决方案被转换为文本摘要来指导评分

Result: 使用GPT-5.2和Gemini-3 Pro后端，完整流程与讲师评分的平均绝对差异约为8分，偏差较低，手动审查触发率约为17%。消融实验显示简化提示和移除参考解决方案会显著降低准确性

Conclusion: 结构化提示和参考解决方案的接地对于实现准确可靠的自动评分至关重要，该工作流为手写STEM考试的自动化评分提供了可行的解决方案

Abstract: Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves $\approx$8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of $\approx$17% at $D_{\max}=40$. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.

</details>


### [45] [Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection](https://arxiv.org/abs/2601.00789)
*Shukesh Reddy,Srijan Das,Abhijit Das*

Main category: cs.CV

TL;DR: 本文探索了将自监督学习作为辅助任务来优化深度伪造检测主任务的方法，通过融合自监督辅助任务的特征表示，在跨数据集评估中实现了更好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造检测面临泛化能力不足的问题，需要探索如何利用自监督学习作为辅助任务来增强主任务的性能，特别是提高跨数据集的检测能力。

Method: 研究不同的训练方案组合，将自监督学习作为辅助任务，融合自监督辅助任务的特征表示，形成结合自监督和主任务优势的独特表示。

Result: 在DF40、FaceForensics++、Celeb-DF、DFD、FaceShifter、UADFV等多个数据集上实验，结果显示该方法在跨数据集评估中相比当前最先进的检测器具有更好的泛化性能。

Conclusion: 融合自监督辅助任务的特征表示是一种强大的特征表示方法，能够充分发挥自监督学习和主任务的潜力，为深度伪造检测问题带来更好的性能。

Abstract: In this work, we attempted to unleash the potential of self-supervised learning as an auxiliary task that can optimise the primary task of generalised deepfake detection. To explore this, we examined different combinations of the training schemes for these tasks that can be most effective. Our findings reveal that fusing the feature representation from self-supervised auxiliary tasks is a powerful feature representation for the problem at hand. Such a representation can leverage the ultimate potential and bring in a unique representation of both the self-supervised and primary tasks, achieving better performance for the primary task. We experimented on a large set of datasets, which includes DF40, FaceForensics++, Celeb-DF, DFD, FaceShifter, UADFV, and our results showed better generalizability on cross-dataset evaluation when compared with current state-of-the-art detectors.

</details>


### [46] [Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI](https://arxiv.org/abs/2601.00794)
*Wenhui Chu,Nikolaos V. Tsekos*

Main category: cs.CV

TL;DR: 提出LNU-Net和IBU-Net两种新型深度学习架构用于心脏MRI左心室分割，基于U-Net改进，分别采用层归一化和实例-批量归一化组合，在805张MRI图像上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 左心室分割对心脏图像临床量化和诊断至关重要，需要更精确的分割方法来提高诊断准确性。

Method: 基于U-Net架构提出两种改进模型：LNU-Net在每个卷积块应用层归一化；IBU-Net在第一个卷积块结合实例和批量归一化，并将结果传递到下一层。使用仿射变换和弹性变形进行图像数据增强。

Result: 在包含45名患者805张MRI图像的数据集上评估，提出的方法在Dice系数和平均垂直距离指标上优于其他最先进方法。

Conclusion: LNU-Net和IBU-Net是有效的左心室分割架构，通过不同的归一化策略改进了分割性能，为心脏图像分析提供了更好的工具。

Abstract: Left ventricle (LV) segmentation is critical for clinical quantification and diagnosis of cardiac images. In this work, we propose two novel deep learning architectures called LNU-Net and IBU-Net for left ventricle segmentation from short-axis cine MRI images. LNU-Net is derived from layer normalization (LN) U-Net architecture, while IBU-Net is derived from the instance-batch normalized (IB) U-Net for medical image segmentation. The architectures of LNU-Net and IBU-Net have a down-sampling path for feature extraction and an up-sampling path for precise localization. We use the original U-Net as the basic segmentation approach and compared it with our proposed architectures. Both LNU-Net and IBU-Net have left ventricle segmentation methods: LNU-Net applies layer normalization in each convolutional block, while IBU-Net incorporates instance and batch normalization together in the first convolutional block and passes its result to the next layer. Our method incorporates affine transformations and elastic deformations for image data processing. Our dataset that contains 805 MRI images regarding the left ventricle from 45 patients is used for evaluation. We experimentally evaluate the results of the proposed approaches outperforming the dice coefficient and the average perpendicular distance than other state-of-the-art approaches.

</details>


### [47] [AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction](https://arxiv.org/abs/2601.00796)
*Jiewen Chan,Zhenjun Zhao,Yu-Lun Liu*

Main category: cs.CV

TL;DR: AdaGaR提出了一种用于单目视频动态3D场景重建的统一框架，通过自适应Gabor表示和时序连续性约束，解决了现有方法在细节捕捉和运动平滑性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在三个主要问题：1）使用单一高斯基元具有低通滤波特性，限制了高频细节捕捉；2）标准Gabor函数存在能量不稳定问题；3）缺乏时序连续性约束导致插值时出现运动伪影。

Method: 1）自适应Gabor表示：通过可学习频率权重和自适应能量补偿扩展高斯函数，平衡细节捕捉和稳定性；2）时序连续性：使用三次Hermite样条配合时序曲率正则化确保平滑运动演化；3）自适应初始化：结合深度估计、点跟踪和前景掩码建立稳定的点云分布。

Result: 在Tap-Vid DAVIS数据集上取得SOTA性能（PSNR 35.49，SSIM 0.9433，LPIPS 0.0723），在帧插值、深度一致性、视频编辑和立体视图合成等任务上表现出强大泛化能力。

Conclusion: AdaGaR通过统一解决频率自适应性和时序连续性问题，显著提升了动态3D场景重建的质量和稳定性，为单目视频重建提供了有效的解决方案。

Abstract: Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [48] [Reinforcement learning with timed constraints for robotics motion planning](https://arxiv.org/abs/2601.00087)
*Zhaoan Wang,Junchao Li,Mahdi Mohammad,Shaoping Xiao*

Main category: cs.RO

TL;DR: 提出了一种基于自动机的强化学习框架，用于在MDP和POMDP中合成满足MITL时序逻辑规范的控制策略，通过将MITL公式转换为定时LDGBA并与决策过程同步，构建适合Q学习的乘积模型。


<details>
  <summary>Details</summary>
Motivation: 动态不确定环境中的机器人系统需要满足复杂任务序列和严格时间约束的规划器。MITL提供了表达时间约束的形式化框架，但由于随机动态和部分可观测性，将MITL与强化学习集成仍然具有挑战性。

Method: 将MITL公式转换为定时限确定广义Büchi自动机(Timed-LDGBA)，并与底层决策过程同步构建乘积定时模型；采用简单但表达性强的奖励结构来强制时序正确性；使用Q学习算法进行策略合成。

Result: 在三个仿真研究中验证：5×5网格世界MDP、10×10网格世界POMDP和办公室服务机器人场景。结果表明，该框架能持续学习满足严格时间约束的策略，可扩展到更大状态空间，在部分可观测环境中保持有效。

Conclusion: 提出的统一框架能够可靠地合成满足MITL时序规范的控制策略，在随机转移和部分可观测条件下表现良好，展示了在时间关键和不确定环境中机器人规划的潜力。

Abstract: Robotic systems operating in dynamic and uncertain environments increasingly require planners that satisfy complex task sequences while adhering to strict temporal constraints. Metric Interval Temporal Logic (MITL) offers a formal and expressive framework for specifying such time-bounded requirements; however, integrating MITL with reinforcement learning (RL) remains challenging due to stochastic dynamics and partial observability. This paper presents a unified automata-based RL framework for synthesizing policies in both Markov Decision Processes (MDPs) and Partially Observable Markov Decision Processes (POMDPs) under MITL specifications. MITL formulas are translated into Timed Limit-Deterministic Generalized Büchi Automata (Timed-LDGBA) and synchronized with the underlying decision process to construct product timed models suitable for Q-learning. A simple yet expressive reward structure enforces temporal correctness while allowing additional performance objectives. The approach is validated in three simulation studies: a $5 \times 5$ grid-world formulated as an MDP, a $10 \times 10$ grid-world formulated as a POMDP, and an office-like service-robot scenario. Results demonstrate that the proposed framework consistently learns policies that satisfy strict time-bounded requirements under stochastic transitions, scales to larger state spaces, and remains effective in partially observable environments, highlighting its potential for reliable robotic planning in time-critical and uncertain settings.

</details>


### [49] [Compositional Diffusion with Guided search for Long-Horizon Planning](https://arxiv.org/abs/2601.00126)
*Utkarsh A Mishra,David He,Yongxin Chen,Danfei Xu*

Main category: cs.RO

TL;DR: CDGS通过将搜索嵌入扩散去噪过程，解决了组合生成模型中的模式平均问题，在机器人操作、全景图像和长视频生成等任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 组合生成模型在处理长时程任务分布时面临关键挑战：当局部分布是多模态时，现有组合方法会平均不兼容的模式，产生既不可行又不连贯的计划。

Method: CDGS将搜索直接嵌入扩散去噪过程，通过基于种群的采样探索局部模式的多样化组合，使用基于似然的过滤修剪不可行候选，并通过重叠段之间的迭代重采样强制全局一致性。

Result: CDGS在七个机器人操作任务上匹配了oracle性能，优于缺乏组合性或需要长时程训练数据的基线方法。该方法能跨领域泛化，通过有效的局部到全局消息传递实现连贯的文本引导全景图像和长视频生成。

Conclusion: CDGS通过将搜索与扩散过程相结合，有效解决了组合生成模型中的模式平均问题，为长时程任务规划提供了一种强大的解决方案。

Abstract: Generative models have emerged as powerful tools for planning, with compositional approaches offering particular promise for modeling long-horizon task distributions by composing together local, modular generative models. This compositional paradigm spans diverse domains, from multi-step manipulation planning to panoramic image synthesis to long video generation. However, compositional generative models face a critical challenge: when local distributions are multimodal, existing composition methods average incompatible modes, producing plans that are neither locally feasible nor globally coherent. We propose Compositional Diffusion with Guided Search (CDGS), which addresses this \emph{mode averaging} problem by embedding search directly within the diffusion denoising process. Our method explores diverse combinations of local modes through population-based sampling, prunes infeasible candidates using likelihood-based filtering, and enforces global consistency through iterative resampling between overlapping segments. CDGS matches oracle performance on seven robot manipulation tasks, outperforming baselines that lack compositionality or require long-horizon training data. The approach generalizes across domains, enabling coherent text-guided panoramic images and long videos through effective local-to-global message passing. More details: https://cdgsearch.github.io/

</details>


### [50] [SLAP: Slapband-based Autonomous Perching Drone with Failure Recovery for Vertical Tree Trunks](https://arxiv.org/abs/2601.00238)
*Julia Di,Kenneth A. W. Hoffmann,Tony G. Chen,Tian-Ao Ren,Mark R. Cutkosky*

Main category: cs.RO

TL;DR: SLAP系统为无人机提供了一种温和的垂直树干栖息方法，包含视觉检测、姿态控制、弹性抓取器和故障恢复机制，在室内实验中达到75%的栖息成功率。


<details>
  <summary>Details</summary>
Motivation: 现有垂直表面栖息方法主要针对轻量级机械设计，缺乏系统级集成，且通常需要高速、激进的着陆操作，这对搭载敏感电子设备的无人机很危险。需要一种适合大型无人机的温和栖息方法。

Method: 开发了SLAP系统，包含：视觉栖息点检测器、IMU栖息故障检测器、软栖息姿态控制器、光学近距离检测系统，以及由市售拍手环制成的带有微刺的快速主动弹性抓取器。

Result: 在1.2公斤商用四旋翼无人机上验证，室内自主飞行实验中，在真实橡树段上20次飞行达到75%栖息成功率，在2次诱导故障飞行中实现100%故障恢复。

Conclusion: SLAP系统为大型无人机提供了一种安全、温和的垂直树干栖息解决方案，具有较高的栖息成功率和可靠的故障恢复能力。

Abstract: Perching allows unmanned aerial vehicles (UAVs) to reduce energy consumption, remain anchored for surface sampling operations, or stably survey their surroundings. Previous efforts for perching on vertical surfaces have predominantly focused on lightweight mechanical design solutions with relatively scant system-level integration. Furthermore, perching strategies for vertical surfaces commonly require high-speed, aggressive landing operations that are dangerous for a surveyor drone with sensitive electronics onboard. This work presents the preliminary investigation of a perching approach suitable for larger drones that both gently perches on vertical tree trunks and reacts and recovers from perch failures. The system in this work, called SLAP, consists of vision-based perch site detector, an IMU (inertial-measurement-unit)-based perch failure detector, an attitude controller for soft perching, an optical close-range detection system, and a fast active elastic gripper with microspines made from commercially-available slapbands. We validated this approach on a modified 1.2 kg commercial quadrotor with component and system analysis. Initial human-in-the-loop autonomous indoor flight experiments achieved a 75% perch success rate on a real oak tree segment across 20 flights, and 100% perch failure recovery across 2 flights with induced failures.

</details>


### [51] [Vehicle Painting Robot Path Planning Using Hierarchical Optimization](https://arxiv.org/abs/2601.00271)
*Yuya Nagai,Hiromitsu Nakamura,Narito Shinmachi,Yuta Higashizono,Satoshi Ono*

Main category: cs.RO

TL;DR: 本文提出了一种分层优化方法，用于自动化车辆喷漆过程中多机械臂的路径规划，将问题分解为上层VRP式分配和下层详细路径规划，能够满足喷漆工艺的特殊约束。


<details>
  <summary>Details</summary>
Motivation: 在车辆生产工厂中，喷漆过程使用多个机械臂同时对传送线上的车身进行喷漆。目前喷漆路径设计仍然是工程师耗时的手动任务，需要自动化和减少设计时间。传统机器人路径规划技术（如焊接中使用的）无法直接应用于喷漆过程的独特约束。

Method: 将喷漆路径设计表述为分层优化问题：上层子问题类似于车辆路径问题（VRP），负责将车身区域分配给机械臂；下层子问题涉及详细的路径规划。该方法允许在每层使用不同的优化算法，并通过设计变量表示、约束、修复算子和初始化过程，灵活处理车辆喷漆过程的特定约束。

Result: 通过对三种商用车型的实验验证，所提出的方法能够自动设计出满足所有车辆喷漆约束的路径，其质量与工程师手动创建的路径相当。

Conclusion: 该分层优化方法成功解决了车辆喷漆路径规划的自动化问题，能够生成满足工艺约束的高质量路径，显著减少了设计时间，为车辆生产中的喷漆过程自动化提供了有效解决方案。

Abstract: In vehicle production factories, the vehicle painting process employs multiple robotic arms to simultaneously apply paint to car bodies advancing along a conveyor line. Designing paint paths for these robotic arms, which involves assigning car body areas to arms and determining paint sequences for each arm, remains a time-consuming manual task for engineers, indicating the demand for automation and design time reduction. The unique constraints of the painting process hinder the direct application of conventional robotic path planning techniques, such as those used in welding. Therefore, this paper formulates the design of paint paths as a hierarchical optimization problem, where the upper-layer subproblem resembles a vehicle routing problem (VRP), and the lower-layer subproblem involves detailed path planning. This approach allows the use of different optimization algorithms at each layer, and permits flexible handling of constraints specific to the vehicle painting process through the design of variable representation, constraints, repair operators, and an initialization process at the upper and lower layers. Experiments with three commercially available vehicle models demonstrated that the proposed method can automatically design paths that satisfy all constraints for vehicle painting with quality comparable to those created manually by engineers.

</details>


### [52] [Pure Inertial Navigation in Challenging Environments with Wheeled and Chassis Mounted Inertial Sensors](https://arxiv.org/abs/2601.00275)
*Dusan Nemec,Gal Versano,Itai Savin,Vojtech Simak,Juraj Kekelak,Itzik Klein*

Main category: cs.RO

TL;DR: WiCHINS：一种结合轮载和车体惯性传感器的轮式底盘惯性导航系统，用于在GNSS受限或光照不佳环境下实现精确的纯惯性导航。


<details>
  <summary>Details</summary>
Motivation: 在GNSS信号受限或光照条件不佳的实际场景中，自主车辆和轮式机器人的导航解决方案可能只能依赖惯性传感器，而惯性测量误差会导致随时间漂移。需要一种能够在挑战性环境中实现鲁棒导航的方法。

Method: 提出WiCHINS系统，将轮载惯性传感器与车体惯性传感器结合。开发了一个三阶段框架，每个阶段使用专用的扩展卡尔曼滤波器，在估计过程中充分利用轮子和车体不同位置的传感器优势。

Result: 使用5个惯性测量单元、总记录时间228.6分钟的数据集进行评估。与四种其他惯性基线方法相比，使用两个轮子和一个车体惯性测量单元时，平均位置误差为11.4米，占平均行驶距离的2.4%。

Conclusion: WiCHINS方法能够在挑战性环境中实现鲁棒导航，有助于弥补纯惯性导航的性能差距，为GNSS受限环境下的自主导航提供了有效解决方案。

Abstract: Autonomous vehicles and wheeled robots are widely used in many applications in both indoor and outdoor settings. In practical situations with limited GNSS signals or degraded lighting conditions, the navigation solution may rely only on inertial sensors and as result drift in time due to errors in the inertial measurement. In this work, we propose WiCHINS, a wheeled and chassis inertial navigation system by combining wheel-mounted-inertial sensors with a chassis-mounted inertial sensor for accurate pure inertial navigation. To that end, we derive a three-stage framework, each with a dedicated extended Kalman filter. This framework utilizes the benefits of each location (wheel/body) during the estimation process. To evaluate our proposed approach, we employed a dataset with five inertial measurement units with a total recording time of 228.6 minutes. We compare our approach with four other inertial baselines and demonstrate an average position error of 11.4m, which is $2.4\%$ of the average traveled distance, using two wheels and one body inertial measurement units. As a consequence, our proposed method enables robust navigation in challenging environments and helps bridge the pure-inertial performance gap.

</details>


### [53] [Replaceable Bit-based Gripper for Picking Cluttered Food Items](https://arxiv.org/abs/2601.00305)
*Prashant Kumar,Yukiyasu Domae,Weiwei Wan,Kensuke Harada*

Main category: cs.RO

TL;DR: 提出可更换夹具头的抓取系统，用于处理杂乱食品包装中的重量控制问题，针对鲑鱼子和意大利面设计了专用夹具头，实现80%和95%以上的重量控制精度


<details>
  <summary>Details</summary>
Motivation: 食品包装行业面临快速变化的食品种类和重量处理挑战，特别是从单件食品到柔性、长条状、杂乱食品的多样化需求

Method: 开发可更换夹具头的抓取系统，配备专门设计的食品附件（夹具头），通过皮带更换系统在不同食品间快速切换，实现对颗粒状、杂乱、缠绕食品的重量控制抓取和投放

Result: 成功抓取意大利面和鲑鱼子，重量控制投放精度分别超过80%和95%，系统能够快速切换不同夹具头，处理多种食品类型

Conclusion: 可更换夹具头的抓取系统能有效解决食品包装中杂乱食品的重量控制问题，具有高精度和快速切换能力，适用于多种食品类型

Abstract: The food packaging industry goes through changes in food items and their weights quite rapidly. These items range from easy-to-pick, single-piece food items to flexible, long and cluttered ones. We propose a replaceable bit-based gripper system to tackle the challenge of weight-based handling of cluttered food items. The gripper features specialized food attachments(bits) that enhance its grasping capabilities, and a belt replacement system allows switching between different food items during packaging operations. It offers a wide range of control options, enabling it to grasp and drop specific weights of granular, cluttered, and entangled foods. We specifically designed bits for two flexible food items that differ in shape: ikura(salmon roe) and spaghetti. They represent the challenging categories of sticky, granular food and long, sticky, cluttered food, respectively. The gripper successfully picked up both spaghetti and ikura and demonstrated weight-specific dropping of these items with an accuracy over 80% and 95% respectively. The gripper system also exhibited quick switching between different bits, leading to the handling of a large range of food items.

</details>


### [54] [LLM-Based Agentic Exploration for Robot Navigation & Manipulation with Skill Orchestration](https://arxiv.org/abs/2601.00555)
*Abu Hanif Muhammad Syarubany,Farhan Zaki Rahmani,Trio Widianto*

Main category: cs.RO

TL;DR: 该论文提出了一个基于LLM的端到端智能探索系统，用于室内购物任务，在Gazebo仿真和真实走廊环境中进行了评估。系统通过检测路标构建轻量级语义地图，利用AprilTags作为定位锚点，LLM根据自然语言购物请求生成决策，ROS控制器执行模块化运动原语。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在室内购物环境中自主导航、理解自然语言指令并执行多商店导航和物品抓取任务的机器人系统。目标是实现端到端的任务执行，同时保持系统的模块化和可调试性。

Method: 系统采用增量式构建轻量级语义地图的方法，在路口检测路标并存储方向与POI的关系。使用AprilTags作为可重复的定位锚点。LLM根据自然语言购物请求在每个路口生成约束离散动作（方向和是否进入商店），ROS有限状态主控制器通过门控模块化运动原语（包括基于局部代价地图的避障、AprilTag接近、商店进入和抓取）来执行决策。

Result: 定性结果表明，集成的系统栈能够执行从用户指令到多商店导航和物体检索的端到端任务执行。系统通过基于文本的地图和记录决策历史保持模块化和可调试性。

Conclusion: 该研究成功开发了一个基于LLM的端到端智能探索系统，能够在室内购物环境中理解自然语言指令并执行复杂导航和抓取任务。系统在仿真和真实环境中都得到了验证，展示了其模块化、可调试的特点和实际应用潜力。

Abstract: This paper presents an end-to-end LLM-based agentic exploration system for an indoor shopping task, evaluated in both Gazebo simulation and a corresponding real-world corridor layout. The robot incrementally builds a lightweight semantic map by detecting signboards at junctions and storing direction-to-POI relations together with estimated junction poses, while AprilTags provide repeatable anchors for approach and alignment. Given a natural-language shopping request, an LLM produces a constrained discrete action at each junction (direction and whether to enter a store), and a ROS finite-state main controller executes the decision by gating modular motion primitives, including local-costmap-based obstacle avoidance, AprilTag approaching, store entry, and grasping. Qualitative results show that the integrated stack can perform end-to-end task execution from user instruction to multi-store navigation and object retrieval, while remaining modular and debuggable through its text-based map and logged decision history.

</details>


### [55] [Priority-Aware Multi-Robot Coverage Path Planning](https://arxiv.org/abs/2601.00580)
*Kanghoon Lee,Hyeonjun Kim,Jiachen Li,Jinkyoo Park*

Main category: cs.RO

TL;DR: 提出优先级感知的多机器人覆盖路径规划（PA-MCPP）问题，通过两阶段框架优化优先级区域的加权延迟和总完成时间


<details>
  <summary>Details</summary>
Motivation: 现有MCPP方法假设区域重要性均匀，但在实际场景中某些区域需要更快关注，因此需要优先级感知的覆盖规划

Method: 两阶段框架：1）贪婪区域分配结合局部搜索和生成树路径规划；2）Steiner树引导的剩余覆盖

Result: 实验表明该方法显著降低优先级加权延迟，同时保持竞争力的总完成时间，对机器人数量和优先级权重具有良好的可扩展性

Conclusion: PA-MCPP框架有效解决了优先级区域覆盖问题，为多机器人系统在非均匀重要性环境中的覆盖任务提供了实用解决方案

Abstract: Multi-robot systems are widely used for coverage tasks that require efficient coordination across large environments. In Multi-Robot Coverage Path Planning (MCPP), the objective is typically to minimize the makespan by generating non-overlapping paths for full-area coverage. However, most existing methods assume uniform importance across regions, limiting their effectiveness in scenarios where some zones require faster attention. We introduce the Priority-Aware MCPP (PA-MCPP) problem, where a subset of the environment is designated as prioritized zones with associated weights. The goal is to minimize, in lexicographic order, the total priority-weighted latency of zone coverage and the overall makespan. To address this, we propose a scalable two-phase framework combining (1) greedy zone assignment with local search, spanning-tree-based path planning, and (2) Steiner-tree-guided residual coverage. Experiments across diverse scenarios demonstrate that our method significantly reduces priority-weighted latency compared to standard MCPP baselines, while maintaining competitive makespan. Sensitivity analyses further show that the method scales well with the number of robots and that zone coverage behavior can be effectively controlled by adjusting priority weights.

</details>


### [56] [Vision-based Goal-Reaching Control for Mobile Robots Using a Hierarchical Learning Framework](https://arxiv.org/abs/2601.00610)
*Mehdi Heydari Shahna,Pauli Mustalahti,Jouni Mattila*

Main category: cs.RO

TL;DR: 提出一个用于大型机器人的安全目标到达控制框架，通过模块化分解系统，结合视觉姿态估计、强化学习运动规划、深度学习建模、鲁棒自适应控制和数学安全监督器，确保系统稳定性和安全性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在机器人应用中有效，但需要大量状态-动作空间探索，期间行为可能不安全，这限制了其在复杂地形上运行的大型机器人中的应用。因此需要设计一个安全的目标到达控制框架。

Method: 将整个系统分解为紧密耦合的功能模块：1)实时视觉姿态估计提供准确机器人状态；2)强化学习运动规划器生成平滑运动指令；3)监督深度学习模型捕捉机器人复杂动力学；4)基于模型的鲁棒自适应控制器确保轮子跟踪运动指令；5)数学安全监督器监控机器人安全。

Result: 提出的框架保证了执行系统的均匀指数稳定性和整个操作的安全性。在6000公斤机器人上的不同场景实验证实了该框架的有效性。

Conclusion: 通过模块化方法结合多种技术，成功实现了大型机器人在复杂地形上的安全目标到达控制，为强化学习在大型机器人系统中的应用提供了可行的解决方案。

Abstract: Reinforcement learning (RL) is effective in many robotic applications, but it requires extensive exploration of the state-action space, during which behaviors can be unsafe. This significantly limits its applicability to large robots with complex actuators operating on unstable terrain. Hence, to design a safe goal-reaching control framework for large-scale robots, this paper decomposes the whole system into a set of tightly coupled functional modules. 1) A real-time visual pose estimation approach is employed to provide accurate robot states to 2) an RL motion planner for goal-reaching tasks that explicitly respects robot specifications. The RL module generates real-time smooth motion commands for the actuator system, independent of its underlying dynamic complexity. 3) In the actuation mechanism, a supervised deep learning model is trained to capture the complex dynamics of the robot and provide this model to 4) a model-based robust adaptive controller that guarantees the wheels track the RL motion commands even on slip-prone terrain. 5) Finally, to reduce human intervention, a mathematical safety supervisor monitors the robot, stops it on unsafe faults, and autonomously guides it back to a safe inspection area. The proposed framework guarantees uniform exponential stability of the actuation system and safety of the whole operation. Experiments on a 6,000 kg robot in different scenarios confirm the effectiveness of the proposed framework.

</details>


### [57] [From 2D to 3D terrain-following area coverage path planning](https://arxiv.org/abs/2601.00614)
*Mogens Plessen*

Main category: cs.RO

TL;DR: 提出了一种三维地形跟随的区域覆盖路径规划算法，能够在保持机械工作宽度的同时，使路径在特定工作高度上跟随地形起伏。


<details>
  <summary>Details</summary>
Motivation: 在三维地形环境中，传统的二维路径规划方法无法有效处理地形起伏问题。特别是在农业等应用场景中，机械需要在特定工作高度上跟随地形，同时保持相邻路径的间距等于机械工作宽度，以实现高效、均匀的区域覆盖。

Method: 1. 使用反距离加权方法生成均匀间距的高程数据；2. 通过局部搜索算法生成相邻路径；3. 确保路径在三维空间中同时满足两个约束：相邻路径间距等于机械工作宽度，路径在特定投影距离（工作高度）上跟随地形。

Result: 算法在实际农业场景的三维数据上进行了验证，成功生成了能够跟随地形起伏的区域覆盖路径。与二维等效算法相比，该算法能够处理三维地形复杂性，确保机械在起伏地形中保持恒定的工作高度和路径间距。

Conclusion: 该三维地形跟随区域覆盖路径规划算法有效解决了在复杂地形环境中的路径规划问题，特别适用于农业机械等需要在特定高度上跟随地形工作的应用场景。算法通过处理高程数据和局部搜索，成功将二维覆盖路径规划扩展到三维空间。

Abstract: An algorithm for 3D terrain-following area coverage path planning is presented. Multiple adjacent paths are generated that are (i) locally apart from each other by a distance equal to the working width of a machinery, while (ii) simultaneously floating at a projection distance equal to a specific working height above the terrain. The complexities of the algorithm in comparison to its 2D equivalent are highlighted. These include uniformly spaced elevation data generation using an Inverse Distance Weighting-approach and a local search. Area coverage path planning results for real-world 3D data within an agricultural context are presented to validate the algorithm.

</details>


### [58] [RoboReward: General-Purpose Vision-Language Reward Models for Robotics](https://arxiv.org/abs/2601.00675)
*Tony Lee,Andrew Wagenmaker,Karl Pertsch,Percy Liang,Sergey Levine,Chelsea Finn*

Main category: cs.RO

TL;DR: 该研究提出了RoboReward数据集和基准，用于评估视觉语言模型在机器人任务中的奖励建模能力，并训练了4B/8B参数的奖励模型，在真实机器人强化学习中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 在真实机器人领域，获取有效的奖励函数通常需要大量人工标注或脆弱的工程化目标设计。虽然视觉语言模型有潜力作为自动奖励模型，但其在真实机器人任务中的有效性尚不清楚。

Method: 1) 构建RoboReward数据集和基准，基于Open X-Embodiment和RoboArena的大规模真实机器人数据；2) 提出负样本数据增强流程，通过反事实重标注和时间裁剪生成校准的负样本和接近成功样本；3) 训练4B和8B参数的视觉语言奖励模型。

Result: 1) 评估显示现有视觉语言模型在所有任务中表现不均，有较大改进空间；2) 训练的4B/8B参数模型在短时程机器人任务中优于更大的视觉语言模型；3) 8B参数奖励模型在真实机器人强化学习中大幅优于Gemini Robotics-ER 1.5，显著缩小了与人工奖励的差距。

Conclusion: 该工作填补了视觉语言模型在机器人奖励建模领域的空白，展示了训练专门奖励模型的有效性，为机器人强化学习提供了新的自动奖励生成方法。

Abstract: A well-designed reward is critical for effective reinforcement learning-based policy improvement. In real-world robotic domains, obtaining such rewards typically requires either labor-intensive human labeling or brittle, handcrafted objectives. Vision-language models (VLMs) have shown promise as automatic reward models, yet their effectiveness on real robot tasks is poorly understood. In this work, we aim to close this gap by introducing (1) \textbf{RoboReward}, a robotics reward dataset and benchmark built on large-scale real-robot corpora from Open X-Embodiment (OXE) and RoboArena, and (2) vision-language reward models trained on this dataset (RoboReward 4B/8B). Because OXE is success-heavy and lacks failure examples, we propose a \emph{negative examples data augmentation} pipeline that generates calibrated \emph{negatives} and \emph{near-misses} via counterfactual relabeling of successful episodes and temporal clipping to create partial-progress outcomes from the same videos. Using this framework, we produce an extensive training and evaluation dataset that spans diverse tasks and embodiments and enables systematic evaluation of whether state-of-the-art VLMs can reliably provide rewards for robotics. Our evaluation of leading open-weight and proprietary VLMs reveals that no model excels across all tasks, underscoring substantial room for improvement. We then train general-purpose 4B- and 8B-parameter models that outperform much larger VLMs in assigning rewards for short-horizon robotic tasks. Finally, we deploy the 8B-parameter reward VLM in real-robot reinforcement learning and find that it improves policy learning over Gemini Robotics-ER 1.5, a frontier physical reasoning VLM trained on robotics data, by a large margin, while substantially narrowing the gap to RL training with human-provided rewards.

</details>


### [59] [DefVINS: Visual-Inertial Odometry for Deformable Scenes](https://arxiv.org/abs/2601.00702)
*Samuel Cerezo,Javier Civera*

Main category: cs.RO

TL;DR: DefVINS是一个视觉惯性里程计框架，通过将刚性IMU锚定状态与非刚性变形图分离，在可变形场景中实现鲁棒的运动估计。


<details>
  <summary>Details</summary>
Motivation: 传统视觉惯性里程计(VIO)基于刚性假设，在可变形场景中容易过拟合局部非刚性运动或产生严重漂移，需要新的方法来处理非刚性变形。

Method: 提出DefVINS框架，将刚性IMU锚定状态与非刚性变形图分离表示；使用标准VIO初始化后，渐进激活非刚性自由度；包含可观测性分析，指导IMU锚定和基于条件数的激活策略。

Result: 消融研究表明，结合惯性约束和可观测性感知的变形激活策略，在非刚性环境中提高了鲁棒性。

Conclusion: DefVINS通过显式分离刚性和非刚性运动，利用惯性测量约束刚性运动，在可变形场景中实现了更鲁棒的视觉惯性里程计。

Abstract: Deformable scenes violate the rigidity assumptions underpinning classical visual-inertial odometry (VIO), often leading to over-fitting to local non-rigid motion or severe drift when deformation dominates visual parallax. We introduce DefVINS, a visual-inertial odometry framework that explicitly separates a rigid, IMU-anchored state from a non--rigid warp represented by an embedded deformation graph. The system is initialized using a standard VIO procedure that fixes gravity, velocity, and IMU biases, after which non-rigid degrees of freedom are activated progressively as the estimation becomes well conditioned. An observability analysis is included to characterize how inertial measurements constrain the rigid motion and render otherwise unobservable modes identifiable in the presence of deformation. This analysis motivates the use of IMU anchoring and informs a conditioning-based activation strategy that prevents ill-posed updates under poor excitation. Ablation studies demonstrate the benefits of combining inertial constraints with observability-aware deformation activation, resulting in improved robustness under non-rigid environments.

</details>


### [60] [Calling for Backup: How Children Navigate Successive Robot Communication Failures](https://arxiv.org/abs/2601.00754)
*Maria Teresa Parreira,Isabel Neto,Filipa Rocha,Wendy Ju*

Main category: cs.RO

TL;DR: 研究探索儿童对机器人重复错误的反应，发现儿童与成人既有相似调整行为，又有更多脱离互动行为，但错误不影响儿童对机器人的感知。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究探讨成人对连续机器人错误的反应，但儿童对此的反应仍知之甚少。本研究旨在探索儿童对机器人社交错误和性能错误的反应，特别是重复对话错误。

Method: 复制Liu等人的连续机器人失败范式，59名8-10岁儿童参与实验。机器人连续三次无法理解儿童提示，通过视频记录分析儿童行为反应。

Result: 儿童与成人反应既有相似又有差异：相似之处包括调整提示、改变语调、情绪化非语言反应增加；差异在于儿童表现出更多脱离行为（如暂时忽略机器人或寻求成人帮助）。错误不影响儿童对机器人的感知。

Conclusion: 儿童对机器人错误表现出更灵活的对话期望，这些发现有助于为年轻用户设计更有效、更适合发展阶段的机器人交互系统。

Abstract: How do children respond to repeated robot errors? While prior research has examined adult reactions to successive robot errors, children's responses remain largely unexplored. In this study, we explore children's reactions to robot social errors and performance errors. For the latter, this study reproduces the successive robot failure paradigm of Liu et al. with child participants (N=59, ages 8-10) to examine how young users respond to repeated robot conversational errors. Participants interacted with a robot that failed to understand their prompts three times in succession, with their behavioral responses video-recorded and analyzed. We found both similarities and differences compared to adult responses from the original study. Like adults, children adjusted their prompts, modified their verbal tone, and exhibited increasingly emotional non-verbal responses throughout successive errors. However, children demonstrated more disengagement behaviors, including temporarily ignoring the robot or actively seeking an adult. Errors did not affect participants' perception of the robot, suggesting more flexible conversational expectations in children. These findings inform the design of more effective and developmentally appropriate human-robot interaction systems for young users.

</details>
