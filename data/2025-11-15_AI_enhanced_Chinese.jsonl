{"id": "2511.10635", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10635", "abs": "https://arxiv.org/abs/2511.10635", "authors": ["Pascal Strauch", "David M\u00fcller", "Sammy Christen", "Agon Serifi", "Ruben Grandia", "Espen Knoop", "Moritz B\u00e4cher"], "title": "Robot Crash Course: Learning Soft and Stylized Falling", "comment": null, "summary": "Despite recent advances in robust locomotion, bipedal robots operating in the real world remain at risk of falling. While most research focuses on preventing such events, we instead concentrate on the phenomenon of falling itself. Specifically, we aim to reduce physical damage to the robot while providing users with control over a robot's end pose. To this end, we propose a robot agnostic reward function that balances the achievement of a desired end pose with impact minimization and the protection of critical robot parts during reinforcement learning. To make the policy robust to a broad range of initial falling conditions and to enable the specification of an arbitrary and unseen end pose at inference time, we introduce a simulation-based sampling strategy of initial and end poses. Through simulated and real-world experiments, our work demonstrates that even bipedal robots can perform controlled, soft falls.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u4eba\u65e0\u5173\u7684\u5956\u52b1\u51fd\u6570\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u53cc\u8db3\u673a\u5668\u4eba\u7684\u53d7\u63a7\u8f6f\u7740\u9646\uff0c\u5e73\u8861\u671f\u671b\u6700\u7ec8\u59ff\u6001\u4e0e\u51b2\u51fb\u6700\u5c0f\u5316\uff0c\u5e76\u4fdd\u62a4\u5173\u952e\u673a\u5668\u4eba\u90e8\u4ef6\u3002", "motivation": "\u5c3d\u7ba1\u53cc\u8db3\u673a\u5668\u4eba\u5728\u9c81\u68d2\u8fd0\u52a8\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u4ecd\u6709\u6454\u5012\u98ce\u9669\u3002\u5927\u591a\u6570\u7814\u7a76\u5173\u6ce8\u9632\u6b62\u6454\u5012\uff0c\u800c\u672c\u6587\u4e13\u6ce8\u4e8e\u6454\u5012\u73b0\u8c61\u672c\u8eab\uff0c\u65e8\u5728\u51cf\u5c11\u7269\u7406\u635f\u4f24\u5e76\u8ba9\u7528\u6237\u63a7\u5236\u673a\u5668\u4eba\u7684\u6700\u7ec8\u59ff\u6001\u3002", "method": "\u63d0\u51fa\u673a\u5668\u4eba\u65e0\u5173\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5e73\u8861\u671f\u671b\u6700\u7ec8\u59ff\u6001\u8fbe\u6210\u3001\u51b2\u51fb\u6700\u5c0f\u5316\u548c\u5173\u952e\u90e8\u4ef6\u4fdd\u62a4\uff1b\u5f15\u5165\u57fa\u4e8e\u4eff\u771f\u7684\u521d\u59cb\u548c\u6700\u7ec8\u59ff\u6001\u91c7\u6837\u7b56\u7565\uff0c\u4f7f\u7b56\u7565\u5bf9\u5e7f\u6cdb\u7684\u521d\u59cb\u6454\u5012\u6761\u4ef6\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u5728\u63a8\u7406\u65f6\u6307\u5b9a\u4efb\u610f\u672a\u89c1\u8fc7\u7684\u6700\u7ec8\u59ff\u6001\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5373\u4f7f\u662f\u53cc\u8db3\u673a\u5668\u4eba\u4e5f\u80fd\u6267\u884c\u53d7\u63a7\u7684\u8f6f\u7740\u9646\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u53cc\u8db3\u673a\u5668\u4eba\u80fd\u591f\u5b9e\u73b0\u53d7\u63a7\u8f6f\u7740\u9646\uff0c\u901a\u8fc7\u5e73\u8861\u59ff\u6001\u63a7\u5236\u548c\u51b2\u51fb\u4fdd\u62a4\u6765\u51cf\u5c11\u6454\u5012\u65f6\u7684\u7269\u7406\u635f\u4f24\u3002"}}
{"id": "2511.10615", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10615", "abs": "https://arxiv.org/abs/2511.10615", "authors": ["Shruti Singh Baghel", "Yash Pratap Singh Rathore", "Sushovan Jena", "Anurag Pradhan", "Amit Shukla", "Arnav Bhavsar", "Pawan Goyal"], "title": "Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals", "comment": "8 pages", "summary": "Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u7684SmolVLM2\u6a21\u578b\uff08500M\u548c2.2B\uff09\u5728\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7528\u6237\u89c6\u9891\u63cf\u8ff0\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u4e24\u4e2a\u4e13\u95e8\u9488\u5bf9BLV\u53ef\u8bbf\u95ee\u6027\u8bc4\u4f30\u7684\u65b0\u6846\u67b6\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u548c\u79fb\u52a8\u8bbe\u5907\u90e8\u7f72\u65b9\u6848\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u63cf\u8ff0\uff0c\u4f46\u5176\u9ad8\u5185\u5b58\u3001\u8ba1\u7b97\u548c\u90e8\u7f72\u9700\u6c42\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4f9d\u8d56\u8be6\u7ec6\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u63cf\u8ff0\u7684\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7528\u6237\u7fa4\u4f53\u3002", "method": "\u4f7f\u7528500M\u548c2.2B\u53c2\u6570\u7684SmolVLM2\u53d8\u4f53\u5728\u4e24\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\uff08AVCaps\u6237\u5916\u548cCharades\u5ba4\u5185\uff09\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5f15\u5165\u4e86\u591a\u4e0a\u4e0b\u6587BLV\u6846\u67b6\u548c\u5bfc\u822a\u8f85\u52a9\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u56db\u79cd\u4e0d\u540c\u63d0\u793a\u8bbe\u8ba1\u7b56\u7565\uff0c\u5e76\u5728\u667a\u80fd\u624b\u673a\u4e0a\u90e8\u7f72FP32\u548cINT8\u7cbe\u5ea6\u53d8\u4f53\u3002", "result": "\u8bc4\u4f30\u4e86\u6a21\u578b\u5728\u7a7a\u95f4\u5b9a\u5411\u3001\u793e\u4ea4\u4e92\u52a8\u3001\u52a8\u4f5c\u4e8b\u4ef6\u548c\u73af\u5883\u6c1b\u56f4\u7b49\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u7684\u8868\u73b0\uff0c\u4ee5\u53ca\u79fb\u52a8\u8bbe\u5907\u4e0a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5b9e\u9645\u6027\u80fd\u7ea6\u675f\u3002", "conclusion": "\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u79fb\u52a8\u90e8\u7f72\u6d4b\u8bd5\uff0c\u4e3a\u5f00\u53d1\u9002\u5408BLV\u7528\u6237\u9700\u6c42\u7684\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2511.10629", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10629", "abs": "https://arxiv.org/abs/2511.10629", "authors": ["Aleksandr Razin", "Danil Kazantsev", "Ilya Makarov"], "title": "One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models", "comment": null, "summary": "Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e86Latent Upscaler Adapter (LUA)\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u5728VAE\u89e3\u7801\u524d\u76f4\u63a5\u5728\u751f\u6210\u5668\u7684\u6f5c\u5728\u4ee3\u7801\u4e0a\u6267\u884c\u8d85\u5206\u8fa8\u7387\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u5ef6\u8fdf\u548c\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u96be\u4ee5\u6269\u5c55\u5230\u8d85\u51fa\u8bad\u7ec3\u5206\u8fa8\u7387\uff0c\u76f4\u63a5\u9ad8\u5206\u8fa8\u7387\u91c7\u6837\u6210\u672c\u9ad8\uff0c\u800c\u4f20\u7edf\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5728\u89e3\u7801\u540e\u64cd\u4f5c\u4f1a\u5f15\u5165\u4f2a\u5f71\u548c\u989d\u5916\u5ef6\u8fdf\u3002", "method": "LUA\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u7ec4\u4ef6\u96c6\u6210\uff0c\u65e0\u9700\u4fee\u6539\u57fa\u7840\u6a21\u578b\u6216\u6dfb\u52a0\u989d\u5916\u6269\u6563\u9636\u6bb5\uff0c\u4f7f\u7528\u5171\u4eabSwin\u98ce\u683c\u4e3b\u5e72\u548c\u5c3a\u5ea6\u7279\u5b9a\u50cf\u7d20\u6d17\u724c\u5934\u652f\u63012\u500d\u548c4\u500d\u653e\u5927\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u9012\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u5408\u6210\u3002", "result": "LUA\u5728\u611f\u77e5\u8d28\u91cf\u4e0a\u4e0e\u50cf\u7d20\u7a7a\u95f4SR\u76f8\u5f53\uff0c\u4f46\u89e3\u7801\u548c\u4e0a\u91c7\u6837\u65f6\u95f4\u964d\u4f4e\u8fd13\u500d\uff081024px\u751f\u6210\u4ec5\u589e\u52a00.42\u79d2\uff0c\u800c\u76f8\u540cSwinIR\u67b6\u6784\u7684\u50cf\u7d20\u7a7a\u95f4SR\u9700\u89811.87\u79d2\uff09\uff0c\u4e14\u5728\u4e0d\u540cVAE\u7684\u6f5c\u5728\u7a7a\u95f4\u4e0a\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "LUA\u5728\u4fdd\u6301\u539f\u751f\u9ad8\u5206\u8fa8\u7387\u751f\u6210\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\uff0c\u4e3a\u73b0\u4ee3\u6269\u6563\u7ba1\u9053\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u53ef\u6269\u5c55\u9ad8\u4fdd\u771f\u56fe\u50cf\u5408\u6210\u8def\u5f84\u3002"}}
{"id": "2511.10648", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10648", "abs": "https://arxiv.org/abs/2511.10648", "authors": ["Jiahao Wang", "Weiye Xu", "Aijun Yang", "Wengang Zhou", "Lewei Lu", "Houqiang Li", "Xiaohua Wang", "Jinguo Zhu"], "title": "Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling", "comment": "Accepted to NeurIPS 2025 (The Thirty-Ninth Annual Conference on Neural Information Processing Systems)", "summary": "Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u81ea\u4e00\u81f4\u6027\u91c7\u6837\uff08SCS\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u56e0\u731c\u6d4b\u6b63\u786e\u9009\u9879\u800c\u83b7\u5f97\u76f8\u540c\u5956\u52b1\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u89c6\u89c9\u6270\u52a8\u548c\u8f68\u8ff9\u91cd\u91c7\u6837\u83b7\u5f97\u4e00\u81f4\u6027\u5206\u6570\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u7684\u591a\u9009\u9898\u8bbe\u7f6e\u4e2d\uff0c\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u4e00\u4e2a\u88ab\u5ffd\u89c6\u7684\u969c\u788d\uff1a\u5373\u4f7f\u63a8\u7406\u94fe\u9519\u8bef\u4f46\u731c\u6d4b\u6b63\u786e\u9009\u9879\u7684\u8f68\u8ff9\u4e0e\u771f\u6b63\u6b63\u786e\u63a8\u7406\u7684\u8f68\u8ff9\u83b7\u5f97\u76f8\u540c\u5956\u52b1\uff0c\u8fd9\u5f71\u54cd\u4e86\u5b66\u4e60\u6548\u679c\u3002", "method": "\u63d0\u51fa\u81ea\u4e00\u81f4\u6027\u91c7\u6837\uff08SCS\uff09\u65b9\u6cd5\uff1a\u5bf9\u6bcf\u4e2a\u95ee\u9898\u5f15\u5165\u5c0f\u7684\u89c6\u89c9\u6270\u52a8\uff0c\u5e76\u5bf9\u521d\u59cb\u8f68\u8ff9\u8fdb\u884c\u91cd\u590d\u622a\u65ad\u548c\u91cd\u91c7\u6837\uff0c\u901a\u8fc7\u7ed3\u679c\u8f68\u8ff9\u7684\u4e00\u81f4\u6027\u83b7\u5f97\u53ef\u5fae\u7684\u4e00\u81f4\u6027\u5206\u6570\uff0c\u5728\u7b56\u7565\u66f4\u65b0\u65f6\u964d\u4f4e\u4e0d\u53ef\u9760\u8f68\u8ff9\u7684\u6743\u91cd\u3002", "result": "\u57fa\u4e8eQwen2.5-VL-7B-Instruct\u6a21\u578b\uff0c\u5c06SCS\u96c6\u6210\u5230RLOO\u3001GRPO\u548cREINFORCE++\u7cfb\u5217\u4e2d\uff0c\u5728\u516d\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe7.7\u4e2a\u767e\u5206\u70b9\uff0c\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u3002\u5728Qwen2.5-VL-3B-Instruct\u548cInternVL3-8B\u6a21\u578b\u4e0a\u4e5f\u83b7\u5f97\u663e\u8457\u63d0\u5347\u3002", "conclusion": "SCS\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7ed3\u679c\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u7ea0\u6b63\u4e86\u56e0\u731c\u6d4b\u6b63\u786e\u800c\u83b7\u5f97\u4e0d\u5f53\u5956\u52b1\u7684\u95ee\u9898\u3002"}}
